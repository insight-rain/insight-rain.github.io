# Online Estimation and Manipulation of Articulated Objects

**相关性评分**: 6.0/10

**排名**: #24


---


## 基本信息

- **arXiv ID**: [2601.01438v1](https://arxiv.org/abs/2601.01438v1)
- **发布时间**: 2026-01-04T08:52:56Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Russell Buchanan, Adrian Röfer, João Moura, Abhinav Valada, Sethu Vijayakumar

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种结合视觉先验和本体感知的在线估计方法，用于机器人操作铰接物体，但未明确涉及语言模型或轻量化架构。

## 摘要

From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.

## 详细分析

## 论文摘要：在线估计与操作铰接物体

### 1. 研究背景和动机
服务机器人若要在家庭环境中协助人类完成任务（如烹饪、清洁），必须能够操作常见的铰接物体（如柜门、抽屉）。现有方法主要分为两类：**基于深度学习的视觉预测**（提供先验但不可靠）和**基于交互感知的估计**（需要良好的初始猜测才能开始操作）。然而，许多物体（如外观相同的柜门）的铰接方式无法仅凭视觉可靠预测。本文旨在融合这两种范式，提出一种**在线估计框架**，使机器人能在与物体交互前利用视觉先验，并在交互过程中利用本体感知（运动学和力觉）实时更新估计，从而鲁棒地操作未知铰接物体。

### 2. 核心方法和技术创新
本文提出了一种基于**因子图**的在线估计框架，核心创新点包括：
- **多模态感知融合**：将**基于学习的视觉先验**、**机器人运动学测量**和**腕部力/力矩传感器数据**统一整合到一个因子图中进行优化。
- **不确定性感知的视觉先验**：提出一种新的神经网络，不仅能预测铰接部件的运动流（Articulation Flow），还能预测每个点的**认知不确定性**，并以点云形式作为因子引入图优化。
- **新颖的力感知因子**：当机器人尝试打开物体但遇到阻力时，利用测得的反作用力构建一个**力平面因子**，该因子能约束有效运动方向必须位于与该力正交的平面内，从而纠正错误的初始估计。
- **基于旋量理论的解析模型**：使用旋量（Screw）对铰接（旋转、平移）进行统一参数化建模，并在因子图中通过**铰接因子**将物体部件姿态与铰接参数关联起来。

### 3. 主要实验结果
- **仿真实验**：在PartNet-Mobility数据集上测试，与Flowbot3D相比，本文方法（使用1000个点因子）将单次推理成功率从53.67%提升至57.71%。
- **手引导实验**：在真实机器人上验证运动学估计的准确性。仅需0.5°的旋转或少量平移，估计方向与真实方向的**切线相似度**即可达到0.90以上，证明了在线更新的可行性。
- **全系统共享自主实验**：在真实机器人上操作一个具有四个视觉相同、但铰接方式各异的柜子（包括滑动门）。机器人成功打开了所有类型的门，在20次试验中取得了**75%的成功率**。实验表明，力感知因子对于纠正视觉歧义（如将滑动门误判为拉门）至关重要。

### 4. 研究意义和价值
本研究的意义在于：
- **技术贡献**：首次将**不确定性感知的视觉学习先验**与**基于物理模型的力/运动学交互感知**在统一的因子图框架中深度融合，实现了真正的在线、闭环估计与操作。
- **解决实际挑战**：使机器人能够操作**视觉上模糊**的铰接物体，突破了纯视觉方法或纯交互方法各自的局限性。
- **系统集成与验证**：提供了完整的系统集成方案（包括基于用户点击的共享自主、运动规划与控制），并进行了广泛的真实世界实验验证，推动了铰接物体操作向更实用、更鲁棒的方向发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
论文旨在解决**服务机器人在未知环境中操作铰接物体（如柜门、抽屉）** 的关键挑战。具体而言，它针对两个主要痛点：
1.  **视觉歧义性**：许多铰接物体（如图1中的柜门）在关闭时外观完全相同，仅凭视觉无法可靠预测其运动方式（如旋转、平移或滑动）。
2.  **纯视觉方法的局限性**：现有基于深度学习的视觉预测方法通常是单次、离线的，无法在交互过程中更新估计，且对预测错误缺乏鲁棒性。

### **核心创新点**
论文提出了一种**多模态、在线、闭环的铰接物体估计与操作方法**，其核心创新在于**将学习到的视觉先验与本体感知（力/运动）传感在基于因子图的统一框架中进行融合**。

具体创新点包括：
1.  **不确定性感知的视觉先验因子**：
    - 改进了神经网络（基于PointNet++），使其不仅能预测物体表面点的“运动流”，还能预测每个预测的**不确定性（协方差）**。
    - 设计了新的**点级铰接因子**，将成百上千个带不确定性的运动流预测直接融入因子图进行优化，取代了先前工作中手工拟合平面的粗糙方法。

2.  **新颖的力传感因子**：
    - 引入了一个**力平面因子**，用于处理初始预测完全错误（如试图拉开一个滑动门）的情况。
    - 当机器人施加力而物体不动时，该因子利用测得的反作用力方向，约束有效运动方向必须位于与该力垂直的平面上，从而引导估计向正确的运动模式更新。

3.  **基于因子图的在线多模态融合框架**：
    - 构建了一个统一的因子图，**同步优化**恒定螺丝参数、随时间变化的关节配置以及物体部件的位姿。
    - 框架**分阶段、在线地融合**三种传感模态：
        - **初始化阶段**：使用改进的神经网络对单次点云进行预测，提供带不确定性的初始螺丝参数先验。
        - **交互阶段**：
            - 若物体不动且力超阈值，则添加**力因子**纠正估计。
            - 若物体开始运动，则添加**运动学因子**（来自机器人编码器的末端位姿）和**铰接模型因子**，持续在线更新估计。
    - 实现了**估计与控制闭环**：更新的铰接模型实时反馈给运动生成模块，生成新的轨迹。

4.  **完整的共享自治系统集成**：
    - 实现了一个从用户点击（指定抓取点）、语义分割、初始预测、在线估计到闭环运动生成的完整流水线。
    - 系统只需用户指定“抓哪里”，机器人自主完成“如何开”。

### **解决方案的技术路径**
1.  **建模**：使用**螺丝理论**对铰接（旋转、平移、螺旋）进行统一、紧凑的参数化表示。
2.  **感知融合**：采用**因子图**作为后端，将视觉先验因子、力因子、运动学因子、铰接模型因子全部表达为高斯噪声模型下的残差项，通过求解最大后验概率估计问题，实现多传感器信息的最优融合。
3.  **在线学习**：在机器人物理交互过程中，因子图不断添加新的力/运动学测量节点，并重新优化，实现“边做边学”，逐步修正初始可能错误的视觉猜测。
4.  **运动生成**：利用符号数学工具（Kineverse）将估计出的螺丝模型转化为机器人任务空间约束，通过二次规划实时求解符合铰接运动规律的关节轨迹。

### **实际价值与验证**
- **效果**：在真实的机器人实验中，成功打开了视觉上完全相同的四个柜门（包括此前无法处理的滑动门），在20次全流程试验中取得了**75%的成功率**。
- **优势**：
    - **处理视觉歧义**：不依赖物体外观，通过物理交互揭示真实铰接方式。
    - **鲁棒性强**：在线估计能纠正错误的初始预测，降低了对高精度视觉模型或极其柔顺控制器的依赖。
    - **系统实用**：展示了从感知到控制的完整闭环系统，向实现真正自主的家庭服务机器人迈出了一步。

**总结**：该论文的核心贡献是提出并实现了一个**“看-猜-试-纠”** 的机器人操作范式。它通过一个**不确定性感知的因子图框架**，巧妙地将**数据驱动的视觉学习能力**与**基于模型的物理交互推理能力**结合起来，使机器人能够像人一样，在面对未知铰接物体时，通过试探性交互快速理解其运动机制并完成操作任务。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决服务机器人在操作未知铰接物体（如柜门、抽屉）时，如何**在线、鲁棒地估计其运动学模型**的核心问题。针对现有方法依赖单一视觉先验（易出错）或需要良好初始猜测才能开始交互的局限性，论文提出了一种**融合多模态感知的因子图在线估计框架**。该方法将基于深度学习的视觉预测（提供带有不确定性的初始关节参数先验）、机器人本体感知（运动学）以及腕部力传感器信息统一整合到一个基于旋量理论的解析关节模型中，进行在线联合优化。实验表明，该系统能够成功操作视觉上无法区分关节类型的物体（如一组外观相同但开启方式不同的柜门），在真实机器人实验中实现了对未知铰接物体75%的成功开启率，显著提升了在视觉模糊场景下的操作鲁棒性和成功率。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Online Estimation and Manipulation of Articulated Objects》提出了一种新颖的、融合多模态感知的铰接物体在线估计与操作方法。其核心创新点在于将基于学习的视觉先验与基于物理模型的交互感知（力觉与运动学）相结合，并通过因子图进行在线优化。以下是其相对于已有工作的明确创新点：

### 1. **提出一个融合视觉先验与多模态交互感知的因子图框架**
   - **改进/不同之处**： 以往工作通常将视觉预测（学习）与交互感知（物理模型）视为独立或顺序的步骤。例如，许多基于学习的方法仅做单次视觉预测，而许多交互感知方法需要一个良好的初始猜测才能开始交互。本文**首创性地将不确定性感知的神经网络视觉预测、力觉测量和机器人运动学测量统一到一个因子图优化框架中**，实现了从初始预测到在线更新的无缝衔接。
   - **解决的问题/优势**： 解决了在视觉信息模糊（如外观相同的柜门）时，仅靠视觉无法可靠预测铰接类型的问题。该框架允许机器人先基于视觉做出“最佳猜测”，然后在物理交互中利用力觉和运动信息快速、持续地修正估计，实现了真正的**在线、闭环估计**，提高了系统对未知和视觉模糊物体的操作鲁棒性。

### 2. **引入不确定性感知的视觉铰接性预测网络及新的“铰接性因子”**
   - **改进/不同之处**： 相比作者之前的工作（Buchanan et al., 2024）及Flowbot3D等模型，本文的神经网络不仅预测每个点的运动流（flow），还预测其**异方差不确定性（aleatoric uncertainty）**。更重要的是，**摒弃了以往将网络输出后处理为单一关节参数的“手工”方式**，而是设计了一个新的“铰接性因子”，将成百上千个带不确定性的点级运动流预测直接融入因子图。
   - **解决的问题/优势**：
     - **不确定性建模**： 网络能表达对预测结果的置信度（例如，对滑动门可能误判为旋转门，但在平移方向不确定性高）。这为后续融合其他传感器信息提供了量化的依据。
     - **更优的融合**： 直接使用点级预测避免了手工后处理的信息损失和偏差，允许因子图利用大量数据点共同优化出一个全局最优的关节参数估计，提高了初始预测的准确性（仿真实验显示比Flowbot3D提升超过11%）。

### 3. **首创将力觉测量作为“力平面因子”引入铰接参数估计**
   - **改进/不同之处**： 这是本文一个**全新的贡献**。以往方法在初始预测错误导致机器人无法移动物体时（例如，试图拉开一个滑动门），会陷入僵局。本文提出，当施加力超过阈值而物体未动时，该反作用力方向必然与物体的真实运动方向正交。利用这一几何约束，构造了一个“力平面因子”，将当前的关节运动方向估计投影到与测力方向垂直的平面上。
   - **解决的问题/优势**： **解决了初始预测完全错误时的“死锁”问题**。该因子能有效地将估计从无效的运动方向“推开”，引导其探索其他可能方向。这使得机器人能够成功操作那些视觉上完全无法判断、且初始预测错误的铰接物体（如论文中的底部右侧滑动门），**显著扩展了系统可处理物体的范围**。

### 4. **实现并验证了完整的“共享自主”闭环操作系统**
   - **改进/不同之处**： 相比许多仅关注估计或仅关注控制的工作，本文集成了一个完整的系统：用户仅需点击图像指定抓取点 → 系统自动分割物体、预测初始铰接性 → 在线估计驱动实时运动生成 → 形成“估计-控制”闭环。其运动生成模块基于符号数学（Kineverse）和二次规划（QP），能快速计算符合最新估计的关节轨迹。
   - **解决的问题/优势**： 证明了所提在线估计框架能与机器人控制系统高效集成，实现**全自动的、对未知物体的操作**。系统无需物体类别先验，且在线更新估计使得无需依赖高度柔顺的控制器来补偿预测误差，**简化了控制器的设计**。最终在真实机器人实验中，对视觉模糊的柜门实现了75%的成功开启率。

### 5. **在真实复杂场景中验证了性能提升**
   - **改进/不同之处**： 论文不仅进行仿真验证，更在具有挑战性的真实场景（四个外观相同、铰接方式各异的柜门）中进行了大量实验。**成功开启了所有类型的门**，包括作者之前工作（Buchanan et al., 2024）无法处理的滑动门。
   - **解决的问题/优势**： 通过严格的实物实验，**实证了多模态融合（尤其是力觉）对于解决视觉模糊问题的关键作用**，验证了系统在现实世界中的有效性和鲁棒性。这超越了多数仅停留在仿真或简单场景的工作，具有更高的实际应用价值。

**总结**： 本文的核心创新在于**通过一个统一的、不确定性驱动的因子图框架，巧妙地融合了学习（视觉先验）与推理（物理交互感知）**。其提出的“不确定性感知视觉因子”和“力平面因子”是关键的技术突破，分别提升了初始估计的质量和系统从错误中恢复的能力。最终，这些创新使机器人能够像人类一样，在面对视觉信息不足的铰接物体时，通过“动手试一下”来快速理解并操作物体，向实用的家庭服务机器人迈出了重要一步。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 1. 实验效果概述
论文通过仿真和真实机器人实验，验证了所提出的**在线估计与操作铰接物体**方法的有效性。核心成果是：**在真实硬件实验中，机器人能够自主打开视觉上难以辨别的未知铰接物体（如柜门），成功率达到75%**。系统实现了从视觉先验预测、到力/运动感知在线更新估计、再到闭环控制的完整流程。

### 2. 使用的数据集
- **训练数据**：使用 **PartNet-Mobility 数据集** 在 PyBullet 仿真环境中渲染铰接物体（如柜子、抽屉）并生成带标注的点云数据，用于训练不确定性感知的神经网络。
- **测试数据**：
    - **仿真实验**：使用 PartNet-Mobility 数据集中**未见过的物体**进行测试。
    - **真实实验**：使用一个具有**四个视觉相同但铰接方式不同（两个旋转门、一个抽屉、一个推拉门）的定制柜子**（如图1所示）进行测试。这是评估方法处理视觉模糊性的关键场景。

### 3. 评价指标
1.  **仿真实验成功率**：物体被打开至其运动范围90%即视为成功。计算成功实验的百分比。
2.  **估计精度（手持引导实验）**：使用 **切线相似度** 度量估计运动方向与真实运动方向的一致性。
    - 公式：`J(v_gt, v_est) = 平均( (v_gt/||v_gt||) · (v_est/||v_est||) )`
    - 值域为 `[-1, 1]`，`1` 表示方向完全一致，`0` 表示垂直。
3.  **系统性能指标**：
    - **推理时间**：比较不同方法单次预测的耗时。
    - **真实机器人任务成功率**：在多次试验中，成功完成开门任务的比例。

### 4. 对比的基线方法
1.  **Flowbot3D**：一个先进的、仅基于视觉的深度学习铰接性预测方法。论文在仿真实验中将其作为主要视觉基线进行对比。
2.  **Heppert et al. (2022)**：一个同样使用因子图进行铰接估计的交互感知方法。论文在**手持引导实验**中与其进行估计精度的直接对比。
3.  **作者先前工作 (Buchanan et al., 2024)**：本文是对该工作的重大改进。在真实实验中，新方法成功打开了旧方法无法处理的**推拉门**（图1右下角），并整体提升了鲁棒性和成功率。

### 5. 关键性能结果与结论
#### 仿真实验（表1）
- **与Flowbot3D对比**：
    - 在“单次推理”设置下，本文方法（使用1000个点因子）成功率为 **57.71%**，优于Flowbot3D的 **53.67%**。
    - 在“多次推理”设置下，本文方法达到 **68.37%**，优于Flowbot3D的 **61.47%**。
- **结论**：本文提出的**不确定性感知因子图方法**，通过整合大量点云预测来优化整体解，比依赖单点最大流预测的Flowbot3D更稳定、更准确。但推理时间随因子数量增加而增长。

#### 手持引导实验（图9）
- **与Heppert et al.对比**：
    - 对于旋转关节，两种方法性能相近。
    - 对于棱柱关节，**本文方法明显更优**，能更好地区分棱柱关节与超大半径的旋转关节。
- **估计收敛速度**：仅需 **0.5° 的旋转** 或少量平移，切线相似度均值即可达到 **0.90**；**1.0°** 后达到 **0.97**。这证明了**在线运动感知能快速修正错误的视觉先验**。

#### 真实机器人共享自治实验（图10, 11）
- **成功率**：在20次全流程试验中，成功打开柜门 **15次**，成功率为 **75%**。失败主要源于夹爪打滑，而非估计错误。
- **关键突破**：成功处理了**视觉模糊的推拉门**。当网络错误预测为“拉出”式抽屉时，机器人感知到阻力，**力因子**被激活并修正估计为可能的运动平面，随后微小的运动被**运动学因子**捕获，最终估计收敛到正确的“滑动”模型。
- **在线估计效果**：图11显示，在操作过程中，对铰接参数 `ξ` 的估计及其不确定性（`3σ` 区间）随新测量的加入而快速收敛和细化。

### 6. 主要结论与提升
1.  **多模态融合的有效性**：融合**视觉先验（带不确定性）、力感知和运动学感知**的因子图框架，显著优于仅依赖视觉或仅依赖交互感知的方法。
2.  **处理视觉模糊性**：新引入的**力因子**和**不确定性感知视觉因子**是关键创新，使系统能在视觉线索缺失时，通过物理交互探索并在线修正估计，从而解决了旧方法无法处理的推拉门问题。
3.  **系统闭环集成**：实现了从用户点击、分割、预测、在线估计到轨迹生成的完整**共享自治系统**，并在真实机器人上验证了其闭环运行能力。
4.  **对先前工作的实质性改进**：相比 Buchanan et al. (2024)，本文通过不确定性建模和力感知的引入，**大幅提升了系统的鲁棒性和适用范围**，成功打开了所有四种类型的柜门。

**总结**：论文通过严谨的仿真与真实实验，定量和定性地证明了所提方法在未知铰接物体在线估计与操作任务上的先进性和实用性，核心优势在于利用多传感器融合和在线学习来克服纯视觉方法的局限性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.01438v1)
- [HTML 版本](https://arxiv.org/html/2601.01438v1)
