# LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments

**相关性评分**: 6.0/10

**排名**: #15


---


## 基本信息

- **arXiv ID**: [2601.02905v1](https://arxiv.org/abs/2601.02905v1)
- **发布时间**: 2026-01-06T10:44:19Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Sara Micol Ferraina, Michele Brienza, Francesco Argenziano, Emanuele Musumeci, Vincenzo Suriani, Domenico D. Bloisi, Daniele Nardi

## 关键词

Lightweight Architecture, Inference Efficiency, Edge Deployment

## 一句话总结

LOST-3DSG 是一种轻量级开放词汇3D场景图方法，通过语义跟踪在动态环境中高效追踪对象，适用于机器人应用。

## 摘要

Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.

## 详细分析

## 论文摘要：LOST-3DSG: 动态环境中基于语义追踪的轻量级开放词汇3D场景图

**1. 研究背景和动机**
在机器人领域，动态环境中的物体追踪是一项核心挑战。尽管3D场景图（3DSG）和开放词汇理解取得了进展，但现有方法通常依赖计算和存储成本高昂的基础模型（如CLIP），存储密集的视觉嵌入，导致在动态场景中更新和维护的效率低下，限制了实时应用和可扩展性。因此，亟需一种既能保持开放词汇表达能力，又具备高效性的轻量级表示方法。

**2. 核心方法和技术创新**
本文提出了 **LOST-3DSG**，一个专为动态环境设计的轻量级开放词汇3D场景图。其核心创新在于：
- **轻量级语义表示**：摒弃存储密集的CLIP特征，转而使用**word2vec**和句子嵌入（Sentence Transformer）对物体标签、颜色、材质和细粒度描述等语义属性进行编码，构建紧凑的物体级表示。
- **基于语义的追踪算法**：设计了**Lost相似度函数**，通过加权组合上述多类语义属性的相似度，来判断新观测物体是否为已存在的移动实例，从而实现基于语义而非纯几何的物体身份关联与追踪。
- **双模式场景更新模块**：系统分为**探索**和**追踪**两种模式。探索阶段专注于构建场景图；追踪阶段则主动管理物体生命周期（添加、移动、移除），并利用视锥体进行图维护，有效处理物体消失和位置更新。

**3. 主要实验结果**
在TIAGo机器人真实环境实验中，系统在三个复杂度递增的场景下进行评估：
- **追踪有效性**：在物体移动、消失的动态场景中，系统能有效关联物体身份并更新其位置（更新准确率达94.4%）。
- **组件贡献分析**：消融实验表明，细粒度描述相似度对实例区分最关键，颜色和材质相似度有助于稳定更新，而仅使用标签相似度效果最差。
- **内存效率**：与基于体素的CLIP方法（约641 MB）相比，LOST-3DSG仅需约**3 KB**内存即可表示包含21个物体的复杂场景，内存占用降低数个数量级。

**4. 研究意义和价值**
LOST-3DSG证明了**轻量级的语义属性足以在动态环境中实现有效的开放词汇物体追踪**，为机器人长期场景理解提供了一个高效、可扩展的替代方案。它在保持开放词汇能力的同时，显著降低了计算和存储开销，推动了轻量级、实用化3D场景理解的发展，并为下游任务（如导航、规划）提供了更易集成的结构化世界模型。未来工作可集中于提升语义描述的鲁棒性和更精细的相似度建模。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：LOST-3DSG

### **核心问题**
论文旨在解决**动态环境中机器人实时、高效地进行开放词汇（open-vocabulary）物体语义跟踪**的难题。现有方法（如基于CLIP的3D场景图）虽然表达能力强大，但依赖存储高维、密集的视觉特征嵌入，导致**计算和内存开销巨大**，难以在需要频繁更新的动态场景中实现可扩展和实时应用。

### **核心创新点**
论文的核心创新在于提出了一种**轻量级、基于语义属性的开放词汇3D场景图构建与跟踪方法**，其创新性主要体现在以下三个层面：

1.  **轻量化的开放词汇表示**：
    - **摒弃密集视觉嵌入**：与主流方法（如ConceptGraphs）为每个体素或点存储高维CLIP特征不同，LOST-3DSG仅为每个物体节点存储一组紧凑的**语义属性**（标签、颜色、材质、细粒度描述）。
    - **采用低成本嵌入**：使用`word2vec`（用于标签、材质）和轻量级句子嵌入模型（如`text-embedding-3-small`，用于描述）来编码语义信息，而非计算昂贵的视觉-语言模型（VLM）特征。这从根本上大幅降低了存储和计算需求。

2.  **基于语义属性的跟踪算法（Lost Similarity Function, LSF）**：
    - **多模态相似度度量**：设计了一个加权组合的相似度函数，综合评估**语义相似度**（`s_ℓ`）、**颜色相似度**（`s_c`）、**材质相似度**（`s_m`）和**描述相似度**（`s_d`）。这使得系统能够通过物体的“语义指纹”而非单纯的几何位置来关联跨时间观测的同一物体实例。
    - **实例级区分**：特别强调**细粒度描述（`s_d`）** 的作用，使其能够区分标签、颜色、材质都相同但外观细节不同的物体（如两把不同的“红色木柄锤子”）。

3.  **双模式场景更新机制（Scene Update Module, SUM）**：
    - **探索模式**：专注于增量式构建完整的场景图，不删除物体，避免在信息不全时误删。
    - **跟踪模式**：在场景图构建完成后激活，基于LSF和空间一致性进行主动维护。它能处理物体的**移动、新增、消失**，并引入“不确定对象”集合来处理语义匹配成功但空间位置冲突的歧义情况，后续可进行恢复或清理。

### **解决方案架构**
论文通过一个清晰的流程实现上述创新：

```mermaid
graph TD
    A[感知模块] --> B[输入: RGB-D帧]；
    B --> C[VLM提取物体标签 & 属性]；
    B --> D[开放词汇检测器 & 实例分割]；
    C & D --> E[3D重建与场景图生成]；
    E --> F[输出: 带属性（标签/颜色/材质/描述）的物体节点]；
    
    F --> G[场景更新模块]；
    H[历史3D场景图] --> G；
    G --> I{基于LSF的语义匹配}；
    I --> J[匹配成功?]；
    J -- 是 --> K[更新节点位置/状态]；
    J -- 否 --> L[创建新节点]；
    K & L --> M[更新后的轻量级3D场景图]；
```

1.  **感知模块**：利用VLM（GPT-4-mini）、开放词汇检测器（OWLv2）和分割模型（EfficientViT-SAM）从RGB-D数据中提取物体实例、3D边界框及其语义属性。
2.  **相似度计算**：使用LSF函数计算当前观测物体与场景图中已有物体节点的综合相似度。
3.  **图更新与跟踪**：SUM模块根据LSF分数和运行模式（探索/跟踪），决定是更新现有节点、创建新节点，还是将节点标记为不确定/删除，从而实现动态物体的语义跟踪。

### **实际价值与优势**
- **高效性与可扩展性**：内存占用极低（论文中21个物体的场景仅需约3KB，而对比的CLIP体素方法需要641MB），**内存节省超过5个数量级**，为在资源受限的机器人平台（如TIAGo）上长期运行提供了可能。
- **开放词汇能力**：得益于VLM的标签和属性生成，系统能够理解和跟踪预定义类别之外的物体，增强了在未知、非结构化环境中的适应性。
- **实用化验证**：在真实机器人（TIAGo）和动态室内环境中进行了多复杂度级别的实验，验证了其在物体移动、消失、部分观测等挑战下的有效性和鲁棒性。

**总结**：LOST-3DSG的核心贡献是提出了一种**“以语义属性为中心”** 的范式转变，用紧凑、可解释的语义描述替代笨重的视觉嵌入，在基本保持开放词汇跟踪能力的同时，实现了效率的飞跃，为机器人长期在动态环境中进行实时场景理解提供了一个切实可行的轻量级解决方案。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决动态环境中机器人依赖计算和内存开销巨大的基础模型（如CLIP）进行物体跟踪和场景理解的问题。为此，作者提出了LOST-3DSG，一个轻量级的开放词汇3D场景图框架。其核心创新在于，它摒弃了存储密集视觉嵌入的传统方法，转而利用视觉语言模型提取物体的语义属性（如标签、颜色、材质、描述），并通过融合这些属性的相似度函数来进行语义层面的物体跟踪与场景图更新。实验结果表明，该方法在真实机器人平台上能有效跟踪动态物体，同时将场景语义表示的内存占用从传统CLIP方法的数百兆字节大幅降低至数千字节级别，在保持开放词汇理解能力的前提下，实现了效率与性能的平衡。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## LOST-3DSG 论文创新点分析

这篇论文针对动态环境中基于3D场景图（3DSG）的物体跟踪问题，提出了一种轻量级、开放词汇的方法。其核心创新点在于**用高效的语义属性表示替代了传统方法中计算和存储密集的视觉嵌入**，从而在保持开放词汇能力的同时，显著提升了系统的实时性和可扩展性。以下是其明确的创新点及分析：

---

### 1. **轻量级开放词汇3D场景图表示**
   - **相比以往方法的改进/不同之处：**
     - **以往方法（如ConceptGraphs, DovSG）：** 通常依赖CLIP等大型视觉-语言模型（VLMs）为场景中的每个体素（voxel）或点存储高维（如512维）的稠密语义嵌入。这导致内存占用巨大（论文中举例：一个场景需约641 MB），计算开销高，难以在动态环境中实时更新。
     - **LOST-3DSG的方法：** 摒弃了存储稠密CLIP特征的做法，转而**在物体节点级别存储一组紧凑的语义属性**（包括标签、颜色、材质和一段细粒度描述）。这些属性通过轻量级的`word2vec`和句子嵌入（如`text-embedding-3-small`）进行编码，维度极低。
   - **解决的具体问题/带来的优势：**
     - **解决了内存和计算瓶颈问题。** 论文中对比显示，存储整个场景的语义信息仅需约3 KB，相比CLIP方法（641 MB）降低了超过5个数量级的内存消耗。
     - **使得在资源受限的机器人平台（如TIAGo）上实现实时、持续的动态场景更新成为可能**，提高了系统的实用性和可扩展性。
     - **保留了开放词汇能力。** 通过VLM提取物体标签和属性，系统仍能识别和描述预定义类别之外的物体。

### 2. **基于语义属性的物体跟踪算法（Lost Similarity Function, LSF）**
   - **相比以往方法的改进/不同之处：**
     - **以往方法：** 动态物体跟踪多依赖于几何一致性（如位置、运动轨迹）或基于外观的视觉特征匹配。一些语义跟踪方法可能使用深度卷积特征（如VGG），但仍较为笨重，且未在3DSG框架内直接实现。
     - **LOST-3DSG的方法：** 提出了一个**专为3DSG设计的语义跟踪算法**，其核心是**LSF相似度函数**。该函数通过加权组合四种语义属性的相似度（标签、颜色、材质、描述）来综合判断两个观测是否属于同一物体实例。
   - **解决的具体问题/带来的优势：**
     - **实现了在3DSG内部对动态物体的身份保持（ID persistence）和状态更新。** 这是论文声称的**首个直接在3DSG表示内进行语义物体跟踪的方法**。
     - **增强了跟踪的鲁棒性。** 即使物体位置大幅改变，只要其语义属性（如“红色木质锤子”）保持一致，系统就能将其关联为同一物体，而非误判为新物体。这解决了纯几何方法在物体移动时容易丢失身份的问题。
     - **支持实例级区分。** 细粒度描述相似度（`s_d`）使得系统能够区分同一类别、颜色和材质的多个不同实例（例如，两个不同的“蓝色陶瓷杯子”）。

### 3. **双模式场景更新模块（Scene Update Module, SUM）**
   - **相比以往方法的改进/不同之处：**
     - **以往方法：** 场景图构建和更新策略通常较为单一，可能没有明确区分“建图”和“跟踪”两个阶段，对物体出现、消失、移动的处理逻辑可能不够精细。
     - **LOST-3DSG的方法：** SUM设计了**探索（Exploration）** 和**跟踪（Tracking）** 两种操作模式，并配有明确的**图维护与清理机制**。
       - **探索模式：** 专注于增量式发现和添加物体到3DSG，避免删除物体，以构建完整的环境目录。
       - **跟踪模式：** 主动维护场景一致性，处理物体移动（更新边界框）、消失（基于视锥体清理）以及由语义-空间冲突引发的身份歧义（将旧位置物体标记为“不确定”）。
   - **解决的具体问题/带来的优势：**
     - **更符合机器人作业的实际情况。** 明确区分了建图阶段和长期运行阶段，使系统行为更合理。
     - **减少了误删除。** 清理机制只删除在机器人当前视野内、理应被看到但未被检测到的物体，降低了因遮挡或检测失败导致的错误删除。
     - **提供了处理歧义的机制。** “不确定物体”集合为暂时无法判定的情况提供了缓冲，允许未来观测进行修正，提高了系统的容错性。

### 4. **整体系统集成与实时部署验证**
   - **相比以往方法的改进/不同之处：**
     - **以往方法：** 许多先进的3DSG或开放词汇映射工作侧重于算法本身或仿真验证，在真实机器人平台进行完整系统集成和长期动态环境测试的相对较少。
     - **LOST-3DSG的方法：** 论文不仅在算法层面进行创新，还**构建了一个完整的感知-更新流水线**，并**在真实的TIAGo机器人上进行了部署和评估**。实验设计了从简单到复杂（★ 到 ★★★）的动态场景，系统性地测试了算法在物体移动、消失、高密度、部分可观测等挑战下的性能。
   - **解决的具体问题/带来的优势：**
     - **证明了方法的实用性和有效性。** 通过真实世界实验，验证了所提的轻量级方法能够实际运行于机器人平台，并有效处理动态变化。
     - **提供了全面的评估基准。** 除了跟踪精度（检测、更新、删除的比率），还通过消融实验量化了LSF各组成部分的贡献，并通过内存对比突显了其效率优势，为后续研究提供了扎实的参考。

---

**总结：**
LOST-3DSG的核心创新在于**“轻量化”和“语义化”**。它通过用紧凑的语义属性替代稠密视觉嵌入，并设计相应的语义跟踪与更新策略，在**几乎不损失开放词汇表达能力的前提下，极大地提升了动态3D场景建模与跟踪的效率**。这为解决机器人长期在动态、开放世界中运行所面临的计算资源约束问题，提供了一个新颖且实用的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验设置与数据集
- **实验平台**：在真实室内环境中使用 **TIAGo机器人**（搭载ROS 2 Humble）进行部署。
- **数据集**：**未使用公开标准数据集**，而是设计了**三个自定义的真实世界场景**，复杂度递增：
  1.  **简单场景（⋆）**：3个静态物体，随后移动、改变位置并最终消失。
  2.  **中等场景（⋆⋆）**：21个物体，物体密度高，部分物体在机器人视野外移动或消失，存在部分观测。
  3.  **困难场景（⋆⋆⋆）**：9个物体，环境高度动态，物体频繁在未被直接观测时改变位置，对语义跟踪能力压力测试。

### 二、 评价指标
论文采用了**任务导向的定量指标**，而非标准数据集上的精度/召回率。指标直接衡量系统核心功能的表现：
- **检测（Detections）**：系统成功检测到场景中（应被检测的）物体的比例。
- **删除（Deletions）**：当物体被移走或消失时，系统正确将其从3D场景图（3DSG）中移除的比例。
- **更新（Updates）**：当物体移动时，系统正确更新其在3DSG中位置的比例。

### 三、 基线方法与对比
论文主要进行了两类对比分析：

1.  **与CLIP-based方法的间接对比（核心创新点）**：
    - **对比对象**：依赖于存储密集CLIP视觉特征的方法（如ConceptGraphs, DovSG）。
    - **对比维度**：**内存消耗**，而非直接跟踪性能。
    - **关键结果**：
        - **CLIP方法（ViT-B/32）**：在包含626,140个体素的中等场景中，存储每个体素的CLIP嵌入（512维）需约 **641 MB** 内存。
        - **LOST-3DSG**：仅在物体层级存储语义属性（标签、颜色、材质、描述），同等场景仅需约 **3.3 KB** 内存。
    - **结论**：LOST-3DSG在保持开放词汇表达能力的同时，将内存占用降低了**超过5个数量级**，实现了极致的轻量化。

2.  **消融实验（Ablation Study）**：
    - **对比对象**：LOST-3DSG自身相似度函数（LSF）的不同组件组合。
    - **目的**：分析语义标签（`s_ℓ`）、颜色（`s_c`）、材质（`s_m`）、描述（`s_d`）各自对跟踪性能的贡献。
    - **关键发现**（见表2）：
        - **完整LSF**（所有组件）取得了最佳的更新（0.944）和删除（0.778）性能。
        - **仅使用描述（`s_d`）**：更新性能尚可（0.833），但删除性能大幅下降（0.444），说明细粒度描述利于身份识别，但不足以独立判断物体存亡。
        - **仅使用语义标签（`s_ℓ`）**：性能最差（删除0.333，更新0.556），证实仅靠类别标签无法进行可靠的长期跟踪。
        - **移除颜色和材质**：显著损害更新性能，说明低层次外观特征对于在视觉相似实例间进行时序关联至关重要。

### 四、 关键性能结果与结论
根据表1和正文分析，系统在不同复杂度场景下的表现如下：

| 场景复杂度 | 检测率 | 删除率 | 更新率 | 主要挑战与表现 |
| :--- | :--- | :--- | :--- | :--- |
| **简单（⋆）** | 3/3 (100%) | 3/3 (100%) | 3/3 (100%) | 完美处理所有移动和消失。 |
| **中等（⋆⋆）** | 20/21 (~95%) | 2/3 (~67%) | 1/1 (100%) | 高物体密度和视野外变化导致主要错误来源于语义关联模糊。 |
| **困难（⋆⋆⋆）** | 2/3 (~67%) | 2/3 (~67%) | 13/14 (~93%) | 极端动态环境下，更新性能依然强劲，但检测和删除面临更大挑战。 |

**总体结论**：
1.  **有效性**：LOST-3DSG能够**有效**在动态环境中进行基于语义的物体跟踪，尤其在物体**位置更新**方面表现出高鲁棒性（更新率普遍 >90%）。
2.  **效率优势**：核心贡献在于**极致的轻量化**，相比主流CLIP-based方法，内存占用**降低超过5个数量级**（MB级 vs. KB级），为在资源受限的机器人平台上实现实时、开放词汇的长期场景理解提供了可行路径。
3.  **性能权衡**：轻量化带来显著效率提升的同时，在**最复杂的动态场景**中，跟踪性能（尤其是检测和删除）会出现一定下降。论文指出，这是由于对视觉语言模型（VLM）生成的描述噪声敏感、缺乏时序特征聚合机制所致，但认为这是一个有前景的权衡。

**总结**：论文通过真实机器人实验证明，**无需依赖沉重的密集视觉嵌入模型，仅使用轻量级的物体级语义属性（词向量、句子嵌入），即可在动态环境中实现有效的开放词汇物体跟踪**。其评估虽未基于标准数据集，但通过精心设计的渐进式场景和针对性的指标，清晰地验证了方法在**轻量化（核心创新）和基本跟踪能力**上的有效性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.02905v1)
- [HTML 版本](https://arxiv.org/html/2601.02905v1)
