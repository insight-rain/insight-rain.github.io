# Learning Diffusion Policy from Primitive Skills for Robot Manipulation

**相关性评分**: 6.0/10

**排名**: #22


---


## 基本信息

- **arXiv ID**: [2601.01948v1](https://arxiv.org/abs/2601.01948v1)
- **发布时间**: 2026-01-05T09:56:24Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Zhihao Gu, Ming Yang, Difan Zou, Dong Xu

## 关键词

Vision-Language-Action Model, VLA for Robotics, Lightweight Architecture, Edge Deployment

## 一句话总结

该论文提出了一种基于技能条件的扩散策略（SDP），通过整合视觉-语言模型和轻量级路由器网络，将复杂任务分解为原始技能序列，以提高机器人操作中的动作生成一致性和效率。

## 摘要

Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.

## 详细分析

## 论文摘要：《Learning Diffusion Policy from Primitive Skills for Robot Manipulation》

**1. 研究背景和动机**
在机器人操作领域，扩散策略（Diffusion Policy, DP）已成为生成机器人动作序列的有力工具。然而，现有方法通常依赖**高层级、全局性的语言指令**来直接生成**短期、细粒度的控制信号**，这种指令与动作在粒度上的不匹配容易导致行为模糊或失准。本文认为，将复杂任务分解为一系列**可解释、可复用的基础操作技能**（Primitive Skills，如“向上移动”、“打开夹爪”），能为机器人学习提供更直观、更精确的指导界面。

**2. 核心方法和技术创新**
本文提出了**技能条件扩散策略（SDP）**，其核心创新在于将**显式的技能学习**与**条件化的动作生成**相结合。具体包括：
- **技能抽象与分配**：从多样任务中抽象出8个通用的基础操作技能。设计了一个**轻量级路由器网络**，它基于视觉-语言模型（VLM）对当前观测和指令的编码，动态地为每个状态分配最合适的单一技能。
- **技能条件策略学习**：提出了一种**单技能扩散策略**。通过一个**技能依赖的前馈网络（FFN）层**，将分配到的技能嵌入动态地参数化到扩散策略网络中，从而确保生成的动作与指定技能精确对齐。该方法类似于一个“技能专家”混合模型。
- **组合提示集成（CPE）**：为每个技能设计了统一的文本提示模板，并通过集成方式生成技能提示嵌入，增强了技能表示的区分度与可重用性。

**3. 主要实验结果**
在模拟与真实机器人实验上，SDP均展现出卓越性能：
- **模拟基准（CALVIN & LIBERO）**：在极具挑战性的零样本泛化（ABC→D）和长视野任务（LIBERO-Long）设置下，SDP的成功率和平均连续任务完成长度均显著超越所有基线方法（如MoDE、UniVLA等），创造了新的性能记录。
- **真实世界部署**：在涉及空间感知、工具使用、语义理解的多任务学习，以及对未见物体、视觉干扰物的泛化测试中，SDP均表现出更强的鲁棒性和泛化能力。
- **消融研究与可视化**：实验证实了技能抽象、CPE、技能依赖FFN等关键组件的有效性。技能分配的可视化结果清晰展示了SDP如何将复杂任务分解为技能序列，验证了其**可解释性**。

**4. 研究意义和价值**
本研究为基于技能的机器人学习提供了一个新范式。其价值在于：
- **技术层面**：通过引入显式、可解释的技能层，有效解决了高层指令与底层控制之间的语义鸿沟，实现了更精准、更连贯的机器人行为生成。
- **应用层面**：所提方法在模拟和真实场景中均表现出强大的多任务学习与泛化能力，为机器人适应开放、复杂的现实环境提供了更可靠的解决方案。
- **方法论**：将生成模型（扩散模型）与结构化技能表示相结合，为未来构建更高效、更透明的机器人决策系统提供了重要思路。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
现有基于扩散模型（Diffusion Policy, DP）的机器人操作策略通常将**高层级、全局性的语言指令**直接映射到**短期、细粒度的控制动作**。这种“粒度不匹配”导致动作生成容易出现**模糊、不对齐（misalignment）** 的行为，例如指令“把柠檬放进锅里”无法明确指导“何时闭合夹爪”这样的瞬时操作。

### **核心创新点**
论文提出了 **SDP（Skill-conditioned Diffusion Policy）**，一个**技能条件化的扩散策略**。其核心创新在于**将可解释的、细粒度的“原始技能”学习与条件化的动作规划相结合**，从而弥合高层指令与底层动作之间的语义鸿沟。

具体创新体现在以下三个层面：

1.  **引入结构化、可解释的原始技能空间**
    - 从多样化的任务中抽象出**8个可复用的原始技能**（Primitive Skills），例如：“移动向上”、“打开夹爪”、“平移”、“旋转”等。
    - 这些技能是**细粒度、短视距、可组合**的，为机器人学习提供了一个结构化和人类可理解的动作空间。

2.  **设计轻量级路由器网络进行动态技能分配**
    - 利用视觉-语言模型（VLM）从当前视觉观测和语言指令中提取特征。
    - 设计一个**轻量级的路由器网络**，根据当前状态动态评估并**为每一时刻分配一个最合适的原始技能**。
    - 这使得高层任务被**显式地分解**为一系列技能序列，而非隐式学习，增强了策略的**可解释性和可控性**。

3.  **构建技能依赖的单技能扩散策略**
    - 核心技术创新：**通过分配的技能嵌入动态参数化扩散策略中的前馈网络层**。
    - 具体实现采用一种 **LoRA-like 的FFN层**（公式4），该层的权重由当前技能嵌入通过一个小型MLP生成。
    - 这**显式地建立了原始技能与底层控制信号之间的依赖关系**，确保生成的动作与指定技能高度对齐，实现了“单技能策略”。

### **解决方案框架**
SDP的解决路径清晰分为两步：

1.  **“做什么”（技能规划层）**：
    - **输入**：静态/腕部相机图像 + 高层语言指令。
    - **处理**：VLM提取特征 → 路由器网络计算8个技能的重要性分数 → 选择最高分技能。
    - **输出**：当前时刻应执行的**具体原始技能**。

2.  **“怎么做”（动作执行层）**：
    - **输入**：分配的技能嵌入 + 机器人本体感知（如关节位置） + 时间步等。
    - **处理**：技能嵌入动态生成FFN参数 → 扩散模型（DiT）进行条件化去噪。
    - **输出**：与技能精确对齐的**机器人动作序列**。

### **实际价值与优势**
- **性能提升**：在CALVIN和LIBERO两大仿真基准测试及真实机器人实验中，SDP均显著超越现有SOTA方法，尤其在长视距、多任务和泛化到未见场景方面表现突出。
- **可解释性**：技能分配过程是显式的，研究者可以直观看到机器人每一步打算执行什么技能（如图5可视化），便于调试和理解策略决策。
- **泛化与鲁棒性**：通过组合有限的原始技能，可以完成大量复杂任务。实验表明其对未见物体和视觉干扰物具有更好的鲁棒性。
- **效率权衡**：虽然模型参数量和计算量略有增加，但通过仅使用**4步去噪**（少于基线方法的10步）和高效的技能参数化方式，实现了性能与推理时间的良好平衡。

**总结**：SDP的核心贡献是提出了一种**“技能桥接”** 的新范式。它不直接连接指令与动作，而是引入一个**由人类先验定义、由模型动态选择的技能中间层**，从而实现了更精确、更可解释、泛化能力更强的机器人操作策略。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对现有扩散策略在机器人操作中直接将高层指令映射为短期动作，导致行为模糊或错位的问题，提出了一种**基于技能条件的扩散策略（SDP）**。其核心方法是**将复杂任务分解为八个可解释、可复用的基础操作技能（如“移动”、“抓取”），并设计一个轻量级路由器网络，根据视觉-语言模型的输出来动态为每个状态分配最合适的技能**。然后，通过一个**参数化的单技能扩散策略**，将所分配的技能作为条件来生成精确对齐的动作序列。实验结果表明，该方法在多个模拟基准测试和真实机器人部署中均**显著超越了现有最优方法**，在任务成功率、泛化能力和长时程任务执行方面表现出色，同时其技能分配过程也提供了良好的可解释性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Learning Diffusion Policy from Primitive Skills for Robot Manipulation》提出了一种名为SDP（Skill-conditioned Diffusion Policy）的新方法。其核心创新在于将**可解释的、细粒度的技能学习**与**基于扩散模型的动作生成**相结合，以解决现有方法中高层指令与短期动作之间的粒度不匹配问题。以下是其相对于已有工作的明确创新点：

---

### 1. **引入显式、可组合的原始技能（Primitive Skills）作为中间表示**
- **改进/不同之处**：
    - **以往方法**：大多数语言条件扩散策略（如DiffPolicy、MDT）直接将高层语言指令（如“拿起柠檬放入锅中”）映射到原始动作序列。这种“端到端”映射缺乏中间解释层，指令过于抽象。
    - **本文方法**：显式定义了一组**8个可重用、人类可理解的原始技能**（如“移动向上”、“打开夹爪”、“平移”等）。这些技能通过一个统一的文本模板（如“the robot arm is going to {skill}”）进行描述，并构成一个结构化的、可解释的动作空间。
- **解决的问题/带来的优势**：
    - **解决指令-动作粒度不匹配**：高层指令（任务目标）与低层控制信号（关节角度/速度）之间存在巨大的语义鸿沟。原始技能作为**细粒度的短期指令**，为动作生成提供了更具体、更明确的引导。
    - **提升可解释性与可控性**：策略的决策过程变得可追溯。我们可以知道在每一步，机器人正在执行哪个“技能”，这有助于调试和理解策略行为。
    - **促进技能复用与组合**：这8个技能是跨任务抽象的，可以被灵活组合以完成各种复杂的长时程任务，提高了学习效率和泛化能力。

### 2. **设计轻量级路由器网络，实现基于状态的动态技能分配**
- **改进/不同之处**：
    - **以往方法**：一些工作（如Garg et al., 2022; Liang et al., 2024）使用VQ-VAE等方法**隐式地**学习技能表示（潜在代码），这些代码与具体技能语义的对应关系不明确。
    - **本文方法**：提出一个**轻量级的MLP路由器网络**。它接收来自视觉-语言模型（VLM）的融合表征（包含当前视觉观察和语言指令），并为8个候选技能计算重要性分数，然后通过`top-1`操作选择最合适的技能。
- **解决的问题/带来的优势**：
    - **实现显式、动态的技能选择**：技能分配不再是黑箱，而是基于当前情境（观察+指令）的明确决策。这使得策略能够根据任务进展**自适应地**切换技能。
    - **计算高效**：路由器网络非常轻量，与庞大的扩散模型主干相比，增加的参数量和计算开销很小。
    - **与VLM结合提升语义理解**：利用VLM强大的跨模态理解能力，将高层指令和视觉场景转化为选择具体技能的依据，使技能分配更准确。

### 3. **提出技能依赖的单技能扩散策略，实现技能与动作的精准对齐**
- **改进/不同之处**：
    - **以往方法**：标准的扩散策略中，条件信息（如图像、语言嵌入）通常通过交叉注意力（Cross-Attention）或特征拼接/相加等方式注入到去噪网络中。这些方式对“技能”这种特定条件的建模不够紧密。
    - **本文方法**：创新性地设计了 **“技能依赖的FFN层”** 。被选中的技能嵌入 `z` 通过一个MLP生成一组参数 `(W_z^1, W_z^2)`，这些参数用来动态构造一个额外的、类似LoRA的FFN层，并与原始FFN并联。公式如下：
        ```python
        FFN(x) = W_z^2(SwishGLU(W_z^1 x)) + FFN_ori(x)
        ```
- **解决的问题/带来的优势**：
    - **建立技能-动作的强依赖关系**：这种方法在神经网络架构层面，将特定的技能条件“编织”进了动作生成过程中，确保了生成的动作序列与**当前指定的技能高度对齐**。
    - **实现“单技能策略”**：在每个时间步，策略都专注于执行当前被分配的那个技能，从而产生更连贯、更精确的行为，避免了因指令模糊导致的动作混乱。
    - **参数高效**：采用类似混合专家（MoE）和LoRA的思想，仅通过少量动态生成的参数来调节网络，在保持高性能的同时控制了模型复杂度。

### 4. **构建组合提示集成（CPE）与正交损失，优化技能嵌入学习**
- **改进/不同之处**：
    - **以往方法**：技能或子目标的文本描述通常比较简单或固定，可能无法充分激发预训练VLM/CLIP文本编码器的能力。
    - **本文方法**：
        1. **组合提示集成（CPE）**：为每个技能设计统一的提示模板并进行集成，生成更丰富、更具描述性的文本嵌入 `p`。
        2. **正交损失（Orthogonal Loss）**：在训练中引入额外的损失项 `ℒ_Orth`，旨在最小化不同技能嵌入 `p_i` 之间的余弦相似度。
            ```python
            ℒ_Orth = (1/64) * Σ_i Σ_j Cos(p_i, p_j)
            ```
- **解决的问题/带来的优势**：
    - **提升技能表征的区分度**：CPE使技能的文字描述更准确，有助于VLM和路由器更好地区分不同技能。
    - **促进技能嵌入解耦**：正交损失强制不同技能的嵌入向量在特征空间中尽可能分开，这增强了技能表示的**独立性和可辨别性**，使路由器能做出更清晰的决策。

---

### **总结：创新点带来的整体优势**
1.  **性能提升**：在CALVIN和LIBERO等多个仿真基准测试以及真实机器人实验中，SDP均显著超越了现有SOTA方法，尤其在长时程、多任务和零样本泛化（如ABC→D）场景下优势明显。
2.  **泛化与鲁棒性增强**：通过技能分解，策略学会了可迁移的基本操作单元，因此能更好地处理未见过的物体（如从柠檬泛化到苹果）和存在视觉干扰物的复杂场景。
3.  **新范式**：论文为基于技能的机器人学习提供了一个新范式，即 **“高层指令 → VLM理解 → 显式技能规划 → 技能条件扩散生成精准动作”** ，弥合了任务规划与低层控制之间的鸿沟。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

该论文通过广泛的仿真和真实世界实验，系统地评估了所提出的**技能条件化扩散策略（SDP）**的性能、有效性和可解释性。

### 一、 使用的数据集与评价指标

1.  **仿真基准数据集**：
    *   **CALVIN**：一个包含34个不同任务、24,000条语言标注演示的大规模多任务操作基准。论文采用两种具有挑战性的评估设置：
        *   **ABCD → D**：在环境A、B、C、D上训练，在环境D上零样本评估。
        *   **ABC → D**：在环境A、B、C上训练，在未见过的环境D上零样本评估（更具挑战性）。
    *   **LIBERO**：一个专注于长期、多样化操作任务的基准。论文评估了其四个任务套件：
        *   **LIBERO-Spatial**（空间关系）
        *   **LIBERO-Object**（物体操作）
        *   **LIBERO-Goal**（多样化目标）
        *   **LIBERO-Long**（长期任务）

2.  **真实世界评估**：
    *   使用6自由度Lebai机械臂，设计了**9个任务**来评估**多任务学习能力**和**视觉泛化能力**。

3.  **主要评价指标**：
    *   **任务成功率**：成功完成指定任务的比率。
    *   **平均连续任务完成长度**：在CALVIN的链式任务评估中，机器人平均能连续成功完成的任务数量（最高为5）。这是衡量长期任务执行鲁棒性的关键指标。
    *   **（真实世界）平均成功率**：对每个任务进行多次试验（20次）后的平均成功比率。

### 二、 对比的基线方法

论文与当前最先进的（SOTA）多种策略进行了全面对比：

1.  **基于扩散的策略**：
    *   **DiffPolicy**：基础的CNN骨干扩散策略。
    *   **Octo**：使用统一动作表示处理异构动作空间。
    *   **MDT**：利用扩散模型生成以多模态目标为条件的灵活动作序列。
    *   **MoDE**：结合稀疏专家和噪声条件自注意力机制。

2.  **视觉-语言-动作（VLA）策略**：
    *   **RoboFlamingo**：使用连续动作头预测的VLA模型。
    *   **GR-1**：通过预测未来帧和动作进行预训练。
    *   **OpenVLA**：在大规模数据集上预训练的通才机器人策略。
    *   **UniVLA**：从视频中推导任务中心化动作表示的潜在动作模型。
    *   **（LIBERO专用）MaIL, UniActions**：在LIBERO基准上表现优异的其他先进方法。

### 三、 关键性能提升与结论

#### 1. 在CALVIN基准上的性能（表1）
*   **SDP在所有设置下均 consistently 超越所有SOTA方法**。
*   **在最具挑战性的ABC → D设置（零样本泛化）下**：
    *   完成全部5个连续任务的**成功率高达76.9%**，比之前最好的方法MoDE（62.4%）**高出14.5个百分点**，比UniVLA（56.5%）**高出20.4个百分点**。
    *   **平均连续任务完成长度达到4.49**，显著优于UniVLA的3.80和MoDE的3.92，证明了其在长视野任务中的卓越鲁棒性。
*   **在ABCD → D设置下**：SDP同样取得最佳成绩（86.5%的5任务成功率），平均长度达4.67。
*   **效率**：SDP仅使用**4步去噪**生成动作，比许多基线（如MDT、MoDE通常用10步）更高效。

#### 2. 在LIBERO基准上的性能（表2）
*   SDP在**所有四个任务套件上均取得最高成功率**，平均成功率高达**96.9%**。
*   特别是在最困难的**LIBERO-Long套件**上，SDP是**唯一成功率超过90%（达93.8%）** 的策略，而其他通用方法在此类复杂长期任务上表现挣扎。
*   相比扩散基线的MDT（76.1%）和VLA基线的UniVLA（92.5%），SDP分别取得了**20.8%和4.4%的绝对性能提升**。

#### 3. 真实世界机器人操作性能（图4）
*   **多任务学习**：在涉及空间感知、工具使用和语义理解的6个任务上，SDP**始终表现最佳**，在“打开微波炉放入薯片”、“倒水”等复杂任务上优势尤其明显。
*   **视觉泛化**：
    *   **未见物体**：能较好地操作与训练物体（柠檬）形状相似的未见苹果（成功率75%），但对形状差异大的香蕉泛化能力下降，符合预期。
    *   **视觉干扰物**：在存在复杂视觉干扰物的场景中，SDP的成功率从75%降至65%，**下降幅度远小于基线方法**，显示了其**更强的鲁棒性**。基线方法则因干扰物而性能大幅下滑。

#### 4. 消融实验与有效性分析（表3）
*   **关键组件**：逐步添加**交叉注意力注入先验**、**技能抽象（技能依赖的FFN）**、**组合提示集成（CPE）** 均带来性能持续提升，验证了每个设计的有效性。其中，技能抽象的引入对长期任务（LIBERO-Long）提升尤为显著（+5.5%）。
*   **技能条件化策略**：论文提出的**通过技能参数化FFN层（公式4）** 的方法，在效果上优于简单的加法、拼接或FiLM等条件化策略。
*   **复杂度分析（表4）**：虽然SDP的模型参数量（1017M）和计算量有所增加，但其带来的**性能提升远超额外的计算开销**，且推理时间增加可接受（45.1ms vs. MoDE的30.5ms）。

#### 5. 可解释性可视化（图5）
*   可视化显示，SDP能够**在训练中学习可重用的技能**，并在推理时**按顺序组合这些技能**来完成复杂目标。观察图像与分配技能之间的良好对齐，证明了该方法的**有效性和内在可解释性**。

### 总结
论文通过**定量与定性相结合**的评估，强有力地证明了SDP方法的优越性。其核心结论是：通过**显式地抽象和条件化于可解释的原始技能**，SDP能够生成更精准、更鲁棒的动作，从而在**多任务学习、零样本泛化到新环境、执行长期复杂任务以及抵抗视觉干扰**等多个关键维度上，显著超越现有的先进扩散策略和VLA策略。这为基于技能的机器人学习提供了一个新的有效范式。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.01948v1)
- [HTML 版本](https://arxiv.org/html/2601.01948v1)
