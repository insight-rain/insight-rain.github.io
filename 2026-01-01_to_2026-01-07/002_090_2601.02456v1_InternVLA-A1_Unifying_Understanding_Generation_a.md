# InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation

**相关性评分**: 9.0/10

**排名**: #2


---


## 基本信息

- **arXiv ID**: [2601.02456v1](https://arxiv.org/abs/2601.02456v1)
- **发布时间**: 2026-01-05T18:54:29Z
- **相关性评分**: 9.0/10
- **是否相关**: 是

## 作者

Junhao Cai, Zetao Cai, Jiafei Cao, Yilun Chen, Zeyu He, Lei Jiang, Hang Li, Hengjie Li, Yang Li, Yufei Liu, Yanan Lu, Qi Lv, Haoxiang Ma, Jiangmiao Pang, Yu Qiao, Zherui Qiu, Yanqing Shen, Xu Shi, Yang Tian, Bolun Wang, Hanqing Wang, Jiaheng Wang, Tai Wang, Xueyuan Wei, Chao Wu, Yiman Xie, Boyang Xing, Yuqiang Yang, Yuyin Yang, Qiaojun Yu, Feng Yuan, Jia Zeng, Jingjing Zhang, Shenghan Zhang, Shi Zhang, Zhuoma Zhaxi, Bowen Zhou, Yuanzhen Zhou, Yunsong Zhou, Hongrui Zhu, Yangkun Zhu, Yuchen Zhu

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

InternVLA-A1是一个统一的视觉-语言-动作模型，通过混合专家架构和轻量化设计，在机器人操作任务中实现高效推理和动态预测，显著提升性能。

## 摘要

Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\% improvement in daily tasks and a 40\%-73.3\% boost in dynamic settings, such as conveyor belt sorting.

## 详细分析

## 论文《InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation》详细摘要

### 1. 研究背景和动机
当前主流的视觉-语言-动作模型虽然具备强大的语义理解能力，但普遍缺乏对物理世界动态变化的推理能力。而基于视频预测的世界模型方法，又常因语义基础薄弱和对预测误差敏感而表现脆弱。为了弥合这一鸿沟，实现兼具语义理解与动态预测能力的通用机器人策略，本研究提出了InternVLA-A1模型。

### 2. 核心方法和技术创新
本研究提出了两大核心创新：
- **统一架构**：提出了一种基于**混合专家Transformer**的统一架构，无缝协调了三个专家模块：**场景理解专家**（基于MLLM）、**视觉前瞻生成专家**（预测未来状态）和**动作执行专家**（生成控制指令）。三者通过统一的掩码自注意力机制交互，实现了语义推理与动态预测的紧密耦合。
- **混合数据策略**：构建了一个**分层数据金字塔**，在预训练阶段混合使用大规模合成仿真数据（InternData-A1）和开源真实机器人数据（AgiBot-World），总计超过5.33亿帧。这种策略有效利用了仿真的多样性和真实数据的物理保真度，缩小了仿真到现实的差距。

### 3. 主要实验结果
在12个真实世界机器人任务和仿真基准上的评估表明，InternVLA-A1显著优于当前领先模型（如π₀和GR00T N1.5）：
- **通用任务**：在10个日常操作任务上，3B参数版本的**平均成功率高达75.1%**，比π₀模型绝对提升14.5%。
- **动态任务**：在传送带分拣等动态场景中，优势更为明显，在“快速分拣”和“动态食材抓取”任务上分别取得了**40%和73.3%** 的大幅性能提升。
- **消融实验**：验证了生成专家模块（移除后平均成功率下降19.4%）和混合预训练策略的关键作用。

### 4. 研究意义和价值
InternVLA-A1通过**架构与数据的协同创新**，为机器人操纵提供了一种新范式。其价值在于：
- **技术层面**：首次在统一框架内成功整合了MLLM的语义能力和世界模型的动态预测能力，为解决动态、非结构化环境中的鲁棒操作提供了有效方案。
- **实践层面**：证明了高质量合成数据与真实数据混合训练的有效性，为以可扩展、低成本方式获取多样化训练数据指明了方向，有助于推动通用机器人技术的实际应用。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：InternVLA-A1

### **一、 核心要解决的问题**
论文旨在解决当前主流**视觉-语言-动作（VLA）模型**在机器人操作任务中存在的两大关键瓶颈：
1.  **缺乏物理世界动态推理能力**：基于多模态大语言模型（MLLM）的VLA模型（如 π₀, GR00T）擅长语义理解，但无法有效建模物理规律（如动量、惯性、接触动力学），导致在**动态环境**（如传送带分拣）中表现不佳。
2.  **对场景变化的泛化能力不足**：现有模型严重依赖大规模真实机器人演示数据，但数据收集成本高昂、效率低下，且难以覆盖长尾场景变化，限制了模型的鲁棒性和泛化性。

### **二、 核心创新点**
论文通过**架构创新**与**数据策略创新**的协同，提出了综合解决方案。

#### **1. 模型架构创新：统一的理解-生成-动作框架**
- **创新架构**：提出了 **“混合专家”Transformer（Mixture-of-Transformers, MoT）** 的统一架构，在一个模型中协调三个专家模块：
    - **理解专家**：基于强大的MLLM（InternVL3/Qwen3-VL），负责**语义场景理解**。
    - **生成专家**：基于世界模型思想，负责**视觉前瞻生成**，预测未来帧的潜在表示。它使用专门的VAE编码器（Cosmos）来保留生成任务所需的细粒度空间结构。
    - **动作专家**：基于**流匹配（Flow Matching）** 方法，综合理解专家的语义上下文和生成专家的动态预测，生成精确的机器人控制指令。
- **关键机制**：通过**统一的掩码自注意力机制**，严格规定了信息流：`理解 → 生成 → 动作`。这确保了动作生成既基于语义理解，又考虑了物理动态的因果影响。

#### **2. 训练数据策略创新：混合合成-真实数据金字塔**
- **数据金字塔**：构建了一个分层的训练数据策略，而非单纯堆叠真实数据。
    - **基座（预训练）**：混合使用**大规模合成数据（InternData-A1）** 和**开源真实数据（AgiBot-World）**。合成数据提供低成本、高多样性的场景覆盖；真实数据提供物理保真度。
    - **顶端（后训练）**：使用**小规模、任务专用的真实数据**进行微调，以弥合“仿真到现实”的差距，并适应具体部署环境。
- **核心价值**：这种混合策略**协同了仿真的规模多样性与真实数据的物理真实性**，以可承受的成本实现了对长尾场景的鲁棒性。

#### **3. 训练与优化创新**
- **联合优化目标**：模型通过一个总损失函数进行端到端训练，同时优化**视觉前瞻生成损失（ℒ_gen）** 和**基于流匹配的动作预测损失（ℒ_action）**，迫使模型内化动作-环境动态的因果关系。
- **高效推理设计**：针对生成专家进行了专门优化（如使用卷积压缩潜在表示），使整个模型能在约**13 Hz**的频率下运行，满足实时控制需求。

### **三、 解决方案总结**
论文通过 **“一个统一架构 + 一套混合数据配方”** 的系统性方案，解决了语义理解与动态预测脱节、以及数据稀缺与泛化需求之间的矛盾：
1.  **在模型层面**，用MoT架构将MLLM的“大脑”（语义理解）与世界模型的“眼睛”（动态预测）紧密耦合，赋予模型**动态场景下的预见性决策能力**。
2.  **在数据层面**，用数据金字塔策略将仿真的“广度”与真实的“深度”相结合，为模型提供了**既多样又逼真**的训练经验，从而提升了面对未知场景变化的**泛化鲁棒性**。

### **四、 实际价值与效果**
- **性能提升**：在12个真实世界任务评估中，InternVLA-A1 (3B) 在**日常任务**上平均成功率比 π₀ 提升14.5%，在**高动态任务**（如传送带分拣、动态抓取）上提升幅度高达40%至73.3%，优势极其明显。
- **效率与性价比**：InternVLA-A1 (2B) 以更少的参数量超越了更大的 π₀ (3.3B)，体现了架构和数据策略的高效性。
- **为通用机器人提供新范式**：证明了**统一理解、预测与行动**的架构，结合**混合仿真-真实数据预训练**，是迈向在复杂、动态现实世界中鲁棒工作的通用机器人的一条有效路径。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决现有视觉-语言-动作（VLA）模型在机器人操作中**语义理解与物理动态预测能力割裂**的核心问题，这导致模型在动态场景（如传送带分拣）中表现脆弱。为此，论文提出了 **InternVLA-A1** 模型，其核心创新在于采用一个统一的**混合专家Transformer架构**，将场景理解、视觉前瞻生成和动作执行三个专家模块通过统一的掩码自注意力机制协同工作，从而将多模态大语言模型的语义推理与世界模型的动态预测能力相结合。此外，论文设计了一个**混合合成-真实数据金字塔**进行预训练，以低成本获取场景多样性并弥合仿真到真实的差距。实验表明，该模型在12项真实机器人任务和仿真基准测试中显著领先于π₀、GR00T N1.5等先进模型，在常规任务上平均成功率提升14.5%，在动态任务上提升最高达73.3%，验证了其架构与数据策略的有效性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation》针对现有视觉-语言-动作（VLA）模型的局限性，提出了多项明确的创新，主要集中在**模型架构**和**训练数据策略**两个方面。

### 1. 模型架构创新：统一的“理解-生成-动作”混合专家架构

-   **改进/不同之处**：
    -   **以往方法**：主流VLA模型（如π₀、GR00T）主要基于多模态大语言模型（MLLMs），擅长语义理解但**缺乏物理世界动态推理能力**。另一些方法转向世界模型（如视频预测），但**语义基础薄弱**且对预测错误敏感。
    -   **本文方法**：提出**Mixture-of-Transformers (MoT)** 统一架构，在一个框架内协调三个专家模块：
        1.  **理解专家**：基于MLLM（InternVL3/Qwen3-VL），负责场景语义理解。
        2.  **生成专家**：基于世界模型思想，预测未来视觉状态（视觉前瞻）。
        3.  **动作专家**：基于流匹配（Flow Matching），综合语义理解和预测的动态来生成控制指令。
    -   关键机制：通过**统一的掩码自注意力机制**实现信息流控制（理解 → 生成 → 动作），确保模块间无缝交互。

-   **解决的问题/带来的优势**：
    -   **解决了语义与动态的割裂问题**：首次在单一模型中**紧密耦合了高级语义推理和低级物理动态预测**。
    -   **提升了动态场景下的鲁棒性**：模型能够“想象”动作后果，从而在动态环境（如传送带分拣）中做出更明智的决策。实验证明，在动态任务上性能提升达40%-73.3%。
    -   **保持了高效的实时推理**：通过压缩视觉潜在表示（将32x32特征图压缩至4x4）等设计，模型在保持预测能力的同时，推理速度达到约13 Hz，满足实时控制需求。

### 2. 训练数据策略创新：混合合成-真实数据的“数据金字塔”

-   **改进/不同之处**：
    -   **以往方法**：严重依赖大规模、高成本的**真实机器人演示数据**进行端到端训练。虽然数据规模大（如Open X-Embodiment），但覆盖长尾场景变异成本高昂，且不同机器人形态的数据异构性给联合训练带来挑战。
    -   **本文方法**：构建了一个分层的**数据金字塔**，在训练的不同阶段策略性地混合数据源：
        1.  **预训练阶段**：混合使用**大规模合成数据**（InternData-A1，超过533M帧）和**开源真实数据**（AgiBot-World）。
        2.  **后训练阶段**：使用小规模、高质量的**特定任务真实数据**进行微调，校准模型以适应真实物理部署。
    -   提出了**负载均衡并行训练（LPT）** 策略，高效处理异构数据集混合训练时的内存和I/O挑战。

-   **解决的问题/带来的优势**：
    -   **解决了数据多样性与真实性的权衡问题**：利用合成数据的**高可扩展性和场景多样性**来覆盖长尾配置，同时利用真实数据的**物理保真度**来弥补仿真到真实的差距。
    -   **降低了数据收集成本**：合成数据可以低成本、自动化地生成大量多样化演示，减少了对昂贵、耗时的真人遥操作数据的依赖。
    -   **提升了模型泛化能力**：混合训练策略使模型既能从大规模数据中学习通用技能和物理先验，又能通过特定真实数据精准适应目标领域。消融实验表明，纯合成数据预训练已能达到媲美真实数据预训练的效果，而混合策略在真实世界任务上带来进一步显著提升。

### 3. 生成专家设计创新：面向高效推理的视觉前瞻模块

-   **改进/不同之处**：
    -   **以往方法**：视频预测模型（如扩散模型）生成质量高但**推理速度慢**（通常<6 Hz），无法满足机器人实时控制的高频需求。
    -   **本文方法**：
        -   采用**Cosmos CI 8x8 连续VAE分词器**对图像进行编码，平衡了生成所需的细粒度结构信息和计算效率。
        -   引入**空间压缩层**，将潜在特征图从32x32大幅压缩至4x4（每图仅用16个token表示），极大减少了序列长度。
        -   生成目标定义为预测**未来帧的潜在表示**（`z_{t+m}`），而非像素级图像，降低了生成复杂度。

-   **解决的问题/带来的优势**：
    -   **解决了实时性瓶颈**：上述设计使得视觉前瞻生成能够集成到需要~13 Hz实时推理的VLA管道中，**首次实现了具有高效未来状态预测能力的实时VLA模型**。
    -   **保持了预测对动作的指导价值**：尽管牺牲了部分图像生成的逼真度，但预测的潜在表示足以编码关键的任务动态信息，有效指导动作生成。消融实验证实，移除生成专家会导致平均成功率大幅下降（从77.0%降至57.6%）。

### 4. 动作学习创新：采用流匹配（Flow Matching）目标

-   **改进/不同之处**：
    -   **以往方法**：许多VLA模型使用简单的回归损失或基于分类的方法来预测动作，难以处理复杂、多模态的动作分布。
    -   **本文方法**：动作专家采用**流匹配**目标来学习动作分布。它将动作生成建模为从噪声到专家演示的连续概率路径学习。

-   **解决的问题/带来的优势**：
    -   **更好地处理多模态动作**：相比直接回归，流匹配能更优雅地建模和生成存在多个合理解决方案的动作序列。
    -   **提供更 principled 的学习框架**：与扩散模型有理论联系，但通常训练更稳定，为从噪声到精确动作的迭代去噪过程提供了坚实基础。

### 总结

InternVLA-A1的核心创新在于**系统性**地解决了当前VLA模型的泛化瓶颈：
1.  **架构上**，通过MoT统一了语义理解（MLLM）与物理动态预测（世界模型），弥补了“语义-动态鸿沟”。
2.  **数据上**，通过混合合成-真实的“数据金字塔”策略，以可扩展的方式获得了兼具多样性和物理真实性的训练数据。
3.  **技术上**，在生成模块和动作学习上进行了针对性优化，确保了模型在保持强大预测和推理能力的同时，满足**实时控制**的严苛要求。

这些创新共同使得InternVLA-A1在静态日常任务和**极具挑战性的动态场景**中，均显著超越了π₀、GR00T N1.5等前沿模型，展示了其在推动通用机器人操控向实用化迈进方面的实际价值。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 评估数据集与任务
论文在**真实机器人任务**和**仿真基准**上进行了全面评估。

1.  **真实世界任务（12项）**：
    *   **通用任务（10项）**：涵盖灵巧操作（如`拧瓶盖`、`拉上拉链`、`零件分拣`）和常规操作（如`制作三明治`、`操作烤箱`、`垃圾分类`、`清扫垃圾`、`擦拭污渍`、`放置马克笔`、`放置鲜花`）。
    *   **动态场景专项任务（2项）**：
        *   `快速分拣`：在传送带上抓取、翻转并展示包裹。
        *   `动态食材抓取`：双机器人协调抓取移动传送带上的食材以制作三明治。

2.  **仿真基准**：
    *   **RoboTwin 2.0 基准**：包含50项双手操作任务，评估了`简单`（与演示环境相同）和`困难`（应用了域随机化）两种设置。

### 二、 评价指标
*   **主要指标**：**任务平均成功率**。每个任务在30种预定义设置（如物体位置、场景初始化）下各进行一次试验，计算所有试验的平均成功率。
*   **辅助指标**：在部分任务中报告了**抓取成功率**。在仿真基准中，报告了每个任务在100次试验中的成功率。

### 三、 对比的基线方法
论文与当前领先的视觉-语言-动作模型进行了直接对比：
1.  **π₀ (3.3B)**：基于MLLM和流匹配的通用策略模型。
2.  **GR00T N1.5 (3B)**：采用双系统架构（推理与动作生成解耦）的模型。

### 四、 关键性能提升与结论

#### 1. 通用任务性能（10项任务）
*   **InternVLA-A1 (2B)**：平均成功率 **64.7%**，已超过更大的基线模型 **π₀ (3.3B, 60.6%)**，证明了其架构和数据策略的高效性。
*   **InternVLA-A1 (3B)**：平均成功率 **75.1%**，相比 **π₀ 绝对提升了14.5%**。
    *   在复杂任务上优势显著：`制作三明治`（93.3% vs. 66.7%）、`操作烤箱`（86.7% vs. 73.3%）。
    *   在灵巧任务`拉上拉链`上大幅领先（73.3% vs. 40.0%）。

#### 2. 动态场景专项任务性能
*   **InternVLA-A1 (3B)** 展现出**压倒性优势**：
    *   `快速分拣`：成功率 **80.0%**，相比基线（≤40.0%）**提升至少40个百分点**。
    *   `动态食材抓取`：成功率 **93.3%**，相比基线（≤20.0%）**提升超过73个百分点**。
*   **结论**：基线模型在动态环境中表现严重不足，而InternVLA-A1凭借其**生成专家（视觉预见）模块**，能够有效推理场景动态变化，从而生成前瞻性动作，实现了卓越的鲁棒性。

#### 3. 仿真基准性能（RoboTwin 2.0）
*   **InternVLA-A1 (3B)** 在50项任务上全面超越 **π₀**：
    *   `简单`设置：平均成功率 **65.0%** vs. π₀的54.5%，**提升10.5%**。
    *   `困难`设置：平均成功率 **25.4%** vs. π₀的19.8%，**提升5.6%**。
*   **结论**：模型在具有域随机化的挑战性仿真环境中也表现出更强的泛化能力。

#### 4. 消融实验关键结论
*   **预训练的有效性**：移除预训练阶段，模型平均性能**暴跌51.6%**（从77.0%降至25.4%），证明了大规模混合数据预训练提供了至关重要的操作技能先验。
*   **生成专家的有效性**：移除生成专家（视觉预见模块），模型平均成功率**下降19.4%**（从77.0%降至57.6%），在12个任务中的11个上性能下降，验证了该模块对于整合语义理解与动态预测的核心价值。
*   **混合数据策略的有效性**：
    *   纯合成数据（InternData-A1）预训练的模型，其性能已可匹配甚至超越纯真实数据（AgiBot-World）预训练的模型。
    *   **混合数据**（合成+真实）预训练策略在真实世界任务上取得了最佳效果，特别是在精细操作任务（如`零件分拣`）上带来**20%** 的显著提升，证明了该策略能有效协同仿真的多样性与真实数据的物理保真度。

### 总结
InternVLA-A1通过**统一的“理解-生成-执行”架构**与**混合合成-真实数据预训练策略**，在通用操作任务上显著超越了当前最先进的基线模型，并在**动态场景任务中取得了突破性进展**（提升40%-73.3%），证明了其模型在语义推理与物理动态预测相结合方面的强大能力。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.02456v1)
- [HTML 版本](https://arxiv.org/html/2601.02456v1)
