# Learning to Act Robustly with View-Invariant Latent Actions

**相关性评分**: 6.0/10

**排名**: #17


---


## 基本信息

- **arXiv ID**: [2601.02994v1](https://arxiv.org/abs/2601.02994v1)
- **发布时间**: 2026-01-06T13:14:01Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Youngjoon Jeong, Junha Chun, Taesup Kim

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种基于视觉的机器人策略框架VILA，通过建模潜在动作来学习视角不变的表示，以提高鲁棒性和下游任务性能，但与关键词中的推理效率和边缘部署等具体技术细节关联较弱。

## 摘要

Vision-based robotic policies often struggle with even minor viewpoint changes, underscoring the need for view-invariant visual representations. This challenge becomes more pronounced in real-world settings, where viewpoint variability is unavoidable and can significantly disrupt policy performance. Existing methods typically learn invariance from multi-view observations at the scene level, but such approaches rely on visual appearance and fail to incorporate the physical dynamics essential for robust generalization. We propose View-Invariant Latent Action (VILA), which models a latent action capturing transition patterns across trajectories to learn view-invariant representations grounded in physical dynamics. VILA aligns these latent actions across viewpoints using an action-guided objective based on ground-truth action sequences. Experiments in both simulation and the real world show that VILA-based policies generalize effectively to unseen viewpoints and transfer well to new tasks, establishing VILA as a strong pretraining framework that improves robustness and downstream learning performance.

## 详细分析

## 论文摘要

### 1. 研究背景和动机
基于视觉的机器人策略通常对相机视角的微小变化非常敏感，这阻碍了其在真实世界中的鲁棒部署。现有方法通常在**场景层面**学习视角不变的视觉表示，但这类方法依赖于视觉外观，未能融入对鲁棒泛化至关重要的物理动态信息。因此，需要一种更精准、基于动态变化的方法来提升策略的视角不变性。

### 2. 核心方法和技术创新
本文提出了**视角不变潜在动作（VILA）**框架。其核心创新在于将不变性约束从静态的场景级表示，转移到编码观测间动态变化的**潜在动作**上。VILA采用两阶段训练：
1.  **潜在动作学习**：在标准潜在动作模型基础上，引入**动作引导的对比学习目标**。该方法利用真实动作序列的相似性作为监督信号，通过加权对比损失和全局结构对齐损失，强制不同视角下描述相同动态的潜在动作在隐空间中对齐，从而实现基于动态的视角不变性。
2.  **潜在行为克隆**：训练一个潜在策略，使其能够从当前观测预测潜在动作。该策略随后作为视角鲁棒的视觉编码器，为下游的视觉运动策略提供条件。

### 3. 主要实验结果
在模拟（Robosuite的5个任务）和真实世界（SO-ARM机器人）的实验中，VILA均表现出色：
*   **未见视角泛化**：在“微调”和“冻结”编码器两种设置下，VILA在未见视角上的成功率显著优于Vanilla、CLASS、ReViWo、KYC等基线方法，且性能随视角差异增大而下降更缓慢。
*   **外推视角与真实世界**：在更极端的视角外推测试和真实世界任务中，VILA是唯一能保持有效性能的方法。
*   **未见任务适应**：当将预训练编码器迁移到新任务时，VILA在有限数据下提供了更强的先验，适应性能优于基线。
*   **表示质量分析**：熵分析和UMAP可视化表明，VILA学习到的表示同时具有最高的**视角熵**（视角不变性强）和最低的**动作熵**（动态语义保持性好）。

### 4. 研究意义和价值
VILA提出了一种新颖的视角鲁棒性学习范式，即**在动态层面而非静态外观层面施加不变性约束**。这使模型能够更专注于与控制相关的运动信息，从而更高效地学习视角不变的表示。该方法作为一个强大的预训练框架，能显著提升视觉运动策略对视角变化的鲁棒性以及在下游任务上的学习效率，为在复杂多变真实环境中部署鲁棒的机器人系统提供了有前景的技术路径。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
论文旨在解决**基于视觉的机器人策略对相机视角变化极度脆弱**的问题。现有方法通常在**场景层面**学习视角不变的视觉表示，但这会混淆静态背景外观与任务相关的动态信息，导致泛化能力不足，尤其是在真实世界中视角变化不可避免的情况下。

### **核心创新点**
论文提出了 **VILA（View-Invariant Latent Action）** 框架，其核心创新在于将**视角不变性**的约束从传统的**场景级视觉表示**，转移到了**动态变化的表示**上。具体来说，VILA学习一个**视角不变的潜在动作**，该动作编码了观察到的状态转移模式（即“动态”），而非整个场景的静态外观。

### **解决方案：VILA框架**
VILA通过一个两阶段流程实现上述目标：

#### **第一阶段：潜在动作学习**
1.  **基础潜在动作建模**：借鉴LAOM框架，通过一个**逆向动力学模型（IDM）** 从连续观测 `(o_t, o_{t+k})` 中推断出紧凑的潜在动作 `z`，并用一个**前向动力学模型（FDM）** 预测未来状态。核心损失是未来状态预测的均方误差 `ℒ_LA`。
2.  **动作引导的视角不变性对齐**：这是VILA的关键。为了强制潜在动作 `z` 在不同视角下保持一致，引入了基于**真实动作序列**的监督对齐：
    - **加权对比损失（`ℒ_W-NCE`）**：对于同一时间点、不同视角的观测对，如果它们对应的真实动作序列相似，则强制其潜在动作在嵌入空间中靠近。
    - **结构对齐损失（`ℒ_struct`）**：在批次级别，对齐潜在动作之间的**全局相似性矩阵**与真实动作序列之间的相似性矩阵。
    - **总损失**：`ℒ_VILA = ℒ_LA + λ1 ℒ_W-NCE + λ2 ℒ_struct`

#### **第二阶段：潜在行为克隆**
1.  训练一个**潜在策略 `π_z`**，其输入是当前观测 `o_t`，输出是预测的潜在动作 `z`。该策略通过行为克隆损失 `ℒ_BC` 进行训练，目标是匹配第一阶段IDM推断出的潜在动作。
2.  在微调下游任务时，`π_z` 作为一个**视角鲁棒的视觉编码器**，其输出的潜在动作用于条件化一个下游的视觉运动策略（如扩散策略）。

### **技术贡献与价值**
1.  **范式转变**：从追求“场景外观不变”转向“**动态变化不变**”，更贴合机器人控制的需求。
2.  **高效的表示学习**：潜在动作专注于编码**智能体和物体的运动模式**，比编码整个场景更紧凑、更具语义性。
3.  **强大的泛化能力**：
    - **视角泛化**：在模拟和真实世界实验中，VILA在**未见过的视角**上显著优于基线方法（如CLASS、ReViWo、KYC），即使视角差异很大。
    - **任务适应**：在跨任务迁移学习中（如从“堆叠三个”任务适应到“冲咖啡”任务），VILA学到的表示作为先验，能实现更高效的数据利用。
4.  **可验证的表示质量**：通过**视角熵**（越高越好，表示视角混合均匀）和**动作熵**（越低越好，表示相似动态的特征聚集紧密）的定量分析，证实VILA学到的表示**同时具备高视角不变性和高动态语义一致性**。

**总结**：VILA通过**在潜在动作空间施加以真实动作为引导的视角不变性约束**，创造了一个既对相机视角变化鲁棒、又富含控制相关动态信息的视觉表示。这使其成为一个强大的预训练框架，能显著提升视觉运动策略在视角变化下的鲁棒性和在下游任务上的学习效率。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对视觉驱动机器人策略因相机视角变化而性能急剧下降的核心问题，提出了一种名为**VILA**的预训练框架。该方法的核心创新在于，它不再追求对整个场景图像进行视角不变的表示，而是将不变性约束施加在**潜在动作**上——这是一种编码了连续观测之间动态变化的紧凑表示。VILA通过一个基于真实动作序列的、**动作引导的对比学习目标**，来对齐不同视角下的潜在动作，从而学习到一种根植于物理动态的、视角不变的动态表示。实验结果表明，基于VILA的策略在模拟和真实环境中，都能有效地泛化到未见过的视角，并能更好地迁移到新任务上，显著提升了策略的鲁棒性和下游学习性能。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Learning to Act Robustly with View-Invariant Latent Actions》针对视觉机器人策略对视角变化敏感的核心问题，提出了一种名为VILA的新颖预训练框架。其核心创新点在于将视角不变性的学习从传统的**场景级视觉表征**转移到**动态变化的表征**上。以下是其相对于已有工作的明确创新点：

### 1. **视角不变性的学习对象：从“场景”到“动态”**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：大多数现有工作（如CLASS、ReViWo）旨在学习一个**场景级**的视觉表征，要求一个单一的紧凑特征向量对整个图像（包括静态布局、背景和任务相关运动）进行编码，并在此层面上强制视角不变性。
     - **VILA的创新**：提出视角不变性不应施加在静态的场景表征上，而应施加在**动态变化**本身，即与动作相关的场景变化上。VILA学习一种**潜在动作**，它编码了连续观测之间的变化模式。
   - **解决的具体问题/带来的优势**：
     - **问题**：在场景级强制不变性要求表征对与任务无关的静态外观也保持不变，这既增加了学习难度，也可能导致表征混淆了任务相关的动态信息和无关的视觉外观。
     - **优势**：将不变性聚焦于动态变化，使得学习目标更紧凑、更直接地与控制相关。这释放了模型容量，使其专注于学习**智能体和物体如何运动**，而非整个场景在不同视角下看起来如何，从而更高效地实现鲁棒的视角泛化。

### 2. **利用真实动作序列进行“动作引导”的对齐**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：在多视角对比学习中，通常将同一时间点、不同视角的观测视为正样本对。这种对齐基于**视觉外观的时空一致性**。
     - **VILA的创新**：引入了**动作感知的对比学习目标**。它使用**真实动作序列**作为监督信号，来对齐不同视角下推断出的潜在动作。具体来说，如果两个潜在动作对应的未来真实动作序列相似，则即使它们来自不同视角，也会在潜在空间中被拉近。
   - **解决的具体问题/带来的优势**：
     - **问题**：仅基于视觉外观的对齐可能无法捕捉到对控制至关重要的物理动力学本质，尤其是在外观因视角变化而差异巨大时。
     - **优势**：**将物理动力学（通过动作序列体现）作为对齐的“锚点”**。这确保了学习到的视角不变表征是**基于物理动态的**，而不仅仅是基于视觉相似性。这为下游策略提供了更可靠、与任务执行更相关的状态表示。

### 3. **两阶段框架：解耦表征学习与策略学习**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：许多方法端到端地联合学习视角鲁棒表征和策略，或者将预训练好的场景编码器直接用于策略条件输入。
     - **VILA的创新**：采用清晰的两阶段流程：
       1.  **潜在动作学习阶段**：学习一个视角不变、动作对齐的潜在动作空间。
       2.  **潜在行为克隆阶段**：训练一个“潜在策略”，它从当前观测预测潜在动作。这个潜在策略随后作为**视角鲁棒的视觉编码器**，为下游的视觉运动策略提供条件。
   - **解决的具体问题/带来的优势**：
     - **问题**：端到端方法可能难以平衡视角不变性和任务性能；直接使用场景编码器可能无法充分利用动态中心化表征的优势。
     - **优势**：
       - **模块化与可移植性**：学习到的潜在动作空间成为一个独立的、可重用的接口。潜在策略作为编码器，**继承了潜在动作空间的视角不变性和结构化属性**。
       - **测试时效率**：在部署时，只需要当前观测即可通过潜在策略预测潜在动作，无需访问未来帧或多视角观测，**满足了实时控制的需求**。

### 4. **结合局部与全局结构的复合损失函数**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：可能仅使用标准的对比损失（如InfoNCE）或简单的重建损失来学习表征。
     - **VILA的创新**：设计了一个复合损失函数 `ℒ_VILA = ℒ_LA + λ1ℒ_W-NCE + λ2ℒ_struct`：
       - `ℒ_LA`：基础的潜在动作学习损失，确保潜在动作能预测未来状态变化。
       - `ℒ_W-NCE`：**加权的InfoNCE损失**，利用动作序列相似性作为软权重，精细化潜在动作空间的**局部结构**。
       - `ℒ_struct`：**全局结构对齐损失**，强制潜在动作之间的余弦相似度矩阵与真实动作序列之间的相似度矩阵对齐，以保持**全局一致性**。
   - **解决的具体问题/带来的优势**：
     - **问题**：单一损失函数可能无法同时保证表征的预测能力、精细的局部聚类和整体的几何结构。
     - **优势**：**局部与全局约束相结合**，使得学习到的潜在动作空间不仅具有视角不变性，而且在语义上与动作动力学高度对齐。消融实验证明，两者结合优于单独使用任一组件。

### 总结：核心技术创新价值
VILA通过将**视角不变性约束从静态场景外观转移到动态潜在动作**，并利用**真实动作序列作为对齐的物理基础**，构建了一个更专注于控制本质的预训练框架。其两阶段设计和复合损失函数确保了该表征既**鲁棒**（对视角变化）又**有效**（对下游任务）。实验表明，VILA在**未见视角泛化**（包括极端外推视角）和**跨任务适应**（数据效率高）方面均显著优于现有的场景级不变性方法，在仿真和真实机器人任务中都验证了其实际价值，为构建适应真实世界复杂视角变化的视觉运动系统提供了新思路。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果总结

该论文通过一系列仿真和真实世界实验，系统地评估了所提出的 **VILA** 框架在**视点泛化**和**任务适应**方面的性能。

### 1. 数据集
- **仿真环境 (Robosuite)**：使用了5个任务构建多视角数据集。
    - **Lift**：简单方块抓取。
    - **Square**：精确的拾取与放置。
    - **Stack Three**：多方块堆叠。
    - **Coffee**：将咖啡胶囊放入机器并关闭。
    - **Mug Cleanup**：打开橱柜、放入杯子并关闭。
    - **视角生成**：以原始`agentview`为参考，在方位角 `[-90°, +90°]` 和仰角 `[-15°, +15°]` 上构建5x5网格，共生成25个不同相机位姿。其中10个用于训练，15个用于评估，并额外生成了8个**外推视角**用于更严苛的测试。
- **真实世界 (SO-ARM101机器人)**：自收集了两个任务的单视角数据集，并使用**ZeroNVS**进行新视角合成以扩充数据。
    - **Pick & Place**：将方块放入杯中。
    - **Drawer**：将方块放入抽屉并关闭。
    - **视角生成**：通过改变方位角、垂直平移和光轴平移，生成27个视角，其中4个用于训练，3个未见视角用于评估。

### 2. 评价指标
- **主要指标**：**任务成功率**。在每个视角下进行多次rollout（仿真20次，真实世界10次），计算平均成功率。
- **分析性指标**：
    - **视角熵**：衡量特征表示中不同相机视角的混合程度（越高越好，表示视角不变性越强）。
    - **动作熵**：衡量特征表示根据未来动作序列（动态）的聚类程度（越低越好，表示动态语义保持得越好）。
    - **UMAP可视化**：直观展示特征在视角和动作类别上的分布。

### 3. 对比的基线方法
论文与以下代表性的基线方法进行了全面对比：
- **Vanilla**：直接使用ImageNet预训练的ResNet-18编码器。
- **CLASS**：使用基于真实动作序列距离的加权InfoNCE损失来训练场景级编码器。
- **ReViWo**：通过多视角观测分解来学习视角不变的场景表示。
- **Know Your Camera (KYC)**：将相机参数（外参、内参或Plücker光线）显式地作为策略的输入条件。

**公平性设置**：所有方法使用相同的下游策略（Diffusion Policy）、相同的训练流程和超参数，以隔离编码器表示本身的影响。

### 4. 关键性能结果与结论

#### a) 未见视角泛化 (核心贡献)
- **仿真结果 (表1, 图5)**：
    - **微调设置**：VILA在**所有5个任务**的未见视角上都取得了最高的成功率。在较难的任务（如Square, Coffee）上，优势尤为明显。例如，在`Coffee`任务中，VILA的未见视角成功率为12.65%，而Vanilla基线仅为0.35%。
    - **冻结设置**：VILA是**唯一一个**在所有任务上都能保持非零性能的方法，而其他基线在多个任务上完全失败（成功率0%）。这证明了VILA学习到的表示本身具有极强的视角不变性。
    - **趋势分析**：随着测试视角与最近训练视角的**角度差异增大**，VILA的性能下降速度远慢于所有基线（图5），显示出更稳健的泛化能力。
    - **外推视角**：在更极端的、超出训练范围的相机位姿上，VILA的优势进一步扩大（表2）。例如在`Lift`任务中，VILA成功率高达93.1%，而最佳基线CLASS仅为51.3%。

- **真实世界结果 (表3)**：
    - VILA在`Pick & Place`和`Drawer`任务上的平均成功率分别达到**63.3%** 和 **85.0%**。
    - 所有基线方法表现显著较差，在`Drawer`任务上甚至全部为**0%**。这验证了VILA在真实物理世界中的有效性。

#### b) 未见任务适应 (迁移学习能力)
- **实验设置**：将在`Stack Three`任务上预训练的编码器，迁移到新的`Coffee`任务，并使用少量标注数据进行微调。
- **结果 (图6)**：
    - 在不同大小的标注数据子集（100, 250, 500, 1000条轨迹）上，**VILA始终提供最强的先验**，其微调后的策略成功率最高。
    - 值得注意的是，并非所有多视角预训练都有益：一些场景级不变性方法（如CLASS）的迁移性能有时甚至不如从头训练的Vanilla基线。这表明VILA在**动态层面**施加不变性，能产生更具通用性和可迁移性的表示。

#### c) 表示质量分析 (表5, 图7)
- **视角熵**：VILA在**所有任务**上均取得了**最高的视角熵**（在`Lift`任务中，微调后为2.803/2.548，接近均匀分布上界3.219），证明其表示确实高度混合了不同视角。
- **动作熵**：VILA同时取得了**最低的动作熵**（在`Lift`任务中为0.829），表明其表示在实现视角不变的同时，依然能紧密地聚类具有相似动态（未来动作）的观测。
- **UMAP可视化**：基线方法的特征中，未见视角（尤其是特定视角组）会形成明显的独立聚类；而**VILA的特征则均匀混合**，未见视角与已见视角交织在一起，直观证实了其视角不变性。

### 结论
论文通过定量和定性的实验充分证明：
1.  **VILA在视角泛化上显著优于现有方法**：无论是在仿真还是真实世界，在标准、挑战性乃至外推的相机位姿下，VILA都能实现最稳健的策略性能。
2.  **VILA提供了更优的迁移学习先验**：其基于动态的视角不变表示，比基于场景外观的表示更能适应新任务，尤其在数据有限时优势明显。
3.  **核心机制得到验证**：通过视角熵和动作熵分析，证实了VILA成功地将**视角不变性**与**动态语义保持**这两个有时相互冲突的目标统一在了其潜在动作表示中。

**局限性说明**：论文明确给出了定量结果，但也指出了其依赖多视角观测数据（真实世界需借助NVS模型生成）以及目前主要针对相机位姿变化的局限性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.02994v1)
- [HTML 版本](https://arxiv.org/html/2601.02994v1)
