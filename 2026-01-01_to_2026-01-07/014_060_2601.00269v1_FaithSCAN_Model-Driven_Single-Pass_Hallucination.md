# FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering

**相关性评分**: 6.0/10

**排名**: #14


---


## 基本信息

- **arXiv ID**: [2601.00269v1](https://arxiv.org/abs/2601.00269v1)
- **发布时间**: 2026-01-01T09:19:39Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

FaithSCAN 是一种轻量级网络，通过利用视觉语言模型的内部信号进行单次幻觉检测，以提高视觉问答的忠实性和推理效率。

## 摘要

Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

## 详细分析

## 论文摘要：FaithSCAN: 面向可信视觉问答的模型驱动单次幻觉检测

**1. 研究背景和动机**
视觉问答（VQA）模型常产生流畅但与视觉证据不符的“忠实性幻觉”，严重威胁其在安全关键应用中的可靠性。现有检测方法主要分为两类：依赖外部模型或知识库的验证方法（计算开销大、受限于外部资源质量）和基于不确定性的方法（如重复采样，仅捕捉有限的不确定性层面且效率低下）。两者在效率、鲁棒性和检测性能上均存在固有局限。

**2. 核心方法和技术创新**
本文提出 **FaithSCAN**，一种轻量级网络，通过**单次前向传播**利用VLM内部丰富的信号进行幻觉检测。核心创新包括：
- **多源内部信号融合**：联合建模**词元级解码不确定性**（如对数似然、熵）、**中间视觉表征**以及**跨模态对齐特征**，全面覆盖感知、推理和语言解码中的失败模式。
- **分支证据编码与不确定性感知注意力**：通过独立分支编码异构信号，并使用注意力机制进行自适应聚合。
- **模型驱动的低成本监督**：扩展“LLM即法官”范式至VQA，提出基于视觉自然语言推理（Visual-NLI）的自动标注策略，生成与目标模型内部推理过程对齐的监督信号，无需昂贵的人工标注。

**3. 主要实验结果**
在HalLoc-VQA、POPE、HaloQuest和VQA v2等多个基准上，针对InstructBLIP、LLaVA-NeXT和Qwen3-VL等不同架构的VLM进行了评估：
- **有效性**：FaithSCAN在AUROC、F1等关键指标上显著优于所有基线方法（如语义熵、SelfCheckGPT），最高提升超过10%。
- **效率**：仅需单次前向传播，推理速度远快于需要多次采样或外部调用的方法。
- **分析洞察**：实验表明，不同内部信号（视觉、文本、跨模态）对检测不同类型幻觉（如物体级、关系级、错误前提）具有互补性；幻觉模式因VLM架构而异，揭示了其与多模态推理内部动态的紧密关联。

**4. 研究意义和价值**
本研究证实了幻觉是根植于VLM内部推理动态的结构化现象，而非仅存在于输出层面。FaithSCAN提供了一种**高效、轻量且可解释**的检测方案，无需改变基础模型或引入外部资源。其方法框架为理解多模态幻觉的根本原因提供了新视角，推动了构建更可靠、可信的视觉语言系统的发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：FaithSCAN

### **一、 研究问题与动机**
**核心问题**：如何高效、准确地检测视觉问答（VQA）任务中的**忠实性幻觉**。即，模型生成的答案虽然流畅，但与输入图像中的视觉证据不符。

**现有方法的局限性**：
1.  **外部验证方法**：依赖额外的模型（如物体检测器、LLM法官）或知识库来验证答案与图像的一致性。
    *   **缺点**：计算开销大、性能受外部资源质量限制、无法洞察模型内部推理过程。
2.  **不确定性驱动方法**：通过重复采样或估计模型输出的不确定性（如语义熵）来检测幻觉。
    *   **缺点**：推理成本高（需多次采样）、不稳定、未能充分利用模型内部丰富的异构不确定性信号。

### **二、 核心技术创新**
FaithSCAN 提出了一种**模型驱动的单次前向传播幻觉检测框架**，其创新点主要体现在以下三个方面：

#### **1. 方法范式创新：从“外部/采样”到“内部/单次”**
*   **核心理念**：将幻觉检测视为一个**监督学习分类问题**，而非基于采样的不确定性估计或外部验证。
*   **关键突破**：仅需**单次前向传播**，从冻结的VLM中提取丰富的内部不确定性信号，并训练一个轻量级检测器进行预测。这从根本上解决了现有方法效率低下的问题。

#### **2. 技术架构创新：多源内部信号融合**
FaithSCAN 设计了一个轻量级网络，专门用于融合从VLM内部提取的三种互补的不确定性信号：
*   **Token级生成信号**：解码过程中每个token的对数似然、熵和隐藏层嵌入。反映**语言解码的不确定性**。
*   **视觉语义表示**：视觉编码器提取的原始图像块特征。反映**视觉感知的不确定性**。
*   **跨模态对齐表示**：经过对齐模块投影到语言语义空间的视觉特征。反映**视觉-语言交互对齐的不确定性**。

**融合机制**：
*   **分支式证据编码**：每个信号源通过独立的轻量级编码器（如CNN+池化）压缩为固定维度的嵌入。
*   **不确定性感知注意力融合**：使用注意力机制动态聚合各分支的证据，让检测器学会权衡不同信号源对当前幻觉判断的重要性。

#### **3. 监督信号创新：低成本、模型驱动的标签生成**
为了解决监督学习需要大量标注数据的难题，论文创新性地将 **“LLM-as-a-Judge”范式扩展到多模态领域**。
*   **方法**：使用一个强大的VLM作为“法官”，基于**视觉自然语言推理**（Visual-NLI）协议，判断模型生成的答案与（图像、问题、参考答案）构成的前提之间是**蕴含、矛盾还是不确定**。将“矛盾”和“不确定”视为幻觉标签。
*   **优势**：
    *   **低成本**：自动化生成，无需大规模人工标注。
    *   **模型感知**：生成的标签与**特定VLM的生成行为**和内部状态对齐，而非通用的、模型无关的标签，更适用于检测模型依赖的幻觉现象。
    *   **可靠性**：通过小规模人工验证，证明该自动标签与人类判断具有高度一致性（Cohen‘s Kappa > 0.81）。

### **三、 解决方案总结**
**FaithSCAN 的完整解决方案流程**：
1.  **数据准备**：使用目标VLM在数据集上生成答案，并利用提出的“Visual-NLI法官”自动为每个（图像，问题，生成答案）三元组打上幻觉/非幻觉标签。
2.  **特征提取**：在训练和推理时，对每个样本，运行一次目标VLM的前向传播，同步提取上述三类内部不确定性信号。
3.  **模型训练**：将提取的信号输入FaithSCAN轻量级网络，使用自动生成的标签进行监督训练，学习将内部信号映射到幻觉概率。
4.  **部署推理**：部署训练好的FaithSCAN检测器。对于新的VQA样本，只需运行一次VLM获取内部信号，即可通过FaithSCAN快速输出幻觉检测结果。

### **四、 实际价值与意义**
*   **高效实用**：单次前向传播即可检测，**推理速度远快于基于采样或外部模型的方法**，为实时或大规模VQA应用中的幻觉检测提供了可行方案。
*   **性能优越**：在多个VQA基准测试（HalLoc-VQA, POPE, HaloQuest, VQA v2）和不同VLM架构（InstructBLIP, LLaVA, Qwen-VL）上，**检测效果（AUROC等指标）显著优于现有基线方法**。
*   **机理洞察**：通过消融实验和分析，论文揭示了不同幻觉类型（物体、关系、虚假前提）对应着不同的内部失败路径（感知、推理、交互），并且不同VLM架构的幻觉模式不同。这为理解多模态幻觉的根本原因提供了新见解。
*   **可解释性**：通过梯度归因分析，方法能够定位导致幻觉的关键token或特征，增强了检测过程的**可解释性和诊断价值**。

**结论**：FaithSCAN 通过巧妙地利用VLM自身的“内在自省”能力，实现了一种高效、准确且可解释的幻觉检测新范式，推动了构建更可靠、可信的多模态AI系统的发展。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对视觉问答（VQA）中模型产生流畅但与视觉证据不符的“忠实性幻觉”问题，提出了一种高效、轻量级的检测方法。现有方法主要依赖外部验证（计算开销大）或基于重复采样的不确定性估计（效率低且信号单一），**核心问题**在于如何在保证检测性能的同时，实现高效、鲁棒的幻觉检测。

为此，论文提出了 **FaithSCAN** 框架。其**核心方法**是：在目标VLM的单次前向传播中，提取并融合其内部丰富的多模态不确定性信号（包括词元级解码不确定性、中间视觉表征和跨模态对齐特征），通过分支证据编码和不确定性感知注意力进行建模。同时，论文创新性地将“LLM-as-a-Judge”范式扩展到VQA领域，提出了一种低成本的、模型驱动的监督信号自动生成策略，从而无需昂贵的人工标注即可训练检测器。

实验表明，FaithSCAN在多个VQA基准测试中，在检测效果（如AUROC）和推理效率上均显著优于现有方法。**主要结论**是：VQA中的幻觉是源于视觉感知、跨模态推理和语言解码等内部状态的系统性变化，不同内部信号提供了互补的诊断线索，且幻觉模式因VLM架构而异。这证明了从模型内部视角进行单次检测的可行性与优越性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering》针对视觉问答（VQA）中的忠实性幻觉检测问题，提出了一个名为FaithSCAN的新方法。其核心创新点在于**从模型内部视角出发，通过单次前向传播提取并融合多种不确定性信号，并利用一种低成本、模型感知的监督策略进行训练**。以下是其相对于已有工作的明确创新点：

### 1. **单次前向传播的检测范式**
   - **改进/不同之处**：现有不确定性驱动的方法（如语义熵SE、SelfCheckGPT）通常依赖于**重复采样**（多次生成答案）或**视觉扰动**来估计模型不确定性，导致**高推理成本和不稳定行为**。FaithSCAN则**仅需一次前向传播**，直接从冻结的VLM内部提取不确定性信号。
   - **解决的问题/优势**：
     - **显著提升效率**：避免了重复采样带来的计算开销，使其更适用于**延迟敏感或大规模VQA场景**。
     - **提高稳定性**：不依赖于随机解码或提示配置，减少了方法的不确定性。

### 2. **多源内部不确定性信号的联合建模与融合**
   - **改进/不同之处**：以往方法往往**独立或部分地利用**模型内部信号（如仅关注注意力模式、或仅使用词元级似然）。FaithSCAN**系统性地提取并融合三类互补的内部信号**：
     1. **词元级解码不确定性**（如对数似然、熵、隐藏层嵌入）。
     2. **中间视觉表示**（原始视觉块嵌入）。
     3. **跨模态对齐表示**（对齐到文本语义空间的视觉特征）。
   - **解决的问题/优势**：
     - **更全面的诊断**：幻觉可能源于视觉感知、跨模态推理或语言解码中任一环节的失败。联合建模能捕获**不同推理路径上的幻觉模式**，提供更丰富的诊断线索。
     - **提升检测性能**：实验表明，不同信号具有互补性，融合后能显著提高检测的AUROC等指标。

### 3. **分支式证据编码与不确定性感知注意力融合机制**
   - **改进/不同之处**：论文设计了**轻量级的分支编码器**独立处理每种异构信号，然后通过**不确定性感知的注意力机制**进行动态聚合。这与简单拼接或平均池化等传统融合方式不同。
   - **解决的问题/优势**：
     - **自适应权重分配**：注意力机制能根据当前实例，**动态评估不同不确定性源的重要性**。
     - **保持轻量化**：分支编码器结构简单，整体检测网络参数量小，易于训练和部署。

### 4. **模型驱动的低成本监督信号生成策略**
   - **改进/不同之处**：
     - 现有监督方法要么依赖**昂贵的人工标注**，要么使用**与模型无关的通用幻觉标注**。FaithSCAN创新性地**将“LLM-as-a-Judge”范式扩展到多模态领域**，提出基于**视觉自然语言推理**的模型感知标注方法。
     - 具体而言，使用一个强大的VLM作为“法官”，评估目标VLM生成的答案与**参考答案**在给定图像下的语义关系（蕴含、矛盾、不确定），从而自动生成幻觉标签。
   - **解决的问题/优势**：
     - **低成本获取大规模监督数据**：避免了人工标注的巨额成本。
     - **模型感知的标签**：生成的标签与**目标VLM的具体生成行为和内部推理过程紧密关联**，而非通用的错误案例，这使得监督信号更适用于训练针对该特定模型的检测器。
     - **可靠性验证**：通过小规模人工验证，证明了该自动标注方法与人类判断具有高度一致性（Cohen‘s Kappa > 0.81）。

### 5. **将幻觉检测明确构建为有监督的二分类问题**
   - **改进/不同之处**：许多不确定性方法最终需要**手动设置阈值**将不确定性分数转换为二分类决策。FaithSCAN**直接学习从内部信号到幻觉标签的映射**。
   - **解决的问题/优势**：
     - **简化部署**：直接输出可操作的二分类结果（是否幻觉），无需额外的阈值调优。
     - **学习更高效**：模型专注于从内部信号中提取有用模式，而非校准输出空间的置信度，降低了学习难度。

### 总结的核心优势
通过上述创新点的结合，FaithSCAN实现了 **“高效、有效、可解释”** 的幻觉检测：
- **高效性**：单次前向传播，推理速度远快于基于重复采样或外部验证的方法。
- **有效性**：在多个VQA基准测试和不同骨干VLM上，综合检测性能（AUROC, F1等）显著优于现有方法。
- **可解释性**：通过梯度归因分析等方法，能够可视化哪些词元或内部信号对幻觉决策贡献最大，提供了对VLM失败模式的洞察。
- **泛化性**：方法设计兼容不同架构的VLM（如InstructBLIP, LLaVA, Qwen-VL），并通过多源数据训练展现了较好的跨数据集泛化能力。

**总而言之，FaithSCAN的创新在于其系统性的视角转变：将幻觉检测从依赖外部验证或输出级统计，转变为深入挖掘并融合模型内部生成过程中的多维度不确定性证据，并辅以创新的自监督范式，为实现可靠、实用的VQA系统提供了新工具和新见解。**


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过全面的实验验证了FaithSCAN在视觉问答（VQA）忠实性幻觉检测任务上的**有效性、高效性和泛化能力**。实验设计围绕四个核心研究问题展开，使用了多个数据集、评价指标，并与一系列代表性基线方法进行了对比。

### 一、 使用的数据集
实验在四个具有不同特点的VQA数据集上进行，以覆盖多样化的幻觉类型：
| 数据集 | 样本量（约） | 主要幻觉类型 | 真值验证粒度 |
| :--- | :--- | :--- | :--- |
| **HalLoc-VQA** | 101k (VQA子集) | 物体级、场景级 | 细粒度（Token级） |
| **POPE** | 9k | 物体存在性 | 物体级 |
| **HaloQuest** | 7.7k | 错误前提、推理诱导 | 答案级 |
| **VQA v2** | 1.1M | 自然场景下的视觉基础挑战 | 答案级 |

### 二、 评价指标
为了全面评估检测器的排名质量和决策能力，论文采用了以下互补的指标：
- **AUROC (ROC)**：衡量模型将幻觉样本与忠实样本区分开的整体排序能力。
- **AURAC (RAC)**：评估在拒绝（即舍弃不确定样本）场景下的选择性预测性能。
- **F1@Best (F1@B)**：报告在最优阈值下的分类F1分数。
- **RejAcc@50% (RA@50)**：量化在拒绝最不确定的50%预测后，剩余样本的分类准确率。

### 三、 对比的基线方法
论文与两大类共8种代表性基线方法进行了对比，涵盖了从Token级到语义级的不同不确定性估计策略：
1.  **Token级基线**：
    - **Predictive Entropy (PE)**：预测分布的熵。
    - **Token-level NLL (T-NLL)**：生成Token的负对数似然。
2.  **隐空间基线**：
    - **Embedding Variance (EmbVar)**：视觉条件答案嵌入的方差。
    - **Semantic Embedding Uncertainty (SEU)**：语义嵌入的不确定性。
3.  **语义级基线**：
    - **Semantic Entropy (SE)**：通过对多个生成样本进行语义聚类来估计不确定性。
    - **SelfCheckGPT (多模态适配)**：通过检查多个生成答案之间的一致性来检测矛盾。
    - **P(True) Verification**：通过图像条件的“是/否”提示来评估答案正确性。
4.  **监督学习基线**：
    - **Logistic Regression**：在相同数据集上训练的Logistic回归分类器，作为监督学习的参考。

### 四、 关键性能与结论
实验在三个主流视觉语言模型（**InstructBLIP, LLaVA-8B, Qwen3-VL-8B**）上展开，主要结论如下：

#### 1. **整体有效性 (RQ1)**
- **性能领先**：FaithSCAN在**绝大多数数据集-模型组合上取得了最佳或次佳的检测性能**。例如，在Qwen3-VL-8B模型上，FaithSCAN在HalLoc-VQA数据集的AUROC达到0.810，显著优于最佳基线（P(True)的0.750）。
- **显著提升**：与最强的无监督基线（如SE、P(True)）相比，FaithSCAN在AUROC上实现了**最高达10个百分点的提升**。在选择性预测指标（如RA@50）上，提升更为明显，表明其不确定性分数能更有效地识别应被拒绝的错误答案。
- **效率优势**：FaithSCAN仅需**单次前向传播**即可完成检测。虽然其推理速度因需融合多种嵌入而略慢于最简单的Token级方法（如PE），但**远快于依赖多次采样（如SE、SelfCheckGPT）或外部模型验证（如P(True)）的方法**，实现了效果与效率的平衡。

#### 2. **分布外泛化能力 (RQ1)**
- **跨数据集测试**：当在一个数据集上训练，在另一个不同特性的数据集上测试时，所有方法（包括FaithSCAN）性能均下降，表明幻觉检测存在分布偏移挑战。
- **相对优势**：FaithSCAN在多数跨数据集设置下**表现优于作为对照的Logistic回归基线**，显示出更好的泛化性。
- **多源训练提升**：当使用所有可用数据集进行**联合训练**时，FaithSCAN的分布外性能得到显著提升，表明多样化的训练数据有助于模型学习更通用的幻觉模式。

#### 3. **模型驱动监督的有效性 (RQ2)**
- **与人工标注高度一致**：论文提出的基于Visual-NLI的自动标注方法，其生成的幻觉标签与**小规模人工验证结果具有高度一致性**（各数据集的Agreement在96%-99%之间，Cohen‘s Kappa > 0.81）。这证明了该低成本监督策略的可靠性。

#### 4. **内部信号分析 (RQ3)**
- **信号互补性**：消融实验证实，**Token级解码不确定性是检测幻觉最有效的单一信号**，但**结合视觉特征和跨模态对齐特征能带来额外增益**。这种增益因幻觉类型和模型架构而异（例如，在物体幻觉为主的POPE数据集上，视觉特征贡献更大）。
- **最优层选择**：从语言解码器中提取特征的**中间层（约2/3深度）** 通常能提供最有利于幻觉检测的表示。
- **架构有效性**：**分支编码 + 不确定性感知注意力融合**的架构设计被证明优于简单的特征拼接或平均池化。

#### 5. **实用性与可解释性 (RQ4)**
- **更优的决策曲线**：FaithSCAN的ROC和PR曲线在大部分操作点上均优于基线，意味着在实际应用中设置阈值时，能在相同召回率下获得更高的精度。
- **清晰的分数分离**：可视化显示，FaithSCAN输出的不确定性分数在“幻觉”和“非幻觉”样本间具有**更明显的分布分离**。
- **可解释的归因**：通过梯度归因方法，FaithSCAN能够**高亮导致幻觉的关键Token**，为诊断模型的错误推理过程提供了直观线索。

### 总结
论文通过系统的实验给出了明确的定量结果，核心结论是：**FaithSCAN通过单次前向传播提取并融合VLM内部的多源不确定性信号，在多个基准测试中，以更高的效率实现了比现有方法更优的忠实性幻觉检测性能。** 这项工作不仅提出了一个有效的检测器，还通过分析揭示了不同内部信号与特定幻觉模式之间的关联，为理解多模态幻觉的根源提供了新见解。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.00269v1)
- [HTML 版本](https://arxiv.org/html/2601.00269v1)
