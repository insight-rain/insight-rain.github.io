# Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians

**相关性评分**: 6.0/10

**排名**: #8


---


## 基本信息

- **arXiv ID**: [2601.00678v1](https://arxiv.org/abs/2601.00678v1)
- **发布时间**: 2026-01-02T13:04:47Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种基于动态3D高斯表示的单图像到视频生成框架，通过单次前向传递实现快速、相机引导的视频生成，强调推理效率和潜在边缘部署价值。

## 摘要

Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

## 详细分析

## 论文摘要：Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians

**1. 研究背景和动机**
人类能够仅凭单张图像预测场景的未来动态，而现有的单图像条件视频生成模型在模仿此能力时，往往**缺乏鲁棒的用户可控性**（如修改相机路径），且在准确建模相机运动、保持时间一致性和几何完整性方面存在挑战。尽管利用显式中间3D表示（如点云）进行渲染提供了解决方案，但其两步流程（先重建后注入动态）仍难以实现完全的时间一致性。因此，研究需要一种能够**在单次前向传播中**同时重建3D场景并生成合理物体运动的高效方法。

**2. 核心方法和技术创新**
本文提出了 **Pixel-to-4D** 框架，其核心创新在于构建了一个**动态的4D高斯场景表示**，并设计了高效的端到端生成架构：
- **4D高斯表示**：从单张图像预测每个像素对齐的高斯椭球体参数，不仅包含静态属性（深度、颜色、旋转等），还为其赋予了**3D线性与角速度及加速度**，以建模动态物体的运动。
- **单次前向生成**：通过一个编码器-解码器网络，结合**DINOv2视觉特征**以增强语义理解，在单次前向传播中同时预测静态高斯参数和采样自潜在变量的动态运动参数，无需迭代去噪或后续的扩散模型处理。
- **可控渲染**：利用3D高斯溅射（3D Gaussian Splatting）技术，可根据用户指定的任意相机轨迹，高效、微分地渲染出未来时间点的视频帧，确保了**3D一致性与时间连贯性**。

**3. 主要实验结果**
在KITTI、Waymo、RealEstate10K和DL3DV-10K四个真实世界数据集上的实验表明：
- **性能领先**：在PSNR、LPIPS、SSIM和FVD等多项指标上均超越CameraCtrl、MotionCtrl等基线方法，实现了**最优的视频质量和时间一致性**。
- **高效推理**：由于避免了逐帧扩散，在KITTI数据集上实现了**最快的推理速度（5.9秒）**。
- **消融实验验证**：验证了使用**多个高斯/像素、引入DINOv2特征以及生成式（而非确定性）速度预测**对提升视频质量和动态建模效果的关键作用。

**4. 研究意义和价值**
本研究的意义在于：
- **技术价值**：首次将动态3D高斯表示与单图像视频生成结合，提供了一种**高效、可控且3D一致**的视频预测新范式，解决了现有方法在动态场景与相机控制协同上的难题。
- **应用价值**：为自动驾驶模拟、影视预可视化、AR/VR内容生成等需要**从静态图像生成可控动态场景**的领域提供了强大的工具，显著提升了生成内容的真实感和实用性。
- **启发性**：证明了显式4D场景表示与高效前馈网络结合的可行性，为后续视频生成研究开辟了新的方向。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Pixel-to-4D

### **一、 拟解决的核心问题**
论文旨在解决**单张图像生成可控视频**任务中存在的三个关键挑战：
1.  **用户控制性弱**：现有方法难以根据用户指定的、**任意**的相机轨迹生成视频，限制了在AR/VR、自动驾驶仿真等实际应用中的实用性。
2.  **时间一致性差**：生成的视频帧之间容易出现闪烁、抖动等不一致现象，尤其是在物体运动和相机移动时。
3.  **几何完整性差**：在动态场景和复杂相机运动下，难以保持场景的3D几何结构一致性，导致物体变形或深度信息错误。

现有方法（如基于点云+视频扩散模型的两阶段方法）在控制性、一致性和效率之间难以兼顾。

### **二、 核心创新点**
论文提出了一个名为 **Pixel-to-4D** 的端到端框架，其创新性主要体现在以下三个方面：

#### **1. 新颖的4D场景表示**
- **基础**：基于**3D高斯泼溅（3D Gaussian Splatting, 3DGS）**，这是一种高效、高质量的显式3D表示方法。
- **动态化扩展**：为每个高斯“泼溅”引入了**动态属性**，包括：
    - **线性速度与加速度** (`v`, `a`)
    - **角速度与角加速度** (`ω`, `α`)
- **层级化建模**：通过实例分割掩码，将属于同一动态物体（如汽车）的所有高斯的运动参数进行聚合与平均，确保物体作为一个整体进行**刚体运动**，极大增强了运动的物理合理性与时间一致性。
- **优势**：这种表示能自然地对**遮挡与反遮挡**进行建模，并支持从任意视角和任意未来时间点进行高效、可微的渲染。

#### **2. 高效的单次前馈生成架构**
- **“一步到位”**：仅需**单次前向传播**，即可从单张输入图像同时预测出**静态3D高斯参数**（位置、颜色、透明度等）和**动态运动参数**。
- **概率化运动建模**：由于从单帧预测未来运动具有不确定性，模型采用了一个**条件变分自编码器（CVAE）** 结构。它从高斯分布中采样潜在变量，解码生成多样的、合理的物体运动，而非单一的确定性预测。
- **特征融合**：创新性地融合了**DINOv2视觉基础模型**的特征。DINOv2提供的丰富语义信息，帮助模型更好地理解场景结构，从而提升深度估计、偏移预测和动态物体外观的生成质量。
- **效率对比**：摒弃了需要迭代去噪的**视频扩散模型**来注入物体动态，因此推理速度显著快于现有基线方法。

#### **3. 针对真实世界复杂场景的设计**
- **场景复杂度**：方法专门针对包含**多个动态物体**和**复杂相机运动**（如前进、转弯）的大规模城市场景（如KITTI, Waymo数据集），而非此前多数工作关注的单一物体或轨道相机路径的简单场景。
- **训练与损失**：采用端到端训练，损失函数结合了：
    - **RGB重建损失**（LPIPS, L1）：保证视觉质量。
    - **深度重建损失**：保证几何准确性。
    - **RGB差分损失**：直接优化相邻帧间的变化，增强时间平滑性。
    - **KL散度损失**：规范CVAE的潜在空间。

### **三、 解决方案总结**
**Pixel-to-4D** 的解决方案可以概括为：**“一个显式的4D动态高斯表示 + 一个高效的概率化单次预测网络”**。

1.  **输入**：单张RGB图像 + 用户指定的未来相机轨迹。
2.  **处理**：
    - 网络预测出每个像素对应的多个高斯的静态参数和初始动态参数。
    - 利用实例分割信息聚合物体级运动参数。
    - 根据物理运动方程（公式2，3），将高斯“泼溅”推进到未来任意时间点 `t+δt`，形成4D场景。
3.  **输出**：使用3DGS的高效渲染器，根据用户指定的相机姿态 `δπ`，渲染出未来帧的RGB图像和深度图。

### **四、 实际价值与效果**
- **技术价值**：在**可控性**、**一致性**和**效率**之间取得了突破性平衡，为单图推理动态3D世界提供了新范式。
- **应用价值**：可广泛应用于需要可控视频生成的领域，如：
    - **自动驾驶仿真**：从真实图像生成多样的、可控的未来交通场景。
    - **内容创作**：为游戏、电影快速生成符合导演意图的动态镜头。
    - **机器人规划**：预测环境动态，进行更安全的路径规划。
- **实验效果**：在KITTI、Waymo等多个真实数据集上，在**PSNR、SSIM、LPIPS、FVD**等关键指标上均达到SOTA，同时**推理时间最短**，验证了其优越性。

```mermaid
graph TD
    A[输入: 单张图像 + 目标相机路径] --> B[Pixel-to-4D 网络];
    B --> C[生成 4D 动态高斯场景表示];
    C --> D[基于物理方程将高斯推进到未来时间];
    D --> E[3DGS 渲染器];
    E --> F[输出: 时间一致 & 相机可控的未来视频帧];
    
    subgraph “核心创新”
        C1[创新1: 带运动参数的4D高斯表示] --> C;
        C2[创新2: 单次前馈概率生成网络] --> B;
        C3[创新3: DINOv2特征融合] --> B;
    end
```

**结论**：这篇论文的核心贡献在于提出了一种**高效、可控且物理合理的4D场景生成方法**，通过将动态属性嵌入到显式的高斯表示中，并设计端到端的网络进行一次性预测，成功解决了复杂真实场景下单图生成可控视频的诸多难题。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决**单张图像生成可控视频**任务中，现有方法普遍存在的**相机运动建模不准确、时间一致性差、几何完整性不足**等核心问题。为此，作者提出了 **Pixel-to-4D** 框架，其核心创新在于**从单张图像直接、前向地预测一个动态的4D高斯场景表示**。该方法将3D高斯泼溅（3DGS）扩展为4D，为每个高斯点赋予线性和角速度/加速度，以建模物体运动，并结合实例分割确保动态物体运动的一致性。通过融合DINOv2特征和变分编码器来捕捉运动的不确定性，该框架能够根据用户指定的任意相机轨迹，高效地渲染出未来帧。

实验表明，该方法在KITTI、Waymo等多个真实世界数据集上，在视频质量（PSNR、SSIM、LPIPS、FVD）和推理效率上均超越了现有基线模型，实现了**高保真、时间一致且相机路径精确可控**的视频生成，同时避免了迭代去噪或复杂的两阶段流程带来的问题。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Pixel-to-4D》的创新点分析

这篇论文提出了一种名为Pixel-to-4D的新框架，用于从单张图像生成可控摄像机的视频。其核心创新在于构建了一个**动态的4D高斯场景表示**，并实现了**高效的单次前向传播生成**。以下是其相对于已有工作的明确创新点：

---

### 1. **创新的4D动态高斯场景表示**
   - **改进/不同之处**：
     - **以往方法**：多数基于3D表示的方法（如点云、神经辐射场）要么是静态的，要么通过后续步骤（如视频扩散模型）注入动态，导致**两步流程**（先重建3D，后添加运动）。这些方法常面临**时间不一致**、点云稀疏导致质量下降、或仅限于单物体/简单场景的问题。
     - **本文方法**：提出了一种**4D高斯表示**，每个高斯体不仅包含静态参数（位置、颜色、透明度等），还直接赋予了**3D线性速度与加速度**以及**角速度与角加速度**。这些动态参数在对象分割掩码的指导下进行聚合，确保同一物体的高斯体运动一致。
   - **解决的问题/优势**：
     - **解决了时间一致性与运动连贯性问题**：通过显式建模物理合理的运动（速度/加速度），直接在一个统一的4D表示中捕获动态，避免了后续“注入”运动带来的不一致性。
     - **提升了渲染质量与效率**：3D高斯泼溅（3DGS）本身能自适应填充点云间的空隙，相比稀疏点云，能生成更高质量、更密集的渲染结果。结合动态参数，可直接渲染出未来任意时刻的帧。
     - **支持复杂、多物体的真实场景**：不同于许多局限于单物体或轨道摄像机的图像到4D方法，本方法针对具有多个动态物体（如城市驾驶场景）的大规模场景设计。

### 2. **高效的单次前向传播生成架构**
   - **改进/不同之处**：
     - **以往方法**：许多先进方法依赖于**迭代去噪**（如视频扩散模型）来生成动态内容，或需要**测试时优化**（如基于SDS的方法），导致**推理速度慢**、计算成本高。一些方法采用两阶段流程（3D重建+运动添加），也增加了复杂性和延迟。
     - **本文方法**：设计了一个**端到端的神经网络**，仅通过**单次前向传播**，即可从单张输入图像同时预测出静态高斯参数和不确定的未来运动（通过变分自编码器采样）。然后直接通过可微的高斯泼溅渲染器，根据用户指定的摄像机轨迹生成视频帧。
   - **解决的问题/优势**：
     - **大幅提升推理效率**：避免了耗时的迭代扩散过程或测试时优化。论文数据显示，在KITTI数据集上，本方法推理时间（5.9秒）显著低于对比基线（9.8-17.5秒）。
     - **实现了“相机控制视频生成”的实时性应用潜力**：快速的单次推理使得在需要实时或交互式预测的应用中（如自动驾驶模拟、内容创作）更具实用性。
     - **保持了高质量输出**：尽管推理快，但在PSNR、LPIPS、SSIM、FVD等多个指标上均达到了最先进水平，证明了效率与质量的可兼得。

### 3. **融合大规模预训练视觉特征（DINOv2）**
   - **改进/不同之处**：
     - **以往方法**：大多数基于单图像到3D/4D的方法主要依赖图像编码器从零学习几何和外观特征，或使用特定任务的深度估计器，可能**缺乏高层语义理解**，导致在复杂场景、特别是动态物体外观和几何细节的预测上不够鲁棒。
     - **本文方法**：创新性地将**DINOv2**（一种在大规模数据集上自监督训练的视觉Transformer）提取的语义特征，与专有编码器提取的几何特征（来自RGB图像和估计的深度图）进行**融合**。这些融合特征用于指导静态和动态高斯参数的预测。
   - **解决的问题/优势**：
     - **增强了语义感知与细节保持**：DINOv2提供的丰富语义先验，帮助模型更好地理解场景结构、物体类别和相互关系。消融实验表明，加入DINOv2特征能显著改善动态物体（如车辆）的外观锐度和运动合理性（见图4）。
     - **提升了深度估计与偏移预测的准确性**：语义信息有助于更精确地修正单目深度估计的误差，并优化高斯体在XY平面上的偏移，从而得到更准确的3D布局。
     - **改善了跨数据集的泛化能力**：利用大规模预训练知识，使模型能更好地适应不同、多样的真实世界场景（如KITTI、Waymo、RealEstate10K、DL3DV-10K）。

### 4. **处理运动不确定性的概率化建模**
   - **改进/不同之处**：
     - **以往方法**：许多预测动态的方法采用**确定性回归**来预测运动参数，这忽略了从单帧推断未来运动所固有的**不确定性**，可能导致不自然或平均化的运动效果。
     - **本文方法**：引入了一个**条件变分自编码器（CVAE）模块**。在训练时，它学习从未来帧编码一个潜在分布；在推理时，从该分布中采样潜在变量，用以解码出多样的、合理的速度/加速度。这实现了对**未来运动的多模态预测**。
   - **解决的问题/优势**：
     - **生成更真实、多样的运动**：承认并建模了不确定性，使得生成的物体运动不再是单一确定的，而是符合物理规律和场景上下文的一组可能运动。消融实验显示，其效果优于确定性速度预测版本。
     - **提高了结果的视觉真实感**：概率化采样能产生更生动、更少模糊或平均化伪影的动态内容，这在评估指标（如FVD）和定性结果中均有体现。

---

**总结**：Pixel-to-4D的核心创新在于**将高效的3D高斯表示成功扩展至动态4D领域**，并**通过精心设计的单次前向网络架构实现了高质量、可控、快速的视频生成**。它主要解决了现有方法在**时间一致性、用户摄像机控制精确度、渲染效率、以及对复杂多动态物体场景的适应性**等方面的关键局限。其融合语义先验和概率化运动建模的策略，进一步提升了生成结果的真实性和鲁棒性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 实验目标与核心结论
论文旨在验证 **Pixel-to-4D** 框架在**单图像、相机可控的视频生成**任务上的有效性。核心结论是：该方法在**视频质量（视觉保真度、时间一致性）和推理效率**上均达到了**最先进水平**，能够根据用户指定的相机轨迹，生成具有自然物体动态和几何一致性的未来帧视频。

### 二、 使用的数据集
为了全面评估模型在复杂、真实世界场景下的泛化能力，论文在四个具有不同特点的数据集上进行了实验：

| 数据集 | 场景类型 | 特点 | 用途 |
| :--- | :--- | :--- | :--- |
| **KITTI** | 城市驾驶场景 | 包含快速前向运动的相机，动态物体（车辆、行人） | 训练与评估 |
| **Waymo Open** | 城市驾驶场景 | 多动态物体，序列长，运动模式复杂 | 训练与评估 |
| **RealEstate10K** | 室内外住宅场景 | 相机运动平缓，场景结构多样 | 训练与评估 |
| **DL3DV-10K** | 多样化真实世界环境 | 复杂的相机轨迹和运动模式 | 训练与评估 |

### 三、 评价指标
论文使用了**像素级、感知级和分布级**的多种指标进行综合评估：

1.  **图像质量指标**（用于评估生成帧与真实帧的相似度）：
    *   **PSNR (峰值信噪比) ↑**： 衡量像素级重建精度。
    *   **SSIM (结构相似性指数) ↑**： 评估图像结构信息的保持度。
    *   **LPIPS (学习感知图像块相似度) ↓**： 基于深度网络特征，衡量感知相似度，值越低越好。

2.  **视频质量指标**：
    *   **FVD (Fréchet视频距离) ↓**： 衡量生成视频与真实视频在分布层面的相似度，是评估时间一致性和整体真实性的关键指标。

3.  **几何质量指标**：
    *   **Depth Error (深度误差) ↓**： 平均相对深度误差，用于评估生成的4D表示（动态3D高斯）的几何准确性。

4.  **效率指标**：
    *   **Inference Time (推理时间) ↓**： 生成视频所需的时间（秒），在KITTI数据集上报告。

**注**： 论文明确指出，未使用依赖于静态场景假设的相机运动误差指标（如RotErr, TransErr），因为这些指标在KITTI、Waymo等包含快速前向相机运动和动态物体的数据集上不可靠。

### 四、 对比的基线方法
论文与四种近期先进的**相机引导的图像到视频生成模型**进行了全面对比：

1.  **RealCam-I2V**： 通过单目深度估计生成点云以确保3D一致性，并使用视频扩散模型注入物体动态。
2.  **CamI2V**： 利用对极注意力机制进行相机条件下的视频合成。
3.  **CameraCtrl**： 基于文本到视频扩散模型，使用相机位姿的Plücker嵌入作为条件。
4.  **MotionCtrl**： 将相机位姿信息集成到时序Transformer中，以增强运动控制。

### 五、 关键性能提升与结论
根据论文中的**表1**和**图2**所示的定量与定性结果，Pixel-to-4D方法在所有数据集和绝大多数指标上均**显著优于所有基线方法**。

#### 主要性能提升：
1.  **综合视频质量最优**：
    *   在**Waymo**数据集（多动态物体）上，FVD达到 **30.9**（对比MotionCtrl的43.3），PSNR达到 **19.4**（对比RealCam-I2V的17.0），SSIM达到 **0.553**（对比RealCam-I2V的0.424）。
    *   在包含复杂相机轨迹的**DL3DV-10K**数据集上，FVD为 **36.4**，同样优于所有基线。
    *   低FVD值表明生成视频不仅单帧质量高，而且**时间一致性极佳**，整体分布更接近真实视频。

2.  **推理效率显著提高**：
    *   在KITTI数据集上，Pixel-to-4D的推理时间为 **5.9秒**，远快于CamI2V (17.5秒)、MotionCtrl (11.8秒) 和 CameraCtrl (12.5秒)，与RealCam-I2V (9.8秒) 相比也有明显优势。
    *   这得益于其**单次前向传播**生成4D表示并直接渲染的流程，避免了基线方法中耗时的迭代去噪（视频扩散）过程。

3.  **几何一致性良好**：
    *   论文报告的深度误差表明，模型能够生成几何合理的动态场景，支撑了其渲染多视角一致视频的能力。

#### 消融实验结论：
论文通过系统的消融研究（表2、表3，图3、图4）验证了其核心设计选择的有效性：
*   **动态建模的必要性**： 包含速度/加速度预测的模型（“Ours”）在各项指标上均优于无动态的3D版本（“w/o velocities”），证明了建模物体运动对视频预测至关重要。
*   **生成式速度预测的优势**： 使用VAE对不确定的未来运动进行**生成式采样**（“gen. velocities”），在视觉质量上优于确定性回归（“det. velocities”），能产生更清晰、更真实的物体运动。
*   **多高斯表示的重要性**： 每个像素预测**5个高斯**（“Ours”）相比仅预测1个（“1-Gaussian”），能有效填充场景空隙，在所有评估指标上表现更优，避免了远处物体的模糊和伪影。
*   **DINOv2特征融合的价值**： 融入**DINOv2预训练特征**增强了模型的语义理解能力，特别是在改善动态物体外观和深度估计方面，带来了FVD、LPIPS和SSIM指标的提升。

**总结**： Pixel-to-4D通过其新颖的、可一次性预测的**动态3D高斯4D表示**，在保持高推理效率的同时，实现了在复杂真实世界场景下，**相机轨迹控制精准、时间一致性强、视觉质量高的视频生成**，综合性能超越了现有的主流方法。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.00678v1)
- [HTML 版本](https://arxiv.org/html/2601.00678v1)
