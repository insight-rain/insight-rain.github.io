# Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving

**相关性评分**: 7.0/10

**排名**: #12


---


## 基本信息

- **arXiv ID**: [2601.12142v1](https://arxiv.org/abs/2601.12142v1)
- **发布时间**: 2026-01-17T19:12:34Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao, Peng Lu, Zufeng Zhang, Sifa Zheng

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

这篇论文提出EchoVLA，一种基于音频指令的用户感知VLA模型，通过情感语音-轨迹对微调多模态大模型，提升自动驾驶的语义理解和适应性驾驶行为。

## 摘要

Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\%$ and the collision rate by $74.4\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user's speech.

## 详细分析

## 论文摘要

**论文标题**：Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving

**核心内容**：本文提出了一种名为 **EchoVLA** 的用户感知型视觉-语言-动作模型，旨在通过整合实时音频指令，使自动驾驶系统能够理解并响应用户的即时意图和情感状态，从而实现更安全、更人性化的驾驶行为。

---

### 1. 研究背景和动机
当前基于视觉-语言模型（VLM）或视觉-语言-动作模型（VLA）的自动驾驶系统，通常将语言指令视为**静态的先验信息**，在推理时固定不变。这导致系统必须仅从视觉像素中持续推断不断变化的目标，往往产生**延迟或过于保守的驾驶动作**。因此，研究团队认为，有效的自动驾驶VLA需要一个**在线交互通道**，允许用户通过具体意图实时影响驾驶决策。

### 2. 核心方法和技术创新
本文的核心贡献是提出了 **EchoVLA** 框架，其技术创新主要体现在以下三个方面：
- **多模态指令数据集构建**：基于 nuScenes 数据集，将自车运动描述转换为**时间同步的合成语音指令**，并进一步通过调整语速、音高等参数，合成了包含**紧急**和**犹豫**等不同情感状态的音频-轨迹配对数据。
- **情感感知的轨迹调制**：提出了一种基于**唤醒度**计算的轨迹速度剖面调制方法。通过分析音频特征（如能量、基频、节奏），量化用户情感，并据此动态调整规划轨迹的速度曲线（紧急时加速，犹豫时减速或暂停），而不改变行驶方向。
- **基于多模态大模型的监督微调**：利用构建的**多模态思维链数据集**，对 **Qwen2.5-Omni** 模型进行监督微调。该模型特有的时间对齐多模态旋转位置编码（TMRoPE）和 Thinker-Talker 架构，有效解决了音频与视觉信号之间的**时间对齐难题**，使模型能够根据音频指令和情感进行端到端的推理与轨迹输出。

### 3. 主要实验结果
在 nuScenes 数据集上的开环评估表明，EchoVLA 相比仅依赖视觉感知的基线 VLA 模型（Qwen2-VL-7B）取得了显著提升：
- **轨迹精度**：平均 L2 误差降低了 **59.4%**。
- **安全性**：碰撞率降低了 **74.4%**。
- **情感响应**：定性实验证明，模型能够根据语音中的情感线索（如紧急或犹豫）成功调整驾驶行为（如加速或减速），实现了**情感自适应的驾驶**。

### 4. 研究意义和价值
本研究的意义在于：
- **技术层面**：首次将**实时音频指令与情感分析**深度耦合到自动驾驶 VLA 中，为解决动态意图理解问题提供了新思路。该方法证明了音频信息是对视觉感知的**有效补充而非竞争**。
- **应用价值**：为实现**人车共驾**和**个性化自动驾驶**迈出了关键一步。系统能够理解用户“怎么说”（语义）和“以何种情绪说”（情感），从而做出更符合用户即时期望的驾驶决策，有望提升驾乘安全和体验。
- **局限性**：当前工作依赖于合成语音，未来需要在真实、嘈杂的车内语音指令和多语言场景下进行验证与增强。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：EchoVLA

### **一、 拟解决的核心问题**
当前基于视觉-语言-动作（VLA）模型的自动驾驶系统存在一个关键缺陷：**语言指令在推理时是静态且固定的**。这意味着模型只能从连续的视觉像素流中被动推断驾驶意图，无法实时响应用户动态变化的指令。这导致两个主要问题：
1.  **意图推断延迟或错误**：在存在多种可行驾驶选项的复杂场景中（例如，前方多车道可选），模型可能无法准确、及时地推断出用户当下的真实意图。
2.  **驾驶行为保守**：为避免错误，模型倾向于采取过于保守的驾驶策略，影响通行效率和用户体验。

简言之，论文旨在解决 **VLA模型缺乏与用户进行实时、动态交互通道** 的问题，以实现更**用户感知**和**意图明确**的自动驾驶。

### **二、 核心创新点**
论文提出了名为 **EchoVLA** 的系统，其创新性主要体现在以下三个层面：

1.  **模态创新：引入在线音频指令通道**
    -   **是什么**：在传统的视觉感知基础上，增加了一个**实时音频指令输入通道**。用户可以通过语音直接、即时地表达驾驶意图（如“请在下个路口右转”、“超过前面那辆车”）。
    -   **为什么创新**：将语言从静态的“先验提示”转变为动态的“在线控制信号”，为VLA模型提供了明确的、可随时间变化的意图来源，从根本上解决了意图模糊性问题。

2.  **数据创新：构建多模态思维链（CoT）数据集**
    -   **是什么**：对nuScenes数据集进行增强，生成了**时序对齐的图像-音频-轨迹**三元组数据。关键步骤包括：
        -   **音频合成**：将车辆自运动描述文本转换为语音指令。
        -   **情感注入**：通过调整语速、音高等参数，合成带有**紧急**或**犹豫**情感的语音，并将情感标签与驾驶行为关联。
        -   **轨迹调制**：根据情感标签，对原始轨迹进行**速度剖面调制**（紧急时加速，犹豫时减速/中途停顿），生成情感适配的轨迹。
        -   **CoT构建**：将上述信息组织成“音频分析 → 情感检测 → 轨迹输出”的思维链格式，用于模型训练。
    -   **为什么创新**：不仅让模型学习“听什么”（语义），还学习“怎么听”（情感），实现了**语义与情感的双重理解**，并能输出与之匹配的、更细腻的驾驶行为。

3.  **方法创新：基于原生多模态大模型（MLM）的端到端微调**
    -   **是什么**：选用 **Qwen2.5-Omni** 作为基础模型进行监督微调。该模型本身具备处理交错多模态序列（如图像帧和音频帧）的能力，其 **Time-aligned Multimodal RoPE (TMRoPE)** 机制保证了跨模态的时序对齐。
    -   **为什么创新**：避免了复杂的、手工设计的跨模态融合模块（如论文在消融实验中对比的HuBERT、VQ-VAE等额外编码器）。直接利用MLM的原生多模态理解与生成能力，通过CoT数据微调，**优雅地实现了“听、看、思考、驾驶”的端到端映射**。

### **三、 解决方案总结**
论文通过 **“数据构建”** 与 **“模型微调”** 相结合的方式，系统性地解决了问题：

1.  **问题定义**：识别出现有VLA缺乏实时用户交互通道的瓶颈。
2.  **方案设计**：提出增加在线音频指令作为解决方案。
3.  **数据工程**：为支持该方案，创新性地构建了包含情感语义的多模态CoT数据集，为模型提供学习素材。
4.  **模型实现**：采用具备原生多模态对齐能力的先进MLM（Qwen2.5-Omni），利用上述数据集进行微调，得到最终的EchoVLA模型。
5.  **验证效果**：实验表明，EchoVLA在nuScenes开环测试中，相比纯视觉感知的基线VLA，**平均L2误差降低59.4%，碰撞率降低74.4%**，并能成功根据语音情感调整驾驶速度剖面，证明了其有效性和实用性。

**核心价值**：EchoVLA为自动驾驶系统提供了一种**自然、实时、富情感**的人机交互范式，使自动驾驶从“被动环境适应”向“主动用户协同”迈出了关键一步，显著提升了系统的意图理解准确性、行为合理性和用户体验。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决现有视觉语言动作（VLA）模型在自动驾驶中无法实时响应用户动态意图的问题，其核心在于语言指令在推理时是静态的，导致模型仅依赖视觉感知可能产生延迟或过于保守的驾驶行为。为此，论文提出了 **EchoVLA** 框架，通过引入**在线音频指令通道**，将摄像头视频流与用户实时的语音命令（包括语义内容和情感语调）进行耦合。该方法的关键创新在于构建了一个**多模态思维链（CoT）数据集**：通过对nuScenes数据集进行增强，将自车运动描述合成为同步的语音指令，并通过调整语速、音高等参数注入“紧急”或“犹豫”等情感，进而生成与之对应的、经过速度调制的轨迹数据。基于此数据集，论文对 **Qwen2.5-Omni** 多模态大模型进行监督微调，使其能够根据视觉场景和音频输入，推理用户意图与情感，并输出用户感知的驾驶轨迹。实验结果表明，该方法在nuScenes开环基准测试中，相比仅使用视觉感知的基线模型，**平均L2误差降低了59.4%，碰撞率降低了74.4%**，有效证明了音频指令在补充视觉感知、实现更及时和符合用户意图的驾驶行为方面的价值。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving》提出了一个名为 **EchoVLA** 的用户感知视觉-语言-动作模型。其核心创新在于将**实时音频指令**与视觉感知耦合，使自动驾驶系统能够理解并响应用户的**具体意图**和**情感状态**。以下是其相对于已有工作的明确创新点：

---

### 1. **引入实时、动态的音频指令通道，实现用户意图的在线注入**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：大多数现有的视觉-语言-动作模型将语言（如文本提示）视为**静态先验**，在推理时固定不变。模型只能从像素中持续推断可能变化的目标，导致动作延迟或过于保守。
    - **本文方法**：提出了一个**在线音频通道**，允许用户在推理过程中通过语音实时输入指令（如“左转”、“加速”），动态影响驾驶决策。
- **解决的具体问题/带来的优势**：
    - **解决了意图推断的延迟和歧义问题**。在驾驶场景存在多种可行选择时（如图5所示），纯视觉模型可能无法捕捉用户即时、具体的意图。音频指令提供了明确的语义指导，使轨迹规划能准确遵循用户命令。
    - **实现了真正的人机协同与交互**。为自动驾驶提供了一个自然、直观的人机交互接口，使系统不再是“黑盒”，用户可以在必要时进行干预和引导。

### 2. **提出情感感知的驾驶行为调制，超越语义理解**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：现有的多模态模型主要关注指令的**语义内容**理解，忽略了语音中蕴含的**副语言特征**（如语调、音高、语速）所传达的情感状态。
    - **本文方法**：创新性地将语音的**情感分析**（通过计算“唤醒度”Arousal）与轨迹规划相结合。根据检测到的“紧急”或“犹豫”情绪，动态调制轨迹的**速度曲线**（如图3、7所示）。
- **解决的具体问题/带来的优势**：
    - **实现了更细腻、人性化的驾驶风格适配**。系统不仅能听懂用户“说什么”，还能感知用户“怎么说”，从而调整驾驶行为的激进或保守程度（如紧急时提速，犹豫时减速或中途停顿）。
    - **提升了系统的用户感知能力和乘坐体验**。使自动驾驶行为更能贴合乘客的即时心理状态，可能提高舒适度和信任感。

### 3. **构建了一个多模态思维链数据集与训练框架**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：自动驾驶数据集的构建多依赖于大量人工标注，且模态（视觉、语言）通常是解耦的。训练VLA时，语言-动作的对齐设计往往需要大量手工工程。
    - **本文方法**：
        1. **自动化数据合成**：基于nuScenes数据集，将自车运动描述转化为结构化的自然语言，再通过TTS模型生成同步的音频指令。
        2. **情感-轨迹对扩展**：通过调整合成音频的语速和音高，生成带有不同情感标签（紧急/犹豫）的音频，并据此对原始轨迹进行速度调制，生成匹配的“情感-轨迹”对。
        3. **多模态思维链**：将图像、音频、情感分析、轨迹点以思维链的形式组织，用于监督微调（如图2所示）。
- **解决的具体问题/带来的优势**：
    - **降低了数据构建的成本和复杂性**。利用现有轨迹数据自动化生成对齐的多模态数据，避免了海量真实人声指令的采集和标注。
    - **提供了更丰富的训练信号**。思维链数据迫使模型学习从感知（听、看）到推理（情感分析、意图理解）再到行动（轨迹生成）的完整因果链条，提升了模型的可解释性和决策合理性。

### 4. **利用前沿多模态大模型架构实现高效的跨模态时序对齐**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：将音频嵌入（如通过HuBERT、VQ-VAE、STFT）作为额外令牌输入VLM时，面临音频与图像帧之间**时序分辨率不匹配**的挑战，对齐效果不佳（消融实验表II证实了这一点）。
    - **本文方法**：采用 **Qwen2.5-Omni** 作为基础模型。其**时间对齐的多模态RoPE** 技术能有效交织音频帧和图像帧，保证跨模态的时序对齐。其“思考者-讲述者”架构将高层理解与自回归解码分离，适合处理长序列音频。
- **解决的具体问题/带来的优势**：
    - **解决了音频-视觉流之间的固有时间不对称性问题**。确保了用户指令与视觉场景在时间维度上的精确关联，这对于理解“现在加速”等即时命令至关重要。
    - **实现了更高效的音频-视觉融合**。如表II所示，该方法在平均L2误差和碰撞率上均优于其他音频嵌入方法，证明了其融合方式的有效性。

---

### **总结与核心价值**
本文的核心创新在于将自动驾驶VLA从**静态的、仅基于视觉推断**的系统，转变为**动态的、多模态交互**的“用户感知”系统。它通过**实时音频指令**解决了意图传递的延迟问题，通过**情感分析**实现了驾驶风格的个性化调制，并通过一套**创新的数据合成与模型训练方法**实现了这些能力。最终，在nuScenes开环评测中，相比纯视觉感知基线，**平均L2误差降低59.4%，碰撞率降低74.4%**，显著提升了规划的安全性和准确性。这项工作为构建更安全、更智能、更具交互性的下一代自动驾驶系统开辟了新方向。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果分析

### 数据集与评价指标
- **数据集**：使用 **nuScenes** 数据集进行训练和评估。该数据集包含1000个驾驶场景，提供多传感器（摄像头、激光雷达、雷达）同步数据。
- **评价指标**：
  - **平均L2误差（米）**：预测轨迹与真实轨迹各路径点之间的欧氏距离平均值，衡量轨迹规划的准确性。
  - **碰撞率（百分比）**：沿预测轨迹放置自车边界框，检查与环境中其他物体边界框的重叠比例，评估安全性。
  - **时间范围**：评估1秒、2秒、3秒时间窗口下的性能。

### 基线方法对比
论文将提出的 **EchoVLA** 模型与以下 **仅视觉感知的VLA基线方法** 进行对比：
1. **Qwen2-VL-7B**：基于更大规模（7B参数）的视觉语言模型，仅使用视觉输入。
2. **VLP-UniAD** 与 **RDA-Driver**：其他先进的仅视觉感知VLA方法。

### 关键性能提升与结论
#### 1. **整体性能显著提升**
在nuScenes验证集上，EchoVLA相比 **仅视觉感知的基线（Qwen2-VL-7B）** 实现了：
- **平均L2误差降低59.4%**（从1.43米降至0.58米）。
- **平均碰撞率降低74.4%**（从0.43%降至0.11%）。

#### 2. **详细指标对比（见表I）**
| 感知模态 | 模型 | 平均L2误差 (m) ↓ | 平均碰撞率 (%) ↓ |
|----------|------|-------------------|-------------------|
| 仅视觉 | Qwen2-VL-7B | 1.43 | 0.43 |
| 仅视觉 | VLP-UniAD | 0.74 | 0.16 |
| 仅视觉 | RDA-Driver | 0.80 | 0.32 |
| **视觉+音频** | **EchoVLA (Ours)** | **0.58** | **0.11** |

- **结论**：引入音频指令后，EchoVLA在轨迹精度（L2误差）和安全性（碰撞率）上均显著优于所有仅视觉基线，甚至超越了参数规模更大的模型。

#### 3. **情感识别与行为调制验证**
- **定性实验**（图7）显示，模型能根据音频中的情感（紧急/犹豫）调整速度曲线：
  - **紧急情感**：生成更快的速度曲线。
  - **犹豫情感**：生成更慢的速度曲线，并在轨迹中点附近添加明显的减速。
- **结论**：EchoVLA不仅能理解音频的语义内容，还能捕捉情感线索，实现 **情感自适应驾驶行为**。

#### 4. **消融实验（音频编码方法对比）**
在nuScenes mini集上对比不同音频编码方法：
| 音频编码方法 | 平均L2误差 (m) | 平均碰撞率 (%) |
|--------------|----------------|----------------|
| HuBERT | 0.45 | 0.33 |
| VQ-VAE | 0.48 | 0.32 |
| STFT | 0.60 | 0.46 |
| **Ours (Qwen2.5-Omni原生编码)** | **0.38** | **0.24** |

- **结论**：直接使用Qwen2.5-Omni的 **原生多模态编码与CoT推理机制** 效果最佳，证明了其 **时间对齐多模态融合（TMRoPE）** 的有效性。

### 总结
论文通过系统的实验证明：
1. **音频指令的引入是视觉感知的有效补充**，能显著提升轨迹规划的准确性和安全性。
2. **情感感知能力** 使模型能根据用户状态（紧急/犹豫）动态调整驾驶行为。
3. **所提出的多模态CoT微调方法** 在音频-视觉对齐和推理上优于传统音频编码方案。

**局限性**：当前实验基于合成音频，未在真实嘈杂车载环境下测试；模型为离线非流式处理，存在延迟问题。未来工作将聚焦于真实语音数据收集和流式推理优化。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.12142v1)
- [HTML 版本](https://arxiv.org/html/2601.12142v1)
