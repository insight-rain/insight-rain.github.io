# STEP3-VL-10B Technical Report

**相关性评分**: 8.0/10

**排名**: #3


---


## 基本信息

- **arXiv ID**: [2601.09668v2](https://arxiv.org/abs/2601.09668v2)
- **发布时间**: 2026-01-14T17:58:24Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Ailin Huang, Chengyuan Yao, Chunrui Han, Fanqi Wan, Hangyu Guo, Haoran Lv, Hongyu Zhou, Jia Wang, Jian Zhou, Jianjian Sun, Jingcheng Hu, Kangheng Lin, Liang Zhao, Mitt Huang, Song Yuan, Wenwen Qu, Xiangfeng Wang, Yanlin Lai, Yingxiu Zhao, Yinmin Zhang, Yukang Shi, Yuyang Chen, Zejia Weng, Ziyang Meng, Ang Li, Aobo Kong, Bo Dong, Changyi Wan, David Wang, Di Qi, Dingming Li, En Yu, Guopeng Li, Haiquan Yin, Han Zhou, Hanshan Zhang, Haolong Yan, Hebin Zhou, Hongbo Peng, Jiaran Zhang, Jiashu Lv, Jiayi Fu, Jie Cheng, Jie Zhou, Jisheng Yin, Jingjing Xie, Jingwei Wu, Jun Zhang, Junfeng Liu, Kaijun Tan, Kaiwen Yan, Liangyu Chen, Lina Chen, Mingliang Li, Qian Zhao, Quan Sun, Shaoliang Pang, Shengjie Fan, Shijie Shang, Siyuan Zhang, Tianhao You, Wei Ji, Wuxun Xie, Xiaobo Yang, Xiaojie Hou, Xiaoran Jiao, Xiaoxiao Ren, Xiangwen Kong, Xin Huang, Xin Wu, Xing Chen, Xinran Wang, Xuelin Zhang, Yana Wei, Yang Li, Yanming Xu, Yeqing Shen, Yuang Peng, Yue Peng, Yu Zhou, Yusheng Li, Yuxiang Yang, Yuyang Zhang, Zhe Xie, Zhewei Huang, Zhenyi Lu, Zhimin Fan, Zihui Cheng, Daxin Jiang, Qi Han, Xiangyu Zhang, Yibo Zhu, Zheng Ge

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

STEP3-VL-10B是一个轻量级开源多模态基础模型，通过高效架构和推理优化，在紧凑尺寸下实现前沿性能，适用于边缘部署和机器人应用。

## 摘要

We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\times$-20$\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.

## 详细分析

## Step3-VL-10B 技术报告详细摘要

### 1. 研究背景和动机
当前多模态大语言模型（MLLMs）的发展主要依赖于模型规模的无限扩展，导致前沿模型（如GPT-5.2、Gemini-3-Pro）计算成本高昂，难以实际部署。而轻量级模型（<10B参数）则通常因能力有限，难以在复杂推理和感知任务上取得突破。本研究旨在**重新定义紧凑效率与前沿多模态智能之间的权衡**，提出一个仅10B参数的轻量级开源基础模型，使其在保持高效的同时，达到甚至超越大10-20倍模型的能力。

### 2. 核心方法和技术创新
**Step3-VL-10B**的成功源于两大核心战略设计：
- **统一、全解冻的预训练策略**：在1.2T多模态token上，采用单阶段、全解冻的训练方式，将**语言对齐的感知编码器（PE-lang）**与**Qwen3-8B解码器**集成，建立了内在的视觉-语言协同。
- **规模化后训练与并行推理**：设计了包含两阶段监督微调（SFT）和**超过1000次迭代的强化学习（RL）**的后训练流程。关键创新在于引入了**并行协同推理（PaCoRe）**，在推理时通过并行探索多种视觉假设并进行合成，有效放大了测试时的计算资源，弥补了与百亿参数模型在感知和推理上的性能差距。

### 3. 主要实验结果
尽管仅有10B参数，**Step3-VL-10B**在广泛的多模态和文本基准测试中表现卓越：
- **多模态能力**：在MMBench上达到92.2%，在MMMU上达到80.11%。在复杂数学推理上，MathVision达到75.95%，AIME2025达到惊人的94.43%。
- **对比优势**：其性能不仅全面超越同规模（7B-10B）的开源模型，更**媲美或超越**了参数量大10-20倍的模型（如GLM-4.6V-106B, Qwen3-VL-235B）以及顶尖闭源模型（如Gemini 2.5 Pro, Seed-1.5-VL）。
- **文本能力**：在保持顶尖多模态性能的同时，其文本推理能力（如数学、代码）未受损害，证明了模型设计的有效性。

### 4. 研究意义和价值
**Step3-VL-10B**证明了通过**精心的架构设计、高质量数据构建和创新的训练范式（如大规模RL和PaCoRe）**，可以在紧凑的模型尺寸内实现前沿的多模态智能。它**为社区提供了一个强大且高效的基线模型**，挑战了“模型能力必须依赖规模”的传统观念。其开源的模型权重和详细训练文档，将推动高效多模态AI的研究与应用，特别是在资源受限的实际部署场景中具有重要价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## Step3-VL-10B 技术报告分析

### 一、核心问题与目标
这篇论文旨在解决**轻量级多模态大模型（MLLM）在感知与推理能力上的局限性**。传统上，10B参数以下的模型常被定位为“高效但能力有限”，难以在复杂视觉理解、数学推理等任务上与百亿级甚至千亿级的大模型竞争。Step3-VL-10B 试图**重新定义“紧凑模型”与“前沿性能”之间的权衡**，证明通过精心设计的数据策略、训练方法和推理优化，一个仅10B参数的模型可以达到甚至超越10-20倍于其规模的模型（如GLM-4.6V-106B、Qwen3-VL-235B）以及顶级闭源模型（如Gemini 2.5 Pro、Seed-1.5-VL）的性能。

### 二、核心创新点
#### 1. **统一的、完全解冻的预训练策略**
   - **架构设计**：采用**语言对齐的感知编码器（PE-lang, 1.8B）** 与 **Qwen3-8B解码器** 结合，通过投影层进行16倍空间下采样，并使用多裁剪策略（全局视图+局部裁剪）高效捕获细节。
   - **数据构建**：在1.2T多模态token上训练，覆盖**知识、教育、OCR、定位与计数、VQA、GUI**等多个关键领域，强调数据质量与多样性（如使用概念平衡重采样、马赛克增强等）。
   - **训练方法**：采用**单阶段、完全解冻的训练**，联合优化视觉编码器与语言解码器，建立内在的视觉-语言协同。

#### 2. **规模化后训练与强化学习（RL）**
   - **两阶段监督微调（SFT）**：先以文本为主（9:1比例）建立推理基础，再平衡多模态数据（1:1比例）进行整合。
   - **超过1000次迭代的强化学习**：
     - **RLVR（基于可验证奖励的RL）**：使用GPT-OSS-120B作为答案验证器，提供对格式变化鲁棒、语义等价的奖励信号。
     - **RLHF（基于人类反馈的RL）**：在开放任务上对齐人类偏好。
     - **奖励系统设计**：区分**可验证任务**（基于感知奖励和模型验证）与**不可验证任务**（基于生成奖励模型和行为正则化）。

#### 3. **并行协同推理（PaCoRe）**
   - **测试时计算扩展**：通过并行生成多个视觉假设（16个SeRe rollout），然后进行综合与交叉验证，模拟人类“先提出后细化”的感知过程。
   - **解决“长度缩减”问题**：在感知任务中，RL倾向于减少冗余token，提高确定性；PaCoRe通过显式并行探索，弥补训练数据中缺乏的“认知轨迹”。
   - **性能提升**：在数学推理（如MathVision +5.14%）、空间理解（如All-Angles-Bench +7.50%）等任务上显著优于顺序推理模式。

### 三、解决方案总结
| 关键挑战 | 解决方案 | 技术实现 |
|----------|----------|----------|
| **轻量模型能力有限** | 高质量预训练 + 规模化RL | 1.2T多模态token预训练 + 超过1000次RL迭代 |
| **感知与推理的权衡** | 统一架构 + 并行推理 | PE-lang编码器 + Qwen3-8B解码器 + PaCoRe |
| **训练稳定性与奖励设计** | 分阶段RL + 混合奖励系统 | RLVR（可验证奖励） + RLHF（人类偏好） + 行为正则化 |
| **测试时计算效率** | 动态推理模式切换 | 顺序推理（SeRe）用于一般任务，并行推理（PaCoRe）用于复杂感知/推理 |

### 四、实际价值与性能
- **性能表现**：在60多个基准测试中达到SOTA水平，例如：
  - **MMBench**: 92.2%
  - **MMMU**: 80.11%
  - **AIME2025**: 94.43%
  - **MathVision**: 75.95%
- **效率优势**：以10B参数媲美百亿级模型，为边缘部署、实时应用提供了高性能基础。
- **开源贡献**：完整模型权重与训练细节公开，推动多模态开源生态发展。

### 五、未来方向
论文提出进一步**压缩系统2到系统1**（将显式并行推理内化为直觉式响应），并**弥合现实差距**（通过物理仿真、具身链式思维等实现真实世界 grounding）。

```markdown
**核心结论**：Step3-VL-10B 通过“高质量预训练 + 规模化RL + 并行推理”的三位一体设计，成功打破了轻量模型的能力天花板，为高效多模态AI的实际部署提供了新的技术范式。
```


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇技术报告旨在解决一个核心矛盾：如何在保持模型**轻量化**（约100亿参数）的同时，使其**多模态智能**达到或超越前沿巨型模型（千亿参数级别）的水平。为此，论文提出了一个名为**Step3-VL-10B**的模型，其成功依赖于两个关键的战略性设计：

1.  **统一的预训练策略**：采用**单阶段、全参数解冻**的训练方式，在一个包含1.2万亿多模态令牌的高质量语料库上，联合优化一个语言对齐的感知编码器和一个Qwen3-8B解码器，以建立内在的视觉-语言协同。
2.  **规模化后训练与并行推理**：设计了一个包含超过1000次迭代的强化学习后训练流程。更重要的是，引入了**并行协同推理**范式，在推理时通过并行探索和合成多种视觉假设，将计算资源有效地分配给可扩展的感知推理。

最终，尽管模型尺寸紧凑，**Step3-VL-10B**在广泛的评测中，其性能不仅远超同尺寸模型，而且**媲美甚至超越了参数规模大10-20倍的顶尖开源模型**（如GLM-4.6V-106B, Qwen3-VL-235B）以及部分领先的闭源旗舰模型（如Gemini 2.5 Pro）。它在MMBench上达到92.2%，在MMMU上达到80.11%，并在AIME2025和MathVision等复杂推理任务上分别取得了94.43%和75.95%的优异成绩，证明了通过精巧的设计和训练策略，可以在紧凑的模型尺寸下实现前沿级别的多模态智能。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《STEP3-VL-10B Technical Report》的创新点分析

这篇论文提出并详细阐述了**Step3-VL-10B**模型，这是一个在紧凑模型规模（10B参数）下实现前沿多模态智能的突破性工作。其核心创新点并非单一技术，而是一套**系统性的设计哲学和工程策略**，旨在重新定义效率与性能之间的权衡。以下是其明确的创新点及其价值分析：

---

### 1. **统一的、完全解冻的预训练策略**
- **改进/不同之处**：
    - **统一单阶段训练**：与以往多模态模型常采用“冻结视觉编码器 + 微调”或“多阶段渐进解冻”的策略不同，Step3-VL-10B在1.2T多模态token上，对**整个模型（包括视觉编码器和语言解码器）进行单阶段、完全解冻的端到端预训练**。
    - **语言对齐的感知编码器**：专门选用预训练时已与语言模型对齐的**Perception Encoder (PE-lang)**，而非纯视觉任务优化的编码器（如DINOv3）。
- **解决的问题/带来的优势**：
    - **建立本质的视觉-语言协同**：完全解冻的联合优化迫使视觉和语言表示在早期就深度融合，避免了模态间隙，建立了更强大的跨模态理解基础。
    - **提升数据效率和收敛性**：语言对齐的PE-lang编码器能更快、更有效地桥接视觉特征与文本语义，相比DINOv3在多项下游任务上表现更优，解决了纯视觉编码器在多模态场景下收敛慢的问题。
    - **为后续强化学习奠定高上限**：这种强协同的预训练模型，为后续通过强化学习解锁复杂推理能力提供了一个性能上限更高的“基座模型”。

### 2. **大规模、多阶段的强化学习后训练流水线**
- **改进/不同之处**：
    - **超千次迭代的规模化RL**：实施了超过**1000次迭代**的强化学习，规模远超常规做法。这包括两个主要阶段：**基于可验证奖励的RL**和**基于人类反馈的RL**。
    - **分阶段、分数据类型的奖励系统**：
        - **可验证任务**：使用**模型辅助验证**，利用GPT-OSS-120B作为答案验证器，提供对格式变化鲁棒、能识别语义等价性的奖励信号，替代简单的字符串匹配。
        - **非可验证任务**：采用**生成式奖励模型**，并结合**行为正则化**（如语言一致性惩罚、引用验证惩罚），防止奖励黑客行为，提升安全性和可靠性。
- **解决的问题/带来的优势**：
    - **持续、无饱和的性能提升**：大规模RL驱动模型能力持续线性增长，奖励和下游指标在数百次迭代后仍未饱和，突破了传统微调的性能瓶颈。
    - **解决奖励信号的可靠性与泛化性问题**：模型辅助验证比启发式规则更鲁棒；精细的行为正则化约束了模型在开放生成中的不良倾向，使RL优化更稳定、更对齐人类价值观。
    - **针对性提升不同能力**：RLVR阶段专注于提升数学、科学、感知等**可客观评估的推理能力**；RLHF阶段则在此基础上，针对对话、创意生成等**主观偏好任务**进行对齐，实现能力与安全的平衡。

### 3. **并行协同推理（PaCoRe）作为测试时计算扩展范式**
- **改进/不同之处**：
    - **从序列推理到并行探索**：不同于仅通过延长思维链来扩展测试时计算的“序列推理”，PaCoRe引入**并行计算**。它同时生成多个（如16个）不同的视觉假设或推理路径（“提议”），然后将这些并行结果聚合为一个上下文，让模型进行最终的“合成”与交叉验证。
    - **训练数据构建**：专门为训练PaCoRe能力构建数据，通过“合成过滤”确保任务在并行设置下仍具有挑战性，防止任务被琐碎化。
- **解决的问题/带来的优势**：
    - **有效扩展感知任务的测试时计算**：针对感知任务（如计数、空间理解）中存在的“长度缩减”现象（RL优化后输出变短、更确定），PaCoRe通过并行探索**显式化**了人类视觉认知中“粗略扫描-聚焦验证”的隐含过程，为模型分配了有效的额外计算资源。
    - **显著提升复杂任务性能**：在需要高召回率或多假设验证的任务上（如MathVision, All-Angles-Bench, 计数任务），PaCoRe模式相比标准序列推理模式带来显著性能提升（+5% ~ +7.5%），使10B模型在需要大量计算搜索的任务上媲美甚至超越百亿级模型。
    - **提供一种“系统2”到“系统1”的压缩路径**：PaCoRe作为一种外部化、多智能体式的推理框架，其产生的“思考痕迹”为未来通过自蒸馏技术，将这种昂贵的、慢思考的“系统2”能力内化为高效的、直觉性的“系统1”能力提供了可能。

### 4. **面向感知与推理的高质量、大规模数据构造**
- **改进/不同之处**：
    - **全面且精细的数据域覆盖**：预训练数据不仅包含通用的图文对和交错文档，还系统性地构建了**教育、OCR、定位与计数、GUI交互**等专门领域的大规模高质量数据。
    - **GUI轨迹建模**：在GUI数据中，不仅有关注静态理解的标注（标题、定位），还包含了超过200万条**原子动作轨迹**，用于建模人机交互的动态过程。
    - **数据质量管控**：采用概念平衡重采样、多维度过滤（可检查性、视觉相关性、难度）等策略，确保数据质量和训练稳定性。
- **解决的问题/带来的优势**：
    - **奠定全方位多模态能力的基础**：专门的数据域直接赋能模型在OCR、文档理解、GUI操作、教育解题等**垂直领域达到前沿水平**，摆脱了通用模型在专业任务上的乏力。
    - **实现“感知-行动”闭环**：GUI轨迹数据使模型不仅能“看懂”界面，还能“操作”界面，将视觉感知直接关联到可执行动作，为具身智能和自动化代理提供了强大基础。
    - **支撑RL有效缩放**：高质量、多样化的数据是RL能够持续缩放而不崩溃的前提。严格的数据过滤为RL提供了清晰、可靠的优化信号。

### 5. **模型架构与训练策略的协同设计**
- **改进/不同之处**：
    - **组件选择**：采用**Qwen3-8B**作为语言解码器基座，因其强大的文本生成和可塑性；使用**多裁剪策略**高效处理高分辨率图像，避免可变长度打包的复杂性。
    - **摒弃不必要的复杂性**：论文通过消融实验，**有意识地放弃了**一些在其他工作中显示有效的技术，如**Deepstack**（深度扩展）和**Muon优化器**。原因在于前者带来的收益与计算开销不匹配，后者与AdamW预训练权重的初始化不兼容，影响整体效率。
- **解决的问题/带来的优势**：
    - **在效率与性能间取得最佳平衡**：证明了并非所有先进技术都适用于特定目标（紧凑且高效）。通过实证分析，选择了**最简单且有效**的组合，确保了训练稳定性和最终性能。
    - **强调工程务实性**：这种“做减法”的创新体现了以最终系统性能为导向的工程思维，避免陷入对复杂技术的盲目追求，使整个系统更易于复现和部署。

---

### **总结：核心创新价值**

Step3-VL-10B的核心创新在于**一套完整的、以“高效解锁前沿智能”为目标的系统工程方案**：
1.  **它通过“完全解冻预训练”建立深度模态融合。**
2.  **通过“超大规模RL”持续挖掘模型潜力。**
3.  **通过“PaCoRe”创新性地将测试时计算分配给并行感知探索。**
4.  **通过“高质量专项数据”夯实各项基础能力。**
5.  **通过“务实的架构选择”确保整个流程的高效可行。**

最终，这些创新点共同解决了**紧凑模型在复杂感知与推理能力上长期落后于巨型模型**的根本问题，成功地将一个10B参数模型推至与10-20倍于其规模的顶级开源模型以及Gemini、Seed等闭源旗舰相竞争的水平，为社区提供了一个**强大、高效且可复现的多模态基础模型新基线**。其关于“将计算从预训练转向RL”和“通过物理仿真桥接现实鸿沟”的未来展望，也为下一代多模态模型的发展指明了方向。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过**大规模、多维度**的基准测试，全面评估了Step3-VL-10B的性能。结果表明，该模型在**仅10B参数**的紧凑规模下，实现了**前沿级的多模态智能**，其性能不仅远超同规模模型，甚至能与参数量大10-20倍的巨型模型以及顶级闭源模型相媲美。

### 1. 使用的数据集与评价指标
论文在超过**60个**公开基准上进行了评估，涵盖两大模态：

#### **I. 多模态基准 (9大能力域)**
- **STEM/多模态推理**: MMMU, MathVista, MathVision, MathVerse, DynaMath, We-Math, PhyX, LogicVista, ZeroBench, VisuLogic, HLE。
- **识别/通用VQA**: MMBench (EN/CN), SimpleVQA, MMStar, HallusionBench, MMVP, ReMI, M3GIA, DoYouSeeMe。
- **计数**: CountBench, CountQA, PixMo-Count。
- **指令遵循**: MM-MT-Bench, MIA-Bench, MM-IFEval。
- **多模态代码**: HumanEval-V, Design2Code (含Hard版)。
- **OCR**: OCRBench, OmniOCR, CC-OCR。
- **2D/3D空间理解**: BLINK, CVBench, MMSI-Bench, ERQA, OmniSpatial, All-Angles-Bench, MindCube-tiny, RealWorldQA, SpatialViz-Bench, STARE, CoreCognition, V*, ViewSpatial。
- **文档与图表理解**: CharXiv (RQ), AI2D, CSVQA, OmniDocBench。
- **GUI定位与交互**: ScreenSpot-Pro/V2, OSWorld-G, MMBench-GUI。

#### **II. 文本中心基准 (6大类别)**
- **考试**: MMLU-Pro, GPQA-Diamond, SuperGPQA, LiveBench。
- **数学**: AIME (2024/2025), Beyond-AIME, HMMT25, CNMO 2024, IMO-AnswerBench。
- **代码**: LiveCodeBench。
- **指令遵循**: IFEval, IFBench, MultiChallenge。
- **主观评价**: Arena-Hard-V2, WildBench。
- **医学**: HealthBench。

**核心评价指标**：各基准的标准准确率（如MMBench）、通过率（如HumanEval-V）、归一化编辑距离（如OmniDocBench的NED）等。

### 2. 对比的基线方法
论文进行了两个层面的系统对比：

#### **A. 同规模开源模型 (7B-10B)**
- **GLM-4.6V-Flash (9B)**
- **Qwen3-VL-Thinking (8B)**
- **InternVL-3.5 (8B)**
- **MiMo-VL-RL-2508 (7B)**

#### **B. 更大规模模型与闭源旗舰**
- **超大开源模型**:
    - GLM-4.6V (106B-A12B) – **参数量约10.6倍**
    - Qwen3-VL-Thinking (235B-A22B) – **参数量约23.5倍**
- **顶级闭源模型**:
    - **Gemini-2.5-Pro**
    - **Seed-1.5-VL**

### 3. 关键性能提升与结论

#### **3.1 全面领先同规模模型**
在几乎所有多模态和文本基准上，Step3-VL-10B均取得**最佳或次佳**成绩，确立了10B规模的新SOTA。

- **多模态推理**: 在**MathVision**上达到70.81%，比最强基线（MiMo-VL, 59.65%）高出**超过11个百分点**。
- **视觉识别**: 在**MMBench (EN/CN)** 上分别达到92.05%/91.55%，均为同规模最佳。
- **GUI交互**: 在**ScreenSpot-V2**上达到92.61%，显著领先。
- **文本数学**: 在**AIME2025**上达到87.66%，远超其他同规模模型（最高66.51%）。

#### **3.2 媲美甚至超越超大模型与闭源旗舰**
这是论文最核心的结论：**10B模型达到了百亿乃至千亿参数模型的性能水平**。

- **vs. 超大开源模型 (GLM-4.6V 106B, Qwen3-VL 235B)**:
    - 在**MMMU** (78.11% vs 75.20%)、**MathVision** (70.81% vs 63.50%*) 等关键推理任务上**实现超越**。
    - 在多数感知、OCR、空间理解任务上表现**相当或更优**。

- **vs. 顶级闭源模型 (Gemini-2.5-Pro, Seed-1.5-VL)**:
    - 在启用**并行协同推理 (PaCoRe)** 模式后，Step3-VL-10B在多个高难度任务上**达到或超越**了这些闭源前沿模型的性能。
    - **关键指标举例**:
        - **MathVision**: 75.95% (PaCoRe) **vs** 73.30% (Gemini-2.5-Pro) / 68.70%* (Seed-1.5-VL)
        - **AIME2025**: **94.43%** (PaCoRe) – 这是一个**惊人的成绩**，显著高于Gemini-2.5-Pro (83.59%) 和 Seed-1.5-VL (64.06%)。
        - **HMMT25**: 92.14% (PaCoRe) **vs** 67.71% (Gemini-2.5-Pro) / 51.30% (Seed-1.5-VL)

#### **3.3 并行协同推理 (PaCoRe) 带来显著增益**
论文验证了通过**扩展测试时计算**来提升性能的有效性。相比默认的顺序推理 (SeRe)，PaCoRe模式带来了普遍的性能提升：
- **数学推理**: MathVision (+5.14%), DynaMath (+5.09%)
- **感知任务**: CountQA (+4.6%), OCRBench (+2.25%)
- **空间理解**: All-Angles-Bench (+7.50%), SpatialViz-Bench (+6.52%)

这证明了模型能够有效利用并行探索和合成来攻克复杂任务。

### 4. 核心结论
1.  **效率重定义**: Step3-VL-10B成功**重新定义了紧凑效率与前沿性能之间的权衡**，证明通过精心的架构设计、高质量数据策略和大规模强化学习，小模型也能具备顶尖智能。
2.  **技术路径验证**: 论文的核心技术创新——**统一全解冻预训练**和**超千次迭代的规模化强化学习（含PaCoRe）**——被证明是极其有效的。
3.  **开源标杆**: 该模型为社区提供了一个强大的、高效的基线，表明“紧凑”与“前沿”之间的鸿沟**并非不可逾越**。

**总之**，论文通过详实、全面的定量实验，强有力地支撑了其核心论点：Step3-VL-10B是一个在10B参数规模下实现突破性多模态性能的模型，其评估结果在学术界和工业界都具有很高的参考价值。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.09668v2)
- [HTML 版本](https://arxiv.org/html/2601.09668v2)
