# CLIMP: Contrastive Language-Image Mamba Pretraining

**相关性评分**: 6.0/10

**排名**: #49


---


## 基本信息

- **arXiv ID**: [2601.06891v1](https://arxiv.org/abs/2601.06891v1)
- **发布时间**: 2026-01-11T12:31:55Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Nimrod Shabtay, Itamar Zimerman, Eli Schwartz, Raja Giryes

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

CLIMP是一种基于Mamba的对比视觉语言模型，通过改进架构提高推理效率和轻量化，但与机器人动作模型无直接关联。

## 摘要

Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI's CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP's fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.

## 详细分析

## 论文摘要：CLIMP: Contrastive Language-Image Mamba Pretraining

**1. 研究背景和动机**
传统的对比语言-图像预训练模型（如CLIP）主要基于Vision Transformer（ViT）架构，其自注意力机制存在**二次计算复杂度**、对高分辨率图像处理困难、以及容易受到虚假相关性的影响等问题。同时，其固定的上下文长度（如77个token）也限制了处理长文本描述的能力。为了克服这些限制，本文提出了**CLIMP**，这是首个完全基于**Mamba状态空间模型（SSM）** 的对比视觉-语言模型。

**2. 核心方法和技术创新**
CLIMP的核心创新在于**用Mamba架构完全替换了CLIP中的视觉和文本编码器**：
*   **视觉编码器**：采用**VMamba**，其2D选择性扫描（SS2D）机制能自然地编码空间结构，提供了有利的空间归纳偏置，并且无需位置编码插值即可处理可变输入分辨率。
*   **文本编码器**：采用预训练的**Mamba-1或Mamba-2**语言模型。其自回归特性克服了CLIP的固定上下文限制，支持长文本（密集描述）检索。
*   **整体优势**：该架构在两种模态上都实现了**亚二次方的内存复杂度**，计算效率更高，并自然地支持动态分辨率和长上下文。

**3. 主要实验结果**
在CC12M数据集上训练后，CLIMP在多个基准测试中展现出显著优势：
*   **检索性能**：在31个数据集的零样本检索任务中，CLIMP（Mamba-1）取得了最佳成绩（图像召回率65.5%，文本召回率77.0%）。
*   **分布外（OOD）鲁棒性**：在ImageNet-O等OOD基准上表现突出，**超越OpenAI的CLIP-ViT-B模型7.5%**，尽管训练数据量仅为后者的1/167。
*   **高分辨率处理**：无需微调即可处理高达896×896的输入分辨率，在超高分辨率下的检索准确率比Transformer基线高出最多**18-19%**，同时内存消耗减少5倍，FLOPs降低1.8倍。
*   **密集描述检索**：在文本长度远超77个token的密集描述检索任务（如Flickr8k-R和DOCCI）上，CLIMP consistently优于所有Transformer基线。

**4. 研究意义和价值**
本研究首次系统性地将Mamba架构成功应用于对比视觉-语言预训练框架。结果表明，基于状态空间模型的CLIMP不仅在**检索精度、OOD鲁棒性和计算效率**方面超越了传统的Transformer-based CLIP，还**原生支持可变分辨率和长文本上下文**。这为视觉-语言学习提供了一个有强大竞争力的新架构选择，特别适用于高分辨率、内存受限或需要处理长文本描述的应用场景。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：CLIMP

### **一、 论文拟解决的核心问题**
论文旨在解决传统基于Transformer的CLIP模型存在的三个主要局限性：
1.  **计算效率低**：Vision Transformer的自注意力机制具有**二次方复杂度**（O(L²)），处理高分辨率图像时计算和内存开销巨大。
2.  **鲁棒性不足**：自注意力的成对令牌交互容易学习到**虚假相关性**，导致模型在分布外（OOD）数据上表现脆弱。
3.  **灵活性受限**：
    *   **视觉**：固定分辨率处理。要适应不同输入尺寸，需要复杂的**位置编码插值**（如RoPE）或专门的训练方案（如FlexViT）。
    *   **文本**：受限于固定的上下文长度（如77个令牌），无法有效处理长文本描述（如密集标注）。

### **二、 核心技术创新**
论文提出了 **CLIMP**，这是**首个完全基于Mamba（状态空间模型）的对比视觉-语言预训练模型**。其创新点体现在架构的全面革新：

1.  **双塔Mamba化**：
    *   **视觉编码器**：采用 **VMamba**，利用其**2D选择性扫描**机制捕捉图像的空间结构，提供有益的**空间归纳偏置**。
    *   **文本编码器**：采用**Mamba-1或Mamba-2** 作为自回归语言模型。
    *   **意义**：首次在CLIP框架内实现了视觉与语言编码器均基于SSM，获得了**一致的亚二次方复杂度**优势。

2.  **关键技术机制**：
    *   **选择性状态空间模型**：Mamba的核心是**选择性机制**（使SSM参数依赖于输入），实现了内容感知的线性复杂度推理。
    *   **VMamba的SS2D模块**：通过四个方向的扫描路径，使每个图像块都能集成全局上下文，同时保持线性复杂度，**隐式地处理空间关系**。
    *   **自回归文本编码**：利用Mamba的因果特性，通过**最后非填充令牌池化**来获取文本表示，自然支持**可变长度文本输入**。

### **三、 解决方案与验证**
CLIMP通过上述架构创新，从多个维度验证了其解决方案的有效性：

| 问题维度 | CLIMP的解决方案与验证结果 |
| :--- | :--- |
| **计算效率与灵活性** | **线性复杂度**带来显著优势：<br>- **高分辨率**：在896×896分辨率下，比ViT基线**内存减少5倍**，**FLOPs减少1.8倍**。<br>- **动态分辨率**：**无需位置编码插值或专门训练**，即可处理任意分辨率输入，在16倍训练分辨率下检索准确率最高提升**6.6%**。 |
| **OOD鲁棒性** | **空间归纳偏置减少了对虚假相关的依赖**：<br>- 在**ImageNet-O**上超越OpenAI的CLIP-ViT-B达 **7.5%**（尽管训练数据小167倍）。<br>- 在五个ImageNet变体数据集的平均准确率上取得最佳成绩。 |
| **表示质量与检索** | **更优的嵌入空间几何特性**：<br>- **对齐度更好**：匹配的图像-文本对在嵌入空间中更接近。<br>- **枢纽度更低**：减少了某些嵌入在最近邻列表中占主导的现象。<br>- **结果**：在31个数据集的零样本检索基准上，**图像和文本召回率均超越所有Transformer基线**。 |
| **长文本处理** | **突破77个令牌的限制**：<br>- 在**密集标注检索**任务（Flickr8k-R, DOCCI）上，对平均134-142个令牌的描述，检索性能**显著优于基线**，证明了处理长上下文的能力。 |

### **四、 实际价值与意义**
1.  **高效高分辨率视觉-语言模型**：为需要处理高分辨率图像（如医疗影像、遥感）且计算资源受限的场景提供了可行的解决方案。
2.  **更鲁棒的跨模态对齐**：在分布外数据上更强的泛化能力，使其在开放世界、对抗性环境或数据分布变化的实际应用中更具可靠性。
3.  **支持长文本与密集标注**：拓展了CLIP模型的应用边界，使其能够更好地理解复杂的、描述性的文本，适用于细粒度的图像检索、自动标注等任务。
4.  **为架构探索提供新方向**：证明了SSM在视觉-语言学习领域的潜力，是Transformer之外一个具有竞争力的替代方案，可能推动更高效、更鲁棒的多模态基础模型发展。

**总结**：CLIMP的核心创新在于**用Mamba架构全面取代Transformer**，构建了一个计算高效、天生灵活、且具有更强鲁棒性的对比视觉-语言模型。它不仅在标准基准上表现优异，更在**高分辨率、OOD鲁棒性、长文本处理**等实际挑战性场景中展现出显著优势，具有重要的理论创新价值和实际应用潜力。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决传统基于Transformer的CLIP模型在计算效率（二次复杂度）、处理高分辨率图像（需特殊位置编码或训练）以及抗分布外（OOD）鲁棒性（易受虚假相关性影响）方面的核心局限。为此，论文提出了**CLIMP**，这是首个完全基于Mamba（一种选择性状态空间模型）架构的对比视觉-语言预训练模型，用VMamba和Mamba LLM分别替换了CLIP中的视觉和文本编码器。该方法利用Mamba的线性复杂度、隐式空间归纳偏置和自回归特性，实现了无需调整即可处理可变分辨率图像和长文本。最终，CLIMP在显著更小的数据集上训练，取得了超越基线Transformer模型的效果：在跨模态检索和OOD鲁棒性（如在ImageNet-O上提升7.5%）上表现更优，同时在高分辨率推理时内存和计算开销大幅降低（分别减少5倍和1.8倍），并突破了CLIP的77个token的文本长度限制。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## CLIMP论文创新点分析

这篇论文提出了首个完全基于Mamba架构的对比视觉-语言预训练模型（CLIMP），其核心创新在于用状态空间模型（SSM）完全替代了传统CLIP中的Transformer编码器。以下是其明确的创新点及具体分析：

---

### 1. **架构创新：首个完全基于Mamba的视觉-语言双塔模型**
   - **改进/不同之处**：
     - **以往方法**：所有主流CLIP变体（如OpenCLIP, SigLIP, EVA-CLIP）均使用基于Transformer的视觉编码器（通常是ViT）和文本编码器。
     - **CLIMP**：**同时**将视觉编码器替换为VMamba（基于2D选择性扫描的视觉SSM），将文本编码器替换为Mamba-1或Mamba-2语言模型。这是首个端到端的纯SSM架构视觉-语言模型。
   - **解决的问题/优势**：
     - **统一的高效性**：双塔均具备**次二次方复杂度**（训练时`O(L)`，推理时`O(1)`），克服了Transformer在序列长度`L`上的`O(L²)`计算瓶颈。
     - **架构一致性**：实验表明（见表8），当视觉和文本编码器均采用SSM时，存在**架构协同效应**，能学习到更兼容的跨模态表示，提升了检索性能（如TR@5提升+3.9%）。

### 2. **高分辨率处理的固有灵活性，无需专门设计**
   - **改进/不同之处**：
     - **以往方法**：ViT处理可变分辨率需要复杂的方案：1) 位置编码插值（如RoPE-ViT）；2) 专门的训练策略（如FlexViT的随机patch大小训练）；3) 序列打包（如NaFlex）。这些方法或损害性能，或增加训练复杂度。
     - **CLIMP**：VMamba通过其**2D选择性扫描（SS2D）机制**隐式地处理空间关系。扫描模式本身提供了空间归纳偏置，因此**无需任何位置编码或专门训练**，即可原生支持任意输入分辨率。
   - **解决的问题/优势**：
     - **动态分辨率推理**：在训练分辨率（224×224）上训练的模型，可直接推理更高分辨率（如896×896）的图像，**无需微调**。
     - **性能与效率**：在高分辨率下（896×896），CLIMP的检索性能显著优于所有Transformer基线（IR@5领先+18%以上），同时内存占用降低**5倍**，FLOPs减少**1.8倍**（图3）。解决了ViT在高分辨率下计算开销大、性能下降快的问题。

### 3. **显著提升的分布外（OOD）鲁棒性**
   - **改进/不同之处**：
     - **以往认知**：CLIP的鲁棒性部分依赖于海量训练数据。Transformer的自注意力机制易受虚假相关性的影响。
     - **CLIMP的发现**：即使仅在1200万规模的CC12M数据集上训练，CLIMP在多个OOD基准上（尤其是ImageNet-O）大幅超越了在更大数据集（如LAION-2B，规模大167倍）上训练的CLIP-ViT-B。
   - **解决的问题/优势**：
     - **关键结果**：在ImageNet-O上，CLIMP比最好的Transformer基线（RoPE-ViT）**高出7.5%至9.7%** 的Top-1准确率。这表明**SSM的架构归纳偏置本身对OOD鲁棒性有决定性贡献**，可能减少了对虚假特征的依赖。
     - **实际价值**：为构建更可靠、对分布偏移更不敏感的视觉-语言模型提供了新的架构方向，降低了对数据规模的绝对依赖。

### 4. **突破文本上下文长度限制，支持密集描述检索**
   - **改进/不同之处**：
     - **CLIP的限制**：标准CLIP文本编码器有**77个token的固定上下文窗口**，无法处理长描述。
     - **CLIMP的解决方案**：采用**自回归的Mamba语言模型**作为文本编码器，并采用**最后非填充token池化**策略。由于Mamba的循环特性，最后一个token能访问全文上下文。
   - **解决的问题/优势**：
     - **密集描述检索**：在平均长度超过130个token的Flickr8k-R和DOCCI数据集上，CLIMP的检索性能（R@5）全面优于Transformer基线（最高提升11.8%）。
     - **应用扩展**：使模型能够处理更详细、更自然的图像描述，拓宽了在细粒度图像检索、文档图像理解等需要长文本输入的应用场景。

### 5. **引入并验证了VMamba的空间归纳偏置对视觉-语言任务有益**
   - **改进/不同之处**：
     - **以往工作**：视觉SSM（如VMamba）的归纳偏置已在分类任务中被研究，但其在**视觉-语言对比学习**中的影响尚未探索。
     - **CLIMP的分析**：通过控制实验（表14）证明，VMamba在常规空间顺序数据上表现更好，而在打乱patch顺序的数据上性能显著下降；ViT则相反。这**实证了VMamba具有对空间顺序结构的强归纳偏置**。
   - **解决的问题/优势**：
     - **解释性能提升**：这种**空间局部性和平滑性偏置**有助于模型进行更样本高效的学习，并产生更空间连贯的跨模态对齐（如图2可视化），这直接贡献了其更好的检索性能和可解释性。
     - **理论贡献**：将SSM在视觉任务中的归纳偏置研究延伸到了多模态领域，为未来架构设计提供了依据。

---

### **总结：核心价值**
CLIMP的创新不仅是将一种新架构（Mamba）应用于现有框架（CLIP），而是通过这种替换，**系统性地解决了传统CLIP模型的几个固有痛点**：计算效率与分辨率缩放的矛盾、对虚假相关性的敏感性、文本上下文长度的限制。它证明了SSM在视觉-语言学习中的巨大潜力，为开发更高效、更鲁棒、更灵活的多模态基础模型提供了一个有竞争力的替代方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

论文CLIMP通过全面的实验评估，证明了其作为首个完全基于Mamba的视觉-语言对比预训练模型的有效性。其实验设计严谨，覆盖了标准基准测试、分布外鲁棒性、高分辨率处理、长文本检索等多个维度。

### 一、 主要评估数据集与指标

#### 1. 核心评估基准
- **CLIP-Benchmark**：包含31个数据集，用于评估**零样本分类**和**零样本检索**。
    - **分类指标**：Top-1准确率 (Acc@1)， Top-5准确率 (Acc@5)。
    - **检索指标**：图像召回率@5 (IR@5)， 文本召回率@5 (TR@5)。

#### 2. 分布外 (OOD) 鲁棒性评估
- **数据集**：ImageNet-V2, ImageNet-R, ImageNet-A, ImageNet-O, ImageNet-Sketch。
- **指标**：Top-1/Top-5准确率，重点关注**自然分布偏移**和**对抗性示例**的鲁棒性。

#### 3. 高分辨率与分辨率灵活性评估
- **数据集**：
    - **CLIP-Benchmark子集**：在224, 320, 384分辨率下评估。
    - **高分辨率专用数据集**：NoCaps (平均810×960), Crossmodal-3600 (平均640×520)，在224, 512, 896分辨率下评估。
- **指标**：检索召回率@5 (IR@5/TR@5)。

#### 4. 密集描述 (长文本) 检索评估
- **数据集**：
    - **Flickr8k-Rephrased (Flickr8k-R)**：使用LLM重新表述的详细描述（平均134个token，98.3%超过CLIP的77-token限制）。
    - **DOCCI**：自然生成的详细描述数据集（平均142个token，94.4%超过77-token限制）。
- **指标**：检索召回率@1, @5, @10。

#### 5. 分析与消融实验
- **嵌入几何分析**：在NoCaps上计算**对齐度(Alignment)**、**均匀度(Uniformity)**和**枢纽度(Hubness)**。
- **效率分析**：比较不同分辨率下的**内存占用**和**计算量(FLOPs)**。
- **缩放实验**：评估模型参数量和数据量对性能的影响。

### 二、 对比的基线方法

论文与多种先进的Transformer-based CLIP变体进行了全面对比，所有基线均使用相似的参数量级（视觉编码器约86M参数，文本编码器约1B参数）并在相同数据集(CC12M)上训练。

1.  **ViT-B/16 + LLaMA-3.2-1B**：标准CLIP架构基线。
2.  **RoPE-ViT-B/16 + LLaMA-3.2-1B**：使用旋转位置编码(RoPE)以支持灵活分辨率的ViT变体。
3.  **FlexViT-B/16 + LLaMA-3.2-1B**：通过随机化patch大小训练以支持任意分辨率推理的ViT。
4.  **NaFlex-ViT-B/16 + LLaMA-3.2-1B**：通过序列打包处理原生分辨率而无需调整大小的ViT。
5.  **OpenAI CLIP-ViT-B/16 (在LAION-2B上训练)**：作为OOD鲁棒性的外部强基准。

### 三、 关键性能提升与结论

#### 1. 零样本检索性能领先
- **核心结果**：在CLIP-Benchmark的31个数据集上，CLIMP (VMamba-B + Mamba-1) 取得了**最佳的检索性能**。
    - **图像检索(IR@5)**：65.5%， 比最佳Transformer基线(RoPE-ViT)高出 **+2.1%**。
    - **文本检索(TR@5)**：77.0%， 比最佳Transformer基线高出 **+4.1%**。
- **结论**：完全基于SSM的架构在跨模态对齐任务上可以匹配甚至超越Transformer。

#### 2. 分布外鲁棒性显著提升
- **核心结果**：在五个ImageNet变体数据集的平均准确率上，CLIMP的两个变体包揽了**前两名**。
- **最突出成果**：在**ImageNet-O**（专门设计用于探测虚假相关性的数据集）上表现惊人。
    - CLIMP (Mamba-2)：Top-1准确率 **49.8%**。
    - CLIMP (Mamba-1)：Top-1准确率 **48.1%**。
    - 相比最佳Transformer基线(RoPE-ViT, 40.1%)，分别提升了 **+9.7%** 和 **+8.0%**。
    - **更重要的对比**：CLIMP (在CC12M上训练) 甚至超越了 **OpenAI CLIP-ViT-B/16** (在LAION-2B上训练，数据集大167倍) 的42.3%准确率。
- **结论**：Mamba的架构归纳偏置（如局部性和平滑性）对于抵抗分布偏移、减少对虚假特征的依赖至关重要，其作用可能比单纯扩大训练数据规模更关键。

#### 3. 高分辨率处理优势明显
- **核心结果**：CLIMP**无需任何调整或额外训练**即可处理更高分辨率输入，且性能下降远小于Transformer基线。
    - 在896×896超高分辨率下，CLIMP在NoCaps和Crossmodal-3600上的平均检索性能显著优于所有基线。
    - 相比专门为灵活分辨率设计的RoPE-ViT，CLIMP在896×896分辨率下的图像/文本检索性能优势达到 **+18–19%**。
- **效率优势**：在896×896分辨率下，CLIMP比ViT变体节省 **5倍内存** (10.0 MB vs. 50.4 MB) 和 **1.8倍计算量** (259.7 vs. 457.9 GFLOPs)。

#### 4. 突破CLIP的文本长度限制
- **核心结果**：在密集描述检索任务（Flickr8k-R和DOCCI）上，CLIMP** consistently outperforms**所有Transformer基线。
    - 在DOCCI数据集上，CLIMP (Mamba-1) 的图像检索R@5达到 **67.1%**，比最佳基线(NaFlex)高出 **+11.8%**。
- **结论**：Mamba自回归文本编码器能够有效处理远超CLIP 77-token限制的长文本描述，实现了更好的密集描述检索。

#### 5. 分析验证核心优势
- **嵌入几何分析**：CLIMP展现出**更好的对齐度**和**更低的文本枢纽度**，这直接解释了其优异的检索性能和潜在的OOD鲁棒性。
- **架构协同效应**：消融实验表明，当视觉编码器为VMamba时，搭配Mamba文本编码器能带来显著性能提升（+3.5% Acc@1, +3.9% TR@5），而ViT对文本编码器选择不敏感。这支持了**完全基于SSM的架构设计**。
- **归纳偏置验证**：通过CIFAR-10 patch顺序打乱实验，证实了VMamba依赖于**序列空间结构**作为其归纳偏置，而ViT对此不敏感。这种偏置有助于样本高效学习和更鲁棒的表示。

### 总结
CLIMP的实验评估系统性地证明了其作为Transformer-based CLIP替代方案的**多重优势**：1) **卓越的检索与OOD鲁棒性**，尤其在抵抗虚假相关性方面；2) **天生的高分辨率处理能力**与显著的**计算效率优势**；3) **突破性的长文本处理能力**。这些成果共同表明，基于状态空间模型(SSM)的架构为视觉-语言学习提供了一个**极具前景的新方向**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.06891v1)
- [HTML 版本](https://arxiv.org/html/2601.06891v1)
