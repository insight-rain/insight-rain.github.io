# Mugi: Value Level Parallelism For Efficient LLMs

**相关性评分**: 6.0/10

**排名**: #29


---


## 基本信息

- **arXiv ID**: [2601.10823v1](https://arxiv.org/abs/2601.10823v1)
- **发布时间**: 2026-01-15T19:48:21Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Daniel Price, Prabhu Vellaisamy, John Shen, Di Wu

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

这篇论文提出了一种名为Mugi的基于值级并行（VLP）的轻量级架构，通过优化非线性近似和小批量GEMM操作，显著提升了LLM的推理效率和能效，适用于边缘部署场景。

## 摘要

Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\times$ and $668\times$ for nonlinear softmax operations, and $2.07\times$ and $3.11\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\times$ and embodied carbon by $1.48\times$.

## 详细分析

## 论文《Mugi：面向高效大语言模型的值级并行》摘要

### 1. 研究背景和动机
随着大语言模型（LLM）的广泛应用，其巨大的计算开销和能耗已成为关键瓶颈。传统的值级并行（VLP）技术主要针对大批次、低精度的通用矩阵乘法（GEMM）进行优化，但LLM推理包含大量复杂的非线性操作（如Softmax、SiLU、GELU）以及小批次、非对称量化的GEMM，现有VLP架构无法高效支持这些操作，导致性能和能效低下。

### 2. 核心方法和技术创新
本文提出了名为 **Mugi** 的新型VLP架构，其核心创新包括：
- **非线性操作近似**：首次将VLP推广至非线性函数近似。采用**输入近似**和**以值为中心**的策略，对重要的输入值赋予更高精度，并通过查找表（LUT）和时序订阅机制高效计算。
- **小批次非对称GEMM优化**：针对LLM中流行的权重仅量化（WOQ）和KV缓存量化（KVQ）带来的BF16-INT4非对称GEMM，通过**定制数据格式**和**优化映射**，并结合分组查询注意力（GQA）提升小批次下的硬件利用率。
- **硬件架构协同设计**：Mugi架构**复用同一计算阵列**来执行非线性近似和GEMM，最大化硬件利用率，减少了专用硬件带来的面积和碳足迹开销。

### 3. 主要实验结果
在多个LLM模型上的实验表明，Mugi相比传统方案取得了显著提升：
- **非线性操作**：Softmax操作的吞吐和能效最高提升 **45倍** 和 **668倍**。
- **端到端LLM推理**：吞吐提升 **2.07倍**，能效提升 **3.11倍**。
- **可持续性**：LLM运行的**运营碳排放降低1.45倍**，制造相关的**隐含碳排放降低1.48倍**。

### 4. 研究意义和价值
Mugi工作首次系统地将VLP并行范式成功应用于完整的LLM工作负载，克服了其在非线性计算和小批次非对称计算上的固有局限。所提出的协同设计架构不仅在性能、能效上取得突破，还通过硬件复用显著降低了碳足迹，为未来设计高效、可持续的AI加速器提供了重要的新思路和架构范例。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Mugi - 面向高效LLM的值级并行架构

### **一、 研究背景与核心问题**
这篇论文旨在解决一个关键矛盾：**传统的值级并行（VLP）架构无法高效支持现代大语言模型（LLM）的复杂工作负载。**

VLP 最初为 **大规模批处理、低精度通用矩阵乘法（GEMM）** 设计，通过将数值计算转换为时间上的累加和“订阅”操作，避免了乘法器，从而提升能效。然而，LLM推理具有三个显著特征，使得传统VLP（如Carat）失效：
1.  **复杂的非线性操作**：如Softmax、SiLU、GELU，在LLM中占据显著运行时开销。
2.  **非对称量化成为趋势**：为减少内存占用，LLM普遍采用BF16激活值搭配INT4权重/KV缓存的量化方案（WOQ/KVQ），而传统VLP仅支持对称格式（如FP8）。
3.  **小批量推理**：为保证实时性，LLM推理批处理大小通常很小（例如8），这与传统VLP为大规模批处理优化的设计相悖。

**核心研究问题**：VLP能否被扩展，以高效支持LLM的完整工作负载（包括非线性操作和非对称、小批量GEMM）？

### **二、 核心创新点与解决方案**
论文提出了名为 **Mugi** 的新型VLP架构，通过三大核心创新解决上述问题。

#### **创新点一：面向非线性操作的VLP近似计算**
*   **问题**：传统VLP及硬件加速器缺乏对LLM中关键非线性函数的高效支持。
*   **解决方案**：
    1.  **输入近似与值中心化**：提出 **“输入近似”** ，对输入值（如BF16的尾数）进行有损舍入，而非对输出结果近似。更重要的是，采用 **“值中心化”** 方法，通过分析发现非线性函数的输入指数值通常聚集在一个小范围内（例如Softmax集中在[-3, 4]）。Mugi只为这个“重要值”窗口预计算并存储查找表（LUT），大幅减少LUT大小和计算时间。
    2.  **滑动窗口机制**：为适应不同模型层间输入分布的差异，为每个计算映射动态调整LUT窗口范围，进一步优化精度。
    3.  **VLP化执行**：将非线性计算分解为“值重用”和“时间订阅”两个阶段，在二维处理元件（PE）阵列上并行处理向量输入，实现了高吞吐量的近似计算。

#### **创新点二：针对非对称、小批量GEMM的VLP优化**
*   **问题**：传统VLP映射（激活值在行、权重在列）无法高效处理BF16-INT4格式和小批量。
*   **解决方案**：
    1.  **格式定制与映射转置**：**将INT4权重/KV缓存映射到行，将BF16查询（Q）令牌映射到列**。这完美契合了LLM的优化趋势：
        *   **权重仅量化（WOQ）**：INT4权重行映射充分利用窄位宽数据路径。
        *   **KV缓存量化（KVQ）与分组查询注意力（GQA）**：GQA使得一组（例如8个）Q令牌共享同一KV缓存，恰好可以填满Mugi的8个阵列列，实现了小批量下的高硬件利用率。
    2.  **缓冲区最小化**：通过**广播机制**和**输出缓冲区精简**，将Carat中随阵列规模呈二次增长增长的FIFO缓冲区面积降低了 **4.5倍**。

#### **创新点三：统一的Mugi架构与资源协同**
*   **问题**：传统设计为GEMM和非线性操作配备独立的计算单元（如矩阵单元和向量单元），增加了芯片面积（体现碳）和功耗（运行碳）。
*   **解决方案**：设计单一的 **Mugi PE阵列**，通过可配置的数据流和控制器，**同时支持VLP非线性近似和VLP GEMM操作**。这种硬件共享极大提高了资源利用率，在单一架构上即可完成LLM全流程计算，同时显著降低了碳排放。

### **三、 技术实现路径总结**
1.  **算法层面**：将非线性函数转化为基于LUT和VLP时间订阅的可并行计算模式，并结合值分布分析进行优化。
2.  **数据流与映射层面**：根据LLM量化（WOQ/KVQ）和注意力机制（GQA）的特点，重新设计GEMM在VLP阵列上的数据映射策略。
3.  **硬件架构层面**：设计一个灵活、统一的PE阵列及配套的内存、缓冲和控制逻辑，以支持上述两种计算模式的高效切换和执行。

### **四、 实际价值与评估结果**
Mugi 的创新带来了显著的性能、能效和可持续性提升：
*   **非线性操作**：相比精确向量阵列，Softmax和SiLU的吞吐量提升最高达 **45倍** 和 **668倍**，能效提升达 **481倍** 和 **668倍**。其精度在多数测试模型中优于或媲美传统的分段线性（PWL）和泰勒级数近似法。
*   **完整LLM工作负载**：在Llama 2 70B等模型上，相比传统脉动阵列，**吞吐量提升2.07倍，能效提升3.11倍**。
*   **可持续性**：通过硬件共享，**运行碳排放降低1.45倍，体现碳排放降低1.48倍**。

**结论**：Mugi 成功地将VLP范式从传统的CNN领域推广到更复杂的LLM领域，通过**算法-硬件协同设计**，解决了LLM推理中的非线性计算和小批量非对称GEMM两大效率瓶颈，为下一代高能效AI加速器提供了新的设计思路。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决现有**值级并行（VLP）架构**在支持**大语言模型（LLM）** 推理时面临的三大核心挑战：1) 无法高效处理LLM中的**非线性运算**（如softmax、SiLU、GELU）；2) 与LLM推理中主流的**非对称量化**（如BF16-INT4）和**小批次**计算模式不兼容；3) 为非线性运算和通用矩阵乘法（GEMM）分别设计独立硬件单元，导致能效和可持续性低下。

为此，论文提出了名为 **Mugi** 的新型VLP架构。其核心创新在于：首先，**将VLP思想推广至非线性运算的近似计算**，提出了一种“以值为中心”的输入近似方法，通过查找表（LUT）和时域订阅机制，为重要的输入值分配更高精度。其次，**针对LLM推理优化VLP的GEMM执行**，通过定制数据格式和映射策略，高效支持非对称量化（如权重仅量化、KV缓存量化）和小批次计算，并利用分组查询注意力（GQA）提升硬件利用率。最后，**在单一Mugi架构中协同整合了上述非线性近似和GEMM优化**，实现了硬件资源的复用。

实验结果表明，Mugi架构在LLM推理中取得了显著的性能、能效和可持续性提升：对于非线性softmax运算，吞吐量和能效分别提升高达**45倍**和**668倍**；对于端到端的LLM，吞吐量和能效分别提升**2.07倍**和**3.11倍**；同时，将LLM运行的**运营碳排放**和**隐含碳排放**分别降低了**1.45倍**和**1.48倍**。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Mugi: Value Level Parallelism For Efficient LLMs》的创新点分析

该论文针对现有**值级并行（Value Level Parallelism, VLP）** 架构在支持大语言模型（LLM）推理时的局限性，提出了名为 **Mugi** 的新架构。其核心创新点可归纳为以下四个方面，每个创新点都明确解决了现有方法的关键问题，并带来了显著优势。

---

### 1. **首次将VLP推广至非线性运算的近似计算**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：现有的VLP设计（如Carat）仅针对**线性GEMM运算**优化，无法高效处理LLM中的关键非线性运算（如Softmax、SiLU、GELU）。这些运算通常依赖独立的专用硬件单元（如查找表LUT、分段线性PWL或泰勒级数近似单元）或软件计算，导致效率低下或面积开销大。
    - **Mugi的创新**：提出了 **“VLP非线性近似”** 方法。其核心是 **“输入近似”** 和 **“以值为中心”** 的策略：
        1.  **输入近似**：对输入的尾数（Mantissa）进行低位舍入（如BF16尾数7位舍入为3位），减少时序信号长度，从而降低计算延迟。
        2.  **以值为中心**：通过分析LLM中非线性运算输入值的实际分布（见图4），发现指数（Exponent）往往集中在较小范围内。Mugi仅为这些 **“重要值”** 预计算并存储LUT，而非覆盖整个理论值域，并采用 **滑动窗口机制** 动态适配不同层的值分布。
- **解决的具体问题/带来的优势**：
    - **问题**：LLM中非线性运算（尤其是Softmax）在总运行时占比显著（可达40%以上），现有VLP架构无法加速，成为性能瓶颈。
    - **优势**：
        1.  **高性能与高效率**：实现了**无乘法器**的近似计算，将非线性运算（如Softmax、SiLU）的吞吐量提升高达 **45倍** 和 **668倍**，能效提升达 **481倍** 和 **668倍**（见图11）。
        2.  **高精度**：通过“以值为中心”的近似，在多数模型上取得了比传统PWL、泰勒级数近似更好的端到端精度（困惑度/损失），因为误差主要集中在不重要的输入区域（见图6, 8）。
        3.  **硬件复用基础**：为后续与GEMM共享计算阵列奠定了基础。

### 2. **针对非对称、小批次GEMM的VLP优化**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：Carat等VLP设计针对**大批次、对称精度（如FP8-FP8）** 的GEMM优化。这与LLM推理的**小批次**（为保证实时性）和**非对称量化**（如BF16-INT4）趋势不兼容，导致硬件利用率低下。
    - **Mugi的创新**：
        1.  **数据格式定制与映射翻转**：将**INT4权重/KV缓存**映射到VLP阵列的**行**，将**BF16输入/查询令牌**映射到**列**。这充分利用了LLM令牌维度大、GQA组大小（常为8）与阵列列数匹配的特点。
        2.  **缓冲区最小化**：通过**广播**和**输出缓冲区精简**技术，将Carat中随阵列规模呈二次增长的大面积FIFO缓冲区减少了 **4.5倍**。
        3.  **协同及时优化**：设计原生支持**仅权重量化（WOQ）**、**KV缓存量化（KVQ）** 和**分组查询注意力（GQA）**，这些是当前LLM推理中减少内存占用、提升效率的关键技术。
- **解决的具体问题/带来的优势**：
    - **问题**：LLM推理中，小批次和非对称量化导致传统VLP和传统脉动阵列/SIMD阵列的硬件利用率极低，计算效率差。
    - **优势**：
        1.  **高硬件利用率**：映射策略与GQA（组大小=8）完美契合，即使在小批次（如8）下也能保持高阵列利用率。
        2.  **高能效**：VLP本身消除了乘法操作，结合优化的数据流和缓冲区设计，在投影、注意力、FFN等GEMM操作上，相比传统脉动阵列，实现了最高 **3.11倍** 的能效提升（见图12，表3）。
        3.  **兼容性**：直接支持主流的BF16-INT4量化方案，无需额外转换开销。

### 3. **提出统一的Mugi架构，协同非线性近似与GEMM优化**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：传统AI加速器通常为**GEMM**和**非线性运算**设计**独立的硬件单元**（如分离的矩阵单元和向量单元）。这增加了芯片面积、功耗和制程碳排放。
    - **Mugi的创新**：设计了**单一、可重构的VLP计算阵列**，通过微架构设计，使其能够**分时复用**，既执行VLP非线性近似，又执行VLP GEMM。图9展示了如何通过添加有限的额外硬件（如符号处理、后处理块等）在同一个阵列上支持两种计算模式。
- **解决的具体问题/带来的优势**：
    - **问题**：独立单元导致芯片面积和“隐含碳”排放增加，且资源利用率可能不均衡。
    - **优势**：
        1.  **资源效率最大化**：显著提高了硬件资源的利用率，一套硬件完成LLM全流程计算。
        2.  **面积与碳足迹降低**：通过共享核心计算阵列，减少了总面积开销。实验表明，Mugi相比基线设计，将**运行碳排放降低了1.45倍**，**隐含碳排放降低了1.48倍**（见图15）。
        3.  **完整的LLM支持**：首次在一个VLP架构内完整支持了LLM的前馈（GEMM）和注意力（非线性+GEMM）关键操作。

### 4. **系统级的可扩展性与性能表现**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：传统阵列（脉动/SIMD）在规模扩大时，面积呈二次增长，且在小批次下利用率问题加剧。Carat的缓冲区面积也超线性增长。
    - **Mugi的创新**：通过优化的NoC（片上网络）设计和前述的缓冲区精简，Mugi架构能够高效地扩展到多节点（如4x4, 8x8 NoC布局）。其性能随阵列规模线性增长，而面积增长相对温和。
- **解决的具体问题/带来的优势**：
    - **问题**：加速器难以在保持高效率的同时进行大规模扩展以处理更大模型。
    - **优势**：
        1.  **卓越的可扩展性**：在多节点配置下，Mugi相比单节点放大的传统阵列，展现出更好的吞吐量和能效（见图17）。例如，4x4 NoC的Mugi相比同配置脉动阵列，吞吐量和能效均有大幅领先。
        2.  **实际性能提升**：在端到端的LLM（如Llama 2 70B）推理中，单节点Mugi相比最佳基线，实现了 **2.07倍** 的吞吐量提升和 **3.11倍** 的能效提升（见表3）。非线性运算的延迟几乎可忽略（见图16）。

---

**总结**：Mugi的核心创新在于**扩展了VLP的计算范式**，使其从仅适用于特定GEMM场景，进化为一个**统一、高效、支持LLM全流程工作负载的通用计算架构**。其通过算法-架构协同设计，解决了LLM推理中**非线性计算瓶颈**、**非对称量化与小批次下的低利用率**以及**硬件资源碎片化**三大关键挑战，最终在性能、能效和可持续性（碳足迹）三个维度上实现了显著的综合提升。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 实验数据集与模型
论文使用了多种**大型语言模型（LLMs）** 作为评估对象，具体包括：
- **Llama 2**（7B、13B、70B参数版本）
- **Whisper**（Tiny、Large版本，用于语音识别）
- **SwinV2**（视觉Transformer）
- **ViViT**（视频Transformer）

这些模型覆盖了**文本、语音、图像、视频**多种模态，以验证Mugi架构的通用性。实验主要针对**推理阶段**，批处理大小（Batch Size）设置为1至32，序列长度覆盖128至4096。

### 二、 主要评价指标
1.  **准确性指标**：
    - **困惑度（Perplexity）**：用于评估语言模型（Llama 2, Whisper）的生成质量。
    - **损失值（Loss）**：用于评估视觉/视频模型（Swin, ViViT）的准确性。
2.  **性能与效率指标**：
    - **吞吐量（Throughput）**：单位时间处理的令牌数（Tokens/s）。
    - **能效（Energy Efficiency）**：每微焦耳能量处理的令牌数（Tokens/s/μJ）。
    - **功率效率（Power Efficiency）**：每瓦功率处理的令牌数（Tokens/s/W）。
    - **延迟（Latency）**：端到端推理时间。
3.  **硬件与可持续性指标**：
    - **芯片面积（On-Chip Area）**：以平方毫米（mm²）衡量。
    - **碳排放**：
        - **运营碳排放（Operational Carbon）**：推理运行能耗产生的碳排放。
        - **隐含碳排放（Embodied Carbon）**：芯片制造过程产生的碳排放。

### 三、 对比的基线方法
论文将**Mugi**架构与多种先进的硬件设计方案进行了全面对比：

1.  **非线性操作近似基线**：
    - **精确向量阵列（VA-FP）**：使用MAC单元精确计算非线性函数（如softmax）。
    - **分段线性近似（PWL）**：采用22个分段进行硬件近似。
    - **泰勒级数近似（Taylor）**：采用高达9阶的多项式展开。
    - **Mugi-L**：使用查找表（LUT）而非VLP进行近似的变体。

2.  **GEMM加速基线**：
    - **Carat**：首个提出VLP的工作，但仅支持FP8和大批次GEMM。
    - **脉动阵列（Systolic Array, SA）**：传统矩阵乘法加速器。
    - **SIMD阵列**：单指令多数据阵列。
    - **FIGNA配置**：专为FP-INT乘法优化的PE单元。
    - **NVIDIA Tensor Core**：基于Hopper GPU架构，作为工业界标杆。

3.  **评估设置**：
    - **单节点（Single Node）**：比较核心阵列性能。
    - **多节点（NoC）**：通过4x4或8x8片上网络（NoC）连接多个计算节点，评估可扩展性。
    - **等面积（Iso-Area）对比**：在控制芯片面积相近的条件下比较性能。

### 四、 关键性能提升与结论

#### 1. 非线性操作近似
- **准确性**：在大多数模型上，Mugi的VLP近似达到了**最佳或接近最佳**的困惑度/损失值。其**以值为中心（value-centric）** 的近似方法（重点关注高频出现的指数值范围）显著优于不考虑值分布的PWL和泰勒近似。
- **性能与效率**（以softmax和SiLU为例，对比精确向量阵列）：
    - **吞吐量提升**：高达 **45倍**（softmax）和 **668倍**（SiLU）。
    - **能效提升**：高达 **481倍**（softmax）和 **668倍**（SiLU）。
    - **功率效率提升**：高达 **10.7倍**（softmax）和 **14.8倍**（SiLU）。
- **原因**：Mugi**消除了乘法器**，通过时间编码和值重用进行计算，并与GEMM共享计算阵列，避免了专用向量阵列的带宽限制。

#### 2. GEMM操作
- **性能与效率**（在Llama 2模型上的投影、注意力、FFN层GEMM）：
    - Mugi在吞吐量和能效上**持续优于**脉动阵列和SIMD阵列。
    - 与Carat相比，Mugi通过**映射定制**（将INT4权重映射到行，BF16激活映射到列）和**缓冲区最小化**，更好地适应了LLM的小批次、非对称量化（BF16-INT4）特性，并利用**分组查询注意力（GQA）** 提高了利用率。
- **单节点端到端LLM推理**（Llama 2 70B with GQA）：
    - 对比16x16脉动阵列，Mugi (256) 实现了：
        - **吞吐量提升**：**2.07倍**
        - **能效提升**：**3.11倍**
        - **功率效率提升**：**1.50倍**

#### 3. 可持续性（碳排放）
- Mugi通过**最大化硬件复用**（非线性近似与GEMM共享同一阵列），减少了芯片面积和功耗，从而降低了碳排放：
    - **运营碳排放降低**：**1.45倍**
    - **隐含碳排放降低**：**1.48倍**

#### 4. 多节点可扩展性
- 在4x4 NoC配置下，Mugi相比基线脉动阵列，在吞吐量和能效上保持了显著优势，证明了其架构**具有良好的可扩展性**。

### 五、 核心结论
论文通过详实的实验证明：
1.  **技术创新有效**：提出的**VLP非线性近似方法**和针对**非对称小批次GEMM的优化**是成功的，在保证精度的同时大幅提升了性能与能效。
2.  **架构设计卓越**：**Mugi架构**成功地将上述优化协同在一个单一设计中，实现了硬件资源的极致复用，能够高效支持完整的LLM工作负载（包括非线性操作和GEMM）。
3.  **实际价值显著**：Mugi在**性能、能效和可持续性**三个维度均带来了显著提升，为未来部署高效、环保的LLM推理硬件提供了有前景的解决方案。其设计与时下LLM的主流优化技术（WOQ、KVQ、GQA）紧密结合，具有很高的实用价值。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.10823v1)
- [HTML 版本](https://arxiv.org/html/2601.10823v1)
