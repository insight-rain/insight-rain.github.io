# From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion

**相关性评分**: 7.0/10

**排名**: #15


---


## 基本信息

- **arXiv ID**: [2601.10710v1](https://arxiv.org/abs/2601.10710v1)
- **发布时间**: 2026-01-15T18:59:10Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Cheng Chen, Yuyu Guo, Pengpeng Zeng, Jingkuan Song, Peng Di, Hang Yu, Lianli Gao

## 关键词

Vision-Language Models, Cross-Layer Injection, Lightweight Architecture, Dynamic Fusion, Parameter-Efficient, Multimodal Understanding

## 一句话总结

该论文提出了一种轻量级的动态跨层注入框架，通过自适应多投影和门控融合机制，提升视觉语言模型在多层次视觉特征融合中的推理效率和性能。

## 摘要

Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.

## 详细分析

## 论文摘要：从“一对一”到“多对多”：用于深度视觉-语言融合的动态跨层注入

**1. 研究背景和动机**
当前主流的视觉-语言模型（VLMs）通常采用一种静态且不对称的架构：仅将视觉编码器的最终层输出连接到大型语言模型（LLM）的输入。这种“一对一”的连接方式造成了严重的视觉特征瓶颈，使LLM无法充分利用视觉编码器不同层次（从低级的纹理、边缘到高级的语义概念）的丰富信息，从而限制了模型整合局部细节与全局语义进行连贯推理的能力。

**2. 核心方法和技术创新**
本文提出了**跨层注入（CLI）** 框架，旨在构建一个轻量级、动态的“多对多”桥梁，以连接视觉与语言模态。CLI包含两个协同工作、参数高效的组件：
- **自适应多投影（AMP）模块**：利用低秩适应（LoRA）技术，高效地将来自不同视觉层的特征对齐到共享的语义空间。
- **自适应门控融合（AGF）机制**：作为智能的上下文敏感控制器，它允许LLM在解码过程中，基于其实时上下文，动态查询并选择性地注入最相关的视觉信息（无论是来自浅层的细节还是深层的概念）。

**3. 主要实验结果**
研究将CLI集成到LLaVA-OneVision和LLaVA-1.5两种不同的VLM架构中，并在涵盖文档分析、图表推理、通用视觉感知等能力的18个多样化基准测试上进行了验证。实验结果表明：
- CLI显著且一致地超越了基线模型及其他深度融合策略（如DeepStack和静态一对一融合）。
- 在LLaVA-OV-7B模型上，CLI在LLaVA-in-the-Wild、MME和OCR-Bench等关键基准上分别取得了**6.5**、**3.3**和**4.7**个百分点的性能提升。
- 消融研究证实了AGF动态选择机制的核心作用，以及AMP与AGF的协同效应。

**4. 研究意义和价值**
本工作通过引入动态的“多对多”融合范式，系统性地解决了VLMs中层次化视觉特征利用不足的根本问题。CLI框架具有**轻量级、可扩展和架构无关**的特点，它赋予LLM按需访问完整视觉层次的能力，从而解锁了更深层次的多模态理解。该研究为构建更强大、推理能力更细致的视觉-语言模型提供了一个有效且通用的新范式。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
当前主流视觉-语言模型（VLMs）存在一个根本性的**架构缺陷**：它们仅将视觉编码器的**最后一层输出**（高层语义特征）静态地连接到大型语言模型（LLM）的输入层。这种**“一对一”的静态连接**导致了严重的**视觉特征瓶颈**，使得LLM无法充分利用视觉编码器不同层次（从低层纹理、边缘到高层语义）的丰富信息，从而限制了模型整合局部细节与全局语义进行连贯推理的能力。

### **核心创新点**
论文提出了 **“跨层注入”框架**，旨在将传统的**“一对一”静态连接**转变为**动态的“多对多”桥接**。其核心创新体现在以下两个协同工作的参数高效模块：

1.  **自适应多投影模块**
    - **功能**：使用**低秩自适应**技术，高效地将来自不同视觉层（如第1、7、14层）的特征**对齐**到统一的语言语义空间中。
    - **创新性**：解决了为每一层训练独立投影器带来的参数量爆炸问题，同时避免了单一投影器无法处理跨层特征分布差异的困境。

2.  **自适应门控融合机制**
    - **功能**：这是框架的**核心控制器**。在LLM解码的每一步，该机制允许LLM基于**当前的解码上下文**，动态地查询整个视觉层次库，并**选择性地注入**最相关的视觉信息。
    - **创新性**：使LLM成为一个**主动的观察者**，能够根据实时推理需求，动态决定是“放大”查看早期层的细粒度细节，还是“拉远”利用深层的高层语义，实现了真正**上下文感知**的特征融合。

### **解决方案概述**
论文通过以下方式构建了动态的“多对多”桥接：

- **信息提取**：从视觉编码器的**多个中间层**采样特征，构成一个分层的视觉特征集合 `𝕍`。
- **信息对齐**：通过**AMP模块**，使用LoRA技术对预训练的投影器进行微调，为不同层的视觉特征生成适配的、对齐到文本空间的表示 `𝕍̂`。
- **动态注入**：在LLM解码器的**多个指定层**设置注入点。在每个注入点，**AGF模块**根据当前LLM的隐藏状态 `h_t`，计算一个动态权重 `W`，以此控制将 `𝕍̂` 中的信息以加权和的方式更新到 `h_t` 中（仅更新与视觉token对应的部分）。
- **整体流程**：这使得LLM在生成文本的整个过程中，能够多次、按需地访问和整合不同抽象级别的视觉信息，从而实现了深度的多模态理解。

### **实际价值与技术意义**
- **性能提升**：在LLaVA-OneVision和LLaVA-1.5两个不同架构上集成CLI，在涵盖文档分析、图表推理、数学问题、视觉对话等**18个多样化基准测试**上均取得了显著性能提升。例如，在LLaVA-OV-7B上，LLaVA-in-the-Wild、MME和OCR-Bench分别提升了6.5、3.3和4.7个百分点。
- **架构通用性**：CLI被证明是一种**轻量级、可插拔**的增强范式，能够无缝集成到不同的VLM主干中，提升其多模态融合能力。
- **验证了动态融合的必要性**：通过可视化门控权重，论文实证了LLM与视觉编码器之间确实存在复杂的**交叉连接**需求，而非简单的逐层对齐，为未来VLM架构设计提供了重要见解。
- **高效性**：相较于为每一层训练完整投影器或使用重型融合模块，CLI通过LoRA和轻量级门控机制，以**较低的参数量开销**实现了显著的性能增益。

**总结**：该论文的核心贡献在于**系统性地识别并解决了VLM中视觉特征利用不足的问题**，通过创新的动态“多对多”跨层注入框架，赋予了LLM按需访问完整视觉层次的能力，从而解锁了更深层次、更精准的多模态理解与推理能力。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决当前视觉-语言模型（VLMs）中存在的**视觉特征瓶颈问题**，即传统架构仅将视觉编码器的最终层输出静态地连接到语言模型输入端，导致语言模型无法充分利用视觉编码器不同层次（从局部细节到全局语义）的丰富信息，从而限制了其进行细粒度感知和深度多模态推理的能力。为此，论文提出了一个名为**跨层注入（CLI）** 的轻量级框架，其核心是通过两个协同工作的参数高效模块——**自适应多投影（AMP）** 和**自适应门控融合（AGF）**——在视觉编码器和语言模型解码器之间建立一个动态的“多对多”桥梁，使得语言模型在解码过程中能够根据实时上下文，有选择地从视觉层次结构的任意层注入最相关的视觉信息。实验结果表明，将该框架集成到LLaVA-OneVision和LLaVA-1.5等不同架构后，在涵盖文档分析、图表推理、视觉问答等任务的18个多样化基准测试上均取得了显著的性能提升，验证了动态、选择性融合多层级视觉特征对于实现更深层次多模态理解的有效性和普适性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion》针对当前视觉-语言模型（VLM）的核心架构瓶颈，提出了一个系统性的创新框架。其核心创新点可归纳为以下三条：

---

### 1. **提出“多对多”动态跨层注入（CLI）框架，取代传统的“一对一”静态连接**
   - **相比以往方法的改进/不同之处**：
     - **传统方法**：主流VLM（如LLaVA、Flamingo）采用“**一对一**”静态连接，即仅将视觉编码器（ViT）的**最后一层特征**投影后，输入到大型语言模型（LLM）的**起始位置**。这是一种**静态、单向、粗粒度**的连接。
     - **部分改进方法**：
       - **DeepStack等“一对多”方法**：将单层视觉特征（通常是最后一层）**暴力注入**到LLM的多个解码层。这是一种**上下文无关**的加法，缺乏选择性。
       - **EVLM、Qwen3-VL等“静态一对一”方法**：在预定义的、固定的视觉层和LLM层之间建立**硬连线**的跨注意力连接。这是一种**刚性、不灵活**的映射。
     - **本文CLI框架**：构建了一个**动态的“多对多”桥梁**。它从视觉编码器的**多个层次**（如浅、中、深层）提取特征，并允许LLM在**多个解码层**上，根据**实时解码上下文**，**动态地、有选择地**注入最相关的视觉信息。
   - **解决的具体问题/带来的优势**：
     - **解决了“视觉特征瓶颈”问题**：传统单层连接丢弃了ViT早期层（包含边缘、纹理等细节）和中期层（包含局部结构）的丰富层次化信息，导致VLM的细粒度感知能力严重受限。CLI通过访问**整个视觉层次**，解决了此瓶颈。
     - **实现了“动态按需访问”**：LLM在推理时，可以根据当前处理的任务（例如，浅层解析名词时需要高层语义，深层进行评价推理时需要低层细节），像“主动观察者”一样查询整个视觉特征库。这解决了**静态架构无法适应复杂、多变的多模态推理需求**的问题。
     - **核心优势**：实现了**层次对称的融合**。ViT和LLM都是层次化处理信息的系统，CLI恢复了它们之间内在的对称性，允许跨模态的“交叉连接”，从而支持更全面、更细致的对齐。

### 2. **设计了两个高效、协同的核心模块：自适应多投影（AMP）与自适应门控融合（AGF）**
   - **相比以往方法的改进/不同之处**：
     - **投影模块的改进**：
       - **传统方法**：使用一个**固定的MLP投影器**处理所有视觉特征，无法应对不同视觉层特征分布的显著差异。
       - **本文AMP模块**：在预训练投影器的基础上，为**每个选中的视觉层**引入一个**独立的、参数高效的LoRA适配器**。公式为：`V_k_hat = MLP(V_k) + B_k * A_k(V_k)`。这使得投影器能**自适应地**将不同层次的视觉特征和谐地映射到语言空间，同时避免了为每一层从头训练一个完整投影器带来的巨大参数量和高风险。
     - **融合机制的革新**：
       - **传统/简单方法**：采用**元素相加**或**拼接**，会破坏LLM隐藏状态的原有信息流。
       - **本文AGF模块**：这是一个**基于查询的注意力门控机制**。它通过两个可学习的查询向量，分别从**新的视觉特征**和**LLM当前隐藏状态**中提取关键信息，然后通过一个门控制器（线性层+Sigmoid）计算一个动态权重（0到1之间）。该权重**有选择地**控制新视觉信息对LLM隐藏状态的更新，且通过掩码操作**仅更新与视觉token对应的位置**。
   - **解决的具体问题/带来的优势**：
     - **AMP解决的问题**：解决了**多层级视觉特征与语言空间的对齐难题**。单一的刚性投影器会导致特征错位，而AMP以极小的参数量代价，实现了对不同层次特征的自适应调和，为后续融合提供了“干净”的输入。
     - **AGF解决的问题**：解决了**信息注入的干扰性与无关性问题**。动态门控确保了只有与当前解码上下文最相关的视觉信息才会被整合，防止LLM被无关或冗余的视觉细节淹没，实现了**上下文感知的、精确的融合**。这是CLI性能提升的关键。
     - **协同优势**：AMP为AGF准备了高质量、对齐的多层次特征，AGF则智能地利用这些特征。两者结合，以**轻量级、参数高效**的方式，实现了强大的动态多对多融合。

### 3. **通过广泛的实验验证了框架的通用性、优越性及内部机制的有效性**
   - **相比以往工作的不同之处**：
     - **验证的广度与深度**：论文没有局限于单一模型架构，而是将CLI集成到**两个差异显著的VLM架构**（LLaVA-OneVision 和 LLaVA-1.5）中，并在**18个不同的基准测试**上进行了全面评估，涵盖了图表理解、文档分析、数学推理、综合对话等多个维度。
     - **对比的针对性**：不仅与SOTA模型比较，还进行了**严格的对照实验**，与“一对多”（DeepStack）和“静态一对一”（SLI）这两种代表性的深度融合策略，在**相同基座模型、相同训练数据**下进行公平对比。
     - **深入的可解释性分析**：通过**可视化门控权重热力图**（图1(b)），直观揭示了LLM解码层与视觉编码层之间复杂的“交叉连接”模式，为“多对多”动态交互提供了**经验证据**，而不仅仅是理论假设。
   - **解决的具体问题/带来的优势**：
     - **证明了通用性与有效性**：实验表明，CLI在两种架构上都能带来**一致且显著的性能提升**（例如，在LLaVA-OV-7B上，LLaVA-in-the-Wild提升6.5分，MME提升3.3分）。这证明了CLI是一个**即插即用、架构无关**的增强范式。
     - **明确了方法优势**：对照实验清晰表明，**非自适应**的DeepStack常导致性能下降，**刚性**的SLI提升有限或不稳定，而**动态自适应**的CLI则能实现稳健增益。这强有力地论证了动态选择机制的必要性。
     - **揭示了内部工作原理**：门控权重可视化证实了LLM在推理过程中对视觉信息需求的**动态演变**，使论文的核心论点——需要“多对多”动态连接——有了坚实的实证基础，增加了工作的**可信度和深度**。

---

**总结**：本文的核心创新在于**系统性地提出并验证了一个动态的“多对多”视觉-语言融合新范式**。它通过**AMP和AGF两个精巧的模块**，以参数高效的方式，解决了传统VLM中存在的**静态连接导致的信息瓶颈和层次不对称问题**，从而在多种任务上实现了更深入、更准确的多模态理解。其实验设计严谨，分析深入，不仅展示了性能提升，也阐明了其背后的工作机制。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验效果总结

论文通过大量实验验证了Cross-Layer Injection (CLI) 框架的有效性，在多个视觉-语言理解任务上实现了显著的性能提升。

### 一、 使用的数据集与评价指标

#### 1. 训练数据集
- **来源**：基于LLaVA-OneVision的约320万单图像指令数据池。
- **构成**：从中采样约140万样本，涵盖五大类别：
    - **通用问答与对话**：如ShareGPT4V/4o、Vision FLAN等。
    - **数学与推理**：如Geo170K、MathQA、MathV360K等。
    - **文档、图表与屏幕理解**：如ChartQA、AI2D、DocVQA等。
    - **通用OCR**：如TextCaps、IAM等。
    - **纯语言指令**：如Magpie-Pro，用于保持语言能力。

#### 2. 评估基准（18个）
使用 **LMMs-Eval框架** 进行零样本评估，涵盖三大类任务：
- **图表、文档理解**：AI2D、ChartQA、DocVQA、InfoVQA。
- **感知与多学科推理**：MME、MMBench、MMVet、MathVerse、MathVista、MMMU、GQA、OK-VQA、ScienceQA、SEED-Bench、MM-Star、POPE。
- **现实世界理解与视觉对话**：RealWorldQA、LLaVA-in-the-Wild。

#### 3. 主要评价指标
- **准确率**：用于大多数有确定答案的任务（如多选、VQA）。
- **平均归一化莱文斯坦相似度**：用于文本密集型VQA任务（如DocVQA），对OCR错误更鲁棒。
- **GPT辅助评估**：用于开放对话任务（如LLaVA-in-the-Wild），由GPT-4对回答评分。
- **统一成功率**：用于MME，综合认知与感知分数。
- **平均准确率**：用于MathVerse，综合其三个子任务。

### 二、 对比的基线方法

论文进行了两个层面的对比：

#### 1. 与先进VLMs的横向对比
- **相似规模**：IXC-2.5-7B、InternVL-2-8B。
- **更大规模**：VILA-13B、InternVL-2-26B。
- **目的**：展示CLI增强的模型能达到甚至超越更大模型的性能。

#### 2. 与不同融合策略的受控对比
在**相同基础模型（LLaVA-OV）和相同训练数据**上，对比了三种融合范式：
- **基线投影器**：原始LLaVA架构的静态单层连接（作为重新训练的公平基线）。
- **DeepStack**：一种“一对多”的暴力注入方法，将最终层视觉令牌不加选择地注入多个LLM层。
- **浅层注入**：一种静态“一对一”方法，将前n个视觉层硬连接到前n个LLM层（模拟Qwen3-VL等模型）。

### 三、 关键性能提升与结论

#### 1. 主要定量结果
- **在LLaVA-OV-7B上**：
    - **LLaVA-in-the-Wild**：提升 **+6.5%**。
    - **MME**：提升 **+3.3%**。
    - **OCR-Bench**：提升 **+4.7%**。
    - **总体表现**：CLI增强的LLaVA-OV-7B总分为660.6，与更大的InternVL-2-8B（660.9）性能相当，展示了其高效性。

- **在LLaVA-OV-0.5B上**：
    - CLI同样带来了全面且一致的性能提升，证明了其在不同模型规模上的有效性。

- **在LLaVA-1.5-7B上**：
    - CLI带来了更显著的提升（部分总和提升+7.5和+8.6），表明CLI能有效弥补基础模型在视觉-语言对齐上的不足，具有**架构无关的通用性**。

#### 2. 与替代融合策略的对比结论
- **DeepStack（暴力一对多）**：性能**普遍下降**，甚至低于基线。证明不加选择的特征注入会破坏LLM的表示，是**破坏性**的。
- **浅层注入（静态一对一）**：性能提升**微弱或不一致**。证明将多层次视觉信息限制在LLM浅层，会使负责复杂推理的深层缺乏感知细节，**无法满足高级推理需求**。
- **CLI（动态多对多）**：是**唯一能稳定、显著超越基线**的方法。其自适应门控融合机制能根据解码上下文动态选择最相关的视觉信息，实现了**精准、高效的融合**。

#### 3. 细粒度任务表现
- **OCR与视觉定位**：CLI在需要高空间和纹理精度的任务上表现突出。在OCR任务上为7B模型带来4.7%提升，在全部9个视觉定位基准上均取得稳健增益。
- **复杂推理任务**：在LLaVA-in-the-Wild、MME、SEED-Bench等需要抽象推理和知识整合的任务上提升明显，证明丰富的多层次视觉输入是高级认知任务的**关键组成部分**。

#### 4. 消融研究结论
- **核心组件**：**自适应门控融合是性能提升的关键**。仅使用AGF即可带来显著提升，而仅使用AMP（无门控）则收效甚微。
- **注入密度**：**高密度注入策略（多对多）效果最佳**，验证了让LLM在解码过程中频繁、按需访问完整视觉层次的必要性。
- **数据效率**：CLI在**低数据量（20%）下**即在推理任务上显现优势，在**50%数据量时**综合性能已普遍超越基线，展示了良好的数据效率。

### 总结
论文通过系统、全面的实验证明，**CLI框架通过建立动态的“多对多”桥梁，有效解决了传统VLM中的视觉特征瓶颈问题**。它在多种模型架构、不同规模模型和涵盖感知、推理、对话的广泛任务上，均实现了显著且一致的性能提升，确立了其作为一种**轻量级、可扩展的深度多模态融合新范式**的有效性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.10710v1)
- [HTML 版本](https://arxiv.org/html/2601.10710v1)
