# Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems

**相关性评分**: 6.0/10

**排名**: #55


---


## 基本信息

- **arXiv ID**: [2601.10560v1](https://arxiv.org/abs/2601.10560v1)
- **发布时间**: 2026-01-15T16:23:53Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Xi Shi, Mengxin Zheng, Qian Lou

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种延迟感知的多智能体系统编排框架，通过并行执行和优化关键路径来降低推理延迟，适用于时间敏感场景。

## 摘要

Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS

## 详细分析

## 论文摘要：面向并行多智能体系统的延迟感知编排学习

**1. 研究背景和动机**
基于大语言模型的多智能体系统通过协调多个智能体进行复杂推理，但其多步执行和重复模型调用通常导致高推理延迟，严重限制了在时间敏感场景下的可扩展性和可用性。现有方法主要优化任务性能和推理成本，并通常假设顺序执行，这使得它们在并行执行环境下难以有效控制延迟。本文旨在研究并行执行下具有显式延迟监督的多智能体系统学习型编排。

**2. 核心方法和技术创新**
本文提出了**延迟感知多智能体系统（LAMaS）**框架，其核心创新在于：
- **并行执行支持**：通过移除智能体算子之间不必要的层内依赖，实现层内算子并行执行。
- **关键路径优化**：明确将**关键执行路径**（决定并行环境下端到端延迟的最长依赖序列）作为首要优化目标，而非传统的总计算成本。
- **关键路径感知的信用分配**：在强化学习奖励设计中，仅对位于关键路径上的“瓶颈”算子施加延迟惩罚，实现了更精确的梯度信号引导。
该方法基于概率化智能体超网，通过控制器动态地为每个查询生成层级的执行拓扑图，在保持查询级难度感知的同时，学习构建具有更短关键路径的“宽而浅”的并行结构。

**3. 主要实验结果**
在GSM8K、HumanEval和MATH三个复杂推理基准测试上的实验表明：
- **显著降低延迟**：与当前最先进的多智能体架构搜索方法MaAS相比，LAMaS将关键路径长度减少了**38%至46%**。
- **保持或提升性能**：在实现大幅延迟降低的同时，任务性能（准确率/通过率）与基线相当甚至有所提升。
- **有效性验证**：消融实验证明，仅启用并行执行而不进行延迟感知优化，无法达到相同的延迟降低效果，凸显了显式延迟监督的关键作用。

**4. 研究意义和价值**
本研究首次系统性地指出并解决了多智能体系统在并行执行环境下的延迟优化问题。其价值在于：
- **范式转变**：推动多智能体系统设计从传统的“性能-成本”二维优化，转向“性能-成本-延迟”三维权衡，为构建高效、生产就绪的系统提供了关键设计原则。
- **实际应用**：为对话助手、实时决策等对延迟敏感的应用场景部署高效多智能体系统铺平了道路。
- **未来方向**：鼓励后续研究在算法层面更仔细地考虑并行执行下的拓扑结构与延迟行为。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：LAMaS

### **一、 论文旨在解决的核心问题**
论文明确指出，现有**多智能体系统（Multi-Agent Systems, MAS）** 在追求任务性能和降低推理成本时，**普遍忽视了一个关键瓶颈：并行执行环境下的推理延迟（Latency）**。

具体问题表现为：
- **延迟与成本脱钩**：在并行执行中，端到端延迟由**关键执行路径（Critical Path）** 决定，而非所有智能体调用成本的总和。现有方法（如MaAS）优化总成本，但可能产生“窄而深”的拓扑结构，反而延长了关键路径，导致高延迟。
- **并行潜力未开发**：许多先进框架（如AnyMAC）采用严格的**顺序执行**，无法利用智能体间的并行性来减少实际耗时（Wall-clock Latency）。
- **缺乏细粒度、动态的延迟优化**：支持并行的框架（如Aflow）通常是**静态的、任务级**的优化，无法根据每个查询（Query）的难度动态调整执行图，可能造成资源浪费或深度不足。

**总结**：论文的核心问题是——如何让多智能体系统的**自动化编排（Orchestration）** 在保证准确性的同时，**显式地优化并行执行下的推理延迟**，以适用于对实时性要求高的场景。

### **二、 核心创新点**
论文提出了 **LAMaS（Latency-Aware Multi-agent System）** 框架，其创新主要体现在以下三个层面：

1.  **问题定义与视角创新**：
    - **首次明确指出**：在并行执行设定下，优化准确性和总成本**不足以**控制延迟。必须将**延迟作为一级优化目标**进行显式监督。
    - 引入了**关键执行路径**作为并行环境下延迟的核心度量指标和优化对象。

2.  **方法层面的技术创新**：
    - **支持层内并行执行**：通过重构智能体算子（Operator）间的依赖关系，**移除了同一层内算子不必要的依赖**（如MaAS中精炼算子对同层输出的依赖），使得同层算子可以真正并行执行。
    - **关键路径感知的奖励设计**：
        ```python
        # 全局奖励: R = S - λc * C - λt * T
        # 关键路径感知的算子级奖励分配：
        if operator == critical_operator_at_layer:
            R(o) = S - λc * C - λt * T  # 对关键路径上的算子施加延迟惩罚
        else:
            R(o) = S - λc * C           # 非关键路径算子只考虑准确性和成本
        ```
        - 这种**差异化信用分配**确保延迟惩罚只作用于决定整体延迟的“瓶颈”算子，引导控制器学习生成“宽而浅”（更并行）的拓扑结构，而非“窄而深”的结构。
    - **基于代理的稳定延迟度量**：使用**关键路径长度（CP_len）** 作为延迟的代理指标，它综合了输出令牌数和工具执行时间，避免了真实API调用中排队、限流等不稳定因素对训练信号的干扰。

3.  **框架级整合创新**：
    - 将**并行执行机制**、**查询级（Query-level）动态编排**与**关键路径显式优化**三者结合在一个统一的、可学习的框架内。
    - 控制器使用阈值采样机制动态决定每层激活的算子集合，实现了根据查询难度自适应调整并行宽度和推理深度。

### **三、 解决方案概述**
LAMaS的解决方案是一个端到端的**基于学习的延迟感知编排框架**：

1.  **构建并行化执行环境**：修改基础算子，解除同层内部依赖，实现真正的层内并行。
2.  **定义延迟模型**：形式化定义并行下的延迟 `T = Σ层( max(本层各算子耗时) )`，即关键路径耗时之和。
3.  **训练延迟感知控制器**：
    - **输入**：用户查询。
    - **过程**：控制器（策略网络）以层为单位自回归地采样算子，构建执行图（DAG）。
    - **优化**：使用策略梯度方法，以**关键路径感知的奖励**为学习信号，同时优化任务得分（S）、总成本（C）和代理延迟（T）。
4.  **推理与应用**：训练好的控制器能为每个输入查询生成一个**延迟更优的执行拓扑图**，在保持性能的同时显著缩短关键路径。

### **四、 实际价值与意义**
- **性能提升**：实验表明，在GSM8K、HumanEval、MATH三个基准测试上，LAMaS相比SOTA方法MaAS将**关键路径长度减少了38%-46%**，且任务性能持平或略有提升。
- **为生产部署铺路**：该研究将**延迟效率**提升为与准确性、成本并列的核心设计指标，使多智能体系统更适用于**实时交互助手、在线决策**等对延迟敏感的场景。
- **提供新的优化范式**：论文证明了仅启用并行执行而不进行延迟感知优化是无效的（见消融实验），强调了**显式延迟监督**在多智能体系统设计中的必要性，为后续研究指明了方向。

**结论**：LAMaS的核心创新在于**系统性地提出了并行多智能体系统中的延迟优化问题，并提供了一个通过显式关键路径监督来学习低延迟编排的有效解决方案**，推动了多智能体系统向高效、实用的生产级应用迈进。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决多智能体系统（MAS）在并行执行时推理延迟过高的问题，该问题源于现有方法主要优化任务性能和计算成本，而忽略了并行环境下决定延迟的关键执行路径。为此，论文提出了**LAMaS**框架，其核心创新在于通过移除层内不必要的操作依赖以实现层间并行，并设计了一种**关键路径感知的信用分配**奖励机制，在训练中显式地惩罚最长执行序列，从而引导控制器学习生成具有更短关键路径的并行执行拓扑图。实验表明，该方法在保持甚至提升任务性能的同时，能将关键路径长度显著降低38%至46%，证明了在并行多智能体系统设计中显式优化延迟的有效性和必要性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems》针对并行多智能体系统的延迟优化问题，提出了明确的创新点。以下是逐条分析：

### 1. **问题定义的创新：首次将并行执行下的延迟作为核心优化目标**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：现有的大多数多智能体编排框架（如MaAS、AgentDropout、G-designer）主要优化**任务性能**和**总推理成本**（如总token消耗）。它们通常隐含地假设延迟与总成本成正比（`Latency ∝ ∑Cost_node`），或者仅支持**顺序执行**（如AnyMAC、SeqCV）。
    - **本文方法**：明确指出在**并行执行**环境下，端到端延迟由**关键执行路径**决定，而非所有操作节点的成本总和。因此，仅优化总成本无法有效控制并行下的实际延迟。
- **解决的具体问题/带来的优势**：
    - 解决了现有方法在并行场景下**延迟控制不力**的根本问题。明确了“窄而深”（总成本低但关键路径长）的拓扑结构在并行下可能比“宽而浅”（总成本高但关键路径短）的结构延迟更高。这为设计真正低延迟的多智能体系统提供了正确的优化方向。

### 2. **方法论的创新：提出LAMaS框架，支持层内并行执行并显式优化关键路径**
- **相比以往方法的改进/不同之处**：
    - **架构层面**：修改了基础操作符（如`SelfRefine`）的依赖关系，消除了同一层内操作符之间的不必要依赖，实现了**真正的层内并行执行**。
        - *对比*：在基线MaAS中，即使拓扑是DAG，同一层内的细化操作符也依赖于同层其他操作符的输出，这引入了隐式的同步屏障，导致实际执行仍是顺序或半顺序的。
    - **优化目标层面**：在强化学习奖励函数中，**显式地加入了关键路径延迟惩罚项**（`-λ_t * T`），其中`T`是关键路径的延迟代理。
        - *对比*：以往方法（如MaAS）的奖励函数只包含任务得分和总成本惩罚（`S - λ_c * C`），没有对延迟的直接监督。
- **解决的具体问题/带来的优势**：
    - **显著降低关键路径长度**：实验表明，在GSM8K、HumanEval、MATH三个基准测试上，相比SOTA基线MaAS，关键路径长度减少了**38%至46%**。
    - **维持或提升任务性能**：在大幅降低延迟的同时，准确率（Pass@1或Accuracy）与基线相当甚至略有提升，实现了效率与效果的更好权衡。

### 3. **算法设计的创新：提出关键路径感知的信用分配机制**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：如果使用全局延迟惩罚，会将惩罚均等地分配给轨迹中的所有操作符。这在并行环境下是**错误的信用分配**，因为只有位于关键路径上的“瓶颈”操作符才对最终延迟负责。
    - **本文方法**：设计了**关键路径感知的信用分配**。在每一层中，只对执行时间最长的“关键操作符”施加延迟惩罚，而对非关键路径上的操作符只施加成本惩罚。
        ```python
        # 公式示意
        if o == o*_ℓ (关键操作符):
            R(o) = S - λ_c * C - λ_t * T
        else:
            R(o) = S - λ_c * C
        ```
- **解决的具体问题/带来的优势**：
    - 解决了并行环境下**奖励信号混淆**的问题，使学习信号更精确。消融实验（表5）证明，移除此机制会导致性能下降和关键路径变长，验证了其有效性。
    - 引导控制器更智能地构建拓扑，避免惩罚那些可以并行执行、不增加整体延迟的有益操作，从而更有效地探索“宽而浅”的高并行度结构。

### 4. **评估指标的创新：使用稳定可复现的“关键路径长度”作为延迟代理**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：可能直接测量不稳定的真实API墙钟时间，受网络波动、队列延迟、速率限制影响大，不适合作为优化目标和对比指标。
    - **本文方法**：定义了**关键路径长度**作为延迟代理：`CP_len = Σ_{层ℓ} max_{操作符o}(N_out(o) + γ * t_tool(o))`。它综合了输出token数（代表LLM调用时间）和工具执行时间。
- **解决的具体问题/带来的优势**：
    - 提供了一个**稳定、可复现、与硬件解耦**的延迟度量标准，便于算法优化和公平比较。
    - 抓住了并行执行下延迟的本质，即由各层最慢操作决定的路径总和，使优化目标与真实延迟目标保持一致。

### 总结
本文的核心创新在于**系统性**地将**并行执行环境下的延迟优化**确立为多智能体编排的一等目标，并从**问题定义、框架设计、学习算法、评估指标**四个层面给出了完整的解决方案。它突破了以往工作仅关注顺序执行或粗粒度并行的局限，通过精细化的依赖解除和关键路径优化，实现了多智能体系统在**延迟-性能-成本**三维权衡上的显著提升，为部署实时性要求高的多智能体应用提供了关键技术。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

### 一、 使用的数据集
论文在三个复杂推理基准数据集上进行了评估：
1.  **GSM8K**：小学数学应用题，需要多步推理。
2.  **HumanEval**：代码生成任务，评估从自然语言描述生成Python函数的功能正确性。
3.  **MATH**：竞赛级数学问题，需要更复杂的多步推理。

### 二、 核心评价指标
论文采用了三个维度的评价指标：
1.  **任务性能 (Task Performance)**：
    *   HumanEval：报告 **Pass@1**（一次生成通过测试的比例）。
    *   GSM8K 和 MATH：报告 **准确率 (Accuracy)**。
2.  **执行成本 (API Cost)**：
    *   测量在整个测试集上调用LLM API所产生的 **总货币成本（美元）**。
3.  **延迟代理指标 (Latency Proxy)**：
    *   由于真实API的端到端延迟受网络波动、队列等影响不稳定，论文提出了一个稳定、可复现的代理指标——**关键路径长度 (Critical Path Length, CP len)**。
    *   **计算公式**：`CP_len = Σ_{每层ℓ} max_{该层算子o} (N_out(o) + γ * t_tool(o))`
        *   `N_out(o)`：算子`o`生成的输出token数。
        *   `t_tool(o)`：外部工具执行的挂钟时间（秒）。
        *   `γ`：缩放因子（实验中设为50），将工具时间映射为虚拟token数。
    *   **物理意义**：该指标近似衡量了在并行执行下，决定整体延迟的**关键路径**上的累积“工作量”，值越小代表潜在延迟越低。

### 三、 对比的基线方法
论文与两大类基线方法进行了对比：

**1. 学习型多智能体系统基线 (主要对比对象)**：
*   **MaAS**：当前最先进的多智能体架构搜索方法，支持DAG拓扑，但优化目标为任务性能和总成本，**未显式优化延迟**。这是LAMaS最主要的对比基线。

**2. 固定拓扑的启发式提示方法**：
*   **Generate**：单步生成，无额外推理。
*   **Gen-CoT**：生成显式的思维链后给出最终答案。
*   **CoT*5+SC**：并行运行5个思维链，然后通过自一致性采样答案。

### 四、 关键性能提升与结论

**1. 与主要基线MaAS的对比（核心结论）**：
*   **关键路径长度大幅降低**：在**所有三个数据集**上，LAMaS相比MaAS实现了**38.0% 到 46.1%** 的关键路径长度减少。
    *   GSM8K: **-38.0%** (从1474.6降至913.5)
    *   HumanEval: **-42.4%** (从1810.8降至1042.7)
    *   MATH: **-46.1%** (从2218.5降至1195.8)
*   **任务性能保持甚至提升**：在显著降低CP len的同时，LAMaS在GSM8K和MATH上的准确率与MaAS相当或略有**提升**（GSM8K: 93.13% -> 93.37%；MATH: 51.23% -> 52.26%），在HumanEval上仅有微小下降（93.00% -> 92.11%）。
*   **结论**：这证明了**显式优化延迟（关键路径）能有效引导控制器学习到并行度更高、关键路径更短的执行图**，且不会牺牲任务性能。

**2. 与固定拓扑基线的对比**：
*   **性能-延迟权衡更优**：LAMaS在任务性能上显著优于简单的单步或浅层推理方法（Generate, Gen-CoT）。虽然CoT*5+SC等方法性能接近，但其**成本远高于LAMaS**，且其关键路径长度并未带来相应的性能优势。
*   **结论**：学习型编排（LAMaS）比固定的启发式策略更能有效地在**性能、成本和延迟**之间取得平衡。

**3. 消融实验验证**：
*   **仅启用并行执行不够**：一个消融变体（移除层内依赖允许并行，但训练时不加延迟惩罚 `λ_t=0`）的CP len远高于LAMaS，且成本更高或性能更差。这证明**仅提供并行执行能力，而不在目标中显式优化延迟，控制器不会自动学习到低延迟的拓扑**。
*   **关键路径信用分配有效**：移除关键路径感知的信用分配（将延迟惩罚均匀施加于所有算子），会导致CP len增加和性能轻微下降，证明了该设计对精准优化的重要性。

**4. 准确性-延迟权衡曲线**：
*   通过调节延迟惩罚系数 `λ_t`，LAMaS可以平滑地控制关键路径长度，形成一条**准确性-延迟权衡曲线**，使用户可以根据实际场景需求灵活选择操作点。

### 总结
论文通过系统的实验表明，LAMaS框架成功地将多智能体系统的**关键路径长度（延迟代理）显著降低了38-46%**，同时在多个基准测试上**维持或改进了任务性能**。这强有力地支持了论文的核心论点：在设计高效的多智能体系统时，**必须超越传统的仅优化精度和成本的思路，在并行执行环境下对延迟进行显式监督和优化**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.10560v1)
- [HTML 版本](https://arxiv.org/html/2601.10560v1)
