# STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order

**相关性评分**: 6.0/10

**排名**: #23


---


## 基本信息

- **arXiv ID**: [2601.08107v1](https://arxiv.org/abs/2601.08107v1)
- **发布时间**: 2026-01-13T00:57:45Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Chengyang Gu, Yuxin Pan, Hui Xiong, Yize Chen

## 关键词

offline Reinforcement Learning, fine tune, goal conditioned reinforcement learning

## 一句话总结

STO-RL 是一个利用大型语言模型生成时序子目标序列，通过奖励塑形解决稀疏奖励下离线强化学习长时程任务的框架。

## 摘要

Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.

## 详细分析

## 论文《STO-RL：基于LLM引导的子目标时序顺序的稀疏奖励离线强化学习》详细摘要

### 1. 研究背景和动机
离线强化学习（Offline RL）旨在从预先收集的数据集中学习策略，避免了在线交互的高成本和风险。然而，在处理具有**稀疏奖励**的**长视野任务**时，现有方法面临巨大挑战。传统的目标条件（Goal-Conditioned）和分层（Hierarchical）离线RL方法虽然通过任务分解和生成中间奖励来缓解问题，但通常**忽视了子目标之间的时序依赖关系**，并且依赖于不精确的奖励塑形（Reward Shaping），导致策略学习效率低下且容易陷入次优解。

### 2. 核心方法和技术创新
本文提出了 **STO-RL** 框架，其核心创新在于**利用大型语言模型（LLM）的常识推理能力，为任务生成具有时序顺序的子目标序列**。
- **LLM引导的子目标发现**：通过向LLM输入任务指令和环境描述，自动生成一个有序的子目标序列 `{G1, G2, ..., GK}` 以及一个将每个状态映射到对应子目标阶段（进度索引 `k_t`）的函数 `h`。
- **基于子目标时序的奖励塑形**：提出了一种新颖的**基于势能的奖励塑形（PBRS）** 方法。其势函数定义为 `Φ(s_t) = - (t/T) * (1/k_t)`，其中 `t` 是时间步，`T` 是最大回合长度，`k_t` 是当前状态的进度索引。该设计将稀疏的终端奖励转化为密集信号，并具有以下理论保证：
    1.  **奖励正向进展**：能够实现子目标正向推进（`k_{t+1} > k_t`）的状态转移会获得更高的奖励（**定理1**）。
    2.  **惩罚无效徘徊**：在折扣因子 `γ > (T-1)/T` 的条件下，未能推进子目标的转移会获得负奖励（**定理2**）。
    3.  **偏好更短轨迹**：在成功到达目标的轨迹中，更短的轨迹将获得更高的累积回报（**定理3**）。
- **离线策略训练**：使用上述方法对离线数据集中的奖励进行重塑后，采用标准的离线RL算法（如IQL）进行策略训练，从而高效利用LLM提供的时序结构知识。

### 3. 主要实验结果
在四个稀疏奖励基准任务（离散的`CliffWalking`、`FourRoom`和连续的`PointMaze-UMaze`、`PointMaze-Medium`）上进行了评估：
- **性能提升**：STO-RL在连续任务中取得了最高的成功率（例如，在`PointMaze-UMaze`上为0.68，显著优于IQL的0.24和HIQL的0.47），并生成了更短的轨迹。
- **学习效率**：在所有任务中，STO-RL都表现出**更快的收敛速度**，价值函数景观更清晰，能更早地引导智能体找到最优路径。
- **鲁棒性验证**：消融实验表明，即使LLM生成的子目标序列存在部分错误或不精确，STO-RL凭借其理论严谨的奖励塑形机制，依然能保持有竞争力的性能，展现了良好的鲁棒性。

### 4. 研究意义和价值
STO-RL成功地将**LLM的高层任务理解与规划能力**与**离线RL的低层策略学习能力**相结合，为解决长视野稀疏奖励离线RL问题提供了一个新颖、有效且可扩展的框架。
- **理论贡献**：提出了一个具有严格理论保证的、基于子目标时序的奖励塑形方案，确保了策略最优性并有效引导学习。
- **实践价值**：该方法减少了对精确人工奖励设计或复杂分层结构学习的依赖，通过利用LLM的零样本/少样本推理能力，为在现实世界（如机器人、自动驾驶）中应用离线RL处理复杂任务提供了新思路。
- **启发性**：展示了外部知识模型（如LLM）如何以可解释、结构化的方式（时序子目标）赋能传统RL算法，启发了未来更多结合基础模型与强化学习的研究方向。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：STO-RL

### **一、 论文拟解决的核心问题**
论文旨在解决**稀疏奖励下的离线强化学习（Offline RL）在长视野任务中的低效问题**。具体而言，现有方法面临两大挑战：
1.  **子目标生成困难**：传统的目标条件或分层离线RL方法难以从有限的离线数据中自动发现**具有时序依赖关系**的有效子目标序列。
2.  **奖励塑形不精确**：常用的奖励塑形方法（如基于距离的势函数）可能引入局部最优解，误导策略学习，导致最终策略性能不佳。

### **二、 核心技术创新点**
STO-RL 的核心创新在于**将大语言模型的世界知识与时序感知的奖励塑形理论相结合**，为离线RL提供了一个结构化的学习框架。其创新点可分解为：

- **LLM引导的时序有序子目标生成**：
    - **方法**：利用LLM的自然语言理解与推理能力，根据任务描述和环境上下文，**零样本或少样本**地生成一个**具有明确时间顺序**的子目标序列 `{G1, G2, ..., GK}`。
    - **关键输出**：不仅生成子目标，还提供一个映射函数 `h: S → {1,...,K}`，将数据集中的每个状态 `s_t` 映射到其对应的“进度索引” `k_t`。这为引入时序结构奠定了基础。

- **子目标时序感知的势基奖励塑形**：
    - **核心设计**：设计了一个新颖的势函数：`Φ(s_t) = - (t / T) * (1 / k_t)`。
        - `t / T`：鼓励智能体尽快完成任务（时间紧迫性）。
        - `1 / k_t`：鼓励智能体尽快推进到后续子目标（进度紧迫性）。
    - **理论保证**：
        1.  **正向进度偏好**（Theorem 1）：相比未推进子目标进度的转移，能实现**正向进度**（`k_{t+1} > k_t`）的状态转移会获得严格更高的塑形奖励。
        2.  **惩罚冗余轨迹**（Theorem 2 & 3）：在折扣因子 `γ > (T-1)/T` 的条件下，未能推进进度的转移会获得**负奖励**。因此，更短的成功轨迹比更长的成功轨迹获得更高的累积回报，从而引导智能体寻找最优（最短）路径。
    - **优势**：此方法将稀疏的终端奖励转化为密集的、理论上有保障的中间信号，同时避免了引入次优解。

- **框架级创新**：**统一了高层规划与低层学习**。STO-RL 利用LLM进行高层任务分解和时序规划，然后通过理论严谨的奖励塑形将这种时序结构**注入**到离线数据集中，从而指导底层策略的优化。这桥接了LLM的符号化、常识性知识与RL的低层连续决策。

### **三、 解决方案流程**
STO-RL 的解决路径清晰，形成了一个完整的闭环：

1.  **输入**：任务的自然语言描述 `l`、环境状态空间 `S`、离线数据集 `D`。
2.  **LLM引导分解**：构建提示词，让LLM输出**时序有序的子目标序列**和**状态-子目标索引映射函数 `h`**。
3.  **奖励塑形与数据增强**：对离线数据集 `D` 中的每一个转移 `(s_t, a_t, s_{t+1})`：
    - 根据映射函数 `h` 得到 `k_t` 和 `k_{t+1}`。
    - 计算势函数值 `Φ(s_t)` 和 `Φ(s_{t+1})`。
    - 生成稠密塑形奖励：`r' = r + γΦ(s_{t+1}) - Φ(s_t)`。
    - 用 `r'` 替换原始稀疏奖励 `r`，得到增强数据集 `D'`。
4.  **离线策略训练**：使用任何标准的离线RL算法（论文中使用IQL）在增强数据集 `D'` 上训练最终策略 `π*`。

### **四、 实际价值与验证**
- **性能提升**：在四个稀疏奖励基准任务（CliffWalking, FourRoom, PointMaze-UMaze, PointMaze-Medium）上，STO-RL 在**成功率、学习速度和轨迹长度**上均优于先进的基线方法（IQL, GC-BC, HIQL）。
- **学习加速**：价值函数图谱显示，STO-RL 能产生更清晰、更准确的价值梯度，引导智能体更快收敛。
- **强鲁棒性**：消融实验表明，即使LLM生成的子目标序列**不完美或包含噪声**（如个别子目标导向死胡同），STO-RL 凭借其理论奖励塑形机制，依然能保持有竞争力的性能。这降低了方法对LLM输出完美性的依赖，提升了实用性。
- **可扩展性**：框架兼容离散和连续动作空间，且不依赖于特定的离线RL算法，具有良好的通用性。

### **总结**
**STO-RL 的核心创新是创造性地利用LLM作为“时序规划器”，生成具有因果关系的子目标结构，并设计了一种与之紧密耦合的、具有理论最优性保证的势基奖励塑形方法，从而将高层语义规划无缝地转化为驱动离线策略优化的稠密学习信号。** 它有效地解决了长视野稀疏奖励离线RL中“不知如何分解”和“不知如何奖励”两大痛点，为结合大型基础模型与强化学习提供了一个新颖且有效的范例。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**稀疏奖励下的离线强化学习（Offline RL）在长视野任务中效果不佳**的核心问题。现有方法通常难以生成有效的子目标序列，且忽略了子目标间的时序依赖关系，导致策略学习效率低下。

为此，论文提出了 **STO-RL 框架**，其核心创新在于**利用大语言模型（LLM）根据任务指令生成具有时序顺序的子目标序列及其状态映射**，并基于此结构设计了一种**基于势能的奖励塑形（PBRS）方法**。该方法将稀疏的终端奖励转化为密集的、鼓励按顺序完成子目标的奖励信号，从而在理论上避免了次优解，并利用增强后的数据集高效训练离线策略。

实验结果表明，该方法在多个离散和连续环境的稀疏奖励基准测试中，**在收敛速度、最终成功率和轨迹长度方面均显著优于现有的离线目标条件RL和分层RL基线方法**，并且对LLM生成的不完美子目标序列展现出良好的鲁棒性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## STO-RL 论文创新点分析

这篇论文提出了一种名为 **STO-RL** 的离线强化学习框架，旨在解决稀疏奖励、长视野任务中的关键挑战。其核心创新在于**将大语言模型（LLM）的规划能力与基于势函数的奖励塑形（PBRS）理论相结合，并特别强调了子目标之间的时序依赖关系**。以下是其相对于已有工作的明确创新点：

---

### 1. **利用 LLM 生成具有明确时序顺序的子目标序列**
   - **相比以往方法的改进/不同之处**：
     - **传统方法**：大多数离线目标条件强化学习（Goal-Conditioned RL）或分层强化学习（HRL）方法依赖于从数据中**自主学习**子目标（例如，通过 hindsight relabeling、向量量化或内在奖励）。这些方法通常**忽略或难以建模子目标之间的时序依赖关系**，导致生成的子目标序列可能不符合任务的实际进展逻辑。
     - **现有 LLM 应用**：已有工作利用 LLM 进行高层次规划或生成奖励函数，但往往**将两者割裂**。例如，LLM 规划器生成子目标后，依赖预训练的低层策略执行，缺乏与奖励信号的深度整合；或者 LLM 用于奖励塑形时，未充分利用其生成的**时序结构**信息。
     - **STO-RL 的做法**：**直接提示 LLM**，根据任务指令和环境描述，生成一个**具有明确时间顺序的子目标序列** `{G1, G2, ..., GK}`，并同时输出一个**映射函数 `h`**，将每个状态 `s_t` 映射到其对应的子目标进度索引 `k_t`。这**显式地捕获了子目标之间的因果和时序依赖**。
   - **解决的具体问题/带来的优势**：
     - **解决了子目标发现中的时序一致性问题**：为 agent 提供了符合任务逻辑的、结构化的中间步骤指导，避免了子目标顺序混乱导致的学习效率低下和策略次优。
     - **降低了子目标发现的难度**：利用 LLM 强大的常识推理和语言理解能力，**无需从稀疏、有噪声的离线数据中进行困难的自主发现**，实现了“零样本”或“少样本”的高质量规划。
     - **为后续的奖励塑形提供了可靠的结构基础**：时序顺序是后续设计势函数的关键输入。

### 2. **提出基于子目标时序顺序的势函数奖励塑形（Subgoal-Temporal-Order-Aware PBRS）**
   - **相比以往方法的改进/不同之处**：
     - **传统奖励塑形**：常见的基于距离的势函数（如到目标的欧氏距离）可能引入**局部最优**，误导策略学习。标准的 PBRS 虽然能保证策略最优性不变，但如何设计一个能有效反映长视野任务进展的势函数仍然是一个挑战。
     - **STO-RL 的做法**：设计了一个**新颖的势函数**：`Φ(s_t) = - (t / T) * (1 / k_t)`。其中 `t` 是时间步，`T` 是最大回合长度，`k_t` 是 LLM 提供的当前状态所属的子目标阶段索引。
     - **关键机制**：该势函数同时编码了**时间紧迫性**（`t/T` 项，鼓励尽早完成）和**子目标进度**（`1/k_t` 项，鼓励向后续子目标推进）。由此得到的塑形奖励 `r'` 会**严格偏好**能产生“正向进展”（`k_t < k_{t+1}`）的状态转移。
   - **解决的具体问题/带来的优势**：
     - **将稀疏的终端奖励转化为密集的、具有时序一致性的学习信号**：使 agent 在每一步都能获得关于其是否朝着正确方向前进的反馈，极大加速了离线策略优化。
     - **理论保证偏好更优轨迹**：论文通过 **Theorem 1, 2, 3** 严格证明了：
       1.  实现“正向进展”的转移会获得更高的塑形奖励。
       2.  在折扣因子 `γ` 足够大时，未产生进展的转移会获得**负奖励**，从而被抑制。
       3.  **更短的成功轨迹会获得更高的累计回报**。这直接鼓励 agent 寻找更高效、更直接的路径，避免了在次优解中徘徊。
     - **与 LLM 规划深度整合**：奖励塑形不再依赖人工设计或简单的几何距离，而是建立在 LLM 提供的**高层次任务语义分解**之上，使奖励信号与任务逻辑对齐。

### 3. **构建了一个统一框架，将 LLM 规划与离线 RL 训练无缝衔接**
   - **相比以往方法的改进/不同之处**：
     - **传统范式**：LLM 规划与 RL 训练往往是**松耦合**的。例如，LLM 作为独立的规划模块，其输出（子目标）作为固定指令给 RL agent 执行，两者之间没有基于奖励的反馈闭环。在离线设定下，这种脱节可能导致规划与数据分布不匹配。
     - **STO-RL 的做法**：提出了一个清晰的端到端流程（如 Algorithm 1 和图1所示）：
       1.  **LLM 引导的时序结构提取**：输入任务描述，得到 `(G, h)`。
       2.  **数据集增强**：利用 `h` 和提出的势函数，为离线数据集 `D` 中的**每一个转移**计算塑形奖励 `r'`，生成增强数据集 `D'`。
       3.  **标准离线 RL 训练**：在 `D'` 上使用任何离线 RL 算法（如 IQL）训练最终策略。
     - **关键点**：LLM **仅在前置阶段使用一次**，用于生成任务分解的结构知识。之后，**所有学习过程都在增强后的静态数据集上进行**，完全符合离线 RL 的设定，无需在线交互或调用 LLM。
   - **解决的具体问题/带来的优势**：
     - **解决了 LLM 与低层策略的脱节问题**：通过奖励塑形，将 LLM 的高层知识**注入**到 agent 的学习目标中，使低层策略在训练过程中自然学会遵循 LLM 提出的时序结构，实现了高层规划与低层执行的**对齐**。
     - **提供了一个实用且可扩展的解决方案**：框架模块化，LLM 部分和离线 RL 算法部分可以替换。实验表明，即使 LLM 生成的子目标序列不完美，该框架仍能保持稳健性能，降低了对 LLM 输出精度的苛刻要求。
     - **充分利用了离线数据**：整个方法的核心是对静态数据集的**再标注**，而非与环境交互，安全且高效。

### 4. **实证验证了方法对不完美 LLM 输出的鲁棒性**
   - **相比以往方法的改进/不同之处**：
     - **常见假设**：许多依赖 LLM 的工作隐含假设其输出是完美或近乎完美的。
     - **STO-RL 的验证**：论文专门设计了**消融实验**（第4.3节），使用多个不同的 LLM（ChatGPT 5.0, Grok 3, DeepSeek-R1, Qwen3-Max）生成可能包含错误或误导性子目标的序列，并测试 STO-RL 的性能。
   - **解决的具体问题/带来的优势**：
     - **证明了方法的实用性**：即使 LLM 的规划能力有限或受环境干扰产生部分错误，STO-RL 框架中的奖励塑形机制仍然能够引导策略学习到有效的行为，性能下降可控。这增强了该方法在现实复杂场景中的适用性。
     - **揭示了时序结构的部分有效性即可带来增益**：只要 LLM 生成的序列**大体上**保持了正确的时序顺序，就能为学习提供有价值的指导。

---

## 总结
STO-RL 的核心创新在于**创造性地将 LLM 的语义规划能力（产生时序子目标序列）与强化学习的经典理论（势函数奖励塑形）相结合**，形成了一个针对稀疏奖励离线 RL 问题的完整解决方案。它主要解决了：
1.  **子目标发现的时序依赖难题** -> 通过 LLM 零样本生成。
2.  **稀疏奖励下的低学习效率难题** -> 通过融入时序信息的 PBRS 提供密集、定向的反馈。
3.  **高层规划与低层学习脱节的难题** -> 通过统一的框架，用奖励信号对齐两者。

其实验在离散和连续控制任务上均展示了**更快的收敛速度、更高的成功率和更短（更优）的求解轨迹**，验证了这些创新点的有效性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文STO-RL在四个稀疏奖励的离线强化学习基准任务上进行了全面评估，证明了其有效性。以下是详细的实验设置、对比方法和关键结果。

### 一、 使用的数据集与评价指标

#### **1. 数据集**
实验使用了四个经典的稀疏奖励目标到达任务，涵盖离散和连续空间：
- **离散动作空间任务**（来自标准RL库）：
    - **CliffWalking**：4x12网格世界，起点(3,0)，终点(3,11)，悬崖区域(3,1-10)。
    - **FourRoom**：11x11网格世界，分为四个房间，起点(0,0)，终点(10,10)。
- **连续动作空间任务**（来自D4RL基准）：
    - **PointMaze-UMaze**：U型迷宫，2自由度小球导航任务。
    - **PointMaze-Medium**：更复杂的迷宫，导航难度更高。

**数据集生成方式**：每个任务通过混合**专家策略**（使用值迭代或PPO训练）和**随机策略**（以一定概率执行）收集1000条轨迹，构成离线数据集。数据集的平均成功率反映了任务的挑战性（例如，FourRoom数据集成功率仅14%）。

#### **2. 评价指标**
- **主要指标**：
    1.  **成功率 (Success Rate)**：智能体在测试中成功到达目标的比例。
    2.  **平均步数/轨迹长度 (Average Steps/Trajectory Length)**：成功轨迹的平均步数，衡量策略的**效率**。
- **辅助指标**：
    - **学习曲线 (Learning Curves)**：训练迭代过程中的成功率变化，衡量**收敛速度**。
    - **值函数可视化 (Value Maps)**：定性分析学习到的价值景观。

### 二、 对比的基线方法
论文与三类先进的离线RL方法进行了对比：
1.  **Implicit Q-Learning (IQL)**：经典的离线RL算法，无分层或子目标分解。
2.  **Goal-Conditioned Behavioral Cloning (GC-BC)**：基于行为克隆的目标条件模仿学习方法。**为公平比较，GC-BC使用了与STO-RL相同的LLM生成的子目标**。
3.  **Hierarchical Implicit Q-Learning (HIQL)**：最新的分层离线目标条件RL方法，联合学习表示函数、高层策略和底层策略。

### 三、 关键性能提升与结论

#### **1. 整体性能优势**
在四个任务上，STO-RL在**成功率**和**轨迹效率**上均表现出色或具有竞争力。

| 方法 | CliffWalking | FourRoom | PointMaze-UMaze | PointMaze-Medium |
| :--- | :--- | :--- | :--- | :--- |
| | 成功率 / 平均步数 | 成功率 / 平均步数 | 成功率 / 平均步数 | 成功率 / 平均步数 |
| **数据集 (平均)** | 0.50 / 69.9 | 0.14 / 97.2 | 0.49 / 191.2 | 0.51 / 415.1 |
| **IQL** | 1.00 / **13.0** | 1.00 / **20.0** | 0.24 / 199.3 | 0.17 / 467.7 |
| **GC-BC** | 1.00 / **13.0** | 1.00 / **20.0** | 0.21 / 198.1 | **0.58** / 406.8 |
| **HIQL** | 1.00 / **13.0** | 1.00 / **20.0** | 0.47 / 197.0 | 0.38 / 416.6 |
| **STO-RL (Ours)** | 1.00 / **13.0** | 1.00 / **20.0** | **0.68** / **195.1** | 0.55 / **380.7** |

**核心结论**：
- **在简单离散任务中**：所有方法（包括STO-RL）都能找到最优路径（最短步数）并达到100%成功率。**但STO-RL的收敛速度最快**（见图4学习曲线）。
- **在复杂连续任务中**：STO-RL展现出**显著优势**。
    - 在**PointMaze-UMaze**上，STO-RL的成功率（0.68）远超所有基线（最高0.47），且轨迹最短。
    - 在**PointMaze-Medium**上，STO-RL的成功率（0.55）与表现最好的GC-BC（0.58）相当，但**平均轨迹长度（380.7步）显著更短**，比GC-BC短约26步。这证明了STO-RL在**避免次优解、寻找更高效路径**方面的能力。

#### **2. 更快的收敛速度（回答研究问题2）**
学习曲线（图4, 图7）清晰表明，无论是在离散还是连续任务中，STO-RL都能以更少的训练迭代次数达到高成功率。这归因于其基于时间顺序的奖励塑形提供了**更密集、更准确的学习信号**，加速了策略优化。

#### **3. 对不完美LLM输出的鲁棒性（消融实验）**
论文评估了使用不同LLM（ChatGPT 5.0, Grok 3, DeepSeek-R1, Qwen3-Max）生成子目标序列时STO-RL的性能。
- **发现**：即使LLM生成的子目标序列存在部分错误（如将智能体导向死胡同），只要**时间顺序大体正确**，STO-RL仍能保持有竞争力的性能（见图8，在Medium任务上各LLM的成功率均高于大部分基线）。
- **结论**：STO-RL框架对LLM生成子目标的质量具有一定的**容错性和鲁棒性**，这增强了其实用性。

### 四、 性能提升的原因分析
论文通过理论分析和实验验证，将STO-RL的优势归因于两个核心创新点的结合：
1.  **LLM引导的时间有序子目标分解**：提供了符合任务逻辑的、结构化的中间步骤，克服了传统方法难以发现有效子目标或忽略其时间依赖性的问题。
2.  **基于子目标时间顺序的势能奖励塑形**：
    - **定理1**：为正进展（向下一子目标前进）的转移提供严格更高的奖励。
    - **定理2 & 3**：在适当折扣因子下，对非正进展的转移施加惩罚，从而**激励更短、更高效的轨迹**。
    - 这共同作用，在离线数据集中创造了**密集且时间一致的奖励信号**，引导智能体学习更优策略。

**总结**：STO-RL通过巧妙结合LLM的世界知识与理论严谨的奖励塑形，在稀疏奖励的离线RL任务中，实现了**更高的成功率、更快的收敛速度以及更优（更短）的解决方案**，为解决长视野、稀疏奖励的离线决策问题提供了一个有效且鲁棒的框架。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.08107v1)
- [HTML 版本](https://arxiv.org/html/2601.08107v1)
