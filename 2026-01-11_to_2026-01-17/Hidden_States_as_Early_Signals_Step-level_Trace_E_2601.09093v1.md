# Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling

**相关性评分**: 7.0/10

**排名**: #10


---


## 基本信息

- **arXiv ID**: [2601.09093v1](https://arxiv.org/abs/2601.09093v1)
- **发布时间**: 2026-01-14T02:54:55Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Zhixiang Liang, Beichen Huang, Zheng Wang, Minjia Zhang

## 关键词

Inference Efficiency, Inference Acceleration, Lightweight Architecture

## 一句话总结

该论文提出STEP框架，通过基于隐藏状态的步级评估和动态剪枝，显著提升大型语言模型推理效率，减少延迟并提高准确性。

## 摘要

Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process has relied on similarity-based or confidence-based pruning, but these signals do not reliably indicate trace quality. To address these limitations, we propose STEP: Step-level Trace Evaluation and Pruning, a novel pruning framework that evaluates reasoning steps using hidden states and dynamically prunes unpromising traces during generation. We train a lightweight step scorer to estimate trace quality, and design a GPU memory-aware pruning strategy that triggers pruning as the GPU memory is saturated by KV cache to reduce end-to-end latency. Experiments across challenging reasoning benchmarks demonstrate that STEP reduces end-to-end inference latency by 45%-70% on average compared to self-consistency while also improving reasoning accuracy. Our code is released at: https://github.com/Supercomputing-System-AI-Lab/STEP

## 详细分析

## 论文摘要：利用隐藏状态作为早期信号：面向高效测试时扩展的步骤级轨迹评估与剪枝

**1. 研究背景和动机**
大型语言模型（LLMs）通过测试时扩展（如自洽性方法）生成多条推理轨迹来提升推理能力，但这也带来了巨大的计算开销和端到端延迟。现有加速方法主要依赖基于相似性或置信度的剪枝，但这些信号无法可靠指示轨迹质量。此外，现有方法忽视了推理系统层面的瓶颈：当多条长轨迹的KV缓存耗尽GPU内存时，系统会将轨迹放入等待队列，导致严重的等待延迟，这构成了端到端延迟的主要瓶颈。

**2. 核心方法和技术创新**
本文提出了 **STEP** 框架，其核心创新在于：
- **步骤级轨迹评估**：利用推理模型在步骤边界处的**隐藏状态**作为早期质量信号，训练一个轻量级的步骤评分器（一个两层MLP），在生成过程中实时评估每个推理步骤的质量。
- **内存感知的剪枝触发机制**：摒弃了基于固定阈值或时间表的剪枝触发方式，改为**以GPU内存利用率为信号**。当KV缓存即将耗尽内存时，立即剪枝当前得分最低的轨迹，释放其资源，从而彻底消除等待队列和由此产生的延迟。
- **动态剪枝与加权投票**：在生成过程中动态计算轨迹级平均分，并在内存饱和时剪枝最差的轨迹。最终，根据各完成轨迹的得分进行加权投票得出最终答案。

**3. 主要实验结果**
在AIME-25、HMMT-24/25、GPQA-D等具有挑战性的推理基准上，使用Qwen3-4B、DeepSeek-R1-8B、Phi-4(14B)等模型进行验证：
- **效率**：与标准的自洽性方法相比，STEP平均降低**45%–70%** 的端到端推理延迟。
- **精度**：在显著加速的同时，推理准确率相比自洽性提升了**+0.4 到 +7.5 个百分点**，在多数设置下优于或持平其他剪枝基线方法（如Slim-SC、DeepConf）。
- **关键分析**：实验表明，基于隐藏状态的评分器在推理早期就能有效区分正确与错误轨迹；性能提升主要源于**消除了由内存引起的等待时间**，而不仅仅是减少了生成的总令牌数。

**4. 研究意义和价值**
本研究的意义在于：
- **技术贡献**：首次系统性地将**隐藏状态这一内部表征**用于实时轨迹质量评估，并结合**推理系统内存管理**来指导剪枝决策，为高效测试时扩展提供了新范式。
- **实际价值**：STEP在几乎不引入额外计算开销的前提下，实现了精度与速度的双重提升，使得LLM在复杂推理任务上的大规模并行采样部署变得更加可行和经济。
- **启发方向**：工作强调了在算法设计中考虑底层系统行为（如内存瓶颈）的重要性，为未来高效LLM推理系统的设计提供了重要思路。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文想解决的核心问题**
论文旨在解决大语言模型（LLM）在**测试时扩展**（Test-Time Scaling，如Self-Consistency）中面临的两个关键瓶颈：
1.  **计算效率低下**：生成多条冗长的推理轨迹（trace）并进行多次采样，导致计算开销巨大、端到端延迟极高。
2.  **现有剪枝方法不可靠**：现有的加速方法（基于相似性或置信度剪枝）所使用的信号无法可靠地指示推理轨迹的质量，可能误剪正确的轨迹或保留错误的轨迹。

### **核心创新点**
论文提出了 **STEP框架**，其创新性主要体现在以下两个层面：

#### **1. 算法层面：基于隐藏状态的早期轨迹质量评估**
- **新信号源**：摒弃了不可靠的文本相似性或模型置信度，转而利用LLM在**推理步骤边界处的隐藏状态**作为评估轨迹质量的早期信号。论文发现，这些隐藏状态编码了丰富的推理动态信息，且**在早期步骤就已包含足够区分轨迹优劣的信息**。
- **轻量级评估器**：训练一个轻量级的**两步MLP“步骤评分器”**，输入为步骤结束标记的隐藏状态，输出为该步骤正确的概率。该评分器计算开销极低，可实现实时评估。
- **渐进式评分**：对单个轨迹，将其所有已生成步骤的评分进行平均，得到**轨迹级评分**。这种聚合方式比单步评分更稳定，能反映推理过程的一致性。

#### **2. 系统层面：GPU内存感知的剪枝触发机制**
- **洞察新瓶颈**：论文指出，端到端延迟的主要瓶颈并非仅仅是生成的令牌数量，更关键的是**GPU内存中KV缓存的饱和**所导致的**轨迹等待队列**。当内存不足时，推理引擎（如vLLM）会将部分轨迹挂起，造成大量等待时间（论文中观察到占延迟的59%）。
- **创新的触发策略**：将**GPU内存利用率**作为剪枝触发信号。当KV缓存积累导致GPU内存将满时，立即剪枝当前**轨迹级评分最低**的轨迹，并释放其KV缓存。
- **直接消除等待**：这种机制直接避免了因内存不足而产生的轨迹排队和挂起-恢复开销，从系统层面大幅降低了延迟。

### **解决方案总结**
STEP通过 **“算法-系统”协同设计** 来解决效率问题：
1.  **如何判断剪谁？** -> 使用基于隐藏状态的轻量级步骤评分器，对并行生成的每条推理轨迹进行实时质量评估。
2.  **何时触发剪枝？** -> 利用GPU内存饱和作为触发信号，动态地剪除最不具潜力的轨迹，优先释放系统资源。
3.  **最终如何决策？** -> 在所有轨迹生成或剪枝完毕后，使用基于轨迹级评分的**加权投票**来聚合最终答案。

### **实际价值**
- **显著提升效率**：在多个高难度推理基准测试上，相比标准的Self-Consistency，STEP平均降低**45%–70%** 的端到端推理延迟。
- **同时提升精度**：由于能更早、更准确地识别并剪除错误轨迹，并将资源集中于优质轨迹，STEP在多数情况下还带来了**+0.4 到 +7.5个百分点**的准确率提升。
- **实用性强**：方法轻量，易于集成到现有推理服务框架（如vLLM）中，为LLM复杂推理任务的高效实际部署提供了新思路。

**简而言之，STEP的核心贡献是：利用隐藏状态这一更可靠的早期信号来评估推理质量，并创新性地将系统级资源（GPU内存）状态作为剪枝控制器，从而在显著加速的同时，提升了大模型测试时扩展的推理精度。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决大语言模型在测试时扩展（如自洽性方法）中，因并行生成多条冗长推理轨迹而导致的**计算开销巨大和端到端延迟过高**的核心问题。现有基于相似性或置信度的剪枝方法信号不可靠，且忽略了推理系统（如KV缓存导致GPU内存饱和引发的排队等待）这一关键延迟瓶颈。

为此，论文提出了 **STEP框架**，其核心创新在于两点：1) **基于隐藏状态的步级评估**：训练一个轻量级MLP评分器，利用推理步骤边界处的隐藏状态作为早期信号来评估轨迹质量；2) **GPU内存感知的触发式剪枝**：以GPU内存饱和为信号触发剪枝，动态移除评分最低的轨迹以释放KV缓存，从而从根本上消除排队等待延迟。

实验表明，STEP在多个高难度推理基准测试上，相比标准的自洽性方法，在**平均降低45%–70%端到端延迟**的同时，还**将推理准确率提升了0.4到7.5个百分点**，实现了效率与性能的双重提升。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文提出的 **STEP** 框架在高效测试时扩展（Test-Time Scaling）领域，相对于已有工作，具有以下明确的创新点：

---

### 1. **利用隐藏状态作为早期质量评估信号**
- **相比以往方法的改进/不同之处：**
    - **以往方法：** 现有加速并行扩展的方法主要依赖两类不可靠的信号进行剪枝：
        1.  **基于置信度的方法**：使用模型输出的词元级置信度（如logits）来评估轨迹质量。但模型可能存在错误校准，对错误答案也表现出高置信度。
        2.  **基于多样性的方法**：基于推理轨迹文本的表面相似性进行剪枝，以保持答案多样性。但这可能错误地剪掉文本不同但推理正确且答案一致的轨迹。
    - **STEP的创新：** 提出使用模型**内部的隐藏状态**（特别是推理步骤边界处的最后一层隐藏状态）作为评估推理质量的早期、可靠信号。论文发现，即使在推理早期，隐藏状态也已包含丰富的、可区分轨迹正确与否的信息。
- **解决的具体问题/带来的优势：**
    - **解决了信号可靠性问题：** 隐藏状态比输出置信度或文本相似性更能反映模型内部的“推理动态”和潜在的正确性，从而提供了更可靠的轨迹质量评估依据。
    - **实现了早期精准剪枝：** 由于信号在早期步骤即有效，使得框架能够在生成过程中尽早识别并终止没有希望的推理轨迹，避免了为错误的长轨迹浪费计算资源。

### 2. **提出基于GPU内存感知的剪枝触发机制**
- **相比以往方法的改进/不同之处：**
    - **以往方法：** 现有的剪枝方法通常基于固定的时间表或预定义的置信度阈值来触发剪枝。它们主要关注**减少生成的词元数量**，但**忽视了推理系统层面的瓶颈**。
    - **STEP的创新：** 创新性地将**GPU内存利用率**作为剪枝触发信号。具体来说，当KV缓存的积累导致GPU内存趋于饱和、系统即将因内存不足而将轨迹放入等待队列时，STEP立即触发剪枝。
- **解决的具体问题/带来的优势：**
    - **解决了系统级延迟瓶颈：** 论文通过分析发现，在并行生成长推理轨迹时，**等待时间**（因内存不足导致轨迹被挂起和恢复）占端到端延迟的绝大部分（约59%），而非解码本身。传统方法无法消除此等待队列。
    - **显著降低端到端延迟：** 通过内存感知触发剪枝，STEP能立即释放最差轨迹的KV缓存，防止等待队列的形成，从而**从根本上消除了内存引起的等待开销**。这是STEP实现45%-70%延迟降低的关键因素之一。

### 3. **设计轻量级步骤评分器与集成剪枝框架**
- **相比以往方法的改进/不同之处：**
    - **以往方法：** 评估通常是在轨迹完成后进行，或使用与生成模型相同的庞大模型进行内部验证，计算开销大。
    - **STEP的创新：**
        1.  **轻量级步骤评分器：** 设计了一个简单的两层MLP模型，仅以步骤边界隐藏状态为输入，预测该步骤所属轨迹的正确性概率。该评分器计算开销极低（论文分析其FLOPs开销可忽略不计）。
        2.  **动态轨迹级评分：** 对单个轨迹，将其所有已生成步骤的评分进行平均，作为该轨迹的当前质量分数。这种聚合方式比仅看最后一步分数更稳定，能反映推理过程的一致性。
        3.  **集成剪枝与投票：** 将上述评分器与内存触发机制结合，形成完整的动态剪枝算法。最终答案通过已完成轨迹的加权投票（权重为轨迹最终评分）得出。
- **解决的具体问题/带来的优势：**
    - **实现了高效实时的质量监控：** 轻量级评分器允许在生成过程中对每一步进行近乎实时的、低成本的质量评估。
    - **提升了最终答案的准确性：** 基于隐藏状态评分的加权投票，比传统的多数投票或基于输出置信度的加权投票更可靠，从而在加速的同时，**反而提升了多个基准测试上的推理准确率**（提升0.4到7.5个百分点）。
    - **提供了更优的精度-延迟权衡：** 如图4所示，STEP在不同计算预算下，都能达到比基线方法（Self-Consistency, Slim-SC, DeepConf）更高的精度，或在相同精度下延迟显著更低。

---

**总结而言，STEP的核心创新在于：**
1.  **信号层面**：从不可靠的外部指标转向更可靠的模型内部隐藏状态。
2.  **系统层面**：从只关注算法词元减少转向解决实际的系统内存瓶颈。
3.  **框架层面**：将轻量级早期评估与系统感知的动态资源管理无缝集成。

这些创新共同解决了现有测试时扩展方法**延迟高**和**资源浪费严重**的核心问题，在**大幅提升效率（降低延迟）的同时，还提高了推理的准确性**，为实现高效、实用的复杂推理LLM部署提供了新思路。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验数据集与评价指标

#### 1. 数据集
论文在四个具有挑战性的推理基准数据集上进行了评估：
- **AIME-25**： 2025年美国数学邀请赛问题。
- **HMMT-24/25**： 2024年和2025年哈佛-麻省理工数学锦标赛问题。
- **GPQA-Diamond**： 研究生水平的通用科学推理问答基准。

#### 2. 评价指标
- **准确性**： 最终答案的正确率。
- **效率**：
    - **平均生成令牌数**： 每个问题平均生成的令牌数量（`Token ↓`）。
    - **端到端推理延迟**： 每个问题的总推理时间，单位为秒（`Lat. ↓`）。

### 二、 对比的基线方法
论文将提出的 **STEP** 方法与以下四种基线方法进行了全面对比：
1.  **Chain-of-Thought**： 标准的思维链提示，生成单一路径。
2.  **Self-Consistency**： 并行生成多条推理路径并进行多数投票。
3.  **Slim-SC**： 基于推理链间相似性进行剪枝的自洽方法。
4.  **DeepConf**： 利用模型内部置信度信号进行动态剪枝的方法。

### 三、 关键性能提升与结论

#### 1. 准确性提升
- **STEP在大多数模型和数据集组合上取得了最高的准确率**。
- 具体提升示例：
    - 在 **HMMT-25** 数据集上，相比 Self-Consistency，STEP 在三个模型（Qwen3-4B, DeepSeek-8B, Phi-4-14B）上的准确率分别提升了 **5.0%**、**3.3%** 和 **0.8%**。
    - 在通用科学推理数据集 **GPQA-Diamond** 上，STEP 也取得了具有竞争力的准确率，证明了其泛化能力。

#### 2. 效率（延迟）显著降低
- **STEP平均减少了45%–70%的端到端推理延迟**。
- 具体示例：
    - 在 **Phi-4-reasoning-plus (14B)** 模型上处理 **HMMT-24** 问题时，STEP 仅用 **630秒** 达到了58.3%的准确率，而 Self-Consistency 需要 **2405秒**，实现了 **3.8倍的加速**。
    - 即使与同样旨在加速的 **DeepConf** 和 **Slim-SC** 相比，STEP 在保持相当或更高准确率的同时，**延迟也更低**。

#### 3. 核心结论
- **双重优势**： STEP 不仅通过基于隐藏状态的早期评估有效剪除了低质量推理路径，从而**提升了准确率**；还通过**GPU内存感知的触发机制**，消除了因KV缓存占满导致的等待队列，从而**大幅降低了延迟**。
- **效率来源分析**： 论文通过剖析（表2）指出，加速来源于两方面：
    1.  **减少生成令牌数**： 通过早期剪枝，减少了不必要的长错误路径的生成。
    2.  **消除等待时间**： 这是STEP相比其他剪枝方法（如DeepConf、Slim-SC）的**关键优势**。这些方法虽然也减少了令牌数，但未能完全消除因内存调度产生的等待时间，而STEP的**内存触发剪枝机制**则彻底解决了这一问题。
- **可扩展性**： 如图4所示，在不同计算预算（采样路径数N=16, 32, 64）下，STEP 始终能提供**更优的“准确率-延迟”权衡曲线**，在任意给定的时间预算下达到更高的准确率。

### 四、 其他重要分析结果
1.  **评分器排序能力**： 基于隐藏状态的步骤评分器，即使在推理早期，其区分正确与错误路径的能力也**显著优于**基于令牌级置信度的方法（图5）。
2.  **对GPU内存的鲁棒性**： 在不同GPU内存利用率限制下（0.5至0.9），STEP的准确率保持稳定（表3），表明其**对内存约束不敏感**，具有部署鲁棒性。
3.  **额外开销可忽略**： 附录D通过计算分析指出，轻量级步骤评分器引入的额外计算开销相对于大语言模型本身的推理开销**微乎其微**（比例低于 `10^{-6}`）。

**总结**： 论文通过系统的实验证明，STEP框架在多个高难度推理基准和不同规模的模型上，**同时实现了推理准确率的提升和端到端延迟的大幅降低**，其核心创新在于将**基于隐藏状态的早期质量评估**与**系统级的GPU内存感知优化**相结合，为高效的测试时扩展提供了有效的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.09093v1)
- [HTML 版本](https://arxiv.org/html/2601.09093v1)
