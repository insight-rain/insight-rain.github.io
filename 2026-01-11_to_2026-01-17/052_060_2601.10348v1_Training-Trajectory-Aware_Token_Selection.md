# Training-Trajectory-Aware Token Selection

**相关性评分**: 6.0/10

**排名**: #52


---


## 基本信息

- **arXiv ID**: [2601.10348v1](https://arxiv.org/abs/2601.10348v1)
- **发布时间**: 2026-01-15T12:45:05Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Zhanming Shen, Jiaqi Hu, Zeyu Qin, Hao Chen, Wentao Ye, Zenan Huang, Yihong Zhuang, Guoshan Lu, Junlin Zhou, Junbo Zhao

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种训练轨迹感知的令牌选择方法，通过优化令牌级训练目标，提升模型推理效率，适用于轻量级架构和边缘部署场景。

## 摘要

Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency, yet in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We further uncover a token-level mechanism: confidence bifurcates into steadily increasing Imitation-Anchor Tokens that quickly anchor optimization and other yet-to-learn tokens whose confidence is suppressed until after the bottleneck. And the characteristic that these two types of tokens cannot coexist is the root cause of the failure in continual distillation. To this end, we propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings: with only hundreds of examples, Qwen3-8B surpasses DeepSeek-R1 on competitive reasoning benchmarks, Qwen3-32B approaches Qwen3-235B, and T3-trained LLaDA-2.0-Mini exceeds its AR baseline, achieving state-of-the-art performance among all of 16B-scale no-think models.

## 详细分析

## 论文摘要：Training-Trajectory-Aware Token Selection (T3S)

**1. 研究背景和动机**
随着具备长链思维（CoT）能力的大语言模型（LLM）不断涌现，**推理蒸馏**成为将昂贵推理能力转化为可部署效率的关键途径。然而，在**前沿场景**（学生模型已具备较强推理能力）下，简单的持续蒸馏往往收效甚微，甚至导致性能退化。本文发现了一个普遍存在的训练现象：**模仿冲击**——尽管训练损失单调下降，但所有性能指标都会在几乎相同的**模仿瓶颈**处急剧下降，随后才缓慢恢复。这揭示了持续推理蒸馏失败的根本原因。

**2. 核心方法和技术创新**
本文提出**训练轨迹感知的令牌选择**方法。其核心创新在于：
- **机制诊断**：通过分析训练轨迹，发现令牌置信度会分化为两类：**模仿锚定令牌**（在瓶颈前置信度持续上升，快速主导优化）和**待学习令牌**（在瓶颈前置信度被抑制）。这两类令牌的梯度存在**对抗性**，无法良性共存。
- **方法设计**：T3S利用模仿瓶颈附近的轨迹信号，在令牌级别重构训练目标：
    - **对于自回归模型**：在损失函数中**屏蔽模仿锚定令牌**，为待学习令牌清除优化路径。
    - **对于扩散语言模型**：通过**联合掩码**，强制模型在随机可见条件下反复练习待学习令牌。

**3. 主要实验结果**
T3S在多种设置下取得显著且一致的性能提升：
- **高效数据利用**：仅使用**数百个**训练样本，Qwen3-8B在竞争性推理基准上超越了其教师模型DeepSeek-R1，Qwen3-32B的性能接近Qwen3-235B。
- **模型普适性**：在扩散语言模型（dLLM）上，T3S训练的LLaDA-2.0-Mini大幅提升了推理准确率，并在16B规模的无思维模型中达到了**最先进的性能**。
- **消融验证**：实验表明，基于初始置信度等静态信号的令牌选择方法无法复现T3S的效果，且无法消除“模仿冲击”，证明了**轨迹感知信号**的必要性。

**4. 研究意义和价值**
本研究不仅揭示了持续蒸馏中“模仿冲击”这一关键现象及其背后的令牌级机制，更重要的是提出了一种简单有效的解决方案T3S。该方法**无需增加数据或改变模型架构**，仅通过轨迹感知的令牌选择，便能显著提升蒸馏效率与最终性能，为高效、稳定的前沿模型能力迁移提供了新的技术路径，在自回归与扩散式语言模型上均具有重要的实际应用价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
在**前沿持续蒸馏**场景中，当学生模型本身已具备较强推理能力时，传统的持续知识蒸馏方法往往收效甚微，甚至会导致性能下降。论文旨在探究这种失败的根本原因，并提出解决方案。

### **核心发现与机制**
论文首先识别并命名了一个关键现象：**“模仿冲击”**。
- **现象描述**：在蒸馏训练过程中，尽管训练损失单调下降，但所有性能指标（训练集准确率、下游推理基准）都会在几乎同一阶段**急剧下降至一个瓶颈点**，然后才缓慢恢复。
- **根本原因**：在**词元级别**，模型的置信度会**分化为两类**：
    1.  **模仿锚定词元**：其置信度在训练早期就**稳步上升**，迅速“锚定”了优化方向。
    2.  **待学习词元**：其置信度在早期被**强烈抑制**，直到模仿瓶颈过后才开始提升。
- **关键冲突**：这两类词元的梯度**无法良性共存**。学习模仿锚定词元会对其他词元产生**强烈的抑制效应**，系统性地延迟了真正有益于泛化推理能力的词元的学习。这是持续蒸馏失败的根本原因。

### **核心创新：训练轨迹感知的词元选择**
基于上述机制，论文提出了 **T3S** 方法，其核心思想是：**利用训练轨迹信号重构词元级别的训练目标，为待学习词元清除优化路径**。

#### **T3S 的具体实现**
1.  **识别模仿瓶颈**：在训练过程中，找到**训练准确率最低的检查点**，定义为“模仿瓶颈”。
2.  **词元置信度分析**：使用一个自回归选择器模型，计算每个词元从初始模型到瓶颈检查点的置信度变化 `Δc_t`。
3.  **针对不同模型架构的干预策略**：
    - **对于自回归模型**：**屏蔽模仿锚定词元**（`Δc_t > 0`）。在损失函数中排除这些词元，防止它们在早期主导优化，迫使模型专注于学习那些被抑制的、对推理更重要的词元。
        ```python
        # 伪代码示意：AR-T3S 损失函数
        loss = -log(p_theta(y_t | y_<t, x)) for all t where Δc_t <= 0
        ```
    - **对于扩散语言模型**：**强制掩码待学习词元**（`Δc_t < -τ`）。dLLM 通过随机掩码进行训练，T3S 通过“联合掩码”确保这些待学习词元在每次训练中都被强制预测，从而反复练习生成关键推理词元。
        ```python
        # 伪代码示意：dLLM-T3S 掩码策略
        target_tokens = {t | Δc_t < -τ}  # 待学习词元集合
        final_mask = random_mask OR indicator(target_tokens)  # 联合掩码
        ```

### **实际价值与实验结果**
- **高效性**：仅需**数百个**高质量蒸馏样本，即可实现显著提升。
- **强大效果**：
    - **Qwen3-8B** 在竞争性推理基准上超越了其教师模型 **DeepSeek-R1**。
    - **Qwen3-32B** 达到了与 **Qwen3-235B** 相当的推理水平。
    - **LLaDA-2.0-Mini** 在 16B 规模的无思考模型中达到了 **SOTA 性能**。
- **通用性**：方法在**自回归模型**和**扩散语言模型**上均验证有效，并缓解了灾难性遗忘问题。
- **创新洞察**：突破了传统基于静态置信度（如初始不确定性）进行词元选择的范式，首次提出并验证了**训练轨迹动态信号**对于识别和干预有害优化模式的关键作用。

**总结**：论文的核心创新在于**首次从训练轨迹和词元动力学的角度，揭示了前沿持续蒸馏失败的微观机制，并据此提出了一种轻量、通用且高效的词元级干预方法 T3S**，为高效推理蒸馏开辟了新的技术路径。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**前沿大语言模型在持续推理蒸馏中性能不升反降**的核心问题。研究发现，这是由于训练过程中出现了“模仿冲击”现象：部分“模仿锚点词元”会过早主导优化，抑制其他“待学习词元”的习得，导致模型性能先崩溃后恢复。为此，论文提出了**训练轨迹感知的词元选择方法**，通过分析训练轨迹识别并屏蔽这些有害的锚点词元，从而为真正有益于推理的词元清理优化路径。该方法在自回归和扩散语言模型上均取得显著效果，仅用数百个样本，就能让较小模型在推理基准上超越或接近其教师模型，实现了高效、高质量的推理能力蒸馏。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Training-Trajectory-Aware Token Selection》在高效推理蒸馏领域提出了几个关键的创新点，主要围绕对“持续蒸馏失败”这一现象的深入诊断和提出的解决方案。以下是其相对于已有工作的明确创新点：

### 1. **识别并定义了“模仿冲击”现象及其根本的令牌级机制**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：以往关于高效蒸馏的研究主要关注数据选择（选择哪些样本）、教师混合或课程学习，但通常将蒸馏过程视为一个“黑箱”，关注整体性能，而忽略了训练动态中可能存在的有害阶段。对于持续蒸馏（即学生模型已有较强能力时继续蒸馏）性能下降或提升有限的问题，缺乏深入、可解释的机制性分析。
     - **本文创新**：本文首次系统性地观察并命名了“模仿冲击”现象：在训练损失单调下降的同时，所有性能指标会先急剧下降至一个“模仿瓶颈”，然后才缓慢恢复。更重要的是，**本文深入到令牌级别**，揭示了该现象的根源：令牌置信度会分化为“模仿锚点令牌”和“待学习令牌”。前者在训练早期主导优化并快速“锚定”，同时对后者产生强烈的抑制效应，导致后者的学习被系统性延迟。
   - **解决的具体问题/带来的优势**：
     - **解决了什么问题**：解释了为什么在“前沿区域”（学生已有强推理能力）进行持续的、天真的蒸馏常常收效甚微甚至导致性能下降。它指出了问题不在于缺乏推理信号，而在于优化路径被有害的令牌动态所阻塞。
     - **带来的优势**：为设计更高效的蒸馏算法提供了全新的、基于训练轨迹的诊断视角和理论基础。这使得干预措施能够有的放矢，而非盲目尝试。

### 2. **提出了“训练轨迹感知的令牌选择”方法**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的令牌优先级方法（如RHO-1的Selective Language Modeling，或RL中对高熵令牌的聚焦）大多依赖于**静态信号**，例如初始置信度、令牌熵或局部梯度。这些方法假设“重要”或“难”的令牌在训练开始前就可以被识别。
     - **本文创新**：T3S的核心创新在于**利用训练轨迹本身作为信号**来动态识别令牌。具体来说，它利用“模仿瓶颈”时刻的令牌置信度变化（Δc_t）来区分“模仿锚点令牌”（Δc_t > 0）和“待学习令牌”（Δc_t < -τ）。这种划分是**动态的、与模型训练过程紧密耦合的**。
   - **解决的具体问题/带来的优势**：
     - **解决了什么问题**：解决了静态信号无法可靠识别那些在早期主导优化、并对其他令牌产生抑制的“锚点令牌”的问题（论文在附录F、G通过实验证实了这一点）。静态方法无法捕捉到这种由训练动态产生的、功能性的令牌对抗关系。
     - **带来的优势**：能够更精确地清除优化路径上的障碍。通过掩码“模仿锚点令牌”，T3S迫使模型优先学习那些对泛化性能真正关键但被抑制的“待学习令牌”，从而避免了“模仿冲击”，加速了有效推理能力的迁移。

### 3. **将令牌选择机制统一应用于自回归模型和扩散语言模型**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：令牌选择技术通常在特定架构（如AR或纯扩散模型）的背景下研究，缺乏一个统一的框架能够根据模型训练目标的特点进行适配。
     - **本文创新**：T3S为AR和dLLM设计了不同的操作策略，但核心思想一致——利用轨迹信号进行令牌级干预。
       - **对于AR蒸馏**：采用**掩码策略**，直接从损失函数中排除“模仿锚点令牌”，防止其主导梯度。
       - **对于dLLM训练**：采用**联合掩码策略**，在随机掩码的基础上，强制额外掩码“待学习令牌”，迫使模型在各种部分观测条件下反复练习生成这些关键令牌。
   - **解决的具体问题/带来的优势**：
     - **解决了什么问题**：解决了dLLM训练中，随机掩码可能效率低下、浪费算力在已学会的简单令牌上的问题。为扩散式模型的高效训练提供了一种基于轨迹信号的目标重构方法。
     - **带来的优势**：
       1. **证明了方法的普适性**：创新点不仅限于AR模型，在更具挑战性的dLLM（如LLaDA-2.0-Mini）上也取得了显著效果，甚至使dLLM在推理基准上超过了同规模的AR基线模型。
       2. **提升dLLM效率**：实验表明，T3S训练后的dLLM在推理时生成的令牌数减少了15-25%，说明其学习到了更准确、更高效的内部推理路径。

### 4. **基于“模仿瓶颈”的“恢复残差迁移”分析与“去偏教师混合”应用**
   - **相比以往方法的改进/不同之处**：
     - **RRT分析**：本文没有停留在现象观察，而是通过“恢复残差迁移”实验进行了**因果性验证**。该实验证明，丢弃“模仿瓶颈”之前的所有参数更新，仅保留之后的更新，得到的模型性能反而优于标准蒸馏。这强有力地表明早期更新不仅不必要，而且有害。
     - **去偏教师混合**：基于T3S能去除教师特定有害偏见的洞察，本文创新性地将其应用于**多教师混合蒸馏**。先对每个教师的数据独立应用T3S进行令牌级重构，再混合训练，而非直接混合原始数据。
   - **解决的具体问题/带来的优势**：
     - **RRT**：从参数更新层面证实了“模仿冲击”早期阶段的危害性，为T3S的令牌级干预提供了强有力的动机和支持。
     - **去偏教师混合**：
       - **解决了什么问题**：解决了直接混合多教师数据时，不同教师的表面模式或偏见可能相互干扰的问题。
       - **带来的优势**：实现了增益的叠加而非稀释。实验显示，使用T3S去偏后，混合更多教师的数据能持续提升学生模型性能，最终让Qwen3-8B/32B仅用数百样本就在推理基准上达到或超越了其教师（DeepSeek-R1）乃至更大规模模型（Qwen3-235B）的水平，展示了**卓越的数据效率**。

### 总结
本文的核心创新在于**从动态训练轨迹的视角，重新审视并解决了持续推理蒸馏的瓶颈问题**。它通过识别“模仿锚点令牌”这一关键概念，将问题从宏观的性能下降定位到微观的令牌级梯度竞争与抑制，并据此提出了普适且高效的T3S方法。相比以往基于静态启发式的方法，T3S的轨迹感知特性使其干预更加精准和根本，从而在AR和dLLM两种范式下都实现了显著的性能提升和效率优化。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果总结

该论文通过大量实验验证了所提出的 **T3S（Training-Trajectory-Aware Token Selection）方法** 在持续推理蒸馏任务中的显著效果。核心结论是：**T3S 能够有效解决“模仿冲击”现象，通过屏蔽早期主导优化的“模仿锚点词元”，为“待学习词元”清理优化路径，从而在极少量数据下实现显著的性能提升，甚至使学生模型超越其教师模型。**

### 一、 主要数据集与评价指标

1.  **训练数据集**：
    *   **BOBA-200**：从 BOBA 数学推理基准中选取的 200 个提示。
    *   **S1K-200**：从 S1K 数据集中筛选出的 200 个带可验证答案的数学推理提示。
    *   **目的**：验证在“高效蒸馏”（仅需数百样本）场景下的有效性。

2.  **核心评价指标**：
    *   **AIME24 / AIME25**：数学推理能力的主要评测基准，报告准确率。
    *   **平均准确率 (AVG)**：AIME24 和 AIME25 得分的平均值，作为综合性能指标。

3.  **扩展评估指标（用于验证泛化与遗忘）**：
    *   **泛化推理能力**：PhyBench, GPQA-Diamond, LiveCodeBench, MMLU-Pro。
    *   **遗忘缓解能力**：SuperGPQA, CEval, IFEval。
    *   **扩散语言模型 (dLLM) 评估**：AIME25, MATH500, TheoremQA。

### 二、 对比的基线方法

论文与以下四种主要策略在相同训练配置下进行对比：

1.  **SFT (标准蒸馏)**：学生模型直接在教师模型生成的推理轨迹上进行标准监督微调。
2.  **RRT (恢复残差迁移)**：基于“模仿冲击”分析的初步方法。**仅保留瓶颈期后的参数更新**，丢弃瓶颈期前的所有更新。
3.  **T3S (本文方法)**：在词元级别重构训练目标，**屏蔽“模仿锚点词元”**，专注于优化“待学习词元”。
4.  **-T3S (诊断性消融)**：**反向屏蔽 T3S 选择的词元**（即只训练锚点词元，屏蔽待学习词元），用于验证 T3S 选择机制的有效性和必要性。

### 三、 关键性能提升与结论

#### 1. **自回归模型上的显著提升**
*   **效果**：在 BOBA-200 和 S1K-200 数据集上，无论教师模型是 DeepSeek-R1 还是 QWQ，**T3S 均一致地取得最佳性能**，大幅超越标准 SFT 和 RRT。
*   **关键数据**（以 DeepSeek-R1 蒸馏为例）：
    *   **Qwen3-8B**：在 S1K-200 上，T3S 将平均准确率从基线的 **71.46** 提升至 **76.57**，而标准 SFT 导致性能下降至 **64.17**。
    *   **Qwen3-32B**：使用 T3S 后，平均准确率从基线的 **76.77** 提升至 **78.33**。
*   **突破性结论**：
    *   仅使用数百个样本，**Qwen3-8B (T3S) 在竞争性推理基准上超越了其教师模型 DeepSeek-R1**。
    *   **Qwen3-32B (T3S) 达到了与千亿参数模型 Qwen3-235B 相当的性能水平**，实现了“小模型逼近大模型”的效果。

#### 2. **解决“模仿冲击”现象**
*   **效果**：如图7所示，标准 SFT 训练中出现的 **“性能骤降后缓慢恢复”的模仿冲击曲线，在 T3S 训练中被基本消除**。训练准确率从开始就稳步提升，验证了 T3S 清除了导致性能崩溃的优化障碍。

#### 3. **在扩散语言模型上的有效性**
*   **模型**：LLaDA-2.0-Mini (dLLM)。
*   **效果**：T3S 大幅提升了 dLLM 的推理能力。
    *   在 AIME25 上，准确率从基线的 **30.00** 提升至 **53.33**。
    *   综合平均性能从 **55.95** 提升至 **66.01**，**超越了同规模（~16B）的所有“无思考”自回归基线模型**，达到了该规模下的 **SOTA 性能**。
*   **额外收益**：T3S 还使模型推理更高效，**生成的词元总数减少了约 15-25%**。

#### 4. **泛化与遗忘缓解**
*   **泛化能力**：在 PhyBench、LiveCodeBench 等分布外推理任务上，T3S 相比 SFT 带来了 **一致的性能提升**（平均提升约2-3个百分点），表明其学习到了更通用的推理能力。
*   **遗忘缓解**：在指令遵循和知识保留任务（如 CEval, IFEval）上，T3S 导致的性能下降**远小于标准 SFT**，甚至有时有轻微提升，说明 T3S 能更好地保留基础能力。

#### 5. **方法必要性的验证**
*   **消融实验 (-T3S)**：反向屏蔽导致性能** catastrophic 下降**（例如 AIME25 从 67.08 跌至 25.63），证明 T3S 选择的“待学习词元”至关重要，而锚点词元单独训练有害。
*   **对比静态启发式方法**：使用相同屏蔽比例，但仅依据**初始置信度**（屏蔽最高或最低置信度词元）的方法，**性能远差于 T3S，且无法消除模仿冲击**，证明了**利用训练轨迹信号的必要性**。

### 总结
论文通过系统性的实验表明，T3S 不仅是一种高效的蒸馏技术，更从根本上揭示了持续推理蒸馏中性能下降的机制。其核心价值在于：
*   **数据高效**：仅需数百高质量样本。
*   **性能卓越**：使学生模型达到甚至超越教师模型的推理水平。
*   **通用性强**：适用于自回归和扩散两种语言模型范式。
*   **机制清晰**：通过词元级别的干预，解决了优化路径冲突的根本问题。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.10348v1)
- [HTML 版本](https://arxiv.org/html/2601.10348v1)
