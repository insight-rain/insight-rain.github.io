# CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation

**相关性评分**: 6.0/10

**排名**: #25


---


## 基本信息

- **arXiv ID**: [2601.08010v1](https://arxiv.org/abs/2601.08010v1)
- **发布时间**: 2026-01-12T21:24:45Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Chaoyu Li, Deeparghya Dutta Barua, Fei Tao, Pooyan Fazli

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, offline Reinforcement Learning, constrained reinforcement learning, safe reinforcement learning, goal conditioned reinforcement learning, world model

## 一句话总结

这篇论文提出CASHEW框架，通过迭代轨迹聚合和强化学习训练来稳定多模态推理，但与机器人应用和特定强化学习方法的直接相关性有限。

## 摘要

Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.

## 详细分析

## 论文《CASHEW：通过迭代轨迹聚合稳定多模态推理》详细摘要

### 1. 研究背景和动机
当前，视觉语言模型（VLMs）在多种多模态理解和推理任务上表现出色，但其多步推理过程存在**不稳定**的问题。对同一输入进行多次采样，常常会产生**发散**的推理轨迹和**不一致**的最终预测。这种不稳定性在复杂推理场景中尤为突出，微小的感知或解释错误会在思维链中传播放大。现有的“测试时扩展”方法多采用“采样-选择”范式，即生成多个独立推理链后选择最佳的一条，但**丢弃了被拒绝轨迹中的部分洞见**，且缺乏显式的视觉证据**锚定**，可能导致模型“想得更久”而非“想得更好”。

### 2. 核心方法和技术创新
本文提出了两种互补的方法来稳定多模态推理：

- **Cashew（推理时框架）**：这是一个即插即用的测试时扩展框架。它将推理建模为一个**迭代进化**过程：在每次迭代中，它从一组候选推理轨迹中随机采样子集，并利用**显式视觉验证**（通过预训练的Grounding DINO模型）来过滤掉幻觉的物体/属性级主张，仅保留有视觉证据支持的“关键视觉对象”。随后，模型基于这些经过验证的证据，将子集中的轨迹**聚合**成更高质量的新轨迹。经过多轮迭代，最终将所有轨迹合并为一个稳定、可靠的最终推理轨迹。

- **Cashew-RL（学习型变体）**：通过**后训练**将上述聚合行为**内化**到单一模型中。其训练分为两个阶段：
    1.  **监督微调（SFT）**：学习一个结构化的聚合输出格式（`<think>`推理链、`<visual_keys>`关键视觉对象、`<answer>`最终答案）。
    2.  **强化学习（RL）**：采用**组序列策略优化（GSPO）** 和一种**复合奖励函数**进行训练。该奖励函数鼓励**基于最小充分视觉证据的正确答案**，并包含一个**难度感知的长度惩罚项**，使模型能根据任务难度自适应地分配推理精力。

### 3. 主要实验结果
在涵盖图像理解、视频理解和视频推理的**13个基准测试**上进行了广泛实验，结果显著：
- **Cashew** 作为推理时方法，能稳定提升多种骨干VLM（如Qwen3-VL、InternVL）的性能。例如，在ScienceQA上，Qwen3-VL-4B的准确率从69.5%提升至**93.1%（+23.6个百分点）**；在EgoSchema上，Qwen2.5-VL-7B提升了**+8.1个百分点**。
- **Cashew-RL** 在仅进行一次聚合（`T=1`）时即展现出优于基线的性能，进行三次聚合（`T=3`）时效果更佳，甚至在某些任务上超越了纯推理时聚合的Cashew，证明了其内化聚合策略的有效性。
- 与Self-Consistency、Self-Selector、Self-Synthesizer等先进的测试时扩展基线相比，Cashew在多个基准上均取得了**最佳性能**。

### 4. 研究意义和价值
- **技术创新**：提出了“**迭代聚合**”而非“选择”的新范式，并创新性地将**显式视觉验证**与**强化学习驱动的策略内化**相结合，为解决多模态推理的不稳定性和幻觉问题提供了系统性的解决方案。
- **实际价值**：该方法不依赖于特定模型架构，可作为即插即用的推理增强模块，显著提升现有VLMs在复杂、长序列、需要精细视觉基础的任务（如科学问答、长视频理解）上的**可靠性和准确性**。
- **未来方向**：研究指出了当前方法对底层视觉感知模型准确性的依赖，未来可通过**联合优化感知与聚合策略**或集成更鲁棒的感知系统来进一步提升性能。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：CASHEW

### **一、 研究问题**
论文旨在解决**多模态大模型（VLMs）在多步推理中的不稳定性问题**。具体表现为：
- **推理轨迹发散**：对同一输入进行多次采样，会产生不一致的推理路径和最终答案。
- **视觉幻觉**：模型容易生成缺乏视觉证据支持的陈述。
- **现有方法的局限**：当前的主流“测试时扩展”方法（如多次采样后选择最佳轨迹）只是让模型“思考更久”，而非“思考得更好”，且会丢弃被拒绝轨迹中的部分有用信息。

### **二、 核心创新点**
论文提出了两种互补的方法来稳定多模态推理：

#### **1. CASHEW（推理时框架）**
- **核心思想**：**迭代式轨迹聚合**。将推理视为一个进化过程，而非简单的“采样-选择”。
- **关键机制**：
    - **种群初始化**：从基础VLM生成一组初始推理轨迹。
    - **子集采样与聚合**：在每次迭代中，随机采样部分轨迹作为参考，引导生成新的、更优质的轨迹。
    - **显式视觉验证**：使用预训练的**Grounding DINO**模型，验证推理轨迹中提到的物体是否真实存在于图像/视频中。只有被验证的视觉证据才会用于指导后续聚合。
    - **最终共识**：经过多轮迭代后，将所有轨迹合并，生成一个最终、稳定的推理轨迹和答案。
- **本质**：一种**即插即用**的测试时扩展方法，无需重新训练模型。

#### **2. CASHEW-RL（学习型变体）**
- **核心思想**：通过**强化学习（RL）** 将轨迹聚合的行为**内化**到单个模型中，减少对推理时大量采样的依赖。
- **两阶段训练流程**：
    1.  **监督微调（SFT）**：教会模型**结构化聚合格式**（`<think>`推理链、`<visual_keys>`视觉关键物体、`<answer>`最终答案），建立参考策略。
    2.  **基于GSPO的强化学习**：使用**组序列策略优化**和**复合奖励函数**来优化聚合策略。
- **创新的奖励函数设计**：
    - **答案正确性奖励**：鼓励给出正确答案。
    - **证据选择质量奖励**：平衡精确率与召回率，鼓励模型选择相关且准确的视觉证据，避免无节制地罗列物体。
    - **难度感知的长度惩罚**：根据问题难度自适应地分配推理努力。简单问题鼓励简洁，困难问题允许更长的推理链。
- **课程学习策略**：在RL训练中，逐步增加由当前策略生成的轨迹比例，使模型学会聚合越来越“真实”（含噪声）的候选轨迹，提升鲁棒性。

### **三、 解决方案总结**
论文通过 **“外部框架迭代优化”** 和 **“内部模型能力内化”** 两条路径，共同攻击多模态推理不稳定的核心痛点：

1.  **从“选择”到“合成”**：`Cashew` 框架通过迭代聚合，综合利用所有采样轨迹的信息，而非丢弃其中大部分。
2.  **从“无据”到“有据”**：引入**显式视觉验证**，将推理严格锚定在视觉证据上，有效过滤幻觉。
3.  **从“启发式”到“学习式”**：`Cashew-RL` 通过RL学习最优的聚合策略，并能根据任务难度自适应调整推理深度，实现了更高效、更稳定的单模型推理。

### **四、 实际价值与效果**
- **性能显著提升**：在13个图像和视频理解/推理基准测试上，`Cashew` 和 `Cashew-RL` 均带来了显著且一致的性能提升。例如：
    - 在 **ScienceQA** 上，`Cashew` 为 Qwen3-VL-4B 带来了 **+23.6%** 的惊人提升。
    - 在长视频推理基准 **EgoSchema** 上，`Cashew` 为 Qwen2.5-VL-7B 带来 **+8.1%** 的提升。
- **强通用性**：方法在多种不同规模、不同架构的VLM骨干模型（如InternVL, Qwen-VL系列）上均有效，证明了其作为通用推理稳定器的价值。
- **超越现有方法**：在同等计算预算（采样数N=8）下，`Cashew` 的性能显著优于Self-Consistency（自洽性投票）、Self-Selector（模型选择）和Self-Synthesizer（单轮合成）等主流测试时扩展方法。

**结论**：该论文创新性地将**迭代进化思想**和**显式视觉 grounding** 引入多模态推理，并通过**强化学习内化聚合能力**，为解决VLM推理脆弱性这一关键问题提供了有效且通用的解决方案，具有重要的学术价值和实际应用潜力。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

**核心问题**：当前视觉语言模型（VLMs）在多步推理中存在不稳定性问题，即对同一输入进行多次采样会产生发散且不一致的推理轨迹和最终预测，且容易产生视觉幻觉。

**主要方法**：论文提出了两种互补的方法：
1.  **Cashew**：一个推理时框架，通过**迭代轨迹聚合**来稳定推理。它生成多个候选推理轨迹，利用**显式的视觉验证**（通过Grounding DINO等模型）过滤幻觉步骤，并将推理过程锚定在视觉证据上，通过多轮迭代合成更高质量的最终推理轨迹。
2.  **Cashew-RL**：一个通过后训练学习的变体，旨在将上述聚合行为**内化到单一模型中**。它采用**分组序列策略优化**进行训练，使用一个复合奖励函数，鼓励模型基于最少但充分的视觉证据得出正确答案，并根据任务难度自适应地分配推理努力。

**主要效果**：在涵盖图像理解、视频理解和视频推理的13个基准测试上进行了广泛实验。结果表明，两种方法均能显著提升性能。例如，在ScienceQA基准上，Cashew为Qwen3-VL-4B模型带来了高达**+23.6个百分点的提升**；在EgoSchema基准上，Cashew为Qwen2.5-VL-7B带来了**+8.1个百分点的提升**。Cashew-RL在单次迭代下即可超越基线，并在多次迭代下达到与Cashew相当甚至更优的性能，证明了其有效内化了聚合能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

本文提出了 **CASHEW** 框架，旨在解决多模态大模型（VLMs）多步推理不稳定、易产生幻觉和结果不一致的问题。其核心创新点如下：

---

### 1. **推理范式创新：从“采样-选择”到“迭代-聚合”**
- **改进/不同之处**： 现有测试时扩展方法（如 Self-Consistency, Self-Selector）多采用“生成多个独立推理链 -> 选择最佳答案”的范式，丢弃了被淘汰轨迹中的部分有效信息。CASHEW 则将推理视为一个**进化过程**，在每一轮迭代中，对一组候选推理轨迹进行**合成与精炼**，生成质量更高的新轨迹。
- **解决的问题/优势**： 解决了传统方法**信息利用率低**和“思考更长而非更协同”的问题。通过迭代聚合，模型能整合多条轨迹的互补信息，纠正单条轨迹中的错误，从而产生更稳定、更一致的推理共识。

### 2. **显式视觉验证机制**
- **改进/不同之处**： 在每次聚合步骤前，CASHEW 引入了一个**显式的视觉-文本 grounding 验证步骤**。它使用预训练的 Grounding DINO 模型，对候选推理轨迹中提到的物体和属性进行验证，只保留在图像/视频中有视觉证据支持的“关键视觉对象”作为后续聚合的输入。
- **解决的问题/优势**： 直接解决了多模态推理中的**幻觉问题**。通过将推理步骤锚定在已验证的视觉证据上，迫使模型“基于证据思考”，而非依赖统计巧合或文本偏见进行推理，显著提升了推理的可靠性和事实准确性。

### 3. **可学习的内部化聚合策略 (Cashew-RL)**
- **改进/不同之处**： 除了推理时框架 (Cashew)，本文进一步提出了 **Cashew-RL**，通过**强化学习后训练**，将多轨迹聚合的行为**内部化**到单个模型中。它使用 Group Sequence Policy Optimization (GSPO) 和复合奖励进行训练。
- **解决的问题/优势**：
    - **降低推理成本**： Cashew 需要在测试时对同一输入进行多次采样和迭代，计算开销大。Cashew-RL 学习到的聚合策略使其在单次前向传播中就能模拟出类似“多轨迹聚合”的效果，**减少了对测试时大规模采样的依赖**。
    - **提升泛化与鲁棒性**： 通过课程学习（混合教师轨迹和策略自身轨迹）和复合奖励的优化，模型学会了从嘈杂、不完整的候选轨迹中提取有效信息，形成了更鲁棒的聚合能力。

### 4. **面向任务难度的自适应推理长度惩罚**
- **改进/不同之处**： 在 Cashew-RL 的奖励函数中，设计了一个**难度感知的长度惩罚项**。它根据当前问题在训练中的历史解决率，动态调整对冗长推理的惩罚强度。
- **解决的问题/优势**： 解决了强化学习训练中，模型倾向于生成冗长但无实质内容的推理链以“刷奖励”的问题。它鼓励模型对**简单问题简洁回答**，对**复杂问题分配更多推理步骤**，实现了**自适应计算分配**，使推理过程更高效。

### 5. **统一的、结构化的聚合输出格式**
- **改进/不同之处**： 为 SFT 和 RL 训练设计了一个标准化的输出格式：`<think>推理链</think> <visual_keys>关键视觉对象列表</visual_keys> <answer>最终答案</answer>`。
- **解决的问题/优势**：
    - **稳定训练**： 为 RL 训练提供了清晰、可解析的接口，避免了输出格式崩溃。
    - **显式监督**： 强制模型同时输出推理过程、支撑证据和最终答案，便于计算针对证据选择质量 (`R_key`) 的奖励，实现了对**推理可解释性和视觉 grounding 的端到端联合优化**。

---

## 总结
CASHEW 的核心贡献在于**系统性地将测试时扩展从“选择”升级为“迭代聚合”**，并通过**显式视觉验证**和**可学习的内部化策略**，构建了一个既能提升性能（在 ScienceQA 上最高提升 **+23.6%**），又能增强稳定性、减少幻觉的多模态推理新范式。其实验在涵盖图像、视频理解与推理的 **13 个基准测试**上均展示了显著且一致的提升，证明了该框架的通用性和有效性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验效果概述
论文提出的 **CASHEW** 和 **CASHEW-RL** 方法在广泛的图像和视频理解与推理任务上，显著提升了多模态大模型（VLMs）的推理稳定性和准确性。核心效果是**通过迭代轨迹聚合和视觉验证，有效减少了单一路径推理的不稳定性、幻觉和噪声，实现了更鲁棒、更基于视觉证据的推理**。

### 二、 使用的数据集
论文在 **13个** 多模态基准数据集上进行了全面评估，涵盖三大类任务：

| 任务类别 | 数据集 | 主要特点 |
| :--- | :--- | :--- |
| **图像理解** | ScienceQA, MME, POPE, SEED-Bench (图像子集) | 科学问答、综合评估、目标幻觉检测、通用能力 |
| **视频理解** | Video-MME, LongVideoBench, EgoSchema, MVBench, NExT-QA | 短视频/长视频理解、自我中心视角、多选问答 |
| **视频推理** | VideoMMMU, VSI-Bench, Video-TT, TOMATO | 多学科专业视频、空间推理、时序推理、因果推理 |

### 三、 评价指标
- **主要指标**：**准确率**。对于大多数问答和推理任务，使用与标准答案的**精确匹配**。
- **特定数据集指标**：
    - **MME/Video-MME**：报告感知和认知两个维度的分数。
    - **POPE**：用于评估目标幻觉的准确率。
    - 其他数据集均采用各自标准评估协议下的准确率。

### 四、 对比的基线方法
1.  **主流VLMs基线**：包括 LLaVA-1.5-7B, Qwen-VL-Chat-7B, VILA1.5-13B, LLaVA-Next-7B, LLaVA-OneVision-7B, Qwen3-VL-4B/8B, Qwen2.5-VL-7B, InternVL3.5-8B, VideoLLaMA3-7B 等。用于评估CASHEW作为即插即用推理框架的普适提升效果。
2.  **测试时扩展方法**：在相同多采样设置下（N=8），与以下三种主流方法对比：
    - **Self-Consistency**：对多个采样答案进行多数投票。
    - **Self-Selector**：使用VLM自身评估并选择最佳轨迹。
    - **Self-Synthesizer**：生成一个整合多个候选轨迹信息的新答案（单轮合成）。

### 五、 关键性能提升与结论

#### 1. **CASHEW（推理时框架）效果**
- **显著且一致的性能提升**：在几乎所有测试的骨干模型和数据集上，CASHEW都带来了统计显著的性能提升。
- **关键提升示例**：
    - **ScienceQA**：Qwen3-VL-4B 从 **69.5%** 提升至 **93.1%**（+23.6个百分点）。
    - **MME**：Qwen3-VL-8B 从 **643.2/1720.3** 提升至 **738.2/1772.0**（感知/认知均有大幅提升）。
    - **EgoSchema**：Qwen2.5-VL-7B 从 **57.7%** 提升至 **65.8%**（+8.1个百分点）。
    - **NExT-QA**：Qwen3-VL-4B 从 **73.8%** 提升至 **78.8%**（+5.0个百分点）。
- **结论**：CASHEW作为一种无需训练、即插即用的方法，能有效稳定不同规模VLMs的推理，尤其在需要复杂、多步推理的任务上效果突出。

#### 2. **CASHEW-RL（学习型变体）效果**
- **超越CASHEW**：经过GSPO训练后，CASHEW-RL在多数任务上表现优于纯推理时聚合的CASHEW，表明模型**内化了聚合行为**。
- **关键提升示例**：
    - **ScienceQA**：Qwen3-VL-8B + CASHEW-RL (T=3) 达到 **97.8%**，优于基线（92.9%）和CASHEW（97.7%）。
    - **EgoSchema**：Qwen3-VL-8B + CASHEW-RL (T=3) 达到 **75.5%**，持续提升。
    - **TOMATO**：Qwen3-VL-8B + CASHEW-RL (T=3) 从 **31.5%** 提升至 **34.0%**。
- **结论**：通过强化学习，模型学会了更有效地整合和筛选多轨迹信息，并能根据任务难度自适应分配推理长度，实现了更高效、更鲁棒的单一模型推理。

#### 3. **与测试时扩展基线的对比结果**
- **CASHEW全面领先**：在表3的对比中，CASHEW在所有五个基准上均优于Self-Consistency、Self-Selector和Self-Synthesizer。
- **关键优势**：例如在MME上，CASHEW比次优的Self-Synthesizer高出 **48.2/82.6** 分（感知/认知）。在EgoSchema上领先 **2.7** 个百分点。
- **核心结论**：**迭代聚合与视觉验证**的策略，优于简单的“采样-选择”或单轮合成，证明了“共同思考”比“更长时间思考”更有效。

#### 4. **消融实验的关键结论**
- **视觉验证至关重要**：移除Grounding DINO验证步骤会导致性能下降，尤其在需要细粒度目标推理的任务上。
- **两阶段训练缺一不可**：SFT阶段提供了稳定的输出格式，但**GSPO强化学习阶段是获得跨任务一致性能提升的关键**。仅SFT可能导致某些任务性能回退。
- **超参数选择**：`N=8`（种群大小）和 `T=3`（迭代次数）在性能与计算成本间取得了良好平衡。
- **泛化性**：CASHEW-RL在更小的Qwen3-VL-4B模型上也取得了显著提升（如ScienceQA +26.2个百分点），证明了方法的可迁移性。

### 总结
论文通过大量、严谨的实验证明，**CASHEW框架通过迭代轨迹聚合与显式视觉验证，能显著且稳定地提升VLMs在多模态理解和推理任务上的性能**。其学习变体CASHEW-RL进一步将这种能力内化到模型参数中，实现了更高效的推理。该方法在13个主流基准上超越了现有VLMs和测试时扩展方法，为解决多模态推理的不稳定性和幻觉问题提供了有效方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.08010v1)
- [HTML 版本](https://arxiv.org/html/2601.08010v1)
