# Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning

**相关性评分**: 8.0/10

**排名**: #2


---


## 基本信息

- **arXiv ID**: [2601.09708v1](https://arxiv.org/abs/2601.09708v1)
- **发布时间**: 2026-01-14T18:59:59Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Acceleration, fine tune, world model

## 一句话总结

Fast-ThinkAct 提出了一种通过可言语化潜在规划实现高效推理的视觉-语言-动作框架，显著降低推理延迟，适用于机器人控制任务。

## 摘要

Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.

## 详细分析

## 论文摘要：Fast-ThinkAct

**1. 研究背景和动机**
在具身人工智能（如机器人操控）中，视觉-语言-动作模型需要处理复杂场景并执行自适应动作。现有基于思维链的推理VLA模型虽然提升了泛化能力，但因其冗长的显式推理过程（约250个文本token）导致推理延迟极高（约数秒），无法满足实时应用（如1-15Hz决策频率）的需求。如何在保持强大推理能力的同时，大幅提升效率，成为关键挑战。

**2. 核心方法和技术创新**
本文提出 **Fast-ThinkAct**，一个通过**可言语化的潜在规划**实现高效推理的框架。其核心创新在于：
- **可言语化潜在推理**：采用“教师-学生”蒸馏框架。教师模型（文本VLM）生成显式文本思维链，而学生模型则学习生成**紧凑的连续潜在向量**（如6个）来编码推理过程。
- **偏好引导的蒸馏与视觉轨迹对齐**：利用教师模型强化学习训练中的奖励信号构建偏好对（高质量 vs. 低质量推理），通过一个**言语化器**将学生模型的潜在向量解码为文本，并使用类似DPO的目标进行训练，确保潜在向量能表达高质量推理。同时，通过对齐师生模型在视觉轨迹规划上的隐藏状态，传递关键的**空间视觉规划能力**。
- **推理增强的策略学习**：训练完成后，冻结学生VLM，将其生成的潜在视觉规划（从空间token的KV缓存中提取）作为条件，指导一个扩散Transformer动作模型生成低层机器人动作，从而桥接紧凑推理与动作执行。

**3. 主要实验结果**
在多个具身推理与操控基准测试中，Fast-ThinkAct展现出卓越性能与效率：
- **性能领先**：在LIBERO、SimplerEnv、RoboTwin2.0（复杂双手操控）等机器人任务上，成功率超越包括ThinkAct、MolmoAct在内的先进推理VLA和基础VLA。
- **效率大幅提升**：相比ThinkAct-7B，推理延迟**降低高达89.3%**，实现近一个数量级的加速。
- **保持强大推理能力**：在EgoPlan-Bench2、RoboVQA、OpenEQA等具身推理基准上，性能超过GPT-4V、Gemini等通用模型及专用模型，证明了其有效的长时程规划、小样本适应和故障恢复能力。

**4. 研究意义和价值**
Fast-ThinkAct首次系统性地解决了推理VLA的**效率瓶颈**问题。其价值在于：
- **技术贡献**：提出了将显式文本推理压缩为可言语化潜在表示的通用蒸馏框架，并成功将其与视觉规划、动作执行端到端结合。
- **实际应用**：为需要**实时响应**的具身AI应用（如机器人、自动驾驶）提供了兼具高性能与低延迟的解决方案，推动了推理型VLA走向实用化。
- **研究方向**：为后续研究如何设计更紧凑、更高效的具身推理模型提供了新的思路和基线。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Fast-ThinkAct

### **一、 核心问题**
论文旨在解决**视觉-语言-动作（VLA）模型在具身推理任务中的“效率瓶颈”问题**。具体而言：
- **问题**：现有的“推理型VLA”模型（如ThinkAct）通过生成显式的、冗长的思维链（CoT）来提升任务泛化能力和规划能力，但这导致了**极高的推理延迟**（例如，生成数百个文本token需要数秒）。这严重阻碍了在机器人操控、自动驾驶等需要**实时高频决策（1-15 Hz）** 的具身AI应用中的部署。
- **矛盾**：如何在**大幅压缩推理过程、降低延迟**的同时，**保持甚至提升模型的长时程规划、少样本适应和故障恢复等高级推理能力**。

### **二、 核心创新点**
论文提出了 **Fast-ThinkAct** 框架，其核心创新在于 **“可言语化的潜在规划”** 。这是一种将冗长的显式文本推理，压缩并蒸馏到紧凑的连续潜在空间中进行高效内部推理的方法。

主要创新点可分解为：
1.  **推理形式的根本性转变**：
    - **从“显式”到“潜在”**：摒弃生成冗长的文本CoT，改为让模型（学生）自回归地生成少量（如6个）**连续的潜在向量**作为内部推理过程。
    - **从“串行”到“并行”**：引入**可学习的空间token**，用于并行地预测视觉轨迹（如机械臂末端路径点），替代了原本需要数十个token串行描述的轨迹文本。

2.  **创新的训练机制**：
    - **基于偏好的可言语化蒸馏**：设计了一个“言语化器”LLM，负责将学生模型生成的潜在向量解码回文本。通过**偏好学习目标（`ℒ_verb`）**，利用教师模型（经过强化学习训练）产生的推理链质量差异（通过优势函数区分），引导学生模型的潜在表示更倾向于编码高质量的推理模式，抑制低质量的。
    - **动作对齐的视觉规划蒸馏**：通过**轨迹对齐损失（`ℒ_distill`）**，强制学生模型在潜在空间中对齐教师模型编码的视觉规划信息（关键token的隐藏状态），确保压缩后的潜在表示保留了对于具身控制至关重要的**空间-时序动态信息**。

3.  **推理增强的策略学习**：
    - **桥接规划与执行**：将训练好的学生VLM（负责潜在推理和视觉轨迹生成）**冻结**，并将其潜在规划表示（从空间token的Key-Value缓存中提取）作为条件，注入到扩散Transformer等动作模型中，指导低层动作的生成。这实现了**高效的高层规划与精准的低层执行之间的无缝衔接**。

### **三、 解决方案路径**
1.  **构建高效的推理核心**：
    - **教师模型**：使用GRPO（一种RL方法）训练一个文本教师VLM，使其能生成带有质量标签（奖励信号）的文本推理链。
    - **学生模型**：初始化一个与学生结构相同的VLM。通过**联合优化三个损失**来训练它：
        - `ℒ_verb`：偏好蒸馏损失，确保潜在表示可被解码为高质量推理文本。
        - `ℒ_distill`：视觉规划对齐损失，确保潜在表示包含有效的空间信息。
        - `ℒ_ans`：轨迹预测损失，确保空间token能准确预测路径点。
    - **结果**：学生模型学会了用极少的潜在token和并行的空间token完成高效的内部推理和视觉规划。

2.  **构建高效的动作策略**：
    - 将训练好的学生VLM与一个动作模型（如RDT）连接。
    - **冻结**学生VLM和状态编码器，仅训练动作模型和一个轻量的投影层。
    - 动作模型以学生VLM产生的**视觉潜在规划**为条件，进行模仿学习，生成最终的控制指令。

3.  **实现高效推理**：
    - 在推理时，**只需运行学生VLM和动作模型**。言语化器仅在训练时使用，或用于事后解释。学生VLM快速产出紧凑的潜在表示和空间轨迹，动作模型据此快速生成动作，实现了极低的端到端延迟。

### **四、 实际价值与技术贡献**
- **性能与效率的卓越平衡**：在多个机器人操控（LIBERO, SimplerEnv, RoboTwin2.0）和具身推理（EgoPlan, RoboVQA）基准上，**在取得最高或接近最高成功率的同时，将推理延迟降低了高达89.3%**（相比ThinkAct-7B），实现了数量级的速度提升。
- **解锁实时应用潜力**：将推理频率从约0.1 Hz提升到可满足实时要求的范围，为**需要快速响应的具身智能体（如实时机器人、自动驾驶）部署推理能力**扫除了关键障碍。
- **保留并增强了高级能力**：实验证明，该框架不仅速度快，而且**长时程规划、少样本适应、故障识别与恢复**等依赖深度推理的能力均得到保持甚至加强，表明其潜在表示确实有效编码了关键的规划语义。
- **提供通用框架**：该方法不依赖于特定的VLM或动作模型骨干，展示了将文本推理压缩为高效潜在表示的通用蒸馏范式，对后续高效多模态推理研究具有启发意义。

**总结**：Fast-ThinkAct 的核心创新在于通过 **“偏好引导的可言语化潜在蒸馏”** 这一精巧设计，将VLA模型中耗时的显式文本推理过程，压缩为一个高效、紧凑且信息丰富的内部潜在表示，从而在几乎不损失甚至提升性能的前提下，革命性地提升了推理速度，为实时具身AI应用铺平了道路。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决视觉-语言-行动模型中，显式思维链推理方法因生成冗长推理轨迹而导致的高推理延迟问题，这严重阻碍了在需要实时响应的具身AI应用中的部署。为此，论文提出了 **Fast-ThinkAct** 框架，其核心创新在于通过**可言语化的潜在规划**来实现高效推理。该方法采用师生蒸馏架构，将教师模型生成的冗长文本思维链，通过偏好引导的目标函数和视觉轨迹对齐，压缩为紧凑的连续潜在表示，并训练一个“言语化器”来确保这些潜在表示是可解释的。训练完成后，学生模型生成的潜在规划信息被用于指导下游动作模型的决策，从而将高效的多模态推理与动作执行桥接起来。

实验结果表明，该方法在多个具身操作和推理基准测试中保持了强大的性能（如在长时程规划、少样本适应和失败恢复方面），同时实现了高达 **89.3%** 的推理延迟降低，显著提升了推理型VLA模型的效率。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## Fast-ThinkAct 论文创新点分析

这篇论文针对视觉-语言-动作（VLA）任务中推理模型推理延迟高的问题，提出了一种高效推理框架。其核心创新在于将冗长的显式思维链（CoT）压缩为紧凑、可言语化的潜在表示，从而在保持强大推理能力的同时，大幅提升推理效率。以下是其明确的创新点：

### 1. **提出“可言语化潜在规划”作为高效推理的核心机制**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：如 ThinkAct、MolmoAct 等“推理型 VLA”模型，依赖于生成冗长的、显式的文本思维链（约250个token）进行逐步推理。这直接导致了高昂的推理延迟（秒级）。
     - **Fast-ThinkAct**：不生成显式文本，而是让模型在**连续的潜在空间**中进行推理，生成少量（如6个）紧凑的连续潜在向量（`z`）和并行的空间token。这些潜在表示可以通过一个“言语化器”解码为文本，因此是“可言语化”的。
   - **解决的具体问题/带来的优势**：
     - **核心问题**：解决了显式文本推理带来的**高推理延迟瓶颈**，该瓶颈严重阻碍了在机器人操控等需要实时决策（1-15 Hz）的具身AI应用中的部署。
     - **核心优势**：实现了**高达89.3%的推理延迟降低**（相比ThinkAct-7B），同时性能不降反升。这使得在保持强大规划能力的前提下，满足实时性要求成为可能。

### 2. **设计了“基于奖励偏好的师生蒸馏”框架来学习高质量潜在推理**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：
       1. **监督式CoT方法**（如Embodied CoT）：需要大量人工标注的推理数据，成本高且受数据覆盖范围限制。
       2. **RL微调方法**（如ThinkAct）：使用强化学习（GRPO）生成CoT，但生成的推理轨迹质量参差不齐，且仍需生成冗长文本。
     - **Fast-ThinkAct**：采用**师生蒸馏**框架。教师模型（`ℱ_θ^T`）通过GRPO训练生成带有质量标签（优势函数 `A(τ)`）的文本推理链。学生模型（`ℱ_θ`）则学习生成潜在表示。
        - **关键创新**：引入一个**言语化器LLM（`𝒱_ψ`）**，将学生生成的潜在向量解码回文本空间，并利用从教师模型获得的**奖励偏好信号**（高质量 `τ+` vs 低质量 `τ-`）进行训练（使用类似DPO的目标 `ℒ_verb`）。
   - **解决的具体问题/带来的优势**：
     - **核心问题**：解决了**如何在没有直接监督信号的情况下，将推理压缩到潜在空间**，并确保压缩后的表示能保留高质量、与任务相关的推理模式。
     - **核心优势**：
       1. **偏好引导**：不是简单模仿教师的所有输出，而是有选择地**蒸馏高质量推理，抑制低质量模式**，从而提升了学生模型推理的准确性和简洁性（如图7所示，学生推理更聚焦）。
       2. **可解释性基础**：通过言语化器将潜在表示与文本关联，确保了潜在推理的**可解释性和可控性**，尽管推理时无需生成文本。

### 3. **引入了“动作对齐的视觉规划蒸馏”以传递空间推理能力**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：许多工作侧重于语言或抽象的推理，缺乏将**高层推理与低层视觉-动作空间**紧密耦合的机制。一些方法（如MolmoAct）生成子目标图像或2D轨迹，但仍是显式且计算量大的。
     - **Fast-ThinkAct**：除了语言推理蒸馏，还专门设计了一个**视觉规划对齐损失（`ℒ_distill`）**。它通过最小化教师模型和学生模型在编码**视觉规划**（如预测的轨迹路径点）时的隐藏状态距离，将教师基于轨迹奖励学到的**空间推理能力**直接传递给学生。
     - **并行空间token**：学生模型使用一组可学习的空间token来**并行地**预测K个路径点坐标，取代了教师模型需要自回归生成数十个文本token来描述轨迹的低效方式。
   - **解决的具体问题/带来的优势**：
     - **核心问题**：解决了**如何让潜在表示不仅包含语言逻辑，还能编码对具身控制至关重要的空间和视觉动态信息**。
     - **核心优势**：
       1. **提升动作质量**：确保了紧凑的潜在表示**扎根于视觉规划**，从而能更有效地指导后续的动作生成，提高了任务成功率。
       2. **提升效率**：并行预测轨迹比自回归文本描述**快得多**，进一步降低了延迟。

### 4. **提出了“推理增强的策略学习”来桥接规划与执行**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：许多VLA模型是端到端训练，或将推理与动作生成较为松散地结合。推理模块和策略模块的交互可能不够深入。
     - **Fast-ThinkAct**：在训练好学生VLM（`ℱ_θ`）后，**冻结其参数**，并将其生成的**视觉潜在规划（`c_t`）** 作为条件，注入到扩散Transformer策略模型（`π_ϕ`）的交叉注意力层中。具体来说，`c_t` 来源于学生VLM**浅层**空间token的Key-Value缓存，与状态观测的KV对拼接后共同指导动作生成。
   - **解决的具体问题/带来的优势**：
     - **核心问题**：解决了**如何将高层、抽象的潜在推理，有效地转化为精确、低层的机器人动作指令**。
     - **核心优势**：
       1. **有效桥接**：通过KV缓存注入机制，实现了**规划与执行阶段的深度、高效融合**，使动作模型能充分利用推理模块提供的结构化视觉规划信息。
       2. **训练稳定性与效率**：冻结推理模块后仅微调策略模块，避免了联合训练的复杂性，使学习更稳定，并便于利用大规模机器人动作数据。

### 总结
Fast-ThinkAct 的系统性创新在于：**1）用潜在表示替代显式文本以提升效率；2）用偏好蒸馏确保潜在表示的质量；3）用视觉对齐确保潜在表示的空间可操作性；4）用规划-执行桥接机制确保推理能有效落地为动作。** 这些创新共同解决了推理型VLA的核心矛盾——**“强推理能力”与“低推理延迟”之间的权衡**，使其在长视野规划、少样本适应、失败恢复等任务上保持甚至提升性能的同时，实现了近一个数量级的加速，为实时具身智能应用提供了可行的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 使用的数据集
论文在训练和评估中使用了**多模态、大规模**的数据集，涵盖视觉-语言-动作（VLA）任务的多个方面。

#### 1. 训练数据集
*   **2D视觉轨迹数据**:
    *   **单臂操作**: 来自MolmoAct标注的Open X-Embodiment (OXE)数据集，约130万条轨迹。
    *   **双臂操作**: 从AIST数据集提取的双臂视觉轨迹，约9.2万条样本。
*   **视觉问答与推理数据**:
    *   **RoboFAC**: 用于失败分析与纠正规划，包含约6.4万对QA。
    *   **RoboVQA**: 用于机器人操作视频理解，包含约79.8万对QA。
    *   **ShareRobot**: 大规模机器人任务规划与操作知识数据集，超过100万对QA。
    *   **EgoPlan-Bench**: 用于第一人称视角长时程任务规划，约5.3万视频-文本对。
    *   **Video-R1-CoT**: 带有思维链标注的视频问答数据，约16.5万对，用于通用视觉推理。
    *   **PixMo**: 通用视觉语言数据集，用于防止在具身数据上训练时发生灾难性遗忘。

#### 2. 评估基准
*   **具身推理基准**:
    *   **EgoPlan-Bench2**: 评估第一人称视角任务规划，使用**准确率**。
    *   **RoboVQA**: 评估机器人操作场景的视觉推理，使用**BLEU分数**。
    *   **OpenEQA**: 评估对真实世界环境的零样本空间与功能理解，使用**LLM评分**。
    *   **RoboFAC**: 专门评估失败识别与纠正能力。
*   **机器人操作基准**:
    *   **LIBERO**: 评估多样化操作任务的泛化能力（空间、物体、目标、长时程），使用**任务成功率**。
    *   **SimplerEnv-Google**: 具有强仿真到真实相关性的操作基准，使用**任务成功率**。
    *   **RoboTwin2.0**: 具有挑战性的双臂长时程操作基准，分简单和困难（带域随机化）设置，使用**任务成功率**。

### 二、 对比的基线方法
论文与当前最先进的模型进行了广泛对比，主要分为三类：

1.  **基础VLA模型**:
    *   `OpenVLA`, `π₀`, `RDT`, `Magma`, `DP`, `ACT`。这些模型主要基于模仿学习，缺乏显式推理。

2.  **推理VLA模型**:
    *   `ThinkAct`: 使用强化学习生成冗长文本思维链的推理VLA。
    *   `CoT-VLA`: 使用监督学习生成视觉子目标等结构化推理。
    *   `MolmoAct`: 使用空间表示进行结构化推理。
    *   `RoboBrain2.0`: 大规模预训练的具身模型。

3.  **高效推理方法**:
    *   `ECoT-Lite`: 通过推理丢弃来加速推理。
    *   文本推理的简化基线：如强制教师模型只生成少量文本token，或使用RL长度惩罚。

### 三、 关键性能提升与结论

#### 1. **核心效率优势：大幅降低推理延迟**
*   **主要结论**: Fast-ThinkAct在**保持甚至提升性能**的同时，实现了**数量级**的推理加速。
*   **关键数据**:
    *   相比 `ThinkAct-7B`，推理延迟**降低89.3%**（从7513ms降至805ms）。
    *   相比 `ThinkAct-3B`，推理速度**提升7倍**（从5674ms降至805ms）。
    *   相比 `MolmoAct-7B`，推理延迟降低88.0%。
*   **原因**: 将冗长的文本思维链（~250个token）压缩为**少量可言语化的潜在token（如6个）和并行的空间token**，实现了高效的内部推理。

#### 2. **操作性能：全面超越基线**
*   **LIBERO基准**: Fast-ThinkAct-3B取得了**89.7%** 的平均成功率，显著优于 `ThinkAct-3B` (83.1%)、`MolmoAct-7B` (86.8%) 和 `OpenVLA-7B` (76.5%)。
*   **SimplerEnv-Google基准**: 取得**68.7%** 的成功率，优于 `ThinkAct-7B` (68.3%) 和 `MolmoAct-7B` (64.9%)。
*   **RoboTwin2.0基准（双臂操作）**:
    *   在**简单设置**下，平均成功率**65.7%**，优于 `ThinkAct` (62.4%) 和 `RDT` (56.4%)。
    *   在**困难设置**（域随机化）下，平均成功率**26.4%**，同样优于所有基线。
    *   在**长时程任务**（>270步）上表现尤为突出，证明了其有效的长程规划能力。

#### 3. **推理能力：在具身问答任务上领先**
*   **EgoPlan-Bench2**: Fast-ThinkAct-3B取得**46.4%** 的平均准确率，优于 `ThinkAct-3B` (44.0%) 和甚至部分更大的专有模型（如GPT-4V的32.6%）。
*   **RoboVQA**: 取得**60.8**的BLEU平均分，大幅领先 `ThinkAct-3B` (55.3) 和 `RoboBrain2.0-3B` (46.5)。
*   **OpenEQA**: 取得**51.2**的LLM评分，优于 `ThinkAct-3B` (48.9)。
*   **结论**: 表明压缩的潜在推理不仅没有损失语义理解能力，反而通过偏好蒸馏过滤了冗余信息，产生了更聚焦、高质量的推理模式。

#### 4. **高级能力验证**
*   **失败恢复（RoboFAC）**: 在仿真和真实机器人设置上，分别以**10.9分**和**16.4分**的优势大幅领先最佳基线 `RoboFAC-3B`，证明了其优秀的错误诊断与纠正规划能力。
*   **少样本适应**: 在RoboTwin2.0上仅用每个任务10条演示进行微调，在中长时程任务上显著优于 `ThinkAct` 和 `π₀`，展示了高效推理对快速适应新任务的促进作用。

#### 5. **消融实验结论**
*   **损失函数有效性**: 移除可言语化损失 `ℒ_verb` 或视觉规划蒸馏损失 `ℒ_distill` 都会导致性能下降，验证了二者对于将高质量推理模式和视觉规划能力压缩到潜在空间都是必要的。
*   **与高效文本基线对比**: 强制教师模型生成少量文本token或使用长度惩罚都会导致性能显著下降，而Fast-ThinkAct使用潜在表示则能同时实现高效和高性能，证明了**潜在推理压缩方法的优越性**。

### 总结
Fast-ThinkAct通过**可言语化的潜在规划**这一核心技术，成功解决了现有推理VLA模型**推理延迟高**的核心瓶颈。实验全面证明，该方法在**机器人操作成功率**、**具身推理精度**、**长时程规划**、**失败恢复**和**少样本适应**等关键能力上均达到或超越了最先进的推理VLA模型，同时实现了**高达近90%的推理延迟降低**，为实时具身AI应用提供了高效且强大的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.09708v1)
- [HTML 版本](https://arxiv.org/html/2601.09708v1)
