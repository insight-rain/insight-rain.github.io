# ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation

**相关性评分**: 8.0/10

**排名**: #5


---


## 基本信息

- **arXiv ID**: [2601.08325v1](https://arxiv.org/abs/2601.08325v1)
- **发布时间**: 2026-01-13T08:29:07Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Zhenyang Liu, Yongchong Gu, Yikai Wang, Xiangyang Xue, Yanwei Fu

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

ActiveVLA 是一种通过注入主动感知能力来增强视觉-语言-动作模型的新框架，用于实现精确的3D机器人操作，通过粗到细的范式提升性能。

## 摘要

Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.

## 详细分析

## 论文摘要：ActiveVLA: 将主动感知注入视觉-语言-动作模型以实现精确3D机器人操作

### 1. 研究背景和动机
近年来，基于预训练视觉-语言模型（VLM）的视觉-语言-动作（VLA）模型在机器人操作领域展现出巨大潜力。然而，现有VLA方法大多依赖静态或腕部固定摄像头，提供的是固定、以末端执行器为中心的视角。这种**被动感知**方式无法在执行任务时自适应地选择最优视角或分辨率，严重限制了模型在长时程任务和细粒度操作场景中的性能。为解决这一核心局限，本文提出了**ActiveVLA**，一个旨在为机器人注入**主动感知**能力的新型VLA框架，以实现高精度、细粒度的3D操作。

### 2. 核心方法和技术创新
ActiveVLA采用**由粗到精**的两阶段范式：
- **关键区域定位（粗阶段）**：将3D点云投影到多视角（顶、前、右）的2D正交图像，利用VLM骨干网络（如PaliGemma）生成2D热力图，并通过反投影定位3D场景中的关键任务区域。
- **主动感知优化（精阶段）**：基于定位的关键区域，进行**主动视角选择**和**主动3D放大**。
    - **主动视角选择**：在关键区域周围球面上生成候选视点，通过一个综合了**可见性**（避免遮挡）、**距离**和**视角多样性**的多目标评分函数，动态选择最优的K个观测视角。
    - **主动3D放大**：在最优视角上，通过虚拟渲染缩小视场角，实现对关键区域的“光学变焦”，在不损失像素分辨率的情况下获取局部高分辨率细节。

该框架的创新在于将**主动感知**（决定“看哪里”和“看多细”）与**3D空间推理**、**视觉-语言理解**深度融合，形成了一个闭环的感知-行动管道。

### 3. 主要实验结果
在三个仿真基准测试中，ActiveVLA均取得了最先进的性能：
- **RLBench**：在18个任务上平均成功率高达**91.8%**，在10个任务上排名第一。
- **COLOSSEUM**：在包含多种扰动（如物体颜色、纹理、大小、相机位姿变化）的泛化场景中，平均成功率达**65.9%**，优于所有基线。
- **GemBench**：在评估技能组合与泛化能力的任务上，平均成功率达**51.3%**，创下新纪录。
- **真实机器人实验**：在包含严重遮挡的复杂场景（如从分层抽屉取毛巾、在水果堆中抓取被遮挡的香蕉）中，ActiveVLA展现出强大的泛化能力和高成功率（整体**91.8%**），显著优于现有方法。

消融实验证实了主动视角选择（A-VS）和主动3D放大（A-3Z）两个模块对性能提升的关键作用。

### 4. 研究意义和价值
本研究的意义在于：
- **方法论贡献**：首次在VLA框架中系统性地集成了主动感知机制，为解决机器人操作中的遮挡、细节感知不足等长期挑战提供了新思路。
- **性能突破**：通过动态调整感知策略，ActiveVLA在多个高难度基准上实现了性能的显著提升，证明了主动感知对于实现精确、鲁棒操作的必要性。
- **实用价值**：框架在仿真和真实世界均表现优异，展示了强大的从仿真到现实的迁移能力，为开发能在复杂、非结构化环境中工作的自适应具身智能体迈出了关键一步。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：ActiveVLA

### **一、 核心问题**
现有的大多数视觉-语言-动作模型在机器人操作中存在一个根本性缺陷：**感知是被动的**。它们通常依赖于静态的、安装在手腕上的摄像头，提供固定的、以末端执行器为中心的视角。这导致了两个关键问题：
1.  **无法自适应选择最优视角**：在任务执行过程中，模型不能根据上下文动态调整观察位置，导致在长视野任务或复杂场景中信息获取不全。
2.  **缺乏精细分辨率感知**：对于需要精细操作的任务（如插入、焊接小孔），固定视角无法提供关键区域的高分辨率细节。

这些问题严重限制了VLA模型在**长视野、细粒度、高精度3D操作**中的性能和鲁棒性。

### **二、 核心创新点**
论文提出了 **ActiveVLA** 框架，其核心创新在于将 **“主动感知”** 能力注入到视觉-语言-动作模型中。这并非简单的模块添加，而是一种根本性的范式转变——使机器人从“被动看”变为“主动探”。

具体创新体现在以下三个层面：

1.  **方法论创新：主动感知范式**
    *   **核心理念**：借鉴Richard Gregory的认知理论，将感知视为一个主动的“假设检验”过程。机器人应能主动寻求、选择和验证与任务相关的信息。
    *   **实现方式**：通过一个**由粗到精的两阶段管道**，动态调整感知输入。

2.  **技术创新：两大主动感知机制**
    *   **主动视角选择**：
        *   **功能**：自主决定任务执行过程中的最优相机位姿。
        *   **策略**：基于在粗阶段定位的3D关键区域，在球面上生成候选视角，并通过一个**多目标评分函数**（权衡**可见性**、**距离**和**视角多样性**）选择最优的K个视角。这最大化了对目标的无模态完整观测，同时最小化了遮挡。
    *   **主动3D放大**：
        *   **功能**：在选定的最优视角上，进行虚拟的“光学变焦”，提升关键区域的点云渲染分辨率。
        *   **实现**：通过缩小渲染视场角，在像素分辨率不变的情况下，获得目标区域的放大特写视图。这在不损失几何细节的前提下，为细粒度操作（如精确的夹爪位姿预测）提供了必需的视觉细节。

3.  **架构创新：由粗到精的集成管道**
    *   **粗阶段（定位）**：
        *   将3D点云投影到多视图（顶、前、右）2D正交图像。
        *   利用预训练的VLM骨干网络处理这些图像和语言指令，通过一个**热图预测模块**定位任务相关的3D关键区域。
    *   **精阶段（优化）**：
        *   基于定位的关键区域，启动上述的**主动视角选择**和**主动3D放大**机制。
        *   将优化后的高分辨率多视图输入VLM，预测更精确的末端执行器位置热图，最终由动作解码器输出6-DoF动作。

### **三、 解决方案总结**
**ActiveVLA通过一个闭环的、由粗到精的感知-动作管道，解决了被动感知的局限性：**
1.  **“看哪里”**：先利用3D空间先验进行粗定位，找到任务核心区域。
2.  **“怎么看”**：然后主动选择能最好地观察该区域（无遮挡、视角丰富）的相机位姿。
3.  **“看多细”**：最后对该区域进行虚拟变焦，获取高分辨率细节以指导精确操作。

这种方法使机器人能够**按需获取信息**，从而在复杂、遮挡严重、需要高精度的长视野操作任务中，实现更可靠、更精确的动作预测。

### **四、 实际价值**
1.  **性能提升**：在RLBench、COLOSSEUM、GemBench三个仿真基准测试中均达到最先进水平，特别是在需要精确接触和长序列规划的任务上优势明显。
2.  **强泛化与鲁棒性**：在COLOSSEUM的多种扰动（颜色、纹理、大小、相机位姿等）下表现稳健，证明了其学习到的表征具有强不变性。
3.  **仿真到现实的迁移**：在真实机器人实验中，面对复杂遮挡和非结构化环境，成功完成了多项高精度操作任务，验证了框架的实用性。
4.  **为具身智能指明方向**：论文论证了将主动感知与VLA模型结合是迈向自适应、可靠具身智能的关键一步，为后续研究提供了重要范式。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决现有视觉-语言-动作（VLA）模型在机器人精细操作任务中的一个核心缺陷：**被动感知**。现有模型通常依赖固定或腕部安装的摄像头，无法在执行任务时动态调整观察视角和分辨率，导致在长视野任务和需要精细操作的场景中性能受限。

为此，论文提出了 **ActiveVLA** 框架，其核心创新在于将**主动感知**能力注入VLA模型。该方法采用**由粗到精**的两阶段范式：首先，通过多视角投影和热图预测定位3D场景中的关键区域；其次，基于此区域，通过一个主动视角选择策略来优化观察点（最大化信息相关性与多样性，最小化遮挡），并应用**3D局部放大**机制来提升关键区域的分辨率，从而实现更精细的感知。

实验结果表明，ActiveVLA在RLBench、COLOSSEUM和GemBench三个仿真基准测试中均取得了**最先进的性能**，并在真实世界的复杂、遮挡严重的环境中展现了强大的泛化能力和高成功率。这证明了将主动感知集成到VLA模型中，是提升机器人自适应性和精确操作能力的关键一步。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## ActiveVLA 论文创新点分析

这篇论文提出的 **ActiveVLA** 框架，其核心创新在于将 **主动感知** 能力系统地注入到视觉-语言-动作模型中，以解决现有VLA模型在复杂、长视野、细粒度操作任务中的根本性局限。以下是其明确的创新点及其与以往方法的对比和优势：

### 1. **引入主动感知范式，实现动态、自适应的观察策略**
   - **相比以往方法**：现有VLA模型（如OpenVLA, TriVLA, SpatialVLA）大多依赖**静态、腕部固定或末端执行器中心**的摄像头。它们只能提供固定的、被动的视觉输入，无法根据任务上下文动态调整观察视角或分辨率。
   - **改进/不同之处**：ActiveVLA 赋予机器人**主动选择最优观察视角**和**主动进行3D局部放大**的能力。这是一个从“被动接收”到“主动探索与验证”的根本性范式转变。
   - **解决的具体问题/带来的优势**：
     - **解决长视野任务中的信息获取不足问题**：在复杂多步骤任务中，机器人可以主动选择能揭示关键信息（如被遮挡物体）的视角，从而做出更可靠的决策。
     - **提升细粒度操作的精度**：通过“3D放大”机制，可以虚拟地提高关键操作区域（如小孔、精细结构）的视觉分辨率，从而实现毫米级精度的操作。
     - **增强对遮挡的鲁棒性**：通过主动选择能最大化目标可见性、最小化遮挡的视角，显著提升了在杂乱、遮挡严重环境下的操作成功率。

### 2. **提出“由粗到精”的两阶段主动感知架构**
   - **相比以往方法**：许多3D感知的VLA方法（如RVT, Act3D）虽然利用了3D信息，但感知流程是**单次、静态**的。它们要么直接处理3D点云，要么将多视角2D投影一次性输入模型，缺乏一个**聚焦-细化**的动态过程。
   - **改进/不同之处**：ActiveVLA 设计了一个清晰的两阶段流程：
     1.  **粗粒度阶段**：将3D场景投影到多个正交2D视图，利用预训练VLM（PaliGemma）生成热力图，定位出任务相关的**关键3D区域**。
     2.  **细粒度阶段**：基于定位的关键区域，执行**主动视角选择**和**主动3D放大**，获取高分辨率、信息量最大的局部观察，用于精确的动作预测。
   - **解决的具体问题/带来的优势**：
     - **平衡全局上下文与局部细节**：粗阶段保证了对整体场景的理解，细阶段则专注于执行动作所需的精确几何信息。
     - **提高计算和决策效率**：避免了在无关区域进行高分辨率计算，将计算资源集中在任务关键区域。
     - **实现了感知-行动的闭环**：观察策略（看哪里、看多细）是根据当前任务状态和感知结果动态调整的，形成了一个更智能的感知-行动循环。

### 3. **设计“主动视角选择”策略，优化多视角观测**
   - **相比以往方法**：现有方法要么使用固定视角，要么简单地在预设的几个视角间切换。它们缺乏一个**量化的、目标驱动的标准**来评估和选择“最优”视角。
   - **改进/不同之处**：ActiveVLA 提出了一个基于**多目标评分函数**的主动视角选择策略。该策略从以关键区域为中心的球面上采样候选视点，并从三个维度进行评分：
     - **可见性**：评估视线是否被场景几何体遮挡。
     - **距离**：偏好适中的观察距离以平衡视野和细节。
     - **多样性**：最大化所选视角之间的角度差异，以获得互补的观测信息。
   - **解决的具体问题/带来的优势**：
     - **最大化信息获取**：通过综合评分，系统性地选择能提供最完整、最互补信息的视角组合。
     - **减少感知歧义**：多视角、多样性的观测有助于解决因单一视角遮挡或视角不佳导致的物体形状、位姿歧义。
     - **公式化与可优化**：将视角选择问题转化为一个可量化的优化问题，为后续改进提供了明确的方向。

### 4. **提出“主动3D放大”机制，实现虚拟高分辨率感知**
   - **相比以往方法**：传统方法依赖于物理相机的固定分辨率。在仿真中，虽然可以渲染，但通常也是固定视野。当需要观察微小物体或精细结构时，要么分辨率不足，要么需要预先设置特写相机，缺乏**自适应的、基于需求的**高分辨率感知能力。
   - **改进/不同之处**：ActiveVLA 在选定最优视角后，**在虚拟渲染层面动态缩小视野（FoV）**，对关键区域进行重新渲染。这模拟了光学变焦的效果，在**不增加物理像素、不损失几何信息**的前提下，显著提高了局部区域的**像素分辨率**。
   - **解决的具体问题/带来的优势**：
     - **解决细粒度操作对高分辨率的需求**：例如，将焊枪插入小孔、拧紧小螺丝等任务，需要亚厘米级的定位精度，主动3D放大为此提供了必要的视觉细节。
     - **成本效益高**：无需部署高成本的高分辨率物理相机或在环境中预设多个特写机位，完全通过算法和3D重建实现。
     - **无缝衔接仿真与真实**：该机制在仿真中通过渲染实现，其原理（基于3D点云的局部重渲染）可以较好地迁移到真实世界（通过RGB-D相机重建的点云）。

### 5. **在多个权威基准上实现SOTA性能，并验证了强大的真实世界泛化能力**
   - **相比以往方法**：许多先进方法可能在某个特定基准（如RLBench）上表现良好，但在强调**泛化性、鲁棒性、组合性**的更复杂基准（如COLOSSEUM, GemBench）上性能下降明显，且真实世界验证往往不足。
   - **改进/不同之处**：
     - **全面领先**：在RLBench（平均成功率91.8%）、COLOSSEUM（65.9%）、GemBench（51.3%）三个具有不同侧重点的仿真基准上均取得了**新的最高性能**。
     - **突出鲁棒性**：在COLOSSEUM的多种扰动（物体颜色、纹理、大小、光照、相机位姿等变化）下表现稳健，证明了其学习到的表征具有强大的不变性。
     - **成功的真实世界迁移**：论文展示了在高度遮挡、结构复杂的真实场景中（如从多层抽屉取毛巾、从水果堆中抓被遮挡的香蕉），ActiveVLA能成功完成任务，且成功率显著高于其他基线（如RVT-2, TriVLA）。
   - **解决的具体问题/带来的优势**：
     - **证明了主动感知的实际价值**：实验数据强有力地支撑了“主动感知是提升VLA模型在复杂现实场景中性能的关键”这一核心论点。
     - **提供了可复现的强基线**：为后续在3D机器人操作中研究主动感知的工作设立了一个高性能的基准。
     - **推动了VLA模型走向实用化**：出色的真实世界表现表明，ActiveVLA是迈向能在非结构化环境中可靠工作的通用机器人智能体的重要一步。

**总结**：ActiveVLA 的核心创新不是对VLM主干或动作解码器的微小改进，而是**在感知层面引入了一个动态、自适应、由任务驱动的“主动环”**。它通过**由粗到精的架构、量化的视角选择、虚拟的3D放大**这三个相互支撑的技术模块，系统性地解决了现有VLA模型因被动感知而导致的**信息不足、精度不够、鲁棒性差**三大痛点，从而在性能和泛化能力上实现了显著飞跃。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

论文通过广泛的仿真和真实世界实验，全面评估了 **ActiveVLA** 框架的性能，证明了其在3D精细操作任务中的优越性。

### 一、 使用的数据集与评价指标

#### 1. 仿真基准数据集
- **RLBench**：包含18个长视野、多步骤的机器人操作任务（如插入木桩、打开抽屉、堆叠杯子）。使用**平均成功率**作为核心评价指标，并在多个任务上报告具体成功率。
- **COLOSSEUM**：基于RLBench，引入了12种系统性的扰动（如物体颜色、纹理、大小、光照、相机位姿变化），用于评估模型的**鲁棒性和泛化能力**。评价指标为在各种扰动下的**平均成功率**。
- **GemBench**：一个层次化、组合式的基准测试，包含16个训练任务和44个测试任务，旨在评估模型对**基础动作原语的组合与泛化能力**。评价指标为在不同难度等级（L1-L4）上的**平均成功率**。

#### 2. 真实世界评估
- 在KINOVA GEN2机器人上，使用RealSense D455相机，设置了4个具有**复杂遮挡和精细操作需求**的任务（如从分层抽屉中取毛巾、在遮挡下抓取香蕉）。
- 评价指标为**任务成功率**，并与其他基线方法在相同任务上进行对比。

### 二、 对比的基线方法
论文与一系列先进的机器人操作策略进行了对比，涵盖了2D、3D以及VLA范式的方法：
- **基于2D图像的行为克隆**：`Image-BC (CNN/ViT)`
- **基于体素的3D策略**：`C2F-ARM-BC`, `PerAct`
- **先进的3D操作模型**：`Act3D`, `RVT` / `RVT-2`, `3D Diffuser Actor`
- **最新的视觉-语言-动作模型**：`BridgeVLA` (作为最接近的VLA基线)
- **其他专门模型**：`HiveFormer`, `PolarNet`, `3D-LOTUS(++)`

### 三、 关键性能提升与结论

#### 1. 在仿真基准上的卓越性能
- **RLBench**：
    - **ActiveVLA取得了新的最高水平**，平均成功率达到 **91.8%**，平均排名为 **1.22**（排名越低越好）。
    - 在10个任务上排名第一，尤其在需要高精度的任务（如`Insert Peg`, `Open Drawer`）上表现完美（100%成功率）。
    - 显著优于之前的SOTA方法`BridgeVLA`（88.2%）。

- **COLOSSEUM**：
    - **ActiveVLA再次刷新纪录**，平均成功率为 **65.9%**，平均排名 **1.07**。
    - 在大多数扰动类别（如物体大小`MO-SIZE`: 72.4%，桌面颜色`Table Color`: 78.3%）上均超越所有基线，证明了其强大的**视觉不变性表示和抗干扰能力**。

- **GemBench**：
    - **ActiveVLA达到最佳综合性能**，平均成功率为 **51.3%**，比之前的SOTA (`BridgeVLA`, 50.0%) 提升 **1.3个百分点**。
    - 在核心难度级别L1-L3上均取得最高成功率，展示了其优秀的**技能组合与长视野推理能力**。

#### 2. 在真实世界中的有效迁移
- 在四个精心设计的、遮挡严重的真实机器人任务中，**ActiveVLA取得了压倒性的成功**，平均成功率高达 **91.8%**。
- 相较于强大的基线`RVT-2`（72.5%）和`TriVLA`（64.0%），**ActiveVLA带来了显著的性能跃升**（提升约19-28个百分点），这直接归功于其**主动感知机制**（动态选择视角和3D放大）在部分可观测、复杂场景下的优势。

#### 3. 消融实验验证核心贡献
- **组件分析**：实验证实了**主动视角选择**和**主动3D放大**两个模块缺一不可。同时使用两者时，在RLBench上获得最高成功率（91.8%），而仅使用固定视角或单一模块时性能下降。
- **超参数分析**：确定了最优配置（选择3个视角、放大因子为4），在全局上下文和局部细节之间取得了最佳平衡。

### 四、 核心结论
论文通过定量实验有力地证明了：
1.  **技术创新有效**：将**主动感知**（动态视角选择与3D局部放大）注入VLA框架，能显著提升机器人在**长视野、高精度、强遮挡**场景下的操作性能。
2.  **实现SOTA性能**：在三大仿真基准上全面超越现有方法，确立了新的性能标杆。
3.  **具备实际价值**：模型能够**无缝迁移到真实世界**，在复杂的物理遮挡环境中依然保持高成功率，证明了该框架的实用性和鲁棒性。

**最终效果**：ActiveVLA不仅是一个学术上的性能突破，更通过其“由粗到细”的主动感知范式，为解决现实世界中机器人精细操作的瓶颈问题提供了一个有效且可泛化的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.08325v1)
- [HTML 版本](https://arxiv.org/html/2601.08325v1)
