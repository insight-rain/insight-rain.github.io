# Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning

**相关性评分**: 6.0/10

**排名**: #20


---


## 基本信息

- **arXiv ID**: [2601.10905v1](https://arxiv.org/abs/2601.10905v1)
- **发布时间**: 2026-01-15T23:33:21Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Rajat Ghosh, Debojyoti Dutta

## 关键词

offline Reinforcement Learning, world model

## 一句话总结

该论文提出了一种名为Action Shapley的指标，用于在强化学习中为世界模型选择训练数据，并设计算法提高计算效率，主要与离线强化学习和世界模型相关。

## 摘要

Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.

## 详细分析

## 论文摘要：Action Shapley: 一种用于强化学习中世界模型的训练数据选择度量

### 1. 研究背景和动机
在基于模型的强化学习中，世界模型对于在交互成本高或危险的真实环境中进行仿真至关重要。然而，世界模型的效能和可解释性高度依赖于其训练数据的质量。现有方法缺乏一种系统、无偏的度量来评估和选择对强化学习智能体最终性能贡献最大的训练数据点。本研究旨在解决这一关键问题，提出一种专门用于强化学习世界模型训练的数据选择度量。

### 2. 核心方法和技术创新
本文的核心创新是提出了 **Action Shapley**。该方法将博弈论中的**沙普利值**概念引入强化学习领域，用于量化每个训练数据点对整个“训练数据选择 → 世界模型训练 → 智能体策略学习”端到端流程的**边际贡献**（以智能体获得的累积奖励为评估函数）。

为了应对传统沙普利值计算指数级复杂度的挑战，论文设计了一种**随机化动态算法**。该算法引入了**截止基数**的概念，当评估到一定数量的子集失败（智能体无法达成目标）时即提前终止计算，从而将计算复杂度从最坏情况的 `O(2ⁿ)` 大幅降低。实验表明，该算法实现了超过 **80%** 的计算效率提升。

### 3. 主要实验结果
研究在五个真实世界的动态控制案例中进行了实证验证：
- **案例**：包括虚拟机资源调配、负载均衡、数据库调优、Kubernetes管理和数据中心冷却管理。
- **有效性**：基于Action Shapley值选择出的最优训练数据集（在截止基数内选择平均Action Shapley值最高的数据点）所训练的智能体，其性能显著优于基于最差Action Shapley数据集训练的智能体，并且在多数情况下优于随机选择的数据集。
- **效率与性能**：在四个案例中，仅使用截止基数规定数量的关键数据点训练的“最优Action Shapley智能体”，其性能甚至超过了使用全部可用数据训练的基线智能体，证明了该方法在提升数据效率的同时保证了模型性能。

### 4. 研究意义和价值
本研究首次将沙普利值系统性地应用于强化学习的训练数据评估与选择中。其价值主要体现在：
- **理论贡献**：为强化学习中的数据估值问题提供了一个严谨、可解释的博弈论框架。
- **实践价值**：提出的高效算法使该方法能应用于实际问题，帮助从业者在数据受限、成本高昂的场景下（如云计算、工业自动化）**智能化地筛选高质量训练数据**，降低数据收集成本，提升模型训练效率和最终智能体的性能。
- **推动应用**：通过提升基于模型的强化学习的数据利用效率和可靠性，有助于加速其在各类复杂动态控制系统中的实际部署。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **一、 研究问题**
论文旨在解决**基于模型的强化学习**中一个关键且实际的问题：**如何为世界模型（World Model）高效、公正地选择高质量的训练数据**。

- **背景**：在云计算、数据中心管理、机器人控制等现实应用中，直接与环境交互成本高昂或危险。世界模型通过模拟环境动态，成为MBRL的核心组件。然而，其性能和可解释性**严重依赖于训练数据的质量**。
- **痛点**：现实数据常存在**稀疏、噪声、部分可观测**等问题。传统的数据选择方法（如随机选择或启发式方法）缺乏理论依据，效率低下，且无法量化每个数据点对最终智能体性能的**具体贡献**。

### **二、 核心创新点**
论文的核心创新是提出了 **“Action Shapley”** ，这是一个专为强化学习设计的训练数据选择度量指标，并配套提出了一个高效的随机动态算法。

1.  **理论创新：将Shapley值引入RL数据评估**
    - **概念**：将博弈论中的**Shapley值**概念应用于RL训练数据点的价值评估。Action Shapley值（`φ_k`）衡量的是一个特定训练数据点`k`对于**整个端到端RL训练流程**（从世界模型训练到智能体策略性能）的**边际贡献**。
    - **公式**：`φ_k = C_f * Σ [ (U(d∪{k}) - U(d)) / C(n-1, |d|) ]`，其中`U`是估值函数（本文中为RL智能体获得的**累计奖励**）。
    - **优势**：相比简单的“留一法”，它通过考虑数据点在所有可能子集组合中的贡献，提供了更**稳健、公平和无偏**的价值评估，并能识别冗余或有害数据。

2.  **算法创新：提出高效的随机动态计算算法**
    - **问题**：传统Shapley值计算需要遍历所有数据子集，复杂度为`O(2^n)`，对于实际应用不可行。
    - **解决方案**：提出**Algorithm 1**，一个基于“截止基数”的随机动态算法。
        - **关键概念**：**截止基数**：成功训练一个RL智能体所需的最小数据点数量阈值（`θ`）。
        - **工作原理**：算法采用**自顶向下**的广度优先搜索。它从最大子集开始计算，并设置一个失败计数器（`mem`）。当在某个基数`i`下，遇到`ε`次（用户定义的容错边界，论文中设为1）因数据子集不足导致智能体失败的情况时，算法即终止，并设定`θ = i + 1`。后续更小的子集无需再计算，因为其贡献已被判定为无效（失败）。
    - **效率提升**：该算法将计算复杂度从最坏的`O(2^n)`显著降低。论文通过**计算性能提升比** `P_comp = 1 - (2^θ / 2^n)` 来量化效率，在五个案例中均实现了超过80%的计算效率提升（例如K8s案例达99.9%）。

3.  **策略创新：基于Action Shapley的数据选择策略**
    - **理想数据集标准**：应包含**不超过全局截止基数**（所有数据点`θ_k`的最大值）的数据点，并且这些数据点的**平均Action Shapley值应尽可能高**。
    - **实践**：首先筛选掉**不可或缺**的数据点（缺少它们智能体必然失败），然后在剩余的可选数据点中，选择截止基数数量下平均Action Shapley值最高的组合作为训练集。

### **三、 解决方法与验证**
1.  **方法实现**：
    - 在五个真实的、数据受限的案例研究中验证（VM资源调整、负载均衡、数据库调优、K8s管理、数据中心冷却管理）。
    - 世界模型采用**径向基函数网络**，RL算法采用**SAC-PID**和**PPO-PID**（结合了PID控制的RL算法）。
    - 严格计算每个数据点的Action Shapley值和截止基数。

2.  **实证验证**：
    - **策略有效性**：在五个案例中，**最佳Action Shapley智能体**（基于最高平均AS值的训练集）的累计奖励均显著高于**最差Action Shapley智能体**，并且其性能与多次随机选择中的**最佳表现者相当或更优**。例如，在负载均衡案例中，100个随机智能体里仅有5个优于最佳AS智能体。
    - **对比基线**：在四个案例中，使用**更少数据**（仅截止基数数量）的最佳AS智能体，其性能**优于使用全部数据**训练的基线智能体，证明了其数据选择的高效性和去冗余能力。
    - **算法效率**：如`P_comp`公式所示，算法在实际案例中避免了大量无效计算，实现了计算效率的质的飞跃。

### **四、 实际价值**
- **降低数据成本**：帮助在数据稀缺或收集成本高的场景下，识别出最具价值的数据子集，用更少的数据训练出性能更优的模型。
- **提升模型可解释性**：为“为什么选择这些数据”提供了基于博弈论的、可量化的解释，增强了MBRL系统的可信度和可操作性。
- **促进RL落地**：通过解决数据选择这一实际工程难题，并大幅降低计算开销，为MBRL在动态控制系统（如云资源管理、工业自动化）中的**实际应用扫除了一个关键障碍**。

**总结**：本文的核心贡献在于**理论、算法、策略三位一体**的创新：将Shapley值创造性地应用于RL数据估值（理论），设计了基于截止基数的高效随机算法解决计算瓶颈（算法），并形成了可指导实践的数据选择策略（策略），最终通过详实的实证研究验证了其有效性和效率。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**基于模型的强化学习（MBRL）中世界模型训练数据选择缺乏有效、可解释性指标**的核心问题，提出了一种名为 **Action Shapley** 的数据评估与选择度量方法。该方法将合作博弈论中的沙普利值（Shapley Value）思想引入强化学习领域，通过计算每个训练数据点对整个RL智能体最终累积奖励的边际贡献，来量化其价值。为了应对传统沙普利值计算指数级复杂度的挑战，论文设计了一种**基于截止基数（cut-off cardinality）的随机动态算法**，能显著降低计算成本。通过在五个真实世界动态控制案例（如虚拟机资源调配、数据库调优、数据中心冷却管理等）上的实证验证，该方法不仅**将计算效率提升了超过80%**，而且基于其选择的高价值数据子集训练出的RL智能体，其性能**系统地优于随机选择或使用全部数据的基线方法**，证明了其在数据受限场景下提升模型效能与可解释性的实际价值。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning》在基于模型的强化学习领域提出了明确的创新，主要体现在以下三个方面：

---

### 1. **首次将Shapley值应用于强化学习（RL）的训练数据选择**
- **相比以往方法的改进/不同之处**：
    - **监督学习 vs. 强化学习**：以往的研究（如Ghorbani和Zou, 2019）已将Shapley值用于监督学习中的数据估值，但**没有工作将其应用于强化学习**。强化学习因其动态学习、稳定性问题（如“致命三角”）和序列决策的复杂性，与监督学习有本质区别。
    - **应用场景的转移**：本文将Shapley值的核心思想——衡量个体对整体效用的“边际贡献”——从静态的数据点估值，迁移到动态的、序列化的RL训练流程中。具体来说，它衡量的是一个训练数据点对整个**端到端RL工作流**（包括世界模型训练、策略学习和最终奖励）的贡献，而不仅仅是模型预测精度。
- **解决的具体问题/带来的优势**：
    - **问题**：在基于模型的RL中，世界模型的性能高度依赖于训练数据的质量。然而，在数据稀缺、噪声大、采样不规则的真实场景（如云计算、机器人控制）中，如何**公正、无偏地评估和选择高质量的训练数据**是一个关键挑战。
    - **优势**：Action Shapley提供了一个**原则性的、与算法无关的度量标准**，可以量化每个数据点对最终RL智能体性能的贡献。这解决了以往依赖临时或启发式方法选择数据的问题，为数据选择提供了可解释的理论依据，有助于构建更高效、更稳健的世界模型。

### 2. **提出了计算Action Shapley的随机动态算法，以解决指数复杂度问题**
- **相比以往方法的改进/不同之处**：
    - **计算复杂度**：经典的Shapley值计算需要对所有可能的子集组合进行评估，其复杂度为 `O(2^n)`（n为数据点数量），这在实践中是**不可行的**。
    - **引入“截止基数”**：本文的核心算法创新是提出了 **“截止基数”** 的概念。算法不是遍历所有子集，而是采用**自上而下的广度优先搜索**，并设置一个早期终止条件。当遇到一定数量（用户定义的误差界 `ϵ`）的“智能体失败”情况时，算法即停止，并将当前基数加一设为截止基数。
- **解决的具体问题/带来的优势**：
    - **问题**：传统Shapley值计算在数据点稍多时即面临**组合爆炸**，无法应用于实际规模的RL问题。
    - **优势**：
        1. **大幅提升计算效率**：论文在五个案例研究中验证，该算法实现了**超过80%的计算效率提升**（对比穷举法）。计算性能提升公式 `P_comp = 1 - 2^(θ_max) / 2^n` 表明，当截止基数 `θ_max` 远小于总数据量 `n` 时，计算量呈指数级下降。
        2. **实用性**：使得Action Shapley能够应用于真实世界的、数据量可观的案例（如K8s管理案例有15个数据点，组合数达3003个）。
        3. **识别关键数据**：算法自然地识别出 **“不可或缺”** 的数据点（即缺少它们会导致智能体完全失败），这本身就是一种有价值的数据质量洞察。

### 3. **提出并实证验证了一套基于Action Shapley的训练数据选择策略**
- **相比以往方法的改进/不同之处**：
    - **系统化的选择策略**：以往RL中的数据选择工作分散于主动学习、后悔最小化、样本优化等多个方向，但缺乏一个**统一、可解释的数据估值与选择框架**。本文明确提出：最优训练集应**在不超过全局截止基数的前提下，选择平均Action Shapley值最高的数据点子集**。
    - **端到端的评估标准**：其估值函数 `𝒰` 直接采用RL智能体的**累计奖励**，这确保了数据选择的目标与RL的最终目标（最大化奖励）完全一致。
- **解决的具体问题/带来的优势**：
    - **问题**：如何从有限的数据中挑选出最能提升智能体性能的子集，同时避免收集不必要的数据（成本高、不现实）。
    - **优势**：
        1. **提升样本效率与性能**：在五个真实案例（VM资源调整、负载均衡、数据库调优、K8s管理、数据中心冷却）中，基于Action Shapley选出的最优智能体，其累计奖励** consistently **优于：
            - 基于最差Action Shapley值选出的智能体。
            - 使用全部数据训练的基线智能体（在4/5的案例中胜出）。
        2. **验证策略有效性**：通过大量随机选择对比实验（例如，在K8s案例中对比3003种可能组合中的随机样本），证明**最优Action Shapley智能体的性能优于绝大多数随机选择的数据集**，验证了该策略的统计有效性。
        3. **平衡效率与效果**：该策略通过截止基数控制了模型复杂度（防止过拟合），同时通过高Shapley值确保了数据质量，实现了**在低样本复杂度下的高性能**。

---

**总结**：本文的核心创新在于**将Shapley值这一经典合作博弈理论，创造性地引入并适配到基于模型的RL数据选择难题中**。它不仅提出了理论框架（Action Shapley），还设计了高效的实用算法，并通过严谨的实证研究验证了其相对于临时方法和全数据基线方法的优越性。这项工作为在数据受限的现实场景中部署高效、可靠的模型强化学习系统提供了重要的方法论工具。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验设置概述

#### 1. **数据集**
论文在**五个真实世界的动态控制系统案例研究**中进行了评估，每个案例的数据集均由**不同配置下的时间序列**构成。具体如下：

- **虚拟机（VM）资源调配**：5个训练数据点（`a1` 到 `a5`），每个点代表一个（vCPU数量，内存大小）的配置组合，对应一条24小时、采样间隔1分钟的CPU使用率时间序列。
- **负载均衡**：5个训练数据点（`w1` 到 `w5`），每个点代表（CPU工作线程数，内存工作线程数）的配置组合，对应CPU使用率时间序列。
- **数据库调优**：6个训练数据点（`p1` 到 `p6`），每个点代表（vCPU数量，内存大小）的配置组合，对应CPU使用率时间序列。
- **Kubernetes管理**：15个训练数据点（`r1` 到 `r15`），每个点代表（写入速率，线程数）的配置组合，对应响应延迟时间序列。
- **数据中心冷却管理**：12个训练数据点（`c1` 到 `c12`），每个点代表（冷却器温度设定点，泵压设定点）的配置组合，对应CPU温度时间序列。

**共同特点**：所有数据均为**数据受限**场景下的真实或模拟数据，旨在验证在有限数据下，Action Shapley选择高质量训练子集的有效性。

#### 2. **评价指标**
- **核心指标**：**累计奖励**。这是强化学习（RL）代理在评估阶段（图1步骤d）执行策略后获得的总奖励，直接衡量代理的性能。
- **辅助指标**：
    - **计算效率提升**：通过提出的随机动态算法（算法1）与传统的指数复杂度（`O(2^n)`）计算相比的**计算性能提升百分比**（`P_comp`）。
    - **Action Shapley值**：用于量化每个训练数据点的贡献，正值表示正面贡献，负值表示负面贡献，“ind.”表示该数据点**不可或缺**。
    - **截止基数**：成功训练世界模型所需的最小数据点子集大小。

### 二、 对比方法与基线

#### 1. **内部对比（验证选择策略）**
- **最佳Action Shapley代理** vs. **最差Action Shapley代理**：在根据截止基数选择的候选子集中，对比具有**最高平均Action Shapley值**的子集训练的代理与具有**最低平均Action Shapley值**的子集训练的代理。
- **最佳Action Shapley代理** vs. **随机选择代理**：在多个训练回合（25个回合）中，将最佳Action Shapley代理的性能与从所有可能子集中**随机选择**相同大小子集训练的代理的性能进行比较。

#### 2. **外部对比（验证整体优势）**
- **最佳Action Shapley代理** vs. **基线代理**：基线代理使用**全部可用训练数据**（而非根据截止基数选择的子集）来训练世界模型。这是最直接的“使用所有数据”的朴素方法。

### 三、 关键性能结果与结论

#### 1. **Action Shapley选择策略的有效性**
- **在所有五个案例中，最佳Action Shapley代理的累计奖励均显著高于最差Action Shapley代理**。例如：
    - VM资源调配：-21.9 vs. -30.4
    - 负载均衡：-8.49 vs. -23.2
    - 数据库调优：-2.42 vs. -21
    - K8s管理：-499 vs. -621
    - 冷却管理：-119 vs. -211
- **最佳Action Shapley代理的性能与随机选择中的顶尖代理相当或更优**。在多数案例中，只有**少数**随机选择的子集能训练出性能优于最佳Action Shapley代理的模型。例如，在冷却管理案例中，250个随机子集中仅有9个（3.6%）表现更优。这证明了Action Shapley能稳定地识别出高质量的训练数据子集。

#### 2. **对比“使用所有数据”基线的优势**
- **在五个案例中的四个，最佳Action Shapley代理的性能显著优于使用全部数据的基线代理**：
    - VM资源调配：-21.9 vs. -37.7 **(提升约72%)**
    - 负载均衡：-8.49 vs. -9.1
    - K8s管理：-499 vs. -561.4 **(提升约12.5%)**
    - 冷却管理：-119 vs. -205.1 **(提升约72%)**
- **关键结论**：**更少但质量更高的数据（由Action Shapley选出）可以训练出比使用全部数据更好的世界模型和RL代理。** 这直接应对了数据稀疏、噪声等实际问题，证明了选择性数据使用的巨大价值。

#### 3. **计算效率的巨大提升**
- 论文提出的随机动态算法通过引入**截止基数**和**失败记忆化**，**避免了穷举所有数据子集组合**。
- 计算性能提升 `P_comp` 在五个案例中分别为：50%、75%、75%、99.9%、98.44%。
- **总体结论**：与传统的指数时间计算相比，该算法实现了**超过80%的计算效率提升**，使Action Shapley在实际应用中变得可行。

#### 4. **算法与指标的鲁棒性**
- 使用两种不同的RL算法（SAC-PID和PPO-PID）计算出的Action Shapley值**高度一致**，且得出的截止基数也相同。这表明Action Shapley作为评估指标是**与具体RL算法无关的**，具有很好的泛化能力。

### 总结
论文通过五个真实案例的实证研究，有力地证明了：
1.  **Action Shapley能有效量化并筛选出对RL代理性能贡献最大的训练数据**。
2.  基于此筛选出的**高质量数据子集**，不仅能**大幅减少所需数据量**（降至截止基数），还能训练出**性能优于使用全部数据的代理**。
3.  配套的**高效随机算法**解决了Shapley值计算固有的指数复杂度问题，**计算效率提升超过80%**，为实际部署扫清了障碍。

这些结果为在数据受限、成本高昂的真实世界动态控制系统中，可靠且高效地应用基于模型的强化学习提供了重要的方法论和工具。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.10905v1)
- [HTML 版本](https://arxiv.org/html/2601.10905v1)
