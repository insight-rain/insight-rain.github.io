# Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration

**相关性评分**: 6.0/10

**排名**: #28


---


## 基本信息

- **arXiv ID**: [2601.10744v1](https://arxiv.org/abs/2601.10744v1)
- **发布时间**: 2026-01-11T16:23:22Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Sen Wang, Bangwei Liu, Zhenkun Gao, Lizhuang Ma, Xuhong Wang, Yuan Xie, Xin Tan

## 关键词

fine tune, multimodal LLM, reinforcement learning, long-term memory, embodied exploration, benchmark, lifelong learning

## 一句话总结

这篇论文提出了一个基于多模态大语言模型和强化学习的框架，通过微调来增强具身智能体的长期记忆探索能力，并建立了相应的基准数据集。

## 摘要

An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.

## 详细分析

## 论文摘要

**论文标题：** 《Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration》

### 1. 研究背景和动机
在具身智能领域，理想的智能体应具备终身学习能力，以在动态、未知的复杂环境中执行长时程任务。然而，现有主流研究（如单目标导航、具身问答）多关注任务完成结果，**忽视了探索过程本身以及长期记忆的利用**。这导致智能体缺乏主动探索和基于经验推理的能力，难以实现真正的认知与决策统一。为解决此问题，本文提出了**长时记忆具身探索**这一新范式。

### 2. 核心方法和技术创新
本文的核心贡献包含一个**基准数据集**和一个**强化学习框架**：
*   **LMEE-Bench基准**：构建了一个全新的评估基准，包含**多目标导航**和**基于记忆的问答**两个核心任务，旨在从过程和结果两个维度，全面评估智能体的探索效率、场景理解与记忆利用能力。该数据集规模庞大，涵盖246个物体类别、超过9000个目标和问题。
*   **MemoryExplorer框架**：提出了一种基于多模态大语言模型（MLLM）的强化学习微调方法。其技术创新在于：
    *   **主动记忆查询**：通过强化学习训练MLLM，使其能**主动调用外部记忆检索工具**，根据当前任务和观察查询相关的多模态记忆（图像、文本、位置），而非被动使用记忆。
    *   **多任务奖励函数**：设计了一个融合**动作预测、前沿区域选择和问题回答**的复合奖励函数，将场景理解、记忆利用和规划决策统一优化，激励模型进行自主探索。

### 3. 主要实验结果
在提出的LMEE-Bench和现有的GOAT-Bench上进行了广泛实验：
*   **性能领先**：MemoryExplorer在**多目标导航的成功率（SR）和路径效率（SPL）**，以及**基于记忆的问答准确率**上，均显著优于现有的先进方法（如3D-Mem, Explore-EQA）。
*   **有效性验证**：消融实验证实，**引入主动记忆检索工具**和**多任务奖励设计**是性能提升的关键。可视化结果显示，模型在训练中逐步学会了更准确地调用记忆工具。
*   **现实世界验证**：在真实办公环境的机器人平台上进行了测试，证明了方法具有良好的从仿真到现实的迁移能力。

### 4. 研究意义和价值
*   **范式创新**：推动了具身智能研究从“一次性任务完成”向“**注重探索过程与记忆积累**”的终身学习范式转变。
*   **基准贡献**：发布的LMEE-Bench为社区提供了一个**系统评估智能体记忆与认知能力**的重要平台。
*   **技术突破**：提出的MemoryExplorer框架通过强化学习激发MLLM的**主动记忆查询能力**，为实现能持续学习、适应和进化的通用具身智能体提供了可行的技术路径。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文标题**
《Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration》

### **核心问题**
现有具身智能（Embodied AI）任务（如目标导航、具身问答）大多关注**任务完成结果**，而**忽视了探索过程和长期记忆的利用**。这导致智能体缺乏**终身学习能力**，无法在复杂、动态的未知环境中持续学习、适应和进化。

### **核心创新点**

#### 1. **提出新范式与基准：LMEE**
- **新范式**：提出 **“长期记忆具身探索”**，将**多目标导航**与**基于记忆的问答**统一起来，强调探索过程（记忆构建）与任务结果同等重要。
- **新基准**：构建 **LMEE-Bench** 数据集与评测标准。
    - **数据规模**：涵盖 246 个物体类别、超过 9,000 个目标与问题、1,982 条探索轨迹。
    - **评估维度**：
        - **导航性能**：成功率、路径效率。
        - **记忆能力**：基于探索过程中构建的情景记忆回答问题的准确性。
    - **任务分级**：根据目标数量、距离分为易、中、难三级，全面评估智能体能力。

#### 2. **提出新方法：MemoryExplorer**
一个基于**多模态大语言模型**，通过**强化学习微调**来驱动**主动探索与记忆检索**的框架。

- **核心机制**：
    1.  **主动记忆查询**：模型被训练去**主动生成查询**，调用外部记忆检索工具，从构建的多模态记忆库中获取相关信息，而非被动接受过滤后的记忆。
    2.  **端到端策略学习**：使用 **GRPO** 进行**强化学习微调**，优化一个统一的策略模型，直接输出动作、边界选择和问题答案。

- **关键技术设计**：
    - **多任务奖励函数**：设计了融合四个维度的奖励函数，鼓励模型协同优化。
        ```python
        r_total = w_act * r_action * c +       # 动作准确性
                 w_front * r_frontier * c +   # 边界选择正确性
                 w_ans * r_answer +           # 问题回答准确性
                 w_fmt * r_format            # 输出格式完整性
        ```
        - **一致性系数 `c`**：惩罚动作与边界选择逻辑不一致的情况。
        - **工具使用奖励缩放**：成功调用记忆检索工具时奖励更高，鼓励主动查询。
    - **记忆检索工具**：利用 CLIP 编码器计算文本、图像和位置特征的加权相似度，动态检索最相关的记忆片段。

### **解决方案路径**
1.  **问题定义**：将长期记忆具身探索形式化为一个由 MLLM 策略 `π_θ` 控制的序列决策问题，输入包括任务指令、当前观察、目标导向问题及外部记忆库。
2.  **数据构建**：基于 HM3DSem 场景，利用 LLM/VLM 自动生成多目标导航指令和基于目标的问答对，并通过仿真器生成探索轨迹与多模态记忆库。
3.  **方法实现**：以 Qwen2.5-VL-7B 为基础模型，通过 RFT 训练 MemoryExplorer。模型学会在探索中主动查询记忆，以理解任务进度、规划动作，并最终回答问题。
4.  **评估验证**：在 LMEE-Bench 和 GOAT-Bench 上进行了广泛实验。结果表明，MemoryExplorer 在**长视野导航成功率**和**记忆问答准确率**上均显著优于现有方法（如 3D-Mem, Explore-EQA）。

### **实际价值与意义**
- **学术价值**：为具身智能的**终身学习**研究提供了一个重要的新方向和严谨的评测基准，推动了从“一次性任务完成”到“持续认知与决策”的范式转变。
- **技术贡献**：证明了通过**强化学习微调 MLLM** 来实现**主动记忆检索**的可行性，为构建更自主、更智能的具身智能体提供了有效技术路径。
- **应用潜力**：该方法使智能体能够像人类一样基于经验进行思考和决策，在家庭服务机器人、复杂环境勘探等需要长期交互与适应的场景中具有广阔应用前景。

**总结**：本文的核心创新在于**系统性地提出了“探索过程与记忆利用”这一关键问题**，并通过**构建新基准**和**开发基于强化学习的主动记忆检索框架**，为构建具有终身学习能力的具身智能体提供了切实可行的解决方案。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决具身智能体在长期、复杂任务中**探索过程与记忆利用脱节**的问题。现有研究多关注任务完成结果，而忽视了智能体在探索过程中如何主动构建、查询和利用长期记忆来优化决策。为此，论文提出了 **“长期记忆具身探索”** 新范式，并构建了对应的数据集与基准 **LMEE-Bench**，通过多目标导航和基于记忆的问答来统一评估智能体的探索认知与决策能力。

为解决上述问题，论文提出了 **MemoryExplorer** 方法，其核心是**通过强化学习微调一个多模态大语言模型**，使其学会主动查询外部记忆库。该方法设计了**多任务奖励函数**，联合优化动作预测、前沿区域选择和问题回答，从而激励模型在探索中主动检索记忆以理解任务进度和环境。

实验表明，该方法在 LMEE-Bench 和 GOAT-Bench 上均取得了显著优势，成功率和路径效率优于现有先进模型，有效提升了智能体在长视野任务中的自主探索和主动记忆检索能力，为实现终身学习型具身智能体迈出了关键一步。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration》在具身智能探索领域提出了多项明确的创新，主要体现在**任务范式**、**基准构建**和**方法框架**三个层面。

### 1. 提出新的任务范式：长时记忆具身探索
- **改进/不同之处**：现有主流具身任务（如单目标导航、具身问答）多为“一次性”任务，侧重于任务完成的结果（如是否找到目标），而**忽略了探索过程本身以及记忆的利用**。即使多目标导航任务关注长时程规划，也未能将探索过程与智能体的场景理解和决策统一起来。本文提出的 **LMEE** 范式，将**多目标导航**与**基于记忆的问答**紧密结合，要求智能体在探索过程中动态构建情景记忆，并利用这些记忆回答问题。
- **解决的问题/优势**：解决了现有任务“重结果、轻过程”的局限性，迫使智能体必须**主动利用长时记忆**来支持决策和场景理解。这更贴近现实世界中智能体需要**终身学习**和**持续适应**的需求，促进了认知（记忆与理解）与决策（规划与行动）的统一。

### 2. 构建综合性基准与数据集：LMEE-Bench
- **改进/不同之处**：现有基准（如ObjectNav, EQA, GOAT-Bench）要么只评估导航成功率，要么只评估问答准确性，且大多不评估记忆利用过程。LMEE-Bench是首个**同时评估多目标导航性能（过程与结果）和基于记忆的问答准确性（记忆利用效果）** 的基准。其数据集规模大（246个类别，超9000个目标/问题），且问题类型多样（属性、计数、位置、关系、状态）。
- **解决的问题/优势**：提供了一个**全面、统一**的评估平台，能够**同时衡量智能体的探索效率、记忆构建能力和记忆检索推理能力**。这解决了以往基准评估维度单一、无法全面反映智能体终身学习能力的问题，为未来研究设立了更贴近实际应用的标准。

### 3. 提出新颖的方法框架：MemoryExplorer（基于强化学习的主动记忆检索）
- **改进/不同之处**：现有利用记忆的方法（如3D-Mem, MTU3D）多属于**被动记忆**使用，例如通过模仿学习复现专家轨迹，或使用预定义的过滤策略（如基于物体类别）来管理记忆快照，限制了智能体的自主性和泛化能力。MemoryExplorer的核心创新在于：
    - **主动记忆查询**：通过**强化学习微调**一个多模态大语言模型，使其学会**主动生成查询**，调用外部记忆检索工具，根据当前任务、观察和问题，从记忆库中检索最相关的多模态记忆。
    - **多任务奖励函数**：设计了一个融合**动作预测、前沿区域选择和问题回答**的奖励函数。这不仅鼓励正确的导航行为，还直接奖励准确的记忆利用和答案生成，将场景理解、记忆利用和规划决策**端到端地统一优化**。
- **解决的问题/优势**：
    - **解决了被动记忆利用的局限性**：使智能体能够像人类一样，在不确定时主动“回想”或“查找”过往经验来指导当前行动，显著提升了在未知复杂环境中**自主探索和决策**的能力。
    - **解决了任务与记忆脱节的问题**：通过将目标导向的问答作为训练信号之一，迫使模型在探索过程中就必须关注和记忆与任务目标相关的场景信息，从而**强化了探索的目的性和记忆的相关性**。
    - **带来了性能优势**：实验表明，MemoryExplorer在LMEE-Bench和GOAT-Bench上均取得了显著优于现有方法（如3D-Mem, RA-Mem）的导航成功率（SR）、路径效率（SPL）和问答准确率。

### 4. 技术实现上的关键创新点
- **基于相似度的动态记忆库构建与检索**：记忆条目包含位置、文本和图像特征，通过加权相似度（文本、视觉、空间距离）进行检索，并引入时间一致性过滤，这比简单的基于类别的过滤更灵活、更准确。
- **强化学习微调策略**：采用Group Relative Policy Optimization进行策略优化，专注于优化最终决策输出，而不直接优化中间的工具调用过程，鼓励模型自主思考。
- **一致性惩罚与工具使用激励**：在奖励函数中引入动作与前沿选择的一致性惩罚系数 `c`，以及根据是否成功调用工具进行奖励缩放的因子 `α`，有效提升了策略的逻辑一致性和工具使用的主动性。

### 总结
本文的核心创新在于**系统性**地推进了具身智能向终身学习方向发展：不仅提出了一个**更符合现实需求的新任务范式**和**评估标准**，还提供了一个**实现该范式的有效方法**。其创新点环环相扣：新范式需要新基准来评估，而新基准的挑战又通过融合主动记忆检索与强化学习的新方法来解决。最终，MemoryExplorer框架证明了**通过激励MLLM主动利用长时记忆，可以显著提升智能体在长时程、复杂任务中的探索与决策能力**，这是迈向通用具身智能的关键一步。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果分析

### 一、 使用的数据集与评价指标

#### 1. 数据集
- **LMEE-Bench**： 本文提出的核心基准测试集。基于HM3DSem场景构建，包含**166个任务**、**828个目标**和**406个问题**。任务分为**多目标导航**和**基于记忆的问答**两部分，难度分为**简单、中等、困难**三个等级。
- **GOAT-Bench**： 一个多模态终身导航基准。用于验证模型在更大规模、更复杂场景下的泛化能力。实验中使用了其“Val Unseen”子集（36个场景，278个导航子任务）。

#### 2. 评价指标
- **导航性能**：
    - **成功率**： 代理最终位置距离目标1米以内即视为成功。
    - **路径长度加权成功率**： 在成功率基础上，考虑路径效率的指标。
- **问答性能**：
    - **MLLM-Score**： 针对开放式问题提出的量化指标。使用Qwen3-VL-30B作为评估模型，对预测答案与真实答案进行1-5分评分，并归一化到0-100分。
    - **准确率**： 针对选择题的标准准确率。

### 二、 对比的基线方法

论文与三类基线方法进行了对比：

1.  **检索增强的问答模型**： 在导航后，使用不同MLLM（如LLaVA-OneVision-7B, Qwen2.5-VL-7B, GPT-4o等）结合记忆库进行问答，作为纯问答能力的上限参考。
2.  **MLLM探索模型**：
    - **Explore-EQA**： 经典的探索与问答方法，但**缺乏显式的长期记忆机制**。
    - **3D-Mem**： 利用3D场景记忆进行探索和推理的SOTA方法，但采用**被动的、基于对象类别的记忆过滤策略**。
    - **RA-Mem**： 本文提出的**3D-Mem的增强变体**。它让模型**主动生成查询文本**来检索记忆，比3D-Mem更灵活。
3.  **GOAT-Bench上的模块化基线**： 如Modular GOAT等，作为该基准上的传统强基线。

### 三、 关键性能提升与结论

#### 1. 在LMEE-Bench上的表现
- **总体优势**： 本文提出的 **MemoryExplorer** 在**导航成功率**和**SPL**上全面超越了所有对比的MLLM探索基线（Explore-EQA, 3D-Mem, RA-Mem）。
    - **总导航SR**： **23.53%** (Ours) vs. 20.96% (RA-Mem) vs. 16.91% (3D-Mem)。
    - **总导航SPL**： **14.99%** (Ours) vs. 12.18% (RA-Mem) vs. 6.86% (3D-Mem)。
- **问答性能**： MemoryExplorer在**基于记忆的问答**上也取得了最佳或接近最佳的表现。
    - **MLLM-Score**： **43.62** (Ours) vs. 41.72 (最佳检索增强模型Qwen2.5-VL-7B)。
    - **选择题准确率**： **65.52%** (Ours)，显著高于3D-Mem的41.38%和RA-Mem的58.62%。
- **结论**： 这表明通过**强化学习微调**来激励模型**主动查询记忆**，不仅提升了**探索和导航的效率**，也同步增强了**基于记忆的认知与推理能力**，实现了探索与记忆的统一优化。

#### 2. 在GOAT-Bench上的表现
- **泛化能力验证**： 在更复杂的多目标导航任务上，MemoryExplorer同样展现出显著优势。
    - **成功率**： **46.40%** (Ours) vs. 42.81% (RA-Mem) vs. 37.05% (3D-Mem) vs. 29.5% (最佳模块化基线)。
    - **SPL**： **28.03%** (Ours) vs. 21.95% (RA-Mem) vs. 20.26% (3D-Mem)。
- **结论**： 证明了本文方法在**长视野、多目标**的终身导航任务中具有强大的泛化性和有效性。

#### 3. 消融实验的关键结论
- **主动记忆检索至关重要**： 仅监督导航任务（`only nav`）性能提升有限。**引入记忆检索工具**（`w/ memory tool`）后，各项指标获得**显著提升**（如SR从20.96%升至23.53%）。
- **多任务奖励设计有效**： 包含**动作-边界一致性惩罚**和**工具使用奖励缩放**的多任务奖励函数，是模型性能优于RA-Mem的关键。
- **问题类型多样性促进学习**： 结合**简单进度问题**和**多种类型的选择题**进行训练，比仅使用单一问题类型能带来更好的综合性能。

#### 4. 定性分析与实际验证
- **案例展示**： 论文通过可视化案例证明，MemoryExplorer能够通过多目标探索构建记忆，并在被提问时**准确检索相关记忆片段并给出正确答案**。
- **真实世界测试**： 在办公室场景的实物机器人（ROSMASTER X3）上进行的测试表明，该方法具备**从仿真到现实的迁移能力**。机器人能够完成多目标导航任务，并基于构建的记忆库回答关于场景细节的问题（甚至是任务目标之外的对象）。

### 总结
论文通过系统的实验评估，定量和定性地证明了 **MemoryExplorer框架的有效性**。其核心价值在于：
1.  **性能领先**： 在提出的LMEE-Bench和现有的GOAT-Bench上，导航与问答的**关键指标均优于现有SOTA方法**。
2.  **机制有效**： 消融实验证实了**主动记忆检索**和**精心设计的多任务奖励函数**是性能提升的关键。
3.  **实用潜力**： 真实世界测试初步验证了该方法的**现实可行性与泛化能力**，为开发具备终身学习能力的具身智能体提供了有力的技术路径。

**局限性**： 论文也指出，基于MLLM的方法存在**推理速度慢**的问题，且对于某些需要复杂空间理解的开放性问题，模型的准确性仍有提升空间。这为未来研究指明了方向。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.10744v1)
- [HTML 版本](https://arxiv.org/html/2601.10744v1)
