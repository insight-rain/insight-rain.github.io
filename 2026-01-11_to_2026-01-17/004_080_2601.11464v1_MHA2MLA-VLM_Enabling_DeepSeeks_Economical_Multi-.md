# MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models

**相关性评分**: 8.0/10

**排名**: #4


---


## 基本信息

- **arXiv ID**: [2601.11464v1](https://arxiv.org/abs/2601.11464v1)
- **发布时间**: 2026-01-16T17:45:34Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种参数高效的多模态感知框架MHA2MLA-VLM，通过将现成的视觉语言模型转换为多头部潜在注意力架构，显著压缩KV缓存并加速推理，适用于边缘部署。

## 摘要

As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.

## 详细分析

## 论文摘要：MHA2MLA-VLM: 实现跨视觉语言模型的DeepSeek经济型多头潜在注意力

### 1. 研究背景和动机
随着视觉语言模型（VLMs）处理的任务日益复杂和多模态化，推理过程中键值（KV）缓存的快速增长带来了显著的内存和计算瓶颈。**多头潜在注意力（MLA）** 作为一种有效的KV缓存压缩和推理加速机制，已被证明优于标准多头注意力（MHA）。然而，如何在不进行昂贵预训练的情况下，将现有的VLMs适配到MLA架构，仍是一个未充分探索的挑战。本研究旨在解决这一问题，提出一种参数高效、多模态感知的框架，以实现现成VLMs向MLA架构的低成本迁移。

### 2. 核心方法和技术创新
本文提出了 **MHA2MLA-VLM** 框架，其核心包含两项关键技术：
- **模态自适应部分旋转位置编码（Partial-RoPE）策略**：针对多模态输入（图像、文本、视频），通过基于KL散度的数据驱动方法（MKL），选择性地保留对位置理解最关键的旋转维度，而非简单地沿用文本模型的启发式策略。
- **模态解耦的低秩近似方法（MD-SVD）**：观察到视觉与文本token的低秩空间是正交的，因此对视觉和文本的KV空间进行独立的低秩压缩，以最小化输出激活误差，而非传统的参数距离误差。

此外，框架引入了**参数高效微调（PEFT）**，在转换过程中仅微调少量参数（如查询/键投影矩阵和MLA内部参数），大幅降低了适配成本（例如，将Qwen2.5-VL的适配时间从22小时缩短至9小时）。

### 3. 主要实验结果
在三个代表性VLM（LLaVA-1.5, LLaVA-NeXT, Qwen2.5-VL）上的广泛实验表明：
- **性能保持与高效压缩**：在显著减少KV缓存占用（最高达94.64%）的同时，模型性能损失极小。例如，Qwen2.5-VL在KV缓存减少94.64%后，整体性能仍达79.47，与原始模型相当。
- **数据与参数高效**：仅使用少量监督数据（最高1.8B token）和微调约10%的参数，即可成功完成架构迁移。
- **兼容性与优越性**：MHA2MLA-VLM与KV缓存量化技术无缝集成，实现叠加压缩效益；并且在相同压缩率下，其性能显著优于KV缓存剪枝等基线方法。

### 4. 研究意义和价值
本工作首次成功将MHA到MLA的高效适配从纯文本LLM扩展到多模态VLM领域。所提出的 **MHA2MLA-VLM** 框架为部署大规模VLMs提供了一种**实用、经济且可扩展的解决方案**。它通过创新的多模态感知压缩技术，在几乎不损失模型性能的前提下，大幅降低了推理阶段的内存和计算开销，对于推动VLMs在资源受限环境下的实际应用具有重要价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：MHA2MLA-VLM

### **一、 论文旨在解决的核心问题**
随着视觉-语言模型（VLMs）处理的任务日益复杂（多模态、长上下文），推理过程中的**Key-Value缓存** 急剧膨胀，导致了严重的**内存占用和计算瓶颈**。这限制了VLMs在资源受限环境下的高效部署和应用。

具体来说，论文试图解决一个**关键空白**：虽然DeepSeek提出的**多头潜在注意力** 机制能有效压缩KV缓存并加速推理，但如何将现有的、广泛使用的VLMs（基于标准MHA或GQA架构）**低成本、高效地迁移到MLA架构**，而无需耗费巨资从头预训练，此前尚未得到充分探索。

### **二、 核心创新点**
论文提出了 **MHA2MLA-VLM** 框架，其创新性主要体现在以下三个紧密关联的方面：

#### **1. 模态自适应的部分RoPE策略**
*   **问题**：MLA要求部分注意力头不使用位置编码（NoPE）。先前在纯文本LLM上的部分RoPE策略无法直接用于多模态场景，因为视觉和文本token在RoPE维度上具有不同的特性。
*   **创新**：提出了 **基于KL散度的贡献感知多模态部分RoPE** 策略。
    *   **方法**：通过计算**零掩码每个旋转子空间后注意力分布的KL散度变化**，来量化该子空间对多模态位置理解的重要性。保留最重要的 `r` 个子空间，其余转为NoPE。
    *   **优势**：这是一种**数据驱动、无需训练**的策略，能自适应地为视觉和文本token选择最关键的旋转维度，优于简单的启发式方法（如Head-wise 2-norm）。

#### **2. 模态解耦的低秩近似方法**
*   **问题**：MLA需要对KV进行联合低秩压缩。直接对多模态激活进行单一SVD会因视觉和文本激活尺度、分布不同而相互干扰，导致压缩质量下降。
*   **创新**：提出了 **模态解耦SVD** 方法。
    *   **方法**：**独立地为视觉和文本模态计算其激活的协方差矩阵，并分别进行SVD驱动的低秩近似**，得到两套独立的低秩投影矩阵。
    *   **理论依据**：论文证明了分离优化的最小损失**小于或等于**联合优化的最小损失，为方法提供了理论支撑。
    *   **优化目标**：采用 **最小化输出激活误差**（`min ||WX - W‘X||`），而非最小化参数距离，这显著减少了性能损失和所需微调数据量。

#### **3. 参数高效的轻量化适配框架**
*   **问题**：对整个VLM进行全参数微调以适应新架构成本高昂。
*   **创新**：设计了**两阶段参数高效微调** 流程，极大降低了适配成本。
    *   **阶段一（部分RoPE）**：仅微调 **Query和Key的投影矩阵**，冻结其他所有参数。
    *   **阶段二（低秩近似）**：仅微调 **MLA模块内的参数**。
    *   **效果**：以Qwen2.5-VL为例，仅需微调约6%-10%的参数，训练时间从22小时缩短至9小时（减少59%），实现了**数据和参数的双重高效**。

### **三、 解决方案的总体思路**
MHA2MLA-VLM的解决方案是一个**系统性的、端到端的架构转换流程**：

1.  **输入适配**：通过**模态自适应部分RoPE**，将原始VLM的注意力输入（Query/Key）转换为与MLA兼容的形式（部分维度带RoPE，部分为NoPE）。
2.  **KV压缩**：通过**模态解耦SVD**，为视觉和文本模态分别构建低秩的联合KV潜在表示，大幅减少需要缓存的维度（`d_kv`）。
3.  **高效微调**：采用**两阶段PEFT**对上述改动进行轻量级微调，以恢复模型性能。
4.  **无缝集成**：最终得到的MLA架构VLM，其KV缓存 footprint 显著减小，并且**可以与KV量化等技术进一步结合**，实现叠加的压缩效益。

### **四、 实际价值与意义**
*   **经济效益**：为业界广泛部署的现有VLM（如LLaVA系列、Qwen-VL系列）提供了一条**低成本、高性能的“升级”路径**，使其能享受MLA带来的推理加速和内存节省，无需从头训练。
*   **技术通用性**：框架在**MHA和GQA** 两种注意力架构，以及**标准RoPE和多模态RoPE** 两种位置编码方案上均验证有效，展现了良好的普适性。
*   **部署友好**：大幅降低的KV缓存使VLMs能够在**相同硬件上处理更长的上下文（更多图像或视频帧）**，或是在边缘设备上实现部署，拓宽了应用场景。
*   **开源贡献**：论文提供了完整的代码实现，促进了高效VLM推理技术的开源生态发展。

**总结**：MHA2MLA-VLM的核心创新在于**创造性地解决了将现有VLM经济地迁移至更高效MLA架构的难题**。其通过**模态感知的部分RoPE**和**模态解耦的低秩压缩**这两个关键技术，在保持模型多模态能力的前提下，实现了KV缓存的显著压缩，并通过**参数高效微调**将适配成本降至最低，具有很高的实用价值和推广潜力。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决视觉语言模型（VLM）在推理时因键值（KV）缓存快速增长而导致的内存和计算瓶颈问题。为此，论文提出了一个名为 **MHA2MLA-VLM** 的参数高效、多模态感知的框架，能够将现成的、基于标准多头注意力（MHA）或分组查询注意力（GQA）的 VLM 适配到 DeepSeek 提出的、更高效的**多头潜在注意力（MLA）** 架构上。该框架的核心创新包括：1）一种**模态自适应的部分旋转位置编码（Partial-RoPE）策略**，通过选择性保留关键旋转维度来支持传统和多模态设置；2）一种**模态解耦的低秩近似方法（MD-SVD）**，独立压缩视觉和文本的 KV 空间，以最小化输出激活误差。实验表明，该方法仅需少量监督数据和参数微调，即可在显著减少 KV 缓存占用（例如，在 Qwen2.5-VL 上减少 94.64%）的同时，基本恢复原始模型的性能，并且能与 KV 量化技术无缝结合，实现进一步的效率提升。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models》的核心贡献在于，**首次提出了一种参数高效、多模态感知的框架，能够将现成的、基于传统注意力机制（MHA/GQA）的视觉语言模型（VLM）适配到更高效的DeepSeek MLA架构上，从而显著减少推理时的KV缓存占用，且无需昂贵的从头预训练**。

以下是其相对于已有工作的具体创新点：

### 1. **多模态自适应的部分旋转位置编码策略**
- **改进/不同之处**：
    - **以往方法**：此前关于部分RoPE（Partial-RoPE）的研究仅限于**单模态（纯文本）** 场景。例如，Ji et al. (2025) 的MHA2MLA工作仅适用于LLM，其基于启发式（如头间2-范数贡献）选择保留的旋转维度。
    - **本文方法**：提出了 **“贡献感知的多模态部分RoPE”** 策略。该方法基于**KL散度敏感性**进行数据驱动的、无需训练的频率子空间选择。它**针对多模态输入（图像和文本交错）**，独立评估每个旋转维度对最终注意力分布的影响，从而自适应地为不同模态保留最关键的旋转维度。
- **解决的问题/带来的优势**：
    - **解决了问题**：直接套用为文本设计的部分RoPE策略到VLM上会导致次优的维度分配，因为视觉和文本信息在RoPE维度上具有不同的特征分布。本文方法确保了旋转关键的子空间在多模态上下文中被优先保留。
    - **带来的优势**：实现了从MHA/GQA到MLA架构的**低成本、高效迁移**，并为后续的KV低秩压缩（MLA的核心）做好了准备。实验表明，该策略（MKL）优于最强的文本基线（`S_2-norm`）。

### 2. **模态解耦的低秩近似方法**
- **改进/不同之处**：
    - **以往方法**：现有的SVD驱动压缩方法（如SVD-LLM V2）在应用于VLM时，通常对**视觉和文本token的联合激活矩阵**进行单一的低秩近似。这忽略了多模态数据的内在差异。
    - **本文方法**：提出了 **“模态解耦SVD”**。该方法**独立地为视觉和文本KV空间计算并应用低秩近似**。其理论证明（定理2.1）和实验验证均表明，分别优化两个模态的截断损失之和，总是小于或等于对联合激活进行单一优化所得到的最小损失。
- **解决的问题/带来的优势**：
    - **解决了问题**：视觉和文本激活在尺度上存在显著差异。对它们进行联合SVD时，主导模态会扭曲奇异值分布，从而降低另一个模态的压缩质量。
    - **带来的优势**：
        1. **更低的压缩损失**：通过解耦优化，最大程度地减少了每个模态特有的信息损失。
        2. **更好的性能保持**：在相同的KV缓存压缩率下，相比联合优化方法，本文方法能更好地保持原始模型的性能（见表3消融实验）。
        3. **理论保障**：为多模态模型的高效压缩提供了理论依据。

### 3. **面向输出的激活误差最小化与参数高效微调的结合**
- **改进/不同之处**：
    - **以往方法**：传统的模型压缩/架构迁移通常**最小化参数距离**（如 `min ||W - W'||_F`），或者需要全参数微调，成本高昂。
    - **本文方法**：
        1. **优化目标**：受SVD-LLM V2启发，将低秩近似的目标从最小化参数距离，改为**最小化输出激活误差**（`min ||XW - XW'||_F`）。这直接保证了模型前向传播行为的保真度。
        2. **训练策略**：引入了**两阶段参数高效微调**：
            - **阶段一（部分RoPE）**：仅微调查询和键的投影矩阵，冻结其他所有参数。
            - **阶段二（低秩近似）**：仅微调MLA模块内的参数。
- **解决的问题/带来的优势**：
    - **解决了问题**：
        1. 最小化参数距离不能直接保证模型输出不变，可能导致较大的性能下降。
        2. 对整个VLM进行全参数微调以适应新架构，需要巨大的计算和數據成本。
    - **带来的优势**：
        1. **显著减少性能损失**：激活误差最小化能更直接地维持模型功能，从而减少适配后的性能下降。
        2. **极高的训练效率**：PEFT策略大幅减少了可训练参数量（例如，Qwen2.5-VL仅需微调~6%-10%的参数）和训练时间（例如，训练时间从22小时缩短至9小时，减少59%）。
        3. **数据高效**：仅需少量监督数据（如1.8B token）即可完成架构迁移，远少于从头训练所需的万亿级token。

### 4. **首次系统地将MHA2MLA范式扩展至VLM领域，并验证其普适性与兼容性**
- **改进/不同之处**：
    - **以往方法**：Ji et al. (2025) 的MHA2MLA工作仅证明了在**纯文本LLM**上从MHA/GQA迁移到MLA的可行性。**VLM能否进行类似迁移是一个未探索的开放问题**。
    - **本文方法**：成功构建了 **MHA2MLA-VLM** 框架，并在**三种具有不同代表性的VLM架构**上进行了验证：使用MHA和标准RoPE的LLaVA-1.5、使用GQA和标准RoPE的LLaVA-NeXT、以及使用GQA和**多模态RoPE**的Qwen2.5-VL。
- **解决的问题/带来的优势**：
    - **解决了问题**：填补了高效注意力架构迁移技术在**多模态模型**领域的空白，证明了该技术路线的普适性。
    - **带来的优势**：
        1. **架构通用性**：方法适用于MHA、GQA、标准RoPE和M-RoPE，展现了强大的兼容性。
        2. **显著的KV缓存压缩**：在`d_kv=64`（压缩率90.63%）时，LLaVA-NeXT性能下降仅约2分；Qwen2.5-VL在`d_kv=64`（压缩率94.64%）时性能几乎与原模型持平。
        3. **技术兼容性**：实验证明，MHA2MLA-VLM与**KV缓存量化技术**（如Int4量化）可以无缝结合，实现**复合压缩效益**，在极高压缩率下性能仍优于单纯的Int2量化（见表2）。

---

**总结**：本文的核心创新在于**系统性、高效地解决了将先进但专有的MLA注意力架构“嫁接”到现有各类VLM上的难题**。通过**多模态自适应的部分RoPE**和**模态解耦的低秩近似**两项核心技术，辅以**激活误差最小化的优化目标和PEFT策略**，实现了**以极低的参数和数据成本，大幅压缩KV缓存，同时基本保持原模型多模态性能**的目标。这为在资源受限场景下部署高性能VLM提供了切实可行的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心实验效果
论文提出的 **MHA2MLA-VLM** 框架成功地将三种主流的视觉语言模型（VLMs）从传统的 **MHA/GQA** 架构高效地迁移至 **DeepSeek 的 MLA** 架构。在**显著减少 KV 缓存占用**的同时，**保持了与原模型相当的性能**，并实现了**参数高效和数据高效的微调**。

### 二、 使用的数据集
1.  **训练/微调数据集**：
    *   **LLaVA-1.5**：使用其默认的约 665K 样本的视觉指令微调数据集。
    *   **LLaVA-NeXT 与 Qwen2.5-VL**：统一使用公开的 **LLaVA-NeXT 数据集**（约 778K 样本），以确保微调过程的可比性。
    *   **数据量**：总计使用 **0.5B 至 1.8B 个多模态令牌**，远低于从头训练 MLA 模型所需的万亿级令牌，证明了方法的**数据高效性**。

2.  **评估基准数据集**（共8个）：
    *   **AI2D**：图表推理。
    *   **GQA**：真实世界视觉推理与组合问答。
    *   **POPE**：评估对象幻觉。
    *   **SEED-Bench-IMG**：多模态生成式理解。
    *   **RealWorldQA**：高分辨率真实世界场景理解。
    *   **MMBench**：综合性多模态模型能力评估。
    *   **ChartQA**：图表问答。
    *   **DocVQA**：文档图像问答。
    *   *注*：LLaVA-1.5 因训练数据限制，未评估 ChartQA 和 DocVQA。

### 三、 评价指标
*   **主要指标**：在上述8个基准测试上的**平均得分**，综合反映模型的多模态理解、推理、幻觉控制等能力。
*   **效率指标**：
    *   **KV 缓存内存占用减少百分比**。
    *   微调所需的**参数量占比**（参数效率）。
    *   微调所需的**数据量/令牌数**（数据效率）。
    *   微调**时间**。

### 四、 对比的基线方法
1.  **原始模型**：经过微调后的原始 MHA/GQA 架构 VLM（`d_kv = 256`），作为性能上限参考。
2.  **KV 缓存剪枝方法**：
    *   **H₂O** 和 **TOVA**：通过移除不重要的令牌来压缩 KV 缓存。
3.  **KV 缓存量化方法**：
    *   **Quanto** 和 **HQQ**：将 KV 缓存量化为低精度（如 Int4, Int2）格式以节省内存。

### 五、 关键性能结果与结论
#### 1. **KV 缓存压缩与性能保持**
*   **结论**：MHA2MLA-VLM 能在**大幅压缩 KV 缓存**的同时，**性能损失极小**。
*   **数据**（见表1）：
    *   **Qwen2.5-VL (7B)**：将 `d_kv` 从 256 压缩至 64（**KV 缓存减少 94.64%**），平均性能从 80.75 降至 79.47，仅下降 **1.28 分**。
    *   **LLaVA-NeXT (8B)**：`d_kv=128`（缓存减少 84.38%）时，性能为 70.23，相比原始模型（70.78）仅下降 **0.55 分**。
    *   即使压缩到 `d_kv=32`（缓存减少 93.75%），LLaVA-NeXT 仍能保持 66.72 的平均分。

#### 2. **与基线压缩方法的对比**
*   **结论**：在**相同或更高压缩率**下，MHA2MLA 的性能**显著优于**缓存剪枝方法，并能与缓存量化方法**无缝结合**实现进一步压缩。
*   **数据**（见表2）：
    *   **vs. 剪枝**：在 KV 缓存减少 62.50% 时，MHA2MLA (LLaVA-NeXT) 得分为 **68.75**，而 H₂O 和 TOVA 分别为 **63.38** 和 **60.48**。
    *   **vs. 量化**：MHA2MLA (`d_kv=64`) 结合 Int4 量化（总压缩率 90.63%）得分为 **~68.65**，显著优于仅使用 Int2 量化（压缩率 87.50%）的 **67.21 (Quanto)** 和 **60.71 (HQQ)**。

#### 3. **参数与数据高效性**
*   **结论**：该方法仅需微调**极少部分参数**和**有限数据**即可完成架构迁移，成本极低。
*   **数据**：
    *   **参数效率**：采用两阶段参数高效微调。例如对于 Qwen2.5-VL，阶段一只微调约 **6%** 的参数（Q/K 投影矩阵），阶段二微调约 **10%** 的参数（MLA 内部参数）。
    *   **时间效率**：Qwen2.5-VL 的总微调时间从 22 小时缩短至 **9 小时**（减少 **59%**）。
    *   **数据效率**：仅使用最高 **1.8B 令牌**的指令数据，而非从头训练所需的万亿级令牌。

#### 4. **消融实验验证核心设计**
*   **结论**：论文提出的**模态解耦低秩近似**和**两阶段训练**策略均对最终性能有明确贡献。
*   **数据**（见表3）：
    *   **模态解耦 (Modality Decoupled)**：在 `d_kv=64` 时带来 **+0.45** 的性能提升。
    *   **MD-SVD 初始化**：在 `d_kv=64` 时带来 **+1.23** 的显著提升，并加速训练收敛。
    *   **两阶段训练**：相比单阶段训练，在 `d_kv=64` 时带来 **+0.88** 的性能提升。

#### 5. **部分 RoPE 策略比较**
*   **结论**：提出的基于 KL 散度的模态感知策略 **𝒮_MKL** 优于基于 2-范数的启发式策略 **𝒮_2-norm**。
*   **数据**（见表4）：在 LLaVA-NeXT 上，𝒮_MKL 平均得分为 **70.54**，高于 𝒮_2-norm 的 **70.31**。分析表明 𝒮_MKL 能更有效地保留对多模态位置信息至关重要的高频维度。

### 总结
论文通过系统的实验证明，**MHA2MLA-VLM 是一种实用、高效的 VLM 推理加速方案**。它不仅在多个主流 VLM 和评测基准上实现了 **KV 缓存的大幅压缩（最高 >94%）与性能的出色保持**，而且在**训练成本（参数、数据、时间）上极具优势**。同时，该方法优于传统的缓存剪枝技术，并能与量化技术叠加，为实际部署中的 VLM 高效推理提供了强有力的工具。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.11464v1)
- [HTML 版本](https://arxiv.org/html/2601.11464v1)
