# Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving

**相关性评分**: 8.0/10

**排名**: #1


---


## 基本信息

- **arXiv ID**: [2601.12142v1](https://arxiv.org/abs/2601.12142v1)
- **发布时间**: 2026-01-17T19:12:34Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao, Peng Lu, Zufeng Zhang, Sifa Zheng

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

这篇论文提出EchoVLA，一种基于音频指令的用户感知VLA模型，用于自动驾驶，通过情感语音-轨迹对微调多模态大模型，提升驾驶行为的适应性和安全性。

## 摘要

Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\%$ and the collision rate by $74.4\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user's speech.

## 详细分析

## 论文摘要

**论文标题：** Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving

**核心内容：**

1.  **研究背景与动机：**
    当前基于视觉-语言-动作（VLA）模型的自动驾驶系统，通常将语言指令视为静态的先验知识，在推理时无法更新。这导致模型必须仅从像素中持续推断不断变化的驾驶目标，容易产生**延迟或过于保守的驾驶行为**。为了提供更直接、实时的用户交互通道，本文提出需要一种能够在线接收并响应用户具体意图的VLA系统。

2.  **核心方法和技术创新：**
    本文提出了 **EchoVLA**，一个用户感知的VLA模型，其核心创新在于**将摄像头视频流与实时的音频指令进行耦合**。具体方法包括：
    - **数据集构建：** 在nuScenes数据集基础上，通过将自车运动描述转换为合成语音，生成了时间对齐、意图明确的音频指令。进一步，通过调整语音的节奏、音高等特征，合成了包含**紧急**和**犹豫**等不同情感状态的音频-轨迹对。
    - **多模态思维链（CoT）推理：** 构建了一个包含图像、音频、情感分析和对应轨迹调制的多模态CoT数据集，用于指导模型推理。
    - **模型微调：** 基于**Qwen2.5-Omni**多模态大模型进行监督微调。该模型特有的时间对齐多模态RoPE（TMRoPE）和Thinker-Talker架构，有效解决了音频与视觉信号之间的时间对齐问题，并能根据音频的语义内容和情感上下文输出用户感知的轨迹。

3.  **主要实验结果：**
    在nuScenes数据集的开环基准测试中，EchoVLA展现出显著优势：
    - 与仅使用视觉感知的基线VLA相比，**平均L2轨迹误差降低了59.4%**，**碰撞率降低了74.4%**。
    - 定性实验表明，EchoVLA不仅能根据音频的语义内容（如“左转”）调整轨迹，还能根据语音中的**情感线索**（如紧急或犹豫的语气）调制驾驶行为（如加速或减速），实现了更细腻、适应性的驾驶。

4.  **研究意义和价值：**
    本研究为自动驾驶VLA模型引入了**实时的、多模态的用户交互通道**，突破了语言指令静态化的限制。其价值在于：
    - **技术创新：** 首次系统性地将音频指令（含情感信息）与视觉感知深度融合到自动驾驶决策中，提出了完整的数据合成、模型训练与推理框架。
    - **实用价值：** 显著提升了轨迹预测的准确性和安全性，使自动驾驶系统能够更直接、更自然地理解并响应用户的即时意图和情绪状态，朝着**更安全、更人性化**的交互式自动驾驶迈出了关键一步。
    - **启发意义：** 证明了音频作为互补模态的有效性，为未来整合更多实时交互信号（如手势、生理信号）提供了思路。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
现有基于视觉-语言-动作（VLA）的自动驾驶模型将语言指令视为**静态先验**（推理时固定），导致模型只能从连续变化的像素中推断用户意图，造成**决策延迟或过于保守**的驾驶行为。这缺乏一个让用户能在推理过程中实时、动态地通过自然交互方式影响驾驶决策的通道。

### **核心创新点**
论文提出了 **EchoVLA**，一个**用户感知的VLA模型**，其核心创新在于：
1.  **引入在线音频指令通道**：将**实时音频指令**与视觉感知耦合，作为用户表达动态意图的直接输入。
2.  **情感感知的驾驶行为调制**：不仅理解音频的**语义内容**，还通过分析语调、音高、语速中的情感线索（如紧急、犹豫），来**调制轨迹的速度剖面**，实现更细腻、人性化的驾驶行为。
3.  **基于多模态思维链的数据构建与训练**：创造性地构建了一个包含**情感-语音-轨迹对**的多模态思维链（CoT）数据集，用于监督微调，使模型能进行从音频分析、情感检测到轨迹生成的连贯推理。

### **解决方案**
1.  **数据集构建**：
    - 以 **nuScenes** 数据集为基础，将自车运动描述转换为结构化的自然语言文本，再通过TTS模型合成同步的语音指令。
    - **情感增强**：通过调整基础音频的语速和音高，合成带有“紧急”和“犹豫”情感的语音指令，并使用基于音频特征（RMS能量、基频、节奏、频谱质心）的**唤醒度计算公式**进行情感量化标注。
    - **轨迹调制**：根据情感标签，对原始轨迹进行**速度剖面重参数化**。紧急情感对应更激进的速度曲线，犹豫情感则对应包含减速点的保守速度曲线，从而生成与情感匹配的调制后轨迹。

2.  **模型架构与训练**：
    - 采用 **Qwen2.5-Omni** 作为基础多模态大模型（MLM）。其**时间对齐的多模态RoPE**和**Thinker-Talker架构**能有效处理音频与视觉信号之间的时间对齐问题，并进行高效推理。
    - 使用构建的**多模态CoT数据集**对模型进行监督微调。模型接收前视图像和用户音频，通过CoT推理，输出音频分析、情感检测和用户感知的轨迹航点。

3.  **实验验证**：
    - **开环评估**：在nuScenes数据集上，EchoVLA相比仅使用视觉感知的基线VLA，**平均L2误差降低59.4%，碰撞率降低74.4%**。
    - **定性分析**：展示了模型能根据音频指令（即使与视觉暗示冲突）规划轨迹，并能根据语音中的情感线索调整驾驶行为（如加速或减速）。
    - **消融实验**：验证了直接利用MLM原生音频编码与CoT推理的方法，优于将音频编码为额外token（如HuBERT、VQ-VAE、STFT）的替代方案。

### **实际价值**
- **提升人车交互与安全性**：为用户提供了一个直观、自然的实时干预接口，使自动驾驶系统能更及时、准确地响应人类意图，减少因意图不明确导致的保守或危险行为。
- **迈向情感智能驾驶**：首次在VLA框架中系统性地整合了**情感计算**，使自动驾驶行为不仅能“听懂指令”，还能“体察情绪”，向更人性化、可信任的自动驾驶迈出关键一步。
- **高效多模态融合范式**：提出的基于现有MLM（Qwen2.5-Omni）和CoT数据构建的微调方案，为高效集成音频等时序模态提供了可复现的技术路径，避免了复杂的多模态编码器设计。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对现有基于视觉-语言-动作（VLA）模型的自动驾驶系统将语言指令视为静态先验、无法实时响应用户动态意图的问题，提出了一个名为 **EchoVLA** 的用户感知型VLA框架。其核心创新在于**引入实时音频指令作为在线交互通道**，使模型不仅能理解指令的语义内容，还能通过分析语音的语调、音高和语速来感知用户的情绪状态（如紧急或犹豫）。方法上，论文通过将nuScenes数据集中的自车运动描述转换为合成语音指令，并基于计算出的“唤醒度”为不同情绪配对相应的速度曲线，构建了一个包含多模态思维链（CoT）的数据集，用于对Qwen2.5-Omni多模态大模型进行监督微调。实验结果表明，该框架在nuScenes开环基准测试中，相比仅依赖视觉感知的基线模型，**平均L2误差降低了59.4%，碰撞率降低了74.4%**，有效证明了耦合音频指令能显著提升轨迹规划的准确性与安全性，并实现基于用户情绪的驾驶行为调制。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving》提出了一个名为 **EchoVLA** 的用户感知视觉-语言-动作模型。其核心创新在于**为自动驾驶VLA模型引入了实时的、包含情感信息的音频指令通道**，从而实现了更符合用户意图和状态的驾驶行为。以下是其相对于已有工作的明确创新点：

---

### 1. **从静态语言先验到动态音频指令的交互模式创新**
- **改进/不同之处**：
    - **以往方法**：现有的VLA模型通常将语言指令（如导航目标）视为在推理时固定的**静态先验**。模型只能从连续的视觉像素中推断可能随时间变化的目标，导致决策延迟或过于保守。
    - **本文方法**：提出了一个**在线音频指令通道**，允许用户在推理过程中实时输入语音命令，动态地影响驾驶决策。这使语言从“固定提示”转变为“动态交互接口”。
- **解决的问题/带来的优势**：
    - **解决了意图模糊和延迟问题**：在驾驶场景存在多种可行选择时（例如多个可通行车道），纯视觉模型可能无法准确判断用户即时意图。音频指令提供了明确的语义指导，使模型能输出符合用户**特定时刻意图**的轨迹，减少了因意图推断错误导致的动作延迟或碰撞风险。
    - **提升人机协同与用户体验**：为用户提供了一个自然、直观的实时干预接口，使自动驾驶系统更具交互性和可控性。

### 2. **融合情感分析的音频指令理解与行为调制**
- **改进/不同之处**：
    - **以往方法**：在机器人音频导航等领域，音频指令通常只用于传递**空间或语义目标**（如“去厨房”），很少考虑语音中蕴含的**情感状态**（如急切、犹豫）。
    - **本文方法**：创新性地将语音的**情感特征**（通过语调、音高、语速等声学特征量化为“唤醒度”）作为驾驶行为调制的依据。论文合成了带有情感标签（急切/犹豫）的音频-轨迹对数据集。
- **解决的问题/带来的优势**：
    - **实现了更细腻、人性化的驾驶行为**：模型不仅能理解指令的“内容”（语义），还能理解指令的“方式”（情感）。例如，识别到“急切”情绪时，会生成更快的速度曲线；识别到“犹豫”时，则会在路径中点附近减速。这使自动驾驶行为能更好地适应用户的实时心理状态。
    - **增强了系统的安全性和舒适性**：情感感知使系统能在用户紧急时更果断，在用户不确定时更谨慎，从另一个维度提升了驾驶安全性。

### 3. **基于多模态思维链的数据集构建与模型微调方法**
- **改进/不同之处**：
    - **以往方法**：VLA训练数据的构建多依赖于大量人工标注，过程耗时费力，且设计原则多为经验性。
    - **本文方法**：
        1.  **数据合成**：利用nuScenes数据集中的真实轨迹，通过聚类得到驾驶意图描述，再使用TTS模型转换为音频指令。通过调整合成语音的声学特征（语速、音高）来注入情感，并据此调制原始轨迹的速度剖面，生成“情感-轨迹”配对数据。
        2.  **训练范式**：采用**多模态思维链**进行监督微调。模型（基于Qwen2.5-Omni）的推理过程被设计为：接收图像和音频 → 分析音频内容 → 检测用户情绪 → 输出情绪引导的轨迹航点。这是一个端到端的、可解释的推理链条。
- **解决的问题/带来的优势**：
    - **低成本构建高质量多模态数据**：避免了为“音频-情感-驾驶行为”三元组进行大规模真实数据采集与标注的难题，提供了一种高效的数据扩充方法。
    - **提升模型的可解释性与对齐能力**：CoT设计迫使模型显式地推理音频语义和情感，再据此规划轨迹，增强了决策过程的透明度和与人类指令的对齐程度。

### 4. **针对自动驾驶场景的跨模态时序对齐模型架构应用**
- **改进/不同之处**：
    - **以往方法**：直接将音频通过HuBERT、VQ-VAE等编码为特征序列，与图像特征简单拼接后输入VLM，可能忽视音频与视觉流之间精细的**时间同步**需求。
    - **本文方法**：选择**Qwen2.5-Omni**作为骨干网络，因其具备**时间对齐的多模态RoPE**技术，能够将不同采样率的音频帧和图像帧在时间轴上交错对齐。其Thinker-Talker架构也适合处理长序列音频并执行复杂推理。
- **解决的问题/带来的优势**：
    - **解决了模态间时间尺度不匹配问题**：自动驾驶中，高频音频信号与低频图像帧需要精确对齐以理解“何时”的指令对应“何种”视觉场景。TMRoPE确保了跨模态信息的时序一致性。
    - **高效处理长序列输入**：模型支持流式处理和滑动窗口注意力，为未来实现低延迟的实时音频指令响应奠定了基础。

---

### **总结与核心价值**
本文的核心创新是**将自动驾驶VLA从被动的环境解释器，升级为主动的、支持实时多模态人机交互的协同驾驶伙伴**。通过引入并深度理解音频指令（含情感），EchoVLA显著提升了在开放环境下的意图跟随准确性和驾驶安全性（L2误差降低59.4%，碰撞率降低74.4%）。这为构建更**安全、人性化、可信赖**的下一代自动驾驶系统提供了新的技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果分析

### 数据集与评价指标
- **数据集**：使用 **nuScenes** 数据集进行训练和评估。该数据集包含1000个驾驶场景，提供多传感器（摄像头、激光雷达、雷达）同步数据。
- **评价指标**：
  - **L2误差（米）**：预测轨迹与真实轨迹各路径点之间的平均欧氏距离，衡量轨迹规划的准确性。
  - **碰撞率（%）**：沿预测轨迹放置自车边界框，检查与其他物体真实边界框的重叠比例，评估安全性。
  - **情绪识别与行为调制**：通过合成音频（正常、紧急、犹豫）验证模型能否根据语音情绪调整速度曲线（如加速、减速）。

### 基线方法对比
论文将提出的 **EchoVLA** 与以下 **仅视觉感知的VLA基线** 进行对比：
1. **Qwen2-VL-7B**：基于更大规模（7B参数）的视觉语言模型，仅使用视觉输入。
2. **其他先进视觉驱动方法**（如VLP-UniAD、RDA-Driver），作为性能参考。

### 关键性能提升
在nuScenes开环评估中，EchoVLA相比 **仅视觉基线（Qwen2-VL-7B）** 取得显著提升：

| 指标 | 提升幅度 | 具体数值对比（EchoVLA vs. 基线） |
|------|----------|----------------------------------|
| **平均L2误差** | **降低59.4%** | 从 **1.43米** 降至 **0.58米** |
| **平均碰撞率** | **降低74.4%** | 从 **0.43%** 降至 **0.11%** |

**详细结果（见表I）**：
- **L2误差**：在1秒、2秒、3秒预测时间范围内均优于基线。
- **碰撞率**：在短时预测（1秒）中降至0%，长时预测（3秒）中从1.07%大幅降至0.22%。

### 其他重要结论
1. **情绪引导行为调制有效**：
   - 模型能根据音频中的紧急/犹豫情绪，自动调整轨迹速度曲线（紧急时加速，犹豫时减速或中途停顿），如图7所示。
2. **多模态互补性**：
   - 在驾驶意图明确时（图4），视觉与音频指令结果一致；当意图存在歧义时（图5），**音频指令能纠正纯视觉感知的偏差**，使轨迹更符合用户实时意图。
3. **消融实验验证设计有效性**：
   - 对比其他音频编码方法（HuBERT、VQ-VAE、STFT），本文采用的 **Qwen2.5-Omni内置多模态对齐与CoT推理** 效果最优（L2误差0.38 vs. 0.45-0.60，碰撞率0.24% vs. 0.32-0.46%）。

### 总结
EchoVLA通过 **耦合实时音频指令与视觉感知**，显著提升了轨迹规划的准确性和安全性，同时实现了 **基于用户情绪的适应性驾驶行为调制**。实验表明，音频模态为VLA提供了 **互补的意图信息**，而非与视觉竞争，从而在复杂驾驶场景中实现更人性化、安全的决策。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.12142v1)
- [HTML 版本](https://arxiv.org/html/2601.12142v1)
