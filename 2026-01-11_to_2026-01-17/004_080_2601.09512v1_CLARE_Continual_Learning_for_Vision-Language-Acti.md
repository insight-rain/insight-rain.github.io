# CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion

**相关性评分**: 8.0/10

**排名**: #4


---


## 基本信息

- **arXiv ID**: [2601.09512v1](https://arxiv.org/abs/2601.09512v1)
- **发布时间**: 2026-01-14T14:23:42Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Ralf Römer, Yi Zhang, Angela P. Schoellig

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune

## 一句话总结

CLARE提出了一种基于轻量级适配器路由和扩展的持续学习框架，用于解决视觉-语言-动作模型在机器人任务中的灾难性遗忘问题，无需存储历史数据或任务标识符。

## 摘要

To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.

## 详细分析

## 论文摘要：CLARE: 通过自主适配器路由与扩展实现视觉-语言-动作模型的持续学习

**1. 研究背景和动机**
在机器人领域，通过任务特定数据微调预训练的视觉-语言-动作模型已成为教授复杂操作任务的常见方法。然而，在现实世界的长期运行中，机器人需要持续适应新任务和环境，同时保留已学知识。现有的机器人持续学习方法通常需要存储历史数据、难以处理长任务序列，或依赖任务标识符进行部署。为解决这些局限，本文提出了**CLARE**，一个通用的、参数高效的、无需示例的VLA持续学习框架。

**2. 核心方法和技术创新**
CLARE的核心创新在于其**轻量级模块化架构**与**自主决策机制**：
- **模块化适配器**：在选定的前馈网络层中注入轻量级适配器作为旁路分支，仅在新任务到来时训练新适配器，冻结原有模型参数，从而避免灾难性遗忘。
- **自主动态扩展**：基于层间特征相似性，通过计算重构误差的z分数，**仅在必要时**（当新任务特征与所有历史任务显著不同时）扩展模型，添加新适配器，实现参数数量的亚线性增长（约2%/任务）。
- **无任务标识符的路由**：为每个可扩展层配备一组自编码器判别器。在部署时，通过比较输入特征在各判别器上的重构误差，**动态激活最相关的适配器**，无需预先知道任务标签。

**3. 主要实验结果**
在LIBERO基准测试上的广泛实验表明：
- **性能卓越**：CLARE在整体性能（AUC）上显著优于所有基线方法，甚至超过了可以使用历史数据的经验回放方法。
- **有效抗遗忘**：负向反向迁移值接近零，表明其能几乎完全避免灾难性遗忘。
- **参数高效**：在保持高性能的同时，模型参数仅以极低速率增长。
- **关键发现**：将适配器添加到模型的**编码器部分**（而非解码器）能获得最佳性能。

**4. 研究意义和价值**
CLARE为机器人在开放世界中的长期自主运行提供了一种切实可行的解决方案。其价值在于：
- **实用性**：摆脱了对存储历史数据或获取任务标识符的依赖，更符合现实部署场景。
- **可扩展性**：轻量级、模块化的设计使其易于扩展到未来更大规模的VLA模型。
- **启发性**：证明了在VLA的编码器模块中存储任务特定知识对于持续学习的有效性，为后续研究指明了方向。

该工作推动了持续学习与具身智能的结合，是迈向拥有终身学习能力机器人系统的重要一步。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：CLARE

### **一、 论文旨在解决的核心问题**
论文旨在解决**机器人视觉-语言-动作模型在持续学习场景下面临的“灾难性遗忘”难题**。具体而言：
- **问题背景**：预训练的VLA模型通常需要在特定任务数据上微调才能获得高性能。然而，在现实世界中，机器人需要**持续学习**一系列新任务（如操作新家电、适应新环境），同时**不能遗忘**已学会的技能。
- **现有方法的局限**：
    1.  **基于经验回放的方法**：需要存储大量历史数据，存在存储、隐私和计算开销问题。
    2.  **基于正则化的方法**（如EWC）：在长任务序列中，受限于固定的初始参数容量，性能会下降。
    3.  **基于架构扩展的方法**：通常需要**任务标识符**来在部署时选择正确的模块，这在开放世界的自主操作中是不现实的。
- **核心挑战**：实现一种**无需示例存储、无需任务标识符、参数高效**的VLA持续学习方法。

### **二、 核心技术创新点**
CLARE 通过一个集成的框架解决了上述问题，其创新点主要体现在以下三个紧密关联的机制上：

1.  **轻量级模块化适配器**
    - **技术实现**：在预训练VLA模型选定的前馈网络层中，以**并行旁支**的形式注入轻量级的编码器-解码器结构适配器。
    - **关键特性**：
        - **参数高效**：适配器采用降维-升维结构（`down-proj` + `ReLU` + `up-proj`），参数量远小于基础层。
        - **冻结基础模型**：学习新任务时，**仅训练新添加的适配器**，基础模型和已有适配器的参数全部冻结，从而从根本上保护了已有知识不被覆盖。

2.  **基于自编码器的自主路由机制**
    - **技术实现**：为每个可扩展层配备一组**自编码器判别器**。每个判别器与一个适配器关联。
    - **工作原理**：
        - **训练**：用新任务数据训练新添加的判别器，使其能**最小化重建误差**来识别属于该任务的特征。
        - **推理**：对于当前输入特征，路由器**选择重建误差最小的判别器所关联的适配器**进行激活。
        - **公式表示**：
            ```python
            # 重建误差计算
            e = ||x - D(x)||_2
            # 路由选择
            adapter_selected = B(D_argmin(e))
            ```
    - **核心价值**：实现了**完全自主、无需任务标签**的适配器选择，使机器人能在未知任务场景下自适应地组合已有技能。

3.  **基于特征相似性的动态扩展策略**
    - **技术实现**：并非为每个新任务在所有层都添加适配器，而是根据**特征分布的新颖性**决定是否扩展。
    - **决策流程**：
        1.  计算新任务特征在所有现有判别器上的标准化`z-score`。
        2.  如果**所有**`z-score`都超过阈值`γ`，表明该层特征与所有旧任务都显著不同，则**扩展一个新适配器**，并链接新判别器。
        3.  否则，仅添加一个**辅助判别器**，并将其链接到与当前任务特征最相似的现有适配器上。
    - **核心价值**：
        - **参数增长次线性化**：仅在最需要的地方增加容量，实现了高效的知识共享与复用。
        - **避免特征分布偏移**：通过辅助判别器机制，确保了即使浅层后续扩展了新适配器，深层已训练适配器的输入特征分布依然保持稳定，保证了策略的可靠性。

### **三、 解决方案的整合与价值**
CLARE 将上述三点创新整合为一个完整的持续学习工作流（如算法1所示）：
1.  **动态扩展阶段**：根据`z-score`决定是否添加新适配器或仅添加判别器。
2.  **训练阶段**：用流匹配损失训练新适配器，然后用重建损失训练新判别器。
3.  **部署阶段**：通过自主路由机制动态激活最相关的适配器组合。

**实际价值与实验验证**：
- **高效防遗忘**：在LIBERO基准测试中，CLARE的**负向反向迁移值接近零**，显著优于需要历史数据的经验回放方法。
- **高性能与高参数效率**：在取得最高**AUC**（综合性能）的同时，平均每个任务仅增加约**2%** 的参数。
- **架构洞察**：实验发现，在VLA的**编码器部分**（而非解码器）添加适配器效果最佳，这为理解VLA中任务知识的存储位置提供了新见解。

**总结**：CLARE的核心创新在于提出了一种**“按需扩展、自主路由”** 的持续学习范式。它通过**轻量适配器**隔离任务知识，用**自编码器路由**实现无标签部署，并借**动态扩展**平衡性能与参数效率，为机器人在开放世界中长期、自主地累积技能提供了一个切实可行的框架。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决机器人视觉-语言-动作模型在持续学习场景中面临的核心挑战：如何在不遗忘旧技能的前提下，高效地学习一系列新任务，同时避免依赖历史数据（范例）和任务标识符。为此，论文提出了名为**CLARE**的框架，其核心方法是在预训练模型的部分前馈层中，动态地、按需地插入轻量级适配器模块来存储新任务知识，并设计了一个基于自编码器的路由机制，在部署时根据特征相似性自动激活最相关的适配器。实验结果表明，该方法在无需范例的情况下，显著优于包括基于范例的方法在内的多个基线，实现了对新任务的高性能学习，同时几乎完全避免了灾难性遗忘，且每个任务仅需增加约2%的参数。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

基于对论文《CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion》的深入阅读，其相对于已有工作的明确创新点、改进之处及带来的优势如下：

### 1. **提出了一种免示例（Exemplar-Free）的持续学习框架**
   - **改进/不同之处**：现有的机器人持续学习方法（如经验回放 ER、LOTUS）通常需要存储和重放过去任务的数据（示例），这不仅带来存储和隐私问题，还增加了计算开销。CLARE 完全**不依赖任何过去任务的原始数据**进行学习。
   - **解决的具体问题/优势**：解决了在长期、开放世界部署中，由于存储限制、隐私约束或数据不可用而无法保留历史数据的问题。这使得 CLARE 更适用于真实世界的长期机器人操作场景。

### 2. **引入了自主的、基于特征相似性的动态适配器扩展策略**
   - **改进/不同之处**：以往的模块化/可扩展架构（如 SDP、TAIL）通常为每个新任务预定义或固定地添加新模块。CLARE 则**动态地、有选择地**在模型的特定前馈层（FFN）中添加轻量级适配器。其决策基于**层间特征统计的相似性**（通过计算新任务特征与已有判别器重建误差的 z-score 来判断），仅在新任务特征分布与所有旧任务显著不同时才扩展新适配器。
   - **解决的具体问题/优势**：
     - **参数高效性**：避免了为每个任务线性增加参数，实现了**亚线性**的参数增长（论文中约为每任务增加2%的参数）。
     - **促进知识共享**：当新任务与旧任务相似时，复用现有适配器，促进了任务间的知识迁移，避免了冗余参数。
     - **平衡稳定性与可塑性**：在保留预训练基础表示（稳定性）和增加必要容量以学习新知识（可塑性）之间取得了良好平衡。

### 3. **设计了无需任务标识符的自主路由机制**
   - **改进/不同之处**：许多需要任务标识符的模块化方法（如 SDP）在部署时需要手动或通过元策略选择对应模块，这在开放世界中不现实。CLARE 使用**基于自编码器的判别器**来构建路由机制。每个判别器学习重建其对应任务的特征。在推理时，系统根据**当前输入特征在各判别器上的重建误差**，自动选择误差最小的判别器所链接的适配器进行激活。
   - **解决的具体问题/优势**：
     - **实现完全自主部署**：机器人可以在没有人工提供任务标签的情况下，根据当前观察自主选择最相关的技能，真正适用于开放世界场景。
     - **轻量且可扩展**：路由机制本身也是轻量级的，并且可以随着判别器的增加而扩展。

### 4. **将适配器模块主要注入编码器部分，并验证了其高效性**
   - **改进/不同之处**：论文通过系统的消融实验发现，将适配器添加到模型的**编码器模块**（无论是 Transformer 编码器还是线性投影层）的性能，远优于添加到解码器模块。这与以往在LLM中常修改FFN层的直觉有所不同，是针对VLA模型特性的新发现。
   - **解决的具体问题/优势**：
     - **性能提升**：该设计选择带来了显著的性能提升（AUC和FWT指标高出30-40%），表明VLA模型中，任务特定的知识更有效地被编码在感知和语言理解的早期融合阶段（编码器）。
     - **为VLA架构设计提供洞见**：这一发现为未来设计高效的、可持续学习的VLA模型提供了重要的架构指导。

### 5. **整体框架在保持高性能的同时，显著缓解了灾难性遗忘**
   - **改进/不同之处**：与基线方法（如 SeqFFT、SeqLoRA、PackNet）相比，CLARE 在**整体性能（AUC）** 和**后向负迁移（NBT，衡量遗忘程度的指标）** 上表现突出。即使与可以使用历史数据的优势基线（如 ER、LOTUS）相比，CLARE 在免示例的情况下仍能达到甚至超越其性能。
   - **解决的具体问题/优势**：
     - **证明了免示例持续学习的可行性**：CLARE 的成功表明，通过精巧的模块化设计和自主路由，可以在不存储旧数据的情况下有效避免灾难性遗忘。
     - **综合优势**：该框架同时实现了**高任务性能**、**低遗忘率**、**参数高效性**和**部署自主性**，在多个维度上超越了现有方法。

---
**总结**：CLARE 的核心创新在于将**动态模块化扩展**、**基于特征相似性的自主路由**和**针对VLA模型的高效层选择**相结合，构建了一个**通用、参数高效、免示例且无需任务标识符**的VLA持续学习框架，有效解决了机器人长期部署中**知识保留**与**持续适应**的核心矛盾。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 使用的数据集与评估指标

#### 1. 数据集
- **预训练数据集**：LIBERO-90，包含90个短视距任务。
- **持续学习评估数据集**：LIBERO-10，包含10个顺序到达的长视距任务。这些任务要求机器人理解语言指令并执行不同类型的动作（如抓放、开抽屉、转旋钮）。
- **数据规模**：每个任务提供50个人类专家演示。

#### 2. 评估指标
论文使用三个核心指标来量化持续学习性能：
- **AUC (Area under the success rate curve)**：衡量新任务和旧任务的整体性能。值越高越好。
- **FWT (Forward Transfer)**：衡量学习新任务的能力。值越高越好。
- **NBT (Negative Backward Transfer)**：衡量灾难性遗忘的程度。值越低越好（负值表示旧任务性能在后续学习后反而提升）。

### 二、 对比的基线方法
论文与五种无需任务标识符的持续学习方法进行了对比：
1.  **SeqFFT (Sequential Fully Fine-tuning)**：顺序地对整个模型进行全参数微调。
2.  **SeqLoRA (Sequential Low-Rank Adaptation)**：为每个新任务添加LoRA适配器，训练后合并回基础权重。
3.  **PackNet**：迭代地冻结重要权重，用剩余的自由权重学习新任务。
4.  **ER (Experience Replay)**：存储部分旧数据，与新数据混合训练。**（注：此方法使用了旧数据，具有优势）**
5.  **LOTUS**：构建并扩展技能库，通过元策略组合技能。**（注：此方法也使用了旧数据）**

### 三、 关键性能结果与结论

#### 1. 主要性能提升
- **整体性能最优**：在两种不同的骨干网络（DiT-EncDec 和 DiT-Dec）上，CLARE 的 **AUC 指标均显著高于所有基线方法**。
    - 相比最佳基线 ER，CLARE 的 AUC 绝对值分别高出约 **11%** (DiT-EncDec) 和 **15%** (DiT-Dec)。
- **有效缓解遗忘**：CLARE 的 **NBT 值接近或低于零**，表明其几乎完全避免了灾难性遗忘。
    - 例如，DiT-EncDec 上的 NBT 为 `-0.80 ± 0.76`，意味着旧任务性能在学习新任务后甚至略有提升。
- **高效学习新任务**：CLARE 的 **FWT 与微调全参数的方法（SeqFFT, ER）相当**，表明其能以极少的额外参数有效学习新任务。
    - 每个任务平均仅增加模型总参数约 **1.7% - 2.3%**，实现了参数高效性。

#### 2. 核心结论
- **无需回放数据，性能更优**：CLARE 在**不依赖任何旧数据（exemplar-free）** 的情况下，其整体性能（AUC）甚至超过了可以使用旧数据的强基线 ER 和 LOTUS。这解决了实际部署中数据存储和隐私的难题。
- **自主路由有效**：基于自编码器的路由机制能够在**无需任务标识符**的情况下，动态激活最相关的适配器，实现了完全自主的部署。
- **动态扩展高效**：基于特征相似性（z-score）的动态扩展策略，能够智能地仅在必要时添加新适配器，避免了参数的线性爆炸增长。通过调整阈值 `γ`，可以在性能和参数效率之间进行权衡。
- **编码层是关键**：消融实验表明，将适配器添加到模型的**编码器部分**（而非解码器）能带来最佳性能，说明编码模块更适合存储任务特定知识。

#### 3. 结果可视化支持
论文中的**图4（成功率曲线）** 直观展示了CLARE的优势：其曲线在高位保持平稳，而基线方法（如SeqFFT、SeqLoRA、PackNet）的曲线随着学习新任务而急剧下降，清晰证明了CLARE在克服遗忘方面的有效性。

**总结**：CLARE 在 LIBERO 基准测试上实现了**高性能、低遗忘、参数高效**的持续学习，其核心创新（模块化适配器、自主路由、动态扩展）被实验证实有效，且在不使用旧数据的公平设定下，性能超越了依赖旧数据的先进方法。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.09512v1)
- [HTML 版本](https://arxiv.org/html/2601.09512v1)
