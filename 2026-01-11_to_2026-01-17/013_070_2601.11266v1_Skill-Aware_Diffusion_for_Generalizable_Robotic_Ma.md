# Skill-Aware Diffusion for Generalizable Robotic Manipulation

**相关性评分**: 7.0/10

**排名**: #13


---


## 基本信息

- **arXiv ID**: [2601.11266v1](https://arxiv.org/abs/2601.11266v1)
- **发布时间**: 2026-01-16T13:14:40Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Aoshen Huang, Jiaming Chen, Jiyu Cheng, Ran Song, Wei Pan, Wei Zhang

## 关键词

Diffusion, fine tune, world model

## 一句话总结

该论文提出了一种基于技能感知扩散模型（SADiff）的机器人操作方法，通过引入技能级信息来提升泛化能力，并使用了扩散模型生成运动流。

## 摘要

Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.

## 详细分析

## 论文摘要：Skill-Aware Diffusion for Generalizable Robotic Manipulation

### 1. 研究背景和动机
在机器人操作领域，实现对新物体、新环境的**鲁棒泛化**是一个核心挑战。现有方法（如模仿学习）通常通过扩大数据和网络规模来提升泛化能力，但往往将任务视为独立个体，**忽视了技能层面的共享信息**。本文观察到，同一技能类别下的不同任务具有相似的运动模式，因此提出通过显式建模**技能级信息**来提升数据效率和泛化能力。

### 2. 核心方法和技术创新
本文提出了**技能感知扩散模型（SADiff）**，一个系统性整合技能信息的三阶段框架：
- **技能感知编码**：引入**可学习的技能令牌**，通过注意力机制与视觉、语言、物体边界框等多模态输入交互，提取技能特定表征。
- **技能约束的流生成**：基于技能感知表征，利用一个**扩散模型**生成以目标物体为中心的2D运动流。训练中引入**技能对比损失**和分类损失，确保生成的运动模式与技能语义对齐。
- **技能检索的动作转换**：提出一种新颖的转换策略，通过**检索技能特定的轨迹先验**来优化从2D运动流到3D可执行动作的映射，提升了精度和鲁棒性。
此外，本文构建了**IsaacSkill**数据集，基于高保真仿真平台，围绕5个基础技能（如倾倒、抓放）设计多样化任务，以支持全面的技能中心化评估和零样本仿真到现实迁移。

### 3. 主要实验结果
在仿真和真实世界实验中，SADiff均展现出卓越性能：
- **仿真内分布任务**：平均成功率（92.8%）显著优于基线方法（如Im2Flow2Act的88.0%）。
- **泛化能力**：在背景变化、未见物体实例、跨类别物体及跨机器人本体等泛化设置下，SADiff均保持最高成功率，例如在跨类别泛化中达到82.4%，而部分基线方法完全失败。
- **指令引导的技能适应**：在场景不变仅指令改变的任务中，SADiff成功率达85.0%，证明了其能优先遵循语义指令而非记忆场景关联。
- **零样本仿真到现实迁移**：未经真实数据微调，在真实机器人上平均成功率达76.0%，远超基线方法。
- **可扩展性与可组合性**：能够零样本泛化到训练中未见的技能（如堆叠），并能通过组合已学技能完成长视野任务。

### 4. 研究意义和价值
本工作的价值在于：
- **方法论创新**：首次系统性地将技能级表征整合到编码、生成、执行的完整操作流程中，为提升机器人泛化能力提供了新范式。
- **实际应用价值**：SADiff框架显著降低了对大规模任务特定数据的依赖，通过技能共享实现数据高效学习，并展示了强大的零样本仿真到现实迁移能力，对推动机器人在开放环境中的实际部署具有重要意义。
- **基准贡献**：发布的IsaacSkill数据集为技能中心化的评估和仿真到现实研究提供了高保真、结构化的新基准。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **一、 研究问题**
论文旨在解决**机器人模仿学习中泛化能力不足**的核心挑战。现有方法通常通过扩大数据和网络规模来提升泛化，但存在两个主要问题：
1.  **任务孤立建模**：将每个任务视为独立个体，忽略了不同任务之间共享的、更高层次的**技能级（Skill-Level）信息**。
2.  **2D到3D映射的脆弱性**：在将预测的2D运动表示（如光流）转换为可执行的3D动作时，现有方法（无论是基于学习还是启发式）在鲁棒性和精度之间难以平衡，对相机参数和传感器噪声敏感。

### **二、 核心创新点**
论文提出了 **Skill-Aware Diffusion (SADiff)** 框架，其核心创新在于**系统性地将技能级信息显式地建模并融入到机器人操作的编码、生成和执行全流程中**。具体包含三个关键技术创新和一个数据集贡献：

#### **1. 技能感知编码模块**
*   **创新**：引入了**可学习的技能令牌**，与多模态输入（图像、语言、物体边界框）进行动态交互。
*   **解决方式**：通过多头自注意力（MHSA）和交叉注意力（MHCA）机制，使技能令牌能够从任务输入中提取和融合**技能特异性信息**。这使模型能够捕获同一技能域内不同任务的统一特征，同时保留任务细节。

#### **2. 技能约束的扩散模型**
*   **创新**：提出了一个**多目标训练的扩散模型**，用于生成以技能感知令牌序列为条件的物体中心运动光流。
*   **解决方式**：
    *   **主损失**：标准的去噪损失，确保光流生成的空间精度。
    *   **辅助损失**：
        *   **技能分类损失**：确保为当前任务选择正确的技能令牌。
        *   **技能对比损失**：约束扩散模型在去噪过程中，使其内部特征与正确技能的语义描述对齐，从而区分不同技能的运动模式。
    *   **效果**：生成的2D运动光流不仅精确，而且在语义上与目标技能保持一致，提升了生成内容的鲁棒性和泛化性。

#### **3. 技能检索转换策略**
*   **创新**：设计了一种**利用技能特异性轨迹先验来优化2D到3D映射**的策略，无需额外训练。
*   **解决方式**：
    1.  为每个技能类别离线构建一个**归一化的3D轨迹模板库**。
    2.  在执行时，根据预测的技能类别检索对应的轨迹模板，并依据当前物体的初始位姿进行空间对齐，得到**任务特定的轨迹先验**。
    3.  在将2D光流通过几何优化（最小化重投影误差）转换为3D位姿时，**将轨迹先验作为正则项加入优化目标**（公式16）。
*   **效果**：显著提升了从2D光流到3D动作转换的精度和物理一致性，对深度模糊和传感器噪声更具鲁棒性。

#### **4. 高保真技能中心数据集：IsaacSkill**
*   **创新**：构建了一个基于高保真NVIDIA Isaac Lab平台的新数据集。
*   **解决方式**：数据集围绕 **5个基本机器人技能**（倾倒、抓放、推、滑动打开、铰链打开）进行组织，每个技能包含多个不同物体和场景的任务。
*   **价值**：提供了物理精确的动态和逼真视觉，支持**以技能为中心的全面评估**，并为**零样本模拟到真实迁移**提供了必要基础。

### **三、 解决方案总结**
SADiff通过一个**三阶段框架**系统性地解决了泛化问题：
1.  **编码阶段**：利用可学习技能令牌动态提取技能级上下文信息。
2.  **生成阶段**：通过技能约束的扩散模型，生成与技能语义对齐的、精确的物体中心2D运动光流。
3.  **执行阶段**：通过技能检索转换策略，利用技能轨迹先验指导优化过程，将2D光流稳健地映射为可执行的3D动作序列。

**实际价值**：该方法在模拟和真实世界实验中均展现出卓越的泛化性能（应对背景、物体实例/类别、机械臂本体的变化）和强大的零样本模拟到真实迁移能力，且展示了扩展到未见过技能和组合技能完成长周期任务的潜力，为构建更通用、数据高效的机器人操作系统提供了新思路。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决机器人模仿学习中**泛化能力不足**的核心问题，即现有方法通常独立处理任务、依赖大规模数据且难以适应新物体、新环境和新的机器人本体。为此，论文提出了 **Skill-Aware Diffusion (SADiff)** 框架，其核心创新在于**显式地建模和利用技能层面的信息**来提升泛化能力。该方法通过三个关键组件实现：1) **技能感知编码模块**，利用可学习的技能令牌与多模态输入交互，提取技能特定表征；2) **技能约束扩散模型**，基于技能感知表征生成以目标物体为中心的运动流，并引入技能对比损失来增强鲁棒性；3) **技能检索转换策略**，利用技能特定的轨迹先验来优化从2D运动流到3D可执行动作的映射。此外，论文还构建了高保真仿真数据集 **IsaacSkill** 用于评估。实验结果表明，SADiff 在仿真和真实世界中均显著优于现有基线方法，在背景、物体类别、机器人本体等多种分布外场景下展现出优异的泛化能力，并实现了零样本的仿真到真实迁移，验证了显式技能建模的有效性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Skill-Aware Diffusion for Generalizable Robotic Manipulation》的创新点分析

这篇论文的核心创新在于**系统性地将“技能级”信息引入到机器人模仿学习的全流程中**，以解决现有方法在泛化性、数据效率和鲁棒性方面的不足。其创新点具体如下：

### 1. **提出Skill-Aware Diffusion (SADiff) 整体框架**
- **相比以往方法的改进**：现有方法（如Im2Flow2Act, Track2Act）通常将每个任务视为独立个体进行学习，或仅通过扩大数据规模来提升泛化。SADiff则**首次明确地将技能级表示作为核心线索**，并贯穿于编码、生成、执行三个阶段。
- **解决的问题与优势**：解决了任务间知识难以迁移、对大规模数据依赖性强的问题。通过利用同一技能下不同任务共享相似运动模式的先验，实现了**数据高效**的泛化。模型能够从有限的演示中学习到更本质的技能表示，从而更好地适应新物体、新环境和新的机器人本体。

### 2. **设计技能感知编码模块**
- **相比以往方法的改进**：现有方法通常对多模态输入（图像、语言、边界框）进行独立编码或简单拼接。SADiff引入了**可学习的技能令牌**，并通过多头自注意力/交叉注意力机制，让技能令牌与多模态输入进行动态交互。
- **解决的问题与优势**：解决了传统编码方式难以提取和融合跨任务的共享技能特征的问题。该模块能够**动态地提取与当前任务相关的技能特定信息**，并生成富含语义的技能感知令牌序列，为后续的流生成提供了更精准、更具泛化能力的条件输入。

### 3. **开发技能约束的扩散模型**
- **相比以往方法的改进**：标准的扩散模型（如用于流生成的模型）通常仅使用去噪损失。SADiff在训练中引入了**两个技能特定的辅助损失**：技能分类损失和技能对比损失。
    - **技能分类损失**：确保模型能根据输入正确选择对应的技能令牌。
    - **技能对比损失**：强制模型在去噪过程中产生的中间特征与正确技能的语义描述对齐，同时远离其他技能。
- **解决的问题与优势**：解决了生成的2D运动流可能偏离目标技能语义的问题。通过技能约束，模型能**更清晰地区分不同技能的运动模式**，从而生成更精确、与技能意图更一致的物体中心运动流，提升了生成的鲁棒性和语义保真度。

### 4. **提出技能检索转换策略**
- **相比以往方法的改进**：传统方法将2D运动流映射到3D动作时，通常依赖纯几何优化（如最小化重投影误差）或一个需要大量数据训练的策略网络。SADiff提出了一种**基于优化的、利用技能特定轨迹先验进行精化的策略**。它离线构建了每个技能类别的平均3D轨迹模板，在执行时检索并作为优化约束。
- **解决的问题与优势**：解决了2D到3D映射过程中因深度模糊、传感器噪声和相机参数变化导致的轨迹不准确和不稳定的问题。技能先验作为一个**高层运动模式的软约束**，引导优化过程产生物理上更合理、更平滑的3D轨迹，**显著提升了动作执行的精度和一致性，且无需额外训练**。

### 5. **构建高保真技能中心数据集 IsaacSkill**
- **相比以往方法的改进**：现有机器人操作数据集（如Meta-World, RLBench）多基于简化物理引擎，视觉保真度低，且以任务完成为中心进行评估。IsaacSkill基于**高保真NVIDIA Isaac Lab平台**构建，并**以5个基础技能（倾倒、抓放、推、滑动打开、铰链打开）为中心进行组织**，每个技能包含多个不同物体和场景的任务。
- **解决的问题与优势**：解决了现有数据集**物理真实性不足**、**缺乏针对技能泛化能力的细粒度评估基准**，以及**难以支持零样本仿真到现实迁移**的问题。IsaacSkill提供了逼真的动力学和视觉，使得在仿真中训练的策略能更可靠地迁移到现实世界，并为评估模型的技能级泛化能力提供了标准平台。

### 总结
这些创新点相互关联，共同构成了SADiff框架。其核心思想是：**通过显式地建模和利用技能级抽象，在编码阶段提取共享知识，在生成阶段约束语义一致性，在执行阶段引入物理先验，从而在数据效率、泛化能力、跨本体适应性和仿真到现实迁移等方面，系统性地超越了以往孤立处理任务或仅依赖数据规模的方法。** 实验在仿真和真实世界中的优异表现验证了这些创新的有效性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验数据集与评价指标

#### 1. 数据集
- **核心数据集**：论文构建了全新的**IsaacSkill**数据集，用于训练和评估。
    - **平台**：基于高保真物理模拟器NVIDIA Isaac Lab。
    - **内容**：包含5个基础机器人操作技能（倾倒、抓放、推动、滑开、铰链打开），每个技能下设计3个不同任务，共15个任务。
    - **规模**：总计2400条轨迹演示（每个任务160条）。
    - **特点**：技能中心化设计、物体和环境多样性高、支持零样本模拟到真实迁移。

#### 2. 评价指标
- **主要指标**：**成功率**。在模拟和真实实验中，每个任务/技能进行25次独立测试，计算平均成功率。
- **辅助指标**：在真实实验中，额外报告了**规划时间**和**执行时间**，以评估系统的时间效率。

### 二、 对比的基线方法
论文选择了四种具有代表性的基线方法进行对比，涵盖了不同的技术范式：
1.  **R3M**：一种行为克隆方法，利用预训练的视觉表示（时间对比学习、视频-语言对齐）。
2.  **AVDC**：一种视频预测框架，通过分析预测帧之间的密集像素对应关系来推导动作。
3.  **Track2Act**：一种轨迹中心方法，使用目标条件网格流模型生成查询点的未来轨迹，再通过策略网络映射为动作。
4.  **Im2Flow2Act**：一种基于流的SOTA方法，生成以物体为中心的运动流，并以此作为策略网络的输入来输出动作。这是与SADiff最直接相关的对比方法。

### 三、 关键实验结果与性能提升

#### 1. 模拟实验：分布内性能（表 I）
- **结果**：SADiff在五个技能上的**平均成功率高达92.8%**。
- **对比**：显著超越了所有基线方法，特别是比最接近的竞争者Im2Flow2Act（88.0%）**高出4.8个百分点**。
- **结论**：证明了显式建模技能级信息能有效提升在已知任务上的模仿性能。

#### 2. 模拟实验：鲁棒性与泛化性分析（图 6）
论文在四种泛化场景下测试：
    - **背景泛化**（改变光照和背景）：SADiff成功率 **89.6%**，领先基线。
    - **类内实例泛化**（同类未见物体）：SADiff成功率 **86.4%**，领先基线。
    - **跨类别泛化**（功能相似但类别不同的物体）：SADiff成功率 **82.4%**，而R3M和AVDC完全失败，其他基线下降超过20%。
    - **跨具身泛化**（更换机器人手臂）：SADiff仅下降5.6%，成功率保持高位，而其他方法（如R3M）因本体感觉不匹配而失败。
- **结论**：SADiff在所有泛化设置中均表现最佳，证明了其技能感知设计对克服视觉变化、物体变化和本体变化的强大能力。

#### 3. 模拟实验：指令引导的技能适应（表 II）
- **设置**：场景不变，仅改变语言指令，要求执行与场景视觉先验不符的新技能。
- **结果**：SADiff**平均成功率达85.0%**，远高于Track2Act（41.0%）和Im2Flow2Act（16.0%）。R3M和AVDC完全无法适应（0%）。
- **结论**：SADiff能够优先遵循语义指令而非视觉先验，展现了基于语言灵活调整行为的强大能力。

#### 4. 消融实验（表 III）
移除核心组件导致性能显著下降，验证了其必要性：
    - **移除可学习技能令牌**：平均成功率从86.1%降至**75.7%**。
    - **移除技能对比损失**：平均成功率降至**81.1%**。
    - **移除技能轨迹先验**：性能下降最严重，至**66.9%**。
- **结论**：技能感知编码、约束扩散和先验引导转换三者紧密耦合，是SADiff实现优异泛化的关键。

#### 5. 真实世界实验：零样本模拟到真实迁移（表 IV）
- **设置**：将在模拟中训练的SADiff模型**直接部署**到真实机器人（UR5）上，**无需任何微调**。
- **结果**：SADiff在五个技能上的**平均成功率为76.0%**。
- **对比**：显著优于Im2Flow2Act（54.4%）和Track2Act（50.4%），分别**高出21.6和25.6个百分点**。
- **时间效率**（表 V）：规划时间（约17-23秒）加执行时间（约11-28秒）在可接受范围内，证明了实时部署的可行性。
- **结论**：SADiff成功实现了零样本模拟到真实迁移，其技能级表示和先验有效桥接了模拟与真实之间的域差异（视觉、传感器噪声、本体差异）。

#### 6. 扩展实验：可扩展性与可组合性
- **可扩展性**：SADiff能够零样本泛化到训练中未见的技能（如“堆叠”、“擦拭”），通过组合已有基础技能的运动模式实现。
- **可组合性**：结合VLM规划器，SADiff能够将复杂的长时程任务（如“把苹果放进下层抽屉”）分解为子任务序列并成功执行。

### 四、 总结
论文通过系统性的实验证明，**SADiff框架在机器人操作任务上实现了卓越的泛化性能和零样本模拟到真实迁移能力**。其核心贡献——**技能感知编码、技能约束扩散模型和技能检索转换策略**——在各项对比实验和消融研究中被证实是有效的。与现有SOTA方法相比，SADiff不仅在标准任务上性能更优，更在**背景变化、物体变化、本体变化和指令变化**等极具挑战性的泛化场景中展现出显著优势。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.11266v1)
- [HTML 版本](https://arxiv.org/html/2601.11266v1)
