# Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces

**相关性评分**: 6.0/10

**排名**: #24


---


## 基本信息

- **arXiv ID**: [2601.08271v1](https://arxiv.org/abs/2601.08271v1)
- **发布时间**: 2026-01-13T06:56:53Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Angshul Majumdar

## 关键词

fine tune, offline Reinforcement Learning, constrained reinforcement learning, safe reinforcement learning, goal conditioned reinforcement learning, world model

## 一句话总结

该论文提出稀疏代理控制（SAC）框架，通过凸代理和正则化方法在大规模离散动作空间中实现多项式时间稳定性，与强化学习中的离线、约束、安全和目标条件学习相关，但未直接涉及视觉-语言-动作模型、流匹配或边缘部署。

## 摘要

Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.

## 详细分析

## 论文摘要

### 1. 研究背景和动机
大型语言模型（LLM）作为智能体（Agent）使用时，需要调用大量外部工具（如API、文档），这形成了一个巨大的离散动作空间（可达数万至数百万）。然而，现有的启发式控制方法（如提示工程、手工工作流）缺乏理论保证，无法解释性能如何随工具数量扩展。本文观察到，对于固定任务分布，实际被频繁调用的工具仅占极小的、未知的子集，即存在**潜在稀疏性**。本文旨在为这种具有大规模动作空间但存在潜在稀疏性的智能体控制问题，建立首个形式化的理论框架和可证明的样本高效学习算法。

### 2. 核心方法和技术创新
本文提出了**稀疏智能体控制（SAC）** 框架，将工具选择问题形式化为一个具有块稀疏表示的高维策略学习问题。核心技术创新是采用 **`ℓ₁,₂` 正则化的凸代理目标函数** 来学习策略参数。该方法将大规模动作空间中的控制问题，与高维统计中的压缩感知和稀疏恢复理论联系起来。

### 3. 主要理论结果
1.  **样本高效性**：在策略限制强凸性等假设下，证明了参数估计误差和价值次优性以 `O(k√(log M / T))` 的速率收敛，实现了对动作总数 `M` 的**对数依赖**。
2.  **精确支持恢复**：在满足不相干性和最小信号强度条件下，当样本量 `T ≳ k log M` 时，学习器能以高概率**精确识别**出相关工具集（支持恢复）。
3.  **下界与必要性**：证明了任何不使用显式稀疏性约束的“稠密”策略类，其样本复杂度必然**线性依赖于 `M`**。这从理论上解释了为何纯提示等启发式方法在大工具库中不稳定。
4.  **部分可观测性**：在POMDP设定下，将性能损失分解为稀疏学习误差和信念/表示压缩误差 `ε_b` 两项。这表明LLM等表示学习器的影响被清晰地隔离为一个**加性项** `O(ε_b)`，而不影响对 `M` 的对数依赖关系。

### 4. 研究意义和价值
本文为工具增强型LLM智能体提供了首个具有严格理论保证的稀疏控制框架。其价值在于：
*   **理论贡献**：揭示了大规模动作空间中实现多项式时间稳定控制的**根本条件是稀疏性**，并建立了清晰的相变边界（`T ≍ k log M`）。
*   **实践指导**：为设计可证明稳定的工具路由器和学习算法提供了原理性指导（如使用 `ℓ₁` 正则化），并解释了现有启发式方法不稳定的根源。
*   **分析框架**：清晰分离了表示学习（LLM质量）、统计工具选择和优化实现三个挑战，为后续研究和系统设计奠定了理论基础。文中的扩展（在线、抗噪、组稀疏等）进一步提升了框架的现实适用性。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文标题**：Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces
**作者**：Angshul Majumdar

---

### **一、 核心问题与动机**

论文旨在解决**增强型大语言模型（Agentic LLMs）在巨大离散动作空间中决策的稳定性与可学习性问题**。

*   **现实问题**：现代LLM智能体可以调用成千上万个工具、API或文档（动作空间巨大，M可达10^4-10^6）。然而，现有方法（如提示工程、手工工作流）缺乏理论保证，性能随工具数量增长不稳定。
*   **关键观察**：对于**固定的任务分布**，只有一小部分（k个）工具是真正相关的，绝大多数工具从不被调用。即，在巨大的动作空间中存在**潜在的稀疏性**。
*   **理论空白**：经典控制与学习理论通常假设中小型动作空间，无法处理这种“极端动作维度”的 regime。

因此，论文将问题重新定义为：**在具有潜在稀疏性的极端动作维度下的控制问题**。

---

### **二、 核心创新点**

论文的核心创新在于**提出并形式化了一个名为“稀疏智能体控制”的新理论框架**，将高维统计中的稀疏恢复理论系统性地应用于大规模动作空间的序列决策问题。

1.  **理论框架创新（Sparse Agentic Control, SAC）**：
    *   首次将工具增强型LLM的决策问题形式化为一个**动作空间巨大（M >> 1）但仅有稀疏子集相关（k << M）** 的序列决策问题。
    *   引入了**集合值动作**（选择工具子集）和**预算约束**的模型，更贴合智能体实际行为（如调用多个API、检索多篇文档）。

2.  **方法论创新（ℓ₁,₂正则化策略学习）**：
    *   提出使用**凸代理目标函数**和 **ℓ₁,₂ 块稀疏正则化**来学习策略。策略对每个工具的评分是上下文特征的线性函数，正则化项促使大部分工具的参数为零。
    *   这直接将大规模动作空间的控制问题，转化为一个**可高效求解（多项式时间）的高维M-估计问题**。

3.  **理论贡献创新（建立了尖锐的、压缩感知风格的理论保证）**：
    *   **上界（可行性）**：证明了在合理的假设下（Policy-RSC， 不相干性， beta-min条件）：
        *   **参数估计误差**：以速率 `O(√(k log M / T))` 收敛。
        *   **价值次优性**：以速率 `O(k √(log M / T))` 收敛。
        *   **精确支持恢复**：当样本量 `T ≳ k log M` 时，可以高概率**精确识别**出哪些工具是相关的（`supp(θ̂) = S*`）。
    *   **下界（必要性）**：证明了一个关键的下界：任何**不利用稀疏性结构**的“稠密”策略类，其样本复杂度**必然线性依赖于动作空间大小 M**（即需要 `Ω(M)` 个样本）。这从理论上解释了为何纯启发式的、无稀疏性控制的控制器（如仅靠提示）在工具库增长时会变得不稳定。
    *   **部分可观测性（POMDP）的清晰分解**：将LLM视为**信念/状态压缩器**。理论表明，最终性能损失可分解为：
        ```
        总损失 ≤ 稀疏学习误差 (O(√(k log M / T))) + 表示误差 (O(ε_b))
        ```
        其中 `ε_b` 是LLM压缩器产生的信念近似误差。这**清晰分离了表示能力和统计稀疏学习**的作用。

4.  **扩展性创新**：论文还将核心框架扩展到多种实用场景，并提供了相应的理论分析，包括：
    *   **免调参**（自标准化）
    *   **在线学习**（动态后悔界）
    *   **鲁棒学习**（抗污染，如Medians-of-Means）
    *   **组稀疏/层次稀疏**（对应API分组、参数层次）
    *   **交互感知**（学习工具间的协同效应）

---

### **三、 解决方案与关键技术**

1.  **问题形式化（SAC）**：
    *   **动作空间**：工具全集 𝒯 = {1, ..., M}， 智能体每一步选择一个子集 `a_t`（满足预算约束 `|a_t| ≤ B`）。
    *   **稀疏性假设**：存在一个未知的“活跃集” `S*`（大小为 k），奖励和转移主要依赖于所选工具与 `S*` 的交集。
    *   **策略参数化**：使用线性评分函数 `score_θ(x, a) = Σ_{j∈a} 〈θ_j, ψ(x)〉`， 策略 `π_θ` 由此评分的Gibbs分布或Top-B选择产生。稀疏性体现在参数 `θ` 的支撑集 `supp(θ)` 很小。

2.  **学习算法**：
    *   最小化**凸代理损失** + **ℓ₁,₂ 正则项**：
        ```math
        θ̂ ∈ argmin_θ ℒ̂_T(θ) + λ ‖θ‖_{1,2},  其中 ‖θ‖_{1,2} = Σ_{j=1}^M ‖θ_j‖_2
        ```
    *   这是一个凸优化问题，可用多项式时间算法（如近端梯度下降）求解。

3.  **理论分析工具**：
    *   **基本不等式与稀疏锥**：通过优化条件推导出估计误差 `Δ = θ̂ - θ*` 必然落入一个特殊的**稀疏锥** `C(S*)` 中。
    *   **策略限制强凸性**：假设损失函数在稀疏锥上有足够的曲率，这是参数可识别和快速收敛的关键。
    *   **不相干性条件**：确保不相关工具的特征不能“伪装”成相关工具，从而支持精确恢复。
    *   **原始-对偶见证法**：用于严格证明支持恢复（即正确识别出所有相关工具）。
    *   **价值敏感性**：将参数估计误差与最终策略价值损失联系起来。

---

### **四、 实际价值与意义**

1.  **为智能体系统设计提供理论指导**：论文指出，构建稳定、可扩展的工具增强型LLM系统的核心不是无限扩大模型容量，而是**显式地利用动作空间的稀疏性**，并采用相应的正则化学习算法。
2.  **解释工程现象**：从理论上解释了为什么基于提示的“稠密”控制器不稳定，以及为什么实践中有效的智能体往往只反复使用一小部分核心工具。
3.  **提供可实现的保证**：给出了达到近最优控制所需样本量的明确阈值（`T ≳ k log M`），这为数据收集、课程学习等提供了量化依据。
4.  **分离关注点**：清晰地将性能瓶颈分解为**表示学习质量**（LLM的压缩能力）和**稀疏决策学习**（路由器的统计效率），指导人们有针对性地改进系统。
5.  **启发性框架**：提出的SAC框架和ℓ₁,₂正则化学习器为开发新一代的、具有理论保证的**可学习工具路由器**提供了蓝图，可以替代手工设计的、脆弱的启发式路由逻辑。

**总结**：这篇论文的核心贡献是**首次为“超大动作空间、潜在稀疏性”这一在现实LLM智能体中普遍存在但被理论忽视的范式，建立了一套完整、严谨且具有深刻见解的学习理论**。它证明了稀疏性不仅是经验观察，更是实现多项式时间稳定学习的**必要条件**，并为构建下一代可靠、可扩展的AI智能体奠定了理论基础。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**工具增强型大语言模型（LLM）在巨大离散动作空间（如成千上万个工具、API）中决策时面临的组合爆炸和不稳定性问题**，提出了一个核心观点：**潜在稀疏性是实现多项式时间稳定控制的关键**。论文将这一问题形式化为**稀疏智能体控制（SAC）**框架，其核心方法是**采用凸代理目标函数和 ℓ₁,₂ 正则化来学习稀疏参数化的策略**。理论分析表明，该方法能够实现**压缩感知风格（compressed-sensing-style）的保障**：在仅有一小部分（k个）工具与任务相关的前提下，策略的估计误差、价值次优性和**精确的工具支持集恢复**所需的样本复杂度仅与 **k log M** 成比例，从而实现了对动作空间大小 M 的**对数依赖**。相反，论文证明了任何不利用稀疏性的“稠密”策略类在最坏情况下都需要 **Ω(M)** 的样本，这从理论上解释了为何纯启发式（如仅靠提示）的控制器在大动作空间中会不稳定。此外，论文还将框架扩展到部分可观测性（POMDP）场景，表明LLM等表示学习器的影响仅通过一个**信念/表示误差项 ε_b** 来体现，在性能保障中产生一个**可加的 O(ε_b) 退化项**，同时仍保持了对 M 的对数依赖。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文核心创新点分析

这篇论文《Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces》在**大规模离散动作空间下的智能体控制**领域提出了一个全新的理论框架。其创新点并非针对某个具体的LLM架构或提示工程技巧，而是从根本上**重新形式化了“工具增强型LLM智能体”所面临的核心控制问题**，并提供了具有严格理论保证的解决方案。

以下是其相对于已有工作的明确创新点：

---

### 1. **理论框架创新：提出“稀疏智能体控制”范式**
- **改进/不同之处**：
    - **以往方法**：将工具选择视为语言生成问题（通过提示工程）、启发式工作流设计或有限状态机路由。这些方法缺乏关于最优性、稳定性或样本效率的形式化保证。
    - **本文方法**：将问题形式化为**稀疏智能体控制**。核心假设是：对于固定的任务分布，只有一小部分（`k`个）工具是真正相关的，而动作空间的总大小`M`可以极大（`M ≫ k`）。这借鉴了高维统计和压缩感知中的**稀疏性**思想。
- **解决的问题/优势**：
    - **识别了根本障碍**：将挑战从“不确定性下的推理”重新定义为“具有潜在稀疏性的极端动作维度下的控制”。
    - **定义了新的渐近机制**：`M → ∞, k = O(1), T moderate`。在这个机制下，经典动态规划和朴素探索在信息和计算上都不可行，而本文的理论提供了可行的路径。
    - **为理论分析奠定了基础**：使得对样本复杂度、稳定性和可识别性进行严格分析成为可能。

### 2. **算法创新：基于凸代理和 `ℓ₁,₂` 正则化的策略学习**
- **改进/不同之处**：
    - **以往方法**：依赖启发式、无正则化的策略参数化（可视为“稠密”策略类）。学习过程不稳定，性能随工具数量`M`增长而难以预测。
    - **本文方法**：提出使用**凸代理目标函数**和**`ℓ₁,₂`（分组Lasso）正则化**来学习稀疏策略参数`θ`。策略对工具集的评分是每个工具评分的加和，正则化迫使大部分工具的参数`θⱼ`为零。
- **解决的问题/优势**：
    - **实现多项式时间学习**：优化问题是凸的，可高效求解。
    - **获得对数级样本复杂度**：理论证明，估计误差和价值次优性以 `O~(k√(log M / T))` 的速率缩放，**仅对数依赖于动作空间大小`M`**。这是突破“维度灾难”的关键。
    - **提供可解释的路由器**：学习到的稀疏支持集 `S*` 直接标识出与任务相关的核心工具集，可用于工具剪枝、加速推理和审计。

### 3. **理论贡献一：证明了稀疏策略的精确支持恢复**
- **改进/不同之处**：
    - **以往理论**：在强化学习或控制理论中，很少研究在大规模离散动作空间中**精确识别哪些动作（工具）是相关的**这一问题。
    - **本文方法**：在**策略限制强凸性**、**不相关性**和**最小信号**条件下，证明了`ℓ₁,₂`正则化学习器能够以高概率**精确恢复**最优稀疏策略的支持集（即相关工具集）。
- **解决的问题/优势**：
    - **形式化了“工具识别”**：为从数据中自动发现任务相关工具提供了理论保证。
    - **给出了样本阈值**：证明一旦样本量 `T ≳ k log M`，即可实现精确恢复。这为数据收集提供了量化的指导。
    - **连接了控制与高维统计**：将压缩感知中的支持恢复理论成功迁移到决策策略的识别上。

### 4. **理论贡献二：证明了稠密策略类的必要性下界**
- **改进/不同之处**：
    - **以往认知**：缺乏对非稀疏方法为何在大动作空间中失效的严格解释。
    - **本文方法**：证明了一个**极小极大下界**：任何不利用显式稀疏结构的策略类（稠密类），在最坏情况下都需要 `Ω(M)` 个样本来达到与稀疏方法相当的遗憾/识别性能。
- **解决的问题/优势**：
    - **解释了启发式控制器的不稳定性**：从理论上形式化了为什么纯提示、无结构的“稠密”控制器在`M`增长时会变得不稳定——它们无法避免对`M`的线性依赖。
    - **凸显了稀疏性的必要性**：标题“Sparsity Is Necessary”由此而来。这不仅是充分条件，在所述机制下也是避免指数级样本复杂度的**必要条件**。

### 5. **理论贡献三：将部分可观测性解耦为表示误差**
- **改进/不同之处**：
    - **以往方法**：LLM作为智能体的角色常常被模糊讨论，其语言能力与控制性能纠缠不清。
    - **本文方法**：将POMDP设置下的问题清晰分解。LLM（或任何编码器）被视为一个**信念/状态压缩器** `g`。其性能由一个单一的**表示误差** `ε_b`（近似信念与真实信念的总变差）来刻画。
- **解决的问题/优势**：
    - **清晰分离了责任**：理论表明，最终的价值次优性可分解为：`（稀疏学习误差）+ O(ε_b)`。
    - **保留了关键缩放律**：尽管存在表示误差，但对动作空间大小`M`的**对数依赖性依然保持**。LLM的质量只通过加性项`ε_b`影响最终性能。
    - **提供了原则性视角**：指明了提升智能体性能的两条独立路径：1) 改进稀疏策略学习（统计效率）；2) 改进上下文表示（降低`ε_b`）。

### 6. **系统性扩展：构建了统一的理论工具箱**
- **改进/不同之处**：
    - **以往工作**：针对大规模动作空间的控制理论结果零散，且未与LLM智能体的实际需求（如在线学习、抗噪、结构化工具集）紧密结合。
    - **本文方法**：在核心理论基础上，系统性地扩展了多个实用场景，并均给出了相应的理论保证：
        1.  **免调参SAC**：使用自标准化估计器，避免依赖未知噪声尺度。
        2.  **在线SAC**：处理工具相关性随时间漂移的情况，给出了动态遗憾界。
        3.  **鲁棒SAC**：使用中位数-均值聚合，抵御部分轨迹被污染（如工具故障、对抗提示）。
        4.  **组稀疏/层次稀疏SAC**：处理工具自然分组（如API、端点）和交互协同效应。
- **解决的问题/优势**：
    - **增强了实用性**：表明核心的稀疏学习框架灵活且可扩展，能适应真实部署中的多种挑战。
    - **保持了理论一致性**：所有扩展都在相同的“稀疏性+凸优化”范式下，继承了核心的对数依赖`M`的优点。

---

## 总结：核心价值与贡献

这篇论文的**根本性创新**在于它进行了一次成功的**范式转换**：
它将**工具增强型LLM智能体**的工程挑战，提炼并提升为一个具有清晰**信息-计算权衡**的**高维稀疏控制问题**。

**带来的核心优势**：
1.  **可证明的稳定性与效率**：为智能体在大规模工具库中的可靠运行提供了首个具有严格样本复杂度保证的理论框架。
2.  **原则性设计指南**：解释了为什么某些启发式方法会失败（线性下界），并指明了成功的关键（利用稀疏性+凸正则化）。
3.  **性能分解与归因**：清晰地将最终性能分解为“表示质量”和“稀疏学习”两部分，为系统优化指明了独立的方向。
4.  **实用的算法蓝图**：提出的`ℓ₁,₂`正则化学习器是一个具体的、可实现的算法原型，为构建下一代可验证的稳定智能体控制器奠定了基础。

这项工作超越了大多数专注于经验技巧或特定架构的“智能体”论文，为理解和管理大规模动作空间中的序列决策提供了全新的、坚实的理论基础。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估分析

根据提供的论文内容，**该论文是一篇纯理论分析文章，并未包含任何实验或实证评估部分**。因此，论文没有在具体数据集上实现算法，也没有与基线方法进行定量对比。

### 原因说明
论文的核心贡献在于**理论框架的构建和理论保证的证明**，而非实证验证。作者明确指出：

> **Why this is not an “agent” paper.** Unlike existing agentic LLM approaches, which treat tool selection primarily as a linguistic or architectural design problem, our analysis identifies tool use as a statistical and optimization problem with sharp phase transitions...

这表明论文的定位是**理论计算机科学/统计学习理论**研究，旨在为智能体LLM在大规模动作空间中的行为建立严格的理论基础，解释其内在的统计规律和复杂性边界。

### 理论“效果”与核心结论
尽管没有实验，但论文通过理论推导得出了几个具有深刻实践指导意义的“效果”或结论：

1.  **样本复杂度的相变**：论文证明了，在稀疏性假设下，使用 `ℓ₁,₂` 正则化的策略学习器可以达到 `O~(k√(log M / T))` 的价值次优性。**关键阈值是 `T ≳ k log M`**。一旦交互数据量超过这个阈值，智能体就能稳定地识别出相关工具并实现近优控制。

2.  **稀疏性的必要性（下界证明）**：论文证明了一个关键的下界：任何不使用显式稀疏性约束（即“稠密”策略类，如纯提示工程）的方法，在最坏情况下都需要 `Ω(M)` 个样本才能达到可比性能。这**从理论上解释了为什么纯粹启发式的、基于提示的控制器在工具库增长时会变得不稳定**。

3.  **部分可观测性的影响被隔离**：论文证明，在POMDP设置下，性能损失可以分解为 `（稀疏学习误差）+ O(ε_b)`，其中 `ε_b` 是信念/表示误差。这意味着**LLM等表示模型的质量仅通过一个加性项影响最终性能，而对动作空间大小 `M` 的依赖仍然保持对数级**。这为理解何时强大的状态压缩器（如LLM）有帮助提供了原则性视角。

4.  **支持恢复（工具识别）**：在满足不相干性和最小信号强度条件下，学习到的路由器能够以高概率**精确恢复出相关工具集 `S⋆`**。这为工具剪枝、加速推理和提供可解释的审计追踪提供了理论依据。

### 总结
| 方面 | 说明 |
| :--- | :--- |
| **实验数据集** | 未使用任何具体数据集。论文工作在抽象的理论模型上。 |
| **评价指标** | 理论指标：参数估计误差 (`‖θ̂ - θ⋆‖`)、价值次优性 (`V(θ⋆) - V(θ̂)`)、支持恢复概率 (`Pr(supp(θ̂) = S⋆)`)。 |
| **基线方法** | 理论对比对象是“稠密策略类”（即无显式稀疏性约束的方法，如纯提示控制器）。论文通过下界定理证明了稀疏方法的必要性。 |
| **主要性能结论** | **核心理论贡献**：揭示了在大规模动作空间中实现多项式时间稳定性的**充分必要条件是利用潜在稀疏性**，并提供了实现这一目标的凸优化框架 (`ℓ₁,₂`正则化) 及其严格的理论保证。 |

**实际价值**：该论文的价值不在于提供一个在某个基准测试上刷分的算法，而在于为**设计和分析工具增强型LLM系统提供了一个全新的、坚实的理论基础**。它解释了实践中观察到的现象（如只有少数工具被频繁使用），并指导系统设计者应该关注**稀疏策略学习**和**高质量的状态表示**，而不是仅仅依赖启发式提示或复杂的架构设计。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.08271v1)
- [HTML 版本](https://arxiv.org/html/2601.08271v1)
