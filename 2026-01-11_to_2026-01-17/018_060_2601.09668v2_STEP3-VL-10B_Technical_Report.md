# STEP3-VL-10B Technical Report

**相关性评分**: 6.0/10

**排名**: #18


---


## 基本信息

- **arXiv ID**: [2601.09668v2](https://arxiv.org/abs/2601.09668v2)
- **发布时间**: 2026-01-14T17:58:24Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Ailin Huang, Chengyuan Yao, Chunrui Han, Fanqi Wan, Hangyu Guo, Haoran Lv, Hongyu Zhou, Jia Wang, Jian Zhou, Jianjian Sun, Jingcheng Hu, Kangheng Lin, Liang Zhao, Mitt Huang, Song Yuan, Wenwen Qu, Xiangfeng Wang, Yanlin Lai, Yingxiu Zhao, Yinmin Zhang, Yukang Shi, Yuyang Chen, Zejia Weng, Ziyang Meng, Ang Li, Aobo Kong, Bo Dong, Changyi Wan, David Wang, Di Qi, Dingming Li, En Yu, Guopeng Li, Haiquan Yin, Han Zhou, Hanshan Zhang, Haolong Yan, Hebin Zhou, Hongbo Peng, Jiaran Zhang, Jiashu Lv, Jiayi Fu, Jie Cheng, Jie Zhou, Jisheng Yin, Jingjing Xie, Jingwei Wu, Jun Zhang, Junfeng Liu, Kaijun Tan, Kaiwen Yan, Liangyu Chen, Lina Chen, Mingliang Li, Qian Zhao, Quan Sun, Shaoliang Pang, Shengjie Fan, Shijie Shang, Siyuan Zhang, Tianhao You, Wei Ji, Wuxun Xie, Xiaobo Yang, Xiaojie Hou, Xiaoran Jiao, Xiaoxiao Ren, Xiangwen Kong, Xin Huang, Xin Wu, Xing Chen, Xinran Wang, Xuelin Zhang, Yana Wei, Yang Li, Yanming Xu, Yeqing Shen, Yuang Peng, Yue Peng, Yu Zhou, Yusheng Li, Yuxiang Yang, Yuyang Zhang, Zhe Xie, Zhewei Huang, Zhenyi Lu, Zhimin Fan, Zihui Cheng, Daxin Jiang, Qi Han, Xiangyu Zhang, Yibo Zhu, Zheng Ge

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, offline Reinforcement Learning

## 一句话总结

STEP3-VL-10B是一个轻量级开源多模态基础模型，通过统一预训练和强化学习后训练实现高效视觉语言智能，但未明确涉及动作控制或特定强化学习子领域。

## 摘要

We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\times$-20$\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.

## 详细分析

## Step3-VL-10B 技术报告详细摘要

### 1. 研究背景和动机
当前多模态大语言模型（MLLMs）的发展主要依赖于模型规模的无限扩展，导致前沿模型（如GPT-5.2、Gemini-3-Pro）计算成本高昂，难以实际部署。而轻量级模型（参数<100亿）则通常因能力有限，难以在复杂推理和感知任务上取得突破。本研究旨在**重新定义紧凑效率与前沿多模态智能之间的权衡**，提出一个仅100亿参数的轻量级开源基础模型Step3-VL-10B，旨在以极小的参数量匹敌甚至超越大10-20倍的模型。

### 2. 核心方法和技术创新
该模型的成功源于两大核心战略设计：
- **统一的、完全解冻的预训练策略**：在1.2万亿多模态token上，将**语言对齐的感知编码器（Perception Encoder）**与Qwen3-8B解码器集成，进行单阶段联合优化，建立了**内在的视觉-语言协同**。
- **规模化后训练与并行推理**：设计了包含超过**1000次迭代的强化学习（RL）**的后训练流程。关键创新在于引入了**并行协同推理（PaCoRe）**，在测试时分配计算资源，通过并行探索多样化的视觉假设并进行合成，从而将测试时计算转化为性能增益。

### 3. 主要实验结果
Step3-VL-10B在广泛的基准测试中展现了卓越性能：
- **多模态任务**：在MMBench上达到92.2%，在MMMU上达到80.11%。在复杂数学推理上表现突出，MathVision得分75.95%，AIME2025得分高达94.43%。
- **文本任务**：保持了强大的语言能力，在AIME2025数学竞赛和LiveCodeBench代码基准上分别达到87.66%和75.77%。
- **对比优势**：其性能不仅全面超越了同规模（7B-10B）的开源模型，更**匹敌或超越了参数量大10-20倍的模型**（如GLM-4.6V-106B, Qwen3-VL-235B）以及顶尖的闭源模型（如Gemini 2.5 Pro和Seed-1.5-VL）。

### 4. 研究意义和价值
Step3-VL-10B证明了通过**精心的架构设计、高质量数据构建和创新的训练范式（如PaCoRe）**，可以在不依赖巨大参数量的情况下实现前沿的多模态智能。它**为社区提供了一个强大且高效的基线模型**，其完整的模型权重和训练细节均已开源，有助于推动高效、可部署的多模态AI发展，并缩小开源生态与前沿技术之间的关键差距。这项工作指明了未来通过**强化学习缩放和物理世界 grounding** 来进一步提升模型智能密度和现实交互能力的方向。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## Step3-VL-10B 技术报告分析

### **核心问题与目标**
这篇论文旨在**重新定义“紧凑高效”与“前沿级多模态智能”之间的权衡**。传统上，轻量级模型（<10B参数）虽高效但能力有限，而前沿大模型（如百亿/千亿参数）虽强大但计算成本高昂，难以实际部署。Step3-VL-10B 试图证明，通过精心的架构设计、数据策略和训练方法，一个仅 **10B 参数**的模型可以达到甚至超越 **10-20倍于其规模**的大模型（如 GLM-4.6V-106B, Qwen3-VL-235B）以及顶级闭源模型（如 Gemini 2.5 Pro）的性能。

### **核心创新点**
论文的核心创新主要体现在**两个战略转变**上：

#### **1. 统一的、完全解冻的预训练策略**
- **架构融合**：采用 **1.8B 的语言对齐感知编码器 (Perception Encoder)** 与 **Qwen3-8B 解码器** 结合。感知编码器因其与语言的预对齐特性，能更快地建立视觉与语言的内在协同，克服了传统视觉编码器（如 DINOv3）在多模态场景下收敛慢的问题。
- **数据策略**：在 **1.2T 多模态令牌** 的语料上进行单阶段、完全解冻的训练。数据构建覆盖了**推理**（通用知识、教育）和**感知**（定位、计数、OCR、GUI交互）两大基础能力，确保了模型能力的全面性。
- **训练技巧**：采用多裁剪策略（全局视图+局部裁剪）高效捕获细节，并使用两阶段学习率调度，在后期专注于巩固细粒度感知和推理能力。

#### **2. 规模化后训练与并行协调推理**
这是论文最具突破性的部分，旨在通过**强化学习**和**测试时计算扩展**来解锁前沿能力。

- **超千次迭代的强化学习**：
    - **监督微调**：采用两阶段SFT，先建立文本推理基础，再平衡多模态能力。
    - **强化学习**：进行了超过 **1000次迭代** 的RL，结合了**可验证奖励**和**人类反馈**。
        - **RLVR**：在可验证任务（数学、科学、感知）上训练，使用GPT-OSS-120B作为答案验证器，提供对格式变化鲁棒、语义等价的奖励信号。
        - **RLHF**：在开放任务上对齐人类偏好。
    - **奖励系统设计**：分叉式框架，对可验证任务强调精确性，对非可验证任务使用生成式奖励模型和行为正则化（如防止幻觉、确保语言一致性）。

- **并行协调推理 (PaCoRe)**：
    - **核心思想**：将测试时的计算资源分配给**可扩展的感知推理**，通过并行探索多种视觉假设，然后进行综合。
    - **工作机制**：类似于计算机视觉中的“区域提议网络”，模型首先生成多个并行假设（提议），然后基于这些假设进行多方面的综合验证，得出最终答案。
    - **实际价值**：这使模型能够执行**类似人类的验证行为**（交叉检查、自我验证），显著提升了在需要高召回率的复杂感知和推理任务上的性能（例如，MathVision +5.14%，空间理解任务 +7.50%）。

### **解决方案总结**
论文通过一套**系统性的工程与算法组合拳**解决了“小而强”的挑战：

1.  **基础构建**：选择**语言对齐的视觉编码器**和强大的**语言解码器**，在高质量、多样化的 **1.2T 令牌多模态数据**上进行充分预训练，打下坚实的感知-推理协同基础。
2.  **能力解锁**：通过**大规模、多阶段的强化学习**，尤其是专注于可验证任务的RLVR，持续、无饱和地提升模型的推理精度和确定性。
3.  **性能突破**：引入 **PaCoRe** 这一**测试时计算扩展范式**，将模型从单纯的序列生成，升级为具备并行探索与综合能力的“多智能体”系统，从而攻克了传统轻量模型在复杂感知任务上表现不佳的瓶颈。

### **实际价值与影响**
- **性能标杆**：在仅10B参数下，取得了多项SOTA或接近SOTA的成绩（如 MMBench 92.2%， MMMU 80.11%， AIME2025 94.43%），证明了“紧凑”与“前沿”之间的鸿沟并非不可逾越。
- **开源贡献**：发布了完整的模型套件，为社区提供了一个强大的、高效的基线模型，推动了开源多模态AI的发展。
- **方法论启示**：
    - **强调了RL在后训练中的巨大潜力**，尤其是可验证奖励的规模化应用。
    - **提出了“测试时计算扩展”的新方向**（PaCoRe），为如何让模型更有效地利用计算资源提供了新思路。
    - **揭示了感知任务中的“长度缩减”现象**，并通过PaCoRe将其转化为一种可扩展的感知推理机制。

**结论**：Step3-VL-10B 的成功不在于单一的“银弹”，而在于一套连贯的、以**感知与推理为中心的设计哲学**，结合了**数据质量、架构协同、规模化RL和创新的推理范式**。它标志着多模态模型开发从一味追求参数规模，转向更注重**设计质量、训练效率和计算资源最优分配**的新阶段。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**轻量级多模态大模型在有限参数量下难以实现前沿级感知与推理能力**的核心问题。为此，它提出了一个名为 **Step3-VL-10B** 的10B参数模型，其核心方法基于两大战略转变：首先，采用**统一、全解冻的预训练策略**，在1.2T多模态token上联合优化语言对齐的感知编码器与Qwen3-8B解码器，以建立内在的视觉-语言协同；其次，设计了一个**包含超过1000次迭代强化学习的规模化后训练流程**，并引入了**并行协同推理（PaCoRe）** 来扩展推理时的计算资源，通过并行探索和合成多种视觉假设来提升性能。最终，该模型在多项多模态和文本基准测试中取得了与参数量大10-20倍的顶尖开源模型（如GLM-4.6V-106B, Qwen3-VL-235B）及闭源旗舰模型（如Gemini 2.5 Pro）相媲美甚至超越的性能，例如在MMBench上达到92.2%，在MMMU上达到80.11%，在AIME2025上达到94.43%，证明了通过精心的架构与训练设计，可以在紧凑的模型尺寸下实现前沿的多模态智能。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## Step3-VL-10B 论文的创新点分析

这篇论文提出了一种轻量级（10B参数）但性能强大的多模态基础模型，其核心创新点主要体现在**训练策略**、**推理架构**和**效率-性能权衡**三个方面。以下是对其明确创新点的逐条分析：

---

### 1. **统一的、完全解冻的预训练策略**
- **改进/不同之处**：
    - 传统多模态模型训练常采用“冻结视觉编码器 + 微调语言模型”或分阶段训练策略。本文采用**单阶段、完全解冻**的训练方式，在1.2T多模态token上**联合优化**语言对齐的感知编码器（Perception Encoder）与Qwen3-8B解码器。
    - 使用**多尺度裁剪策略**（全局视图 + 局部裁剪）替代复杂的变长序列打包，简化了训练流程。
- **解决的问题/优势**：
    - **建立内在的视觉-语言协同**：通过端到端联合训练，使视觉编码器与语言解码器在早期就深度融合，避免了模态间的“对齐鸿沟”。
    - **提升数据效率与收敛速度**：语言对齐的感知编码器（PE-lang）相比纯视觉编码器（如DINOv3）在多模态任务上收敛更快、性能更好（见表4）。
    - **保留细粒度视觉信息**：多裁剪策略在不增加序列长度复杂度的前提下，增强了模型对细节的感知能力。

### 2. **规模化后训练：超过1000轮强化学习**
- **改进/不同之处**：
    - 后训练流程包含**两阶段监督微调（SFT）** 和**大规模强化学习（RL）**，其中RL迭代超过**1000轮**，远超常规RLHF规模。
    - RL阶段细分为：**可验证奖励的RL（RLVR）** 和**人类反馈的RL（RLHF）**，并设计了**分叉奖励系统**（可验证任务 vs. 非可验证任务）。
- **解决的问题/优势**：
    - **持续性能提升**：RL训练展示了**无饱和的持续改进**（图2），说明通过规模化RL可以不断挖掘模型潜力，而非依赖单纯扩大预训练数据。
    - **精准奖励引导**：对于可验证任务（如数学、OCR），使用GPT-OSS-120B作为验证器，提供**解析不变、语义等价**的奖励，减少格式噪声；对于非可验证任务，使用生成式奖励模型（GenRM）和**行为正则化**（如引用验证、语言一致性），防止奖励黑客攻击。
    - **解决感知任务的长度缩减问题**：在确定性感知任务（如定位、OCR）中，RL通过**熵减**将概率质量集中到确定性答案上，实现了从“探索性”到“精确性”的转变，提升了Pass@1准确率。

### 3. **并行协同推理（PaCoRe）**
- **改进/不同之处**：
    - 在测试时引入**并行计算扩展**，通过同时生成多个视觉假设（并行提议），然后进行序列交叉验证与合成，模拟了“提议-精炼”的人类视觉推理过程。
    - 这是一种**显式的外部化视觉过程**，将隐式的视觉探索转化为可训练的并行推理轨迹。
- **解决的问题/优势**：
    - **弥补感知推理的“缺失痕迹”**：训练数据中缺乏人类“粗看-聚焦”的显式推理步骤，PaCoRe通过并行生成和合成，为模型提供了学习复杂视觉验证行为的机会。
    - **显著提升复杂任务性能**：在需要高召回率的感知任务（如计数、空间理解）和复杂推理任务（如数学）上，PaCoRe模式相比标准序列推理（SeRe）带来显著提升（例如MathVision +5.14%，All-Angles-Bench +7.50%）。
    - **实现测试时计算的可扩展性**：允许模型在推理时动态分配更多计算资源（通过并行探索），从而在不增加参数量的情况下逼近甚至超越更大模型的性能。

### 4. **高效架构与数据构造**
- **改进/不同之处**：
    - **架构选择**：采用**语言对齐的感知编码器（PE-lang）** 而非通用视觉编码器，与Qwen3-8B解码器通过简单投影器连接。
    - **数据构造**：构建了大规模、高质量、**领域聚焦**的预训练数据集，特别强调**教育、OCR、GUI、 grounding**等需要细粒度感知与推理的领域。例如，GUI数据包含轨迹建模，将视觉元素与可执行动作关联。
- **解决的问题/优势**：
    - **降低模态对齐成本**：PE-lang的预对齐特性大幅减少了视觉-语言融合的训练开销。
    - **针对性能力培养**：高质量的专业领域数据（如数学题、文档、界面）直接赋能模型在对应任务上的前沿性能（如OCRBench 86.75%，GUI任务领先）。
    - **实现紧凑模型下的全面能力**：10B模型在文本、数学、代码、指令遵循等纯文本任务上未出现性能下降（见表2），打破了“多模态训练损害语言能力”的固有印象。

### 5. **重新定义“紧凑”与“前沿”的权衡**
- **改进/不同之处**：
    - 论文核心主张是：通过上述**算法与工程创新**，一个10B参数模型可以达到或超越10-20倍参数量的大型开源模型（如GLM-4.6V-106B, Qwen3-VL-235B），并与顶级闭源模型（Gemini 2.5 Pro, Seed-1.5-VL）竞争。
- **解决的问题/优势**：
    - **提供高性能、可部署的基线**：Step3-VL-10B作为一个完全开源的模型，为社区提供了一个**计算效率高、性能强大**的实用选择，降低了前沿多模态AI的应用门槛。
    - **证明智能不完全依赖于参数规模**：通过精心设计的训练策略（尤其是规模化RL和PaCoRe），可以在小模型内激发出接近“系统2”的深思熟虑能力，为模型效率的研究指明了新方向。

---

## 总结
Step3-VL-10B的主要创新在于**系统性**地改进了多模态模型的训练与推理范式：
1.  **训练上**，通过**完全解冻的联合预训练**和**超大规模RL**，最大化从数据中提取能力。
2.  **推理上**，引入**PaCoRe**，将测试时计算转化为可扩展的并行感知探索与合成。
3.  **效率上**，通过**语言对齐的架构**和**高质量数据**，在10B的紧凑尺寸下实现了接近百亿级模型的综合性能。

这些创新共同解决了轻量级模型在**复杂推理**和**细粒度感知**上能力不足的核心问题，为构建高效、强大的开源多模态基础模型树立了新的标杆。其未来方向（如将PaCoRe内部化为“系统1”直觉、向物理世界模型演进）也颇具前瞻性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过**大规模、多维度**的基准测试，全面评估了 **Step3-VL-10B** 的性能，证明了其在**紧凑模型（10B参数）** 中达到了**前沿水平**，甚至能与**10-20倍参数规模**的更大模型以及顶级闭源模型相媲美。

### 一、 使用的数据集与评价指标

论文在超过 **60个** 公开基准上进行了评估，涵盖了**多模态**和**纯文本**两大领域。

#### 1. 多模态基准（9大能力域）
- **STEM/多模态推理**： MMMU, MMMU-Pro, MathVision, MathVista, LogicVista, DynaMath, ZeroBench, MathVerse, We-Math, VisuLogic, PhyX, HLE。
- **识别/通用VQA**： MMBench (EN/CN), SimpleVQA, MMStar, HallusionBench, MMVP, ReMI, M3GIA, DoYouSeeMe。
- **计数**： CountBench, CountQA, PixMo-Count。
- **指令遵循**： MM-MT-Bench, MIA-Bench, MM-IFEval。
- **多模态代码**： HumanEval-V, Design2Code (Hard)。
- **OCR**： OCRBench, OmniOCR, CC-OCR。
- **2D/3D空间理解**： BLINK, CVBench, MMSI-Bench, ERQA, OmniSpatial, All-Angles-Bench, MindCube-tiny, RealWorldQA, SpatialViz-Bench, STARE, CoreCognition, V*, ViewSpatial。
- **文档与图表理解**： CharXiv (RQ), AI2D, CSVQA, OmniDocBench。
- **GUI定位**： ScreenSpot-Pro, ScreenSpot-V2, OSWorld-G, MMBench-GUI (L2)。

#### 2. 纯文本基准（6大能力域）
- **考试**： MMLU-Pro, GPQA-Diamond, SuperGPQA, LiveBench。
- **数学**： AIME2024/2025, HMMT25, CNMO2024, BeyondAIME, IMO-AnswerBench。
- **代码**： LiveCodeBench。
- **指令遵循**： IFEval, IFBench, MultiChallenge。
- **主观评价**： Arena-Hard-V2, WildBench。
- **医学**： HealthBench。

**核心评价指标**： 绝大多数基准使用**准确率**。部分特定任务使用其官方指标，如OmniDocBench使用**归一化编辑距离（NED）**，GUI任务输出**坐标点**进行匹配。

### 二、 对比的基线方法

论文进行了两个层面的对比：

1.  **同规模开源模型（7B-10B）**：
    - **GLM-4.6V-Flash (9B)**
    - **Qwen3-VL-Thinking (8B)**
    - **InternVL-3.5 (8B)**
    - **MiMo-VL-RL-2508 (7B)**

2.  **更大规模模型与闭源旗舰模型**（用于展示其突破性）：
    - **GLM-4.6V (106B-A12B)** - 开源，约10倍参数
    - **Qwen3-VL-Thinking (235B-A22B)** - 开源，约20倍参数
    - **Gemini-2.5-Pro** - 闭源旗舰
    - **Seed-1.5-VL** - 闭源旗舰

### 三、 关键性能提升与结论

#### 1. 在7B-10B规模内确立新SOTA
**Step3-VL-10B** 在几乎所有多模态和纯文本能力域上均**领先于同规模竞争对手**。
- **多模态推理**： 在**MathVision**上达到70.81%，比第二名（MiMo-VL-RL-2508, 59.65%）高出超过10个百分点。
- **视觉识别**： 在**MMBench (EN/CN)** 上达到92.05%/91.55%，是该规模下的最佳成绩。
- **GUI交互**： 在**ScreenSpot-V2**上达到92.61%，验证了其轨迹建模和强化学习奖励系统的有效性。
- **文本数学**： 在**AIME2025**上达到87.66%，大幅领先同规模模型（第二名66.51%）。

#### 2. 媲美或超越10-20倍参数的大型模型
这是论文最核心的结论，证明了其**高效性**。
- **vs GLM-4.6V (106B)**： 在多数感知、识别和复杂推理任务上**超越**了这款106B参数模型。
- **vs Qwen3-VL (235B)**： 在多项关键指标上保持**竞争力**，例如MathVision (70.81% vs 72.10%)，AIME2025 (87.66% vs 83.96%)。

#### 3. 通过并行协调推理（PaCoRe）达到前沿性能
当启用**PaCoRe**（测试时计算扩展）模式时，模型性能进一步提升，甚至在某些任务上**超越顶级闭源模型**。
- **关键提升示例**：
    - **MMMU**: 从78.11% (**SeRe**) 提升至 **80.11%** (**PaCoRe**)。
    - **MathVision**: 从70.81% 提升至 **75.95%**。
    - **AIME2025**: 从87.66% 提升至惊人的 **94.43%**。
- **对比闭源模型**： 在**AIME2025** (94.43%) 和 **HMMT25** (92.14%) 上，**PaCoRe**模式下的成绩显著超过了**Gemini-2.5-Pro**和**Seed-1.5-VL**。这强有力地证明了**智能并非严格受模型规模限制**，通过创新的训练和推理策略，小模型也能达到顶尖水平。

#### 4. 保持文本能力，无模态权衡
与以往视觉语言模型常牺牲文本能力不同，**Step3-VL-10B**在**纯文本推理**（如数学、代码）上同样表现出色，避免了性能折衷。

### 结论
论文通过详尽的定量实验证明：
- **Step3-VL-10B** 重新定义了**紧凑效率**与**前沿智能**之间的权衡。
- 其成功源于**统一的预训练策略**和**大规模强化学习后训练**，特别是**PaCoRe**推理范式，有效将测试时计算转化为性能增益。
- 该模型不仅在同规模中领先，其性能上限（PaCoRe模式）已可挑战参数规模大一个数量级的开源模型和当前最强的闭源旗舰模型，为社区提供了一个**强大且高效的基线**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.09668v2)
- [HTML 版本](https://arxiv.org/html/2601.09668v2)
