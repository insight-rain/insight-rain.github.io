# $D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness

**相关性评分**: 6.0/10

**排名**: #33


---


## 基本信息

- **arXiv ID**: [2601.09176v1](https://arxiv.org/abs/2601.09176v1)
- **发布时间**: 2026-01-14T05:17:35Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Lang Xiong, Ning Liu, Ao Ren, Yuheng Bai, Haining Fang, BinYan Zhang, Zhe Jiang, Yujuan Tan, Duo Liu

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

这篇论文提出了一种名为D²Prune的剪枝方法，通过双泰勒展开和注意力分布感知来稀疏化大型语言模型，旨在提高推理效率并支持轻量级架构，从而促进边缘部署。

## 摘要

Large language models (LLMs) face significant deployment challenges due to their massive computational demands. % While pruning offers a promising compression solution, existing methods suffer from two critical limitations: (1) They neglect activation distribution shifts between calibration data and test data, resulting in inaccurate error estimations; (2) They overlook the long-tail distribution characteristics of activations in the attention module. To address these limitations, this paper proposes a novel pruning method, $D^2Prune$. First, we propose a dual Taylor expansion-based method that jointly models weight and activation perturbations for precise error estimation, leading to precise pruning mask selection and weight updating and facilitating error minimization during pruning. % Second, we propose an attention-aware dynamic update strategy that preserves the long-tail attention pattern by jointly minimizing the KL divergence of attention distributions and the reconstruction error. Extensive experiments show that $D^2Prune$ consistently outperforms SOTA methods across various LLMs (e.g., OPT-125M, LLaMA2/3, and Qwen3). Moreover, the dynamic attention update mechanism also generalizes well to ViT-based vision models like DeiT, achieving superior accuracy on ImageNet-1K.

## 详细分析

## 论文《D²Prune：基于双泰勒展开与注意力分布感知的大语言模型稀疏化》详细摘要

### 1. 研究背景和动机
大语言模型（LLMs）因其巨大的计算和内存需求，在资源受限设备上的部署面临严峻挑战。模型剪枝作为一种有效的压缩方案，现有方法存在两大关键局限：**（1）忽视了校准数据与测试数据之间的激活分布偏移**，导致误差估计不准确；**（2）忽略了注意力模块中激活值的长尾分布特性**，破坏了模型的关键推理模式。为解决这些问题，本文提出了 **D²Prune** 这一新颖的剪枝方法。

### 2. 核心方法和技术创新
D²Prune 包含两大核心技术：
- **基于双泰勒展开的激活-权重双重敏感剪枝机制**：通过构建关于**权重和激活值**的双变量泰勒展开，联合建模激活变化和权重扰动对输出误差的影响，从而实现了更精确的剪枝掩码选择和权重更新。
- **注意力分布感知的动态权重更新策略**：针对注意力模块中的 Q/K/V 权重，将其更新状态建模为一个组合优化问题。该方法通过困惑度引导的轻量级自适应搜索，**动态确定 Q/K/V 中哪些权重需要更新、哪些保持不动**，在最小化重构误差的同时，有效保留了注意力分数的原始长尾分布。

### 3. 主要实验结果
在 OPT、LLaMA2/3、Qwen3 等多种 LLMs 上的广泛实验表明：
- **性能领先**：在多种稀疏度（50%-80%）下，D²Prune 在语言建模困惑度（PPL）和零样本任务准确率上均**一致优于** SOTA 方法（如 SparseGPT、Wanda）。
- **高稀疏度优势**：在 70%-80% 的高稀疏度下，优势尤为显著，零样本准确率相比基线提升最高达 **40%**。
- **泛化性强**：该动态注意力更新机制能很好地泛化到基于 ViT 的视觉模型（如 DeiT），在 ImageNet-1K 上取得了优异的准确率。
- **有效性验证**：消融实验证实，双泰勒展开和动态更新策略均对性能提升有显著贡献。

### 4. 研究意义和价值
D²Prune 通过**突破“恒定激活”假设**和**首次系统性地在剪枝中保护注意力分布**，显著提升了后训练剪枝的精度与鲁棒性。其核心创新为 LLM 的高效压缩部署提供了更可靠的解决方案，在保持模型性能的同时大幅降低了计算开销，对推动大模型在边缘设备的实际应用具有重要的理论价值与实践意义。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：$D^2Prune$

### **一、 研究背景与核心问题**
这篇论文旨在解决**大语言模型（LLMs）部署时面临的巨大计算和内存开销问题**。模型剪枝是一种有效的压缩方法，但现有方法存在两个关键缺陷：

1.  **激活分布偏移问题**：现有剪枝方法（如SparseGPT、Wanda）通常假设校准数据与下游测试数据的**激活分布相同**（“恒定激活假设”）。然而，LLMs中不同数据集和层之间的激活存在显著分布偏移（如图1所示），导致基于校准数据估计的剪枝误差不准确，从而影响下游任务性能。
2.  **注意力模块的长尾分布被忽视**：Transformer的注意力机制具有**长尾分布特性**（少数关键token主导注意力分数），这对模型推理至关重要。现有方法平等地处理注意力模块中的Q、K、V权重与线性层权重：
    *   **非权重更新方法**（如Wanda）：不更新权重，导致剪枝误差在注意力层累积。
    *   **权重更新方法**（如SparseGPT）：全局更新权重，会**破坏原始的长尾注意力分布**，使注意力分数趋于均匀化（如图2所示），损害模型语义理解能力。

### **二、 核心创新点与技术方案**
论文提出了 **`$D^2Prune$`** 框架，通过两大核心技术解决上述问题。

#### **创新点1：基于双泰勒展开的激活-权重双重敏感剪枝机制**
*   **目标**：更精确地建模剪枝引起的误差，同时考虑**权重扰动**和**激活分布偏移**的影响。
*   **方法**：
    *   将层输出误差函数 `E` 在**权重 `w`** 和**激活 `x`** 两个变量上进行**双泰勒展开**（公式7-9），而传统方法（如OBS、SparseGPT）仅对权重进行单变量泰勒展开。
    *   通过理论推导和实验观察（附录C），假设激活偏移 `δx` 与校准激活 `x` 存在线性关系 (`δx = λx`)，从而将难以直接计算的 `δx` 纳入误差估计。
    *   推导出新的剪枝重要性度量 `S`（公式10），它不仅包含权重的二阶项（类似传统方法），还**显式引入了激活的一阶和二阶偏导项**，能够建模跨层依赖。
*   **效果**：实现了更精确的剪枝掩码选择和权重更新。实验表明，相比传统单变量方法，困惑度（PPL）平均降低约10%，在分布偏移显著的下游任务上，高稀疏度下的准确率提升高达40%。

#### **创新点2：注意力分布感知的动态权重更新策略**
*   **目标**：在剪枝注意力模块的Q、K、V权重时，**平衡误差补偿与长尾分布保持**。
*   **方法**：
    *   将Q/K/V权重的更新状态（更新/不更新）建模为一个**组合优化问题**（公式12）。优化目标是最小化**重构误差**和**注意力分布KL散度**的加权和。
    *   提出一种**基于困惑度（PPL）引导的轻量级自适应搜索方法**：将Q、K、V的更新配置（共3种有效组合：更新其中两个，固定一个）作为搜索空间，选择能使整体模型PPL最低的配置作为该层的最优策略。
    *   研究发现，对于特定模型，最优的Q/K/V更新配置在不同稀疏度下保持一致，且通常与权重中的**异常值（outlier）分布相关**（附录D）。例如，在LLaMA-2-13B中，大多数层的K权重异常值比例最高，因此最优策略往往是**固定K权重，只更新Q和V权重**。
*   **效果**：成功保留了注意力长尾分布。实验表明，相比Wanda（非更新）和SparseGPT（全更新），该方法将注意力分布的KL散度和注意力输出的RMSE平均降低了61%和43%。

### **三、 实际价值与实验效果**
1.  **卓越的压缩性能**：在OPT、LLaMA-2/3、Qwen3等多个LLM上，`$D^2Prune$` 在中等（50%-60%）和高稀疏度（70%-80%）下，在**语言建模困惑度（PPL）和零样本任务准确率**上均一致优于SOTA方法（SparseGPT, Wanda, Pruner-Zero）。例如，在LLaMA-2-7B模型80%稀疏度下，PPL远低于基线（92.68 vs. Wanda的5107.20）。
2.  **强大的泛化能力**：
    *   在LLaMA-2-70B模型50%稀疏度下，零样本准确率甚至**超过了原始稠密模型**。
    *   方法可泛化至**视觉Transformer模型（如DeiT）**，在ImageNet-1K上取得优异精度。
    *   经过**LoRA微调**后，`$D^2Prune$` 剪枝的模型恢复性能最佳，进一步证明了其生成高质量稀疏结构的能力。
3.  **实用性**：作为后训练剪枝方法，无需重训练，计算效率高。其动态注意力更新策略的搜索开销很小（仅需3次前向传播即可确定最优配置）。

### **四、 总结**
`$D^2Prune$` 的核心贡献在于**系统性地识别并解决了LLM剪枝中的两个根本性挑战**：
1.  通过**双泰勒展开**将激活分布偏移纳入误差估计，提升了剪枝决策的**鲁棒性和准确性**。
2.  通过**注意力感知的动态更新策略**，在剪枝过程中**有效保护了对于LLM推理至关重要的注意力长尾模式**。

这项工作不仅提出了一个高性能的剪枝工具，更深化了对Transformer注意力机制在压缩过程中行为特性的理解，为未来面向LLM的高效、保性能压缩技术提供了重要思路。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对大语言模型（LLM）后训练剪枝中存在的两个关键问题：1）传统方法基于“激活恒定”假设，忽略了校准数据与下游任务数据间的激活分布偏移，导致误差估计不准；2）忽视了注意力模块中激活的长尾分布特性，对Q/K/V权重的“全更新或不更新”策略会破坏关键注意力模式或累积误差。为此，论文提出了 **D²Prune** 框架，其核心创新包括：**基于对偶泰勒展开的激活-权重双敏感剪枝机制**，通过联合建模权重扰动和激活偏移来精确估计误差，从而优化掩码选择和权重更新；以及**注意力分布感知的动态权重更新策略**，将Q/K/V的更新状态建模为组合优化问题，通过最小化困惑度来动态选择配置，在补偿剪枝误差的同时有效保持注意力长尾分布。实验表明，该方法在OPT、LLaMA2/3、Qwen3等多种LLM上，尤其在较高稀疏度下，在困惑度和零样本任务准确率上均显著优于SparseGPT、Wanda等现有方法，并能很好地泛化到ViT等视觉模型。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《D²Prune》创新点分析

这篇论文针对大语言模型（LLM）后训练剪枝中的两个核心局限性，提出了两项关键技术创新，并整合为一个统一的剪枝框架。以下是其明确的创新点及其与已有工作的对比、解决的问题和带来的优势：

---

### 1. **基于双泰勒展开的激活-权重双重敏感剪枝机制**
   - **改进/不同之处**：
     - **以往方法**：主流剪枝方法（如SparseGPT、Wanda、OBS）通常基于“恒定激活假设”，即在剪枝过程中将输入激活视为固定不变。它们仅对**权重**进行单变量泰勒展开来近似剪枝误差，忽略了校准数据与下游测试数据之间**激活分布偏移**的影响。
     - **本文方法**：提出**双泰勒展开**，将误差函数同时关于**权重（w）和激活（x）** 进行展开，联合建模激活变化和权重扰动对输出误差的影响。
   - **解决的具体问题/优势**：
     - **问题**：LLM中，不同数据集和层之间的激活分布存在显著偏移（如图1所示），恒定激活假设会导致误差估计不准确，尤其是在高稀疏度下性能急剧下降。
     - **优势**：
       - **更精确的误差估计**：通过引入激活的一阶和二阶偏导项，显式地建模了激活偏移的影响，从而得到更准确的剪枝掩码选择准则和权重更新量。
       - **提升泛化能力**：实验表明，相比传统单变量方法，双泰勒展开在分布偏移显著的下游任务上，能将准确率提升高达40%，困惑度降低约10%。
       - **保持跨层依赖性**：新的重要性度量 $S$ 显式包含了下一层的输入激活 $y$，能够建模跨层依赖，这是Wanda、Pruner-Zero等方法所不具备的。

---

### 2. **注意力分布感知的动态权重更新策略**
   - **改进/不同之处**：
     - **以往方法**：现有方法将注意力模块中的查询（Q）、键（K）、值（V）权重与MLP中的线性层同等对待，采用“全更新”或“全不更新”的二元策略。
       - **全不更新**（如Wanda）：保留原始权重，能较好保持注意力长尾分布，但会导致剪枝误差在层间累积。
       - **全更新**（如SparseGPT）：全局调整权重以补偿误差，但会破坏注意力分数的原始长尾分布（如图2所示），使模型对关键令牌的关注失真。
     - **本文方法**：将Q/K/V权重的更新状态建模为一个**组合优化问题**，提出一种基于困惑度指导的轻量级自适应搜索方法，**动态地为每个注意力层确定最优的更新配置**（即选择Q、K、V中哪一个不更新，其余更新）。
   - **解决的具体问题/优势**：
     - **问题**：注意力机制的长尾分布（少数关键令牌主导注意力分数）对LLM的推理至关重要。传统的“全有或全无”更新策略无法在**误差补偿**和**分布保持**之间取得平衡。
     - **优势**：
       - **平衡误差与分布**：通过动态配置（例如，仅保持K权重不更新，更新Q和V），既能通过部分权重更新来补偿剪枝误差，又能有效保护关键注意力模式。
       - **显著降低分布失真**：实验表明，该策略相比Wanda和SparseGPT基线，平均将注意力分布的KL散度降低了61%，注意力输出的均方根误差降低了43%。
       - **高稀疏度下的鲁棒性**：该策略是模型在70%-80%高稀疏度下性能远超基线（尤其是非权重更新方法）的关键原因。

---

### 3. **统一框架与广泛适用性**
   - **改进/不同之处**：
     - **以往方法**：多数剪枝方法专为LLM设计，或在架构迁移上验证不足。
     - **本文方法**：将上述两个创新点整合为一个统一的剪枝框架 `D²Prune`，并证明了其在多种LLM（OPT, LLaMA2/3, Qwen3）以及**视觉Transformer模型（如DeiT）** 上的有效性。
   - **解决的具体问题/优势**：
     - **问题**：需要一种通用、高效的剪枝方法，能够适应不同架构和任务。
     - **优势**：
       - **架构通用性**：在DeiT模型上的ImageNet-1K分类任务中取得了优于基线的准确率，证明了其动态注意力更新机制对基于Transformer的视觉模型同样有效。
       - **任务通用性**：在语言建模、零样本任务、上下文学习（如GSM8K）等多种评估设置下均取得SOTA性能。
       - **实用高效**：通过理论简化（如忽略Hessian非对角项、引入缩放因子平衡量纲）和搜索空间缩减（基于异常值比例的先验），在保证性能的同时，保持了与SparseGPT相近的计算效率，且显著优于全局剪枝方法SparseLLM。

---

### 总结
`D²Prune` 的核心创新在于：
1.  **理论层面**：突破了“恒定激活假设”，通过双泰勒展开更真实地建模剪枝误差。
2.  **结构层面**：认识到注意力模块的特殊性，设计了动态更新策略来保护其关键语义模式。
3.  **实践层面**：提供了一个高效、通用、高性能的剪枝框架，在多种模型和任务上实现了当前最佳的精度-稀疏度权衡。

这些创新共同解决了后训练剪枝中**误差估计不准**和**注意力模式破坏**两大痛点，使得LLM在极高稀疏度（如80%）下仍能保持可用的性能，为在资源受限设备上部署大模型提供了更强大的压缩工具。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果总结

### 一、 核心评估指标与数据集
论文使用以下指标和数据集对提出的 **$D^2Prune$** 方法进行综合评估：

**1. 主要评估指标：**
- **语言建模能力**：在 **WikiText2** 测试集上计算 **困惑度**。**困惑度越低，模型性能越好**。
- **零样本推理能力**：在多个下游任务上计算 **平均准确率**。**准确率越高，模型泛化能力越强**。

**2. 主要数据集：**
- **校准数据**：使用 **C4** 数据集的训练集。
- **语言建模评估**：**WikiText2** 测试集。
- **零样本任务评估**：涵盖7个基准任务，包括：
    - **BoolQ** (自然语言是/否问答)
    - **HellaSwag** (常识推理)
    - **WinoGrande** (共指消解)
    - **RTE** (文本蕴含)
    - **ARC-c** 与 **ARC-e** (科学问答，挑战版与简易版)
    - **OBQA** (开放书籍问答)

### 二、 对比的基线方法
论文与当前主流的后训练剪枝方法进行了全面对比，分为两类：

1.  **权重更新类方法**：
    - **SparseGPT**：基于Hessian矩阵进行掩码选择和权重更新的一阶剪枝方法。

2.  **非权重更新类方法**：
    - **Wanda**：结合权重幅度和激活范数的重要性度量方法。
    - **Pruner-Zero**：使用符号回归自动发现剪枝评估函数的方法。

此外，论文还在附录中与更复杂的**全局剪枝方法 SparseLLM** 进行了对比，并验证了方法在**视觉Transformer模型（DeiT）**和**最新LLM（Qwen3）**上的泛化能力。

### 三、 关键性能提升与结论

**1. 综合性能全面领先：**
- **$D^2Prune$ 在从低到高（50%-80%）的各种稀疏度下，在困惑度和零样本准确率上均一致优于所有基线方法。**
- **具体提升幅度**：
    - **相比权重更新方法（如SparseGPT）**：平均获得 **3.1%** 的零样本准确率提升，困惑度降低约 **16%**。
    - **相比非权重更新方法（如Wanda, Pruner-Zero）**：在高稀疏度下，困惑度降低幅度**最高可达86%**。

**2. 高稀疏度下的显著优势：**
- 在70%-80%的极高稀疏度下，$D^2Prune$ 的优势尤为明显。例如，在LLaMA-2-7B模型80%稀疏度下：
    - $D^2Prune$ 困惑度为 **92.68**，而SparseGPT为102.43，Wanda高达5107.20。
    - 这证明了其**双泰勒展开误差估计**在高稀疏场景下的精确性。

**3. 超越稠密模型的泛化能力：**
- 在特定情况下，剪枝后的模型甚至**超越了原始稠密模型**。例如，LLaMA-2-70B在50%稀疏度下，$D^2Prune$ 的零样本准确率（71.60%）**高于稠密模型（71.50%）**。这表明剪枝可能起到了正则化作用，去除了冗余噪声。

**4. 注意力模块的有效性验证：**
- 论文提出的**注意力分布感知动态权重更新策略**被证明是有效的。
- 通过动态选择Q、K、V矩阵中哪一个不更新（以保持长尾分布），在几乎所有模型和稀疏度上都取得了最优性能（见表3、11-14）。
- 该方法将注意力分布的KL散度和注意力输出的RMSE平均降低了 **61%** 和 **43%**。

**5. 优秀的泛化性与可扩展性：**
- **跨架构有效**：方法成功应用于视觉Transformer模型 **DeiT**，在ImageNet-1K上取得优于基线的准确率。
- **适用于最新大模型**：在 **Qwen3-8B/14B** 上同样表现出色。
- **上下文学习能力**：在GSM8K数学推理任务的少样本评估中，$D^2Prune$ 也显著优于基线，证明了其保留模型复杂推理能力的效果。

**6. 效率与加速比：**
- **剪枝时间**：与SparseGPT相当，远低于其性能增益带来的价值。
- **推理加速**：在2:4半结构化稀疏模式下，LLaMA-2-7B的线性层矩阵乘法获得了约 **1.3倍** 的推理加速。

### 四、 核心结论
通过系统的实验评估，论文证明了 **$D^2Prune$** 通过**双泰勒展开联合建模权重与激活扰动**，以及**注意力分布感知的动态更新策略**，能够：
1.  **更精确地估计剪枝误差**，尤其是在校准数据与下游数据存在分布偏移时。
2.  **更好地保持注意力机制的关键长尾分布**，从而维持模型的核心推理能力。
3.  **在各种模型规模、稀疏度模式和不同架构上，实现当前最优的剪枝效果**，为大规模语言模型的高效部署提供了强有力的后训练压缩工具。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.09176v1)
- [HTML 版本](https://arxiv.org/html/2601.09176v1)
