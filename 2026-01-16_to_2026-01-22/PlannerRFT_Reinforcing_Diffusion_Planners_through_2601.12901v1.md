# PlannerRFT: Reinforcing Diffusion Planners through Closed-Loop and Sample-Efficient Fine-Tuning

**相关性评分**: 6.0/10

**排名**: #42


---


## 基本信息

- **arXiv ID**: [2601.12901v1](https://arxiv.org/abs/2601.12901v1)
- **发布时间**: 2026-01-19T09:56:05Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Hongchen Li, Tianyu Li, Jiazhi Yang, Haochen Tian, Caojun Wang, Lei Shi, Mingyang Shang, Zengrong Lin, Gaoqiang Wu, Zhihui Hao, Xianpeng Lang, Jia Hu, Hongyang Li

## 关键词

Diffusion, fine tune, offline Reinforcement Learning, world model

## 一句话总结

PlannerRFT 是一个用于自动驾驶中基于扩散的轨迹规划器的样本高效强化微调框架，通过双分支优化和快速模拟器提升性能。

## 摘要

Diffusion-based planners have emerged as a promising approach for human-like trajectory generation in autonomous driving. Recent works incorporate reinforcement fine-tuning to enhance the robustness of diffusion planners through reward-oriented optimization in a generation-evaluation loop. However, they struggle to generate multi-modal, scenario-adaptive trajectories, hindering the exploitation efficiency of informative rewards during fine-tuning. To resolve this, we propose PlannerRFT, a sample-efficient reinforcement fine-tuning framework for diffusion-based planners. PlannerRFT adopts a dual-branch optimization that simultaneously refines the trajectory distribution and adaptively guides the denoising process toward more promising exploration, without altering the original inference pipeline. To support parallel learning at scale, we develop nuMax, an optimized simulator that achieves 10 times faster rollout compared to native nuPlan. Extensive experiments shows that PlannerRFT yields state-of-the-art performance with distinct behaviors emerging during the learning process.

## 详细分析

## 论文摘要：PlannerRFT: 通过闭环与样本高效微调强化扩散规划器

**1. 研究背景和动机**
基于扩散模型的规划器在自动驾驶轨迹生成中展现出类人化潜力。然而，现有方法在强化学习微调（RFT）中面临**模态坍缩**（生成轨迹单一）和**场景适应性差**（探索方向与场景不匹配）的挑战，导致采样效率低下，限制了闭环性能的提升。本文旨在解决这些问题，提出一个样本高效的强化微调框架。

**2. 核心方法和技术创新**
本文提出 **PlannerRFT** 框架，其核心创新在于：
- **策略引导去噪**：引入一个**探索策略**，学习根据场景上下文自适应地调制横向和纵向的引导尺度，从而生成**多模态且场景自适应**的候选轨迹，极大提升了强化学习采样效率。
- **双分支优化**：采用**PPO**优化探索策略，采用**GRPO**优化扩散规划器（DiT）的轨迹分布，实现了探索与利用的协同。
- **生存奖励设计**：提出累积非终止段奖励的生存奖励公式，鼓励规划器推迟失败，改善了在困难场景中的优化稳定性。
- **高效仿真器nuMax**：开发了基于JAX的GPU并行仿真器，相比原生nuPlan实现了**10倍加速**，支撑了大规模闭环训练。

**3. 主要实验结果**
在nuPlan基准测试上的实验表明：
- **性能领先**：PlannerRFT在多个测试集（尤其是包含交互的**反应式交通**设置）上达到了最先进的性能，例如在Test14-hard反应式设置上比预训练基线提升了4.03分。
- **行为改善**：微调后的规划器展现出更安全、高效的类人驾驶行为，如成功处理碰撞、避障和复杂交互场景。
- **消融验证**：策略引导的探索相比均匀或固定探索，在保证训练稳定性的同时获得了更高的性能与样本效率；适中的困难场景比例和生存奖励对效果提升至关重要。

**4. 研究意义和价值**
PlannerRFT为扩散规划器的强化微调提供了一个**高效、稳定且可扩展**的解决方案。其**策略引导的探索机制**有效解决了扩散模型在强化学习中探索能力不足的根本问题。所开发的**nuMax仿真器**为学术界大规模闭环研究提供了有力工具。该工作推动了自动驾驶规划器从模仿学习向通过闭环交互持续优化与适应的发展，对提升自动驾驶系统在开放场景中的**鲁棒性和安全性**具有重要价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：PlannerRFT

### **一、 拟解决的核心问题**
论文旨在解决**基于扩散模型的自动驾驶轨迹规划器在强化学习微调（Reinforcement Fine-Tuning, RFT）中面临的探索效率低下问题**。具体表现为两个关键挑战：
1.  **模态坍缩**：传统扩散规划器在去噪过程中，不同噪声输入会收敛到几乎相同的轨迹，导致生成的候选轨迹**缺乏多样性（多模态性不足）**，无法为RFT提供丰富的优化信号。
2.  **场景失配**：现有的基于固定“锚点”的方法能生成多样轨迹，但这些锚点是**场景无关的**，会产生大量与当前驾驶情境冲突的无效或危险轨迹，引入噪声梯度，**阻碍稳定、高效的强化学习优化**。

### **二、 核心创新点**
论文提出了 **PlannerRFT** 框架，其创新性主要体现在以下三个层面：

#### **1. 方法创新：策略引导的去噪**
- **核心机制**：设计了一个**可学习的探索策略**，用于自适应地调制引导去噪过程的**横向和纵向引导尺度**。
- **实现方式**：
    - **引导去噪**：采用基于能量的分类器引导，向去噪过程注入残差偏移，使模型能在参考轨迹附近生成多样化的轨迹。
    - **探索策略**：该策略以驾驶场景和参考轨迹为条件，输出服从Beta分布的引导尺度参数，从而**实现场景自适应的轨迹采样**。
- **优势**：同时保证了**采样多样性（多模态）** 和**场景一致性（自适应性）**，为后续的强化学习优化提供了高质量、信息丰富的候选轨迹集。

#### **2. 优化框架创新：双分支协同优化**
- **探索策略优化**：使用**近端策略优化**在闭环模拟中在线优化探索策略，使其学会根据长期收益提供更安全、高效的探索方向。
- **轨迹生成器优化**：使用**组相对策略优化**对扩散模型（DiT）的去噪过程进行微调。关键引入了**生存奖励**，累积非终止段的奖励，鼓励规划器推迟失败，改善了在困难场景中的优化稳定性。
- **协同作用**：探索策略负责“怎么探索”（提供高质量的候选集），轨迹生成器负责“探索什么”（优化轨迹本身的质量），两者通过闭环数据协同进化。

#### **3. 工程创新：高性能模拟器 nuMax**
- **目的**：为了解决大规模闭环训练中模拟器吞吐量的瓶颈。
- **实现**：基于Waymax和JAX构建的**GPU并行模拟器**，对nuPlan基准测试进行了校准。
- **效果**：相比原生nuPlan模拟器，实现了**高达10倍的仿真加速**，使得基于大规模交互数据的强化学习微调变得可行。

### **三、 解决方案总结**
PlannerRFT通过一个**集成化的框架**系统性地解决了问题：
1.  **输入**：一个经过模仿学习预训练的扩散规划器。
2.  **增强探索**：在推理架构上“插入”一个**探索策略模块**，实现策略引导的、场景自适应的多模态轨迹采样。
3.  **高效训练**：利用**nuMax模拟器**进行高速闭环仿真，收集交互数据。
4.  **协同优化**：采用**双分支优化**分别更新探索策略和轨迹生成器，并利用**生存奖励**稳定困难场景的训练。
5.  **部署**：训练完成后，移除探索策略和参考模型，**规划器保持原有的扩散结构**，但性能得到提升。这是一种“即插即用”的微调范式。

### **四、 实际价值与意义**
- **性能提升**：在nuPlan基准测试上达到了SOTA性能，特别是在包含动态交互的**反应式交通场景**和**困难测试集**上提升显著，证明了其在提升规划安全性、鲁棒性方面的有效性。
- **行为涌现**：经过微调后，规划器展现出不同于单纯模仿学习的、更**拟人化且合理的驾驶行为**（如更果断的变道、更早的制动避让）。
- **推动领域发展**：为解决扩散模型在强化学习中探索效率低的共性问题提供了新思路。同时，开源的**nuMax模拟器**为学术界在nuPlan上进行大规模闭环RL研究提供了关键工具。
- **实用路径**：框架允许在保持原有规划器部署结构不变的情况下进行性能增强，降低了落地应用的复杂性。

**简而言之，PlannerRFT的核心贡献是提出了一种让扩散规划器“更聪明地探索”的方法，并通过高效的工程实现和优化的训练框架，将这种探索能力转化为更安全、更鲁棒的闭环规划性能。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决扩散规划器在强化学习微调中因轨迹多样性不足（模态坍缩）和探索效率低下而导致优化信号弱、样本效率低的问题。为此，作者提出了 **PlannerRFT** 框架，其核心是通过一个**可学习的探索策略**，在去噪过程中自适应地注入横向和纵向的引导偏移，从而生成既多样又符合场景的候选轨迹，以支持高效的组式强化优化（GRPO）。同时，为了支持大规模闭环训练，论文还开发了加速模拟器 **nuMax**。实验结果表明，该方法在 nuPlan 基准测试中取得了最先进的性能，特别是在交互性强的反应式交通场景下，显著提升了规划的安全性和鲁棒性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

本文《PlannerRFT: Reinforcing Diffusion Planners through Closed-Loop and Sample-Efficient Fine-Tuning》针对自动驾驶中基于扩散模型的轨迹规划器，提出了一套完整的强化学习微调框架。其核心创新点可归纳为以下四个方面：

### 1. **提出“策略引导去噪”机制，实现多模态与场景自适应的探索**
- **改进/不同之处**： 以往扩散规划器存在**模态坍塌**问题（不同噪声输入生成几乎相同的轨迹），导致强化学习采样时探索能力不足。而现有的**锚点方法**（Anchor-based）虽然能生成多样轨迹，但其锚点是固定、场景无关的，会产生大量与环境冲突的噪声轨迹，干扰优化。PlannerRFT创新性地引入了一个**可学习的探索策略**，该策略根据当前驾驶场景和参考轨迹，动态预测并调制**横向和纵向的引导尺度**，从而控制去噪过程。
- **解决的具体问题/带来的优势**：
    - **解决了探索效率问题**： 实现了**多模态**（能生成多种驾驶假设）和**场景自适应**（生成的轨迹与当前场景兼容）的平衡，为后续的组式强化学习优化提供了更高质量、更稳定的候选轨迹集。
    - **提升了采样效率**： 相比均匀随机探索，策略引导的探索能更有效地将搜索方向集中在有潜力的行为空间，减少了无效探索，从而加速了策略优化过程。

### 2. **设计双分支优化框架，协同优化轨迹生成与探索策略**
- **改进/不同之处**： 传统的强化微调通常只优化轨迹生成器本身。PlannerRFT将优化过程解耦为两个并行的分支：
    1.  **轨迹优化**： 使用**组相对策略优化**对扩散去噪过程进行微调，以提升生成轨迹的长期奖励。
    2.  **探索策略优化**： 使用**近端策略优化**在线优化探索策略，使其学会为不同场景分配合适的探索方向。
- **解决的具体问题/带来的优势**：
    - **解决了训练稳定性问题**： 将长时程的闭环行为优化（探索策略）与单步的多轨迹生成优化（轨迹生成器）分离，避免了目标冲突，使训练更加稳定。
    - **实现了协同增效**： 探索策略为轨迹优化提供高质量的探索样本；轨迹优化的进步又为探索策略提供了更准确的长期价值估计，两者形成正向循环。

### 3. **引入“生存奖励”公式，改善困难场景下的优化梯度**
- **改进/不同之处**： 在具有挑战性的场景（如即将发生碰撞）中，如果直接使用终端奖励（碰撞/驶出道路则奖励为0），所有候选轨迹可能都很快失败，导致奖励稀疏、梯度为零，优化停滞。PlannerRFT提出了**生存奖励**，其核心思想是：**只累计轨迹在失败发生前的非终端段奖励**。
- **解决的具体问题/带来的优势**：
    - **解决了稀疏奖励问题**： 即使最终失败，鼓励规划器**推迟失败事件**的行为也能获得正向奖励。这为优化提供了连续的梯度信号。
    - **提升了在困难场景下的性能**： 使规划器在接近失败的边缘场景中，仍能学习到如何做出更优决策以延长安全行驶时间，从而显著提升了在`Test14-hard`等挑战性基准上的鲁棒性。

### 4. **开发高性能仿真器nuMax，实现大规模并行闭环训练**
- **改进/不同之处**： 基于Waymax和V-Max，为nuPlan基准定制开发了**GPU并行仿真器**。通过**场景预缓存**、使用**JAX重写控制器和评分器**、以及设计**PyTorch-JAX混合分布式训练流水线**，大幅提升了仿真吞吐量。
- **解决的具体问题/带来的优势**：
    - **解决了训练效率瓶颈**： 仿真速度达到原生nuPlan仿真器的**10倍**，使得在有限计算资源下进行大规模（4000万步）的闭环强化学习训练成为可能。
    - **为学术研究提供基础设施**： nuMax作为一个开源的高性能仿真平台，降低了自动驾驶强化学习研究的门槛，促进了该领域的发展。

### **总结与核心价值**
PlannerRFT的核心创新在于**系统性**地解决了将扩散模型用于自动驾驶强化学习微调时的关键挑战：**探索效率低**、**训练不稳定**、**奖励稀疏**和**仿真速度慢**。它通过“策略引导去噪”和“双分支优化”实现了**样本高效**的强化学习；通过“生存奖励”增强了在**边缘场景**下的学习能力；并通过nuMax解决了**工程可扩展性**问题。最终，在nuPlan基准测试中取得了领先性能，特别是在交互复杂的**反应式交通**设置下提升显著，证明了其框架在提升自动驾驶规划器安全性、鲁棒性和拟人化驾驶行为方面的有效性与实用价值。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

### 数据集与评价指标
- **数据集**：使用大规模自动驾驶规划基准 **nuPlan**。
  - **训练/微调数据**：从nuPlan中抽取144,494个非重叠场景（10Hz采样率），并根据预训练模型性能构建了三个子集：`Fail`（碰撞/脱轨案例）、`Lt90`（低分案例）、`All`（全部场景）。
  - **测试基准**：
    - **Val14**：常规驾驶场景，评估通用性能。
    - **Test14-hard**：复杂挑战性场景，评估鲁棒性。
    - **Test14-random**：随机场景，用于补充验证。
  - **交通设置**：
    - **非反应式（NR）**：周围车辆按日志轨迹行驶。
    - **反应式（R）**：周围车辆使用IDM模型根据自车行为动态调整，更贴近现实交互。

- **评价指标**：
  - **核心指标**：nuPlan综合评分（0-100分），聚合多个子指标：
    - **安全性**：碰撞（Collisions）、脱轨（Drivable Area Compliance）、时间碰撞（TTC）。
    - **舒适性**：加速度/加加速度限制（Comfort）。
    - **效率**：进度（Progress）、速度（Speed）。
  - **辅助指标**：
    - **轨迹多样性（𝒟）**：基于轨迹组间平均交并比（mIoU）衡量探索能力。
    - **奖励统计**：平均奖励（𝑟¯）和奖励标准差（𝑠𝑟），反映探索稳定性。

### 对比基线方法
论文与多类先进规划器进行了全面对比：
1. **专家/规则基**：
   - **Log-replay**：日志回放（性能上限）。
   - **IDM**：智能驾驶员模型。
   - **PDM-Closed**：经典闭环规划器。
2. **学习基**：
   - **PDM-Open**、**GameFormer**、**PlanTF**、**PLUTO**。
3. **生成式规划器**：
   - **Diffusion Planner**（扩散规划器，作为预训练基线）。
   - **Flow Planner**（流匹配规划器）。

### 关键性能提升与结论
#### 1. **整体性能达到SOTA**
- 在**Test14-hard**（最具挑战性）和**Val14**基准上，PlannerRFT在**反应式（R）设置**中均取得最佳或次佳成绩。
- **显著提升**：相比预训练的Diffusion Planner，在Test14-hard-R上提升**+4.03分**（从68.18→72.21），在Val14-R上提升**+1.66分**（从82.80→84.46）。
- **结论**：闭环强化微调显著提升了在动态交互场景中的规划鲁棒性和安全性。

#### 2. **安全性大幅改善**
- **碰撞减少**：在Test14-hard-R上，碰撞分数从79.05提升至84.93（+5.88）。
- **脱轨合规性**：从94.48提升至95.59。
- **定性验证**：如图4所示，在OOD场景中，PlannerRFT从“碰撞” → “保守绕行” → “安全高效换道”逐步进化，体现了安全与效率的平衡。

#### 3. **探索策略的有效性**
- **策略引导降噪** vs. 其他探索方式：
  - **均匀探索**：多样性高（𝒟=39.78%），但奖励方差大（𝑠𝑟=0.12），训练不稳定，性能最差（65.82分）。
  - **固定Beta分布**：稳定性好，但探索受限，性能中等（70.65分）。
  - **PlannerRFT（自适应）**：在多样性（𝒟=25.34%）、平均奖励（𝑟¯=73.88）和稳定性（𝑠𝑟=0.06）间取得最优权衡，性能最高（72.21分）。
- **结论**：**场景自适应的探索策略**是提升采样效率和稳定性的关键。

#### 4. **微调数据与奖励设计的影响**
- **数据分布**：使用`Lt90`（平衡难易样本）效果最佳，仅用困难样本（`Fail`）会导致性能崩溃，仅用简单样本（`All`）则提升有限。
- **奖励设计**：
  - **生存奖励（Survival Reward）** 比终端奖励更有效，鼓励延迟失败，在硬场景中提供持续梯度。
  - **奖励视野**：4秒视野取得最佳平衡（过短视野信息不足，过长视野收益递减）。
- **引导偏移量λ**：适中值（λ_lat=2.5m, λ_lon=25%）最佳，过小限制探索，过大偏离专家分布。

#### 5. **效率与部署优势**
- **推理速度**：使用5步DDIM，**无需引导模块**时，延迟仅34.27ms，比原Diffusion Planner（86.43ms）快2.5倍，且性能更高。
- **即插即用**：训练时添加探索策略和参考轨迹，部署时移除，保持原始扩散结构，易于集成。

### 未明确给出的定量结果
- **计算成本对比**：未详细列出与基线方法的训练时间/GPU小时对比，但强调了nuMax模拟器**10倍加速**对大规模训练的关键性。
- **真实道路测试**：所有实验均在仿真中进行，未提供实车测试数据。

### 核心结论
**PlannerRFT通过策略引导的自适应探索和双分支优化，在nuPlan基准上实现了SOTA的闭环规划性能，特别是在动态交互场景中显著提升了安全性和鲁棒性，且保持了高效的推理速度。** 该方法为扩散规划器的强化微调提供了样本高效、稳定可扩展的框架。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.12901v1)
- [HTML 版本](https://arxiv.org/html/2601.12901v1)
