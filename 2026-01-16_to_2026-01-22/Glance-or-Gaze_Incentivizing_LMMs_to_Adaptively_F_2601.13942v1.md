# Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning

**相关性评分**: 6.0/10

**排名**: #30


---


## 基本信息

- **arXiv ID**: [2601.13942v1](https://arxiv.org/abs/2601.13942v1)
- **发布时间**: 2026-01-20T13:18:18Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, Yike Guo

## 关键词

fine tune, Reinforcement Learning

## 一句话总结

该论文提出Glance-or-Gaze框架，通过强化学习激励大型多模态模型自适应聚焦视觉搜索，以解决静态知识限制下的复杂查询问题。

## 摘要

Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.

## 详细分析

## 论文《Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning》详细摘要

### 1. 研究背景和动机
大型多模态模型（LMMs）在视觉理解方面取得了显著成功，但其静态参数化知识难以应对涉及长尾实体或动态更新信息的复杂视觉查询。现有的检索增强方法通常采用被动、无差别的全图检索，引入了大量视觉冗余和噪声，且缺乏深度迭代反思，限制了其在复杂视觉问答任务上的有效性。为了克服这些挑战，本文提出了 **Glance-or-Gaze (GoG)** 框架，旨在将LMMs从被动感知转变为主动的视觉规划者。

### 2. 核心方法和技术创新
GoG框架的核心创新在于其**选择性凝视机制**和**双阶段训练策略**：
- **选择性凝视机制**：模型能够动态决策是“瞥视”全局上下文，还是“凝视”高价值区域进行精细搜索，从而在检索前主动过滤无关视觉信息，减少噪声。
- **双阶段训练策略**：
    1. **反思性GoG行为对齐**：通过监督微调，利用精心构建的GoG-Instruct数据集，让模型学会主动选择与跨模态反思的基本范式。
    2. **复杂度自适应强化学习**：基于查询难度分层数据，采用分组相对策略优化（GRPO），进一步优化模型的规划策略，使其能够根据查询复杂度进行智能、迭代的反思和自适应聚焦搜索。

### 3. 主要实验结果
在六个基准测试（包括FVQA、InfoSeek、SimpleVQA、MMSearch、LiveVQA、DynVQA）上的实验表明：
- **性能领先**：GoG-3-8B-Think-RL模型取得了最先进的性能，平均表现显著优于全搜索工作流、提示式GoG代理以及之前的检索增强基线（如MMSearch-R1），提升幅度高达+19.97分。
- **有效性验证**：消融研究证实，选择性凝视机制和复杂度自适应RL训练都是提升性能的关键。RL训练不仅提高了整体准确率，还显著增强了模型选择相关图像区域的能力（裁剪选择准确率提升4.7%-6.8%）以及在错误时触发自我纠正的反思率（从30%提升至70%）。
- **行为演化**：SFT阶段教会模型**何时**以及**如何**使用搜索工具，而RL阶段则鼓励模型进行更复杂的多步骤、混合类型搜索，展现出更强的迭代推理能力。

### 4. 研究意义和价值
本研究提出了一种全新的、完全自主的视觉搜索增强范式，其价值体现在：
- **技术突破**：首次将主动视觉规划与强化学习相结合，使LMMs能够像人类一样“先观察，再聚焦”，实现了从“看到”到“观察”的质变。
- **实际应用**：显著提升了LMMs在知识密集型视觉任务（如识别新产品、理解近期事件）上的准确性和可靠性，减少了幻觉，为构建更强大、更自主的多模态搜索智能体指明了方向。
- **开源贡献**：作者承诺将公开数据和模型，有助于推动该领域的进一步探索和发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Glance-or-Gaze (GoG)

### **一、 论文想解决的核心问题**
当前大型多模态模型（LMMs）在**知识密集型视觉问答**任务中存在两大根本性缺陷：
1.  **静态知识局限**：模型参数化知识是固定的，无法回答涉及**长尾实体**或**实时更新信息**的查询，导致“知识截止”或“幻觉”。
2.  **现有搜索增强方法的低效与噪声**：
    *   **视觉策略低效**：现有方法通常对整张图像进行“无差别检索”，引入大量无关视觉噪声。
    *   **缺乏深度迭代反思**：工具调用多为单次、单向，缺乏基于视觉反馈的迭代验证和自我修正能力，难以处理复杂查询。

### **二、 核心创新点**
论文提出了 **Glance-or-Gaze (GoG)** 框架，其核心创新在于**将LMMs从“被动感知者”转变为“主动视觉规划者”**。

#### **1. 机制创新：选择性凝视**
*   **核心思想**：在检索前，动态决定是“瞥一眼”（**Glance**，分析全局上下文）还是“凝视”（**Gaze**，聚焦于高价值区域进行精细搜索）。
*   **技术实现**：引入 **Selective Gaze** 机制。模型首先通过视觉定位（如Grounding DINO）提出候选区域，然后评估这些区域与问题的相关性，**主动过滤掉无关信息**，只对选定的、最相关的区域进行精确检索。
*   **价值**：从根本上减少了检索引入的视觉冗余和噪声，实现了从“粗看”到“细察”的智能过渡。

#### **2. 训练策略创新：双阶段学习架构**
GoG采用了一个渐进式的训练范式，而非单一的监督微调：

*   **第一阶段：反射式GoG行为对齐**
    *   **目标**：通过监督微调，让模型学会“何时”以及“如何”使用GoG范式（Glance/Gaze决策、工具调用、跨模态反思）。
    *   **关键步骤**：构建 **GoG-Instruct** 数据集。通过**不确定性感知过滤**（只保留模型回答不一致、需要外部验证的难题）和**人工验证的轨迹合成**，确保数据质量。

*   **第二阶段：复杂度自适应的强化学习**
    *   **目标**：超越模仿学习，优化模型的**规划与推理策略**，使其能根据查询的复杂程度自适应地调整搜索行为。
    *   **关键设计**：
        *   **复杂度分层数据**：根据SFT模型的失败率，将训练数据分为不同难度等级（如Level 1：决策边界样本；Level 2：持续失败的困难样本）。
        *   **在困难样本上训练**：论文发现，在更具挑战性的样本上进行强化学习，能最大程度地激发模型发展出复杂、多步的迭代推理策略。
    *   **算法**：采用**组相对策略优化**，奖励结合了答案准确性和格式合规性。

### **三、 如何解决问题（解决方案总结）**
1.  **针对视觉噪声问题**：通过 **Selective Gaze** 机制，在检索链路前端增加了一个“智能过滤器”，实现了**先评估、后检索**的主动视觉规划，取代了被动的整图检索。
2.  **针对静态知识局限**：无缝集成外部搜索工具（图像搜索、文本搜索），使模型能够获取训练数据之外的、最新的或细粒度的知识。
3.  **针对缺乏深度推理问题**：通过**双阶段训练**，特别是**复杂度自适应RL**，激励模型进行**多轮、迭代的“搜索-反思-修正”循环**。RL训练后，模型更倾向于组合使用多种搜索类型，并在初始“凝视”错误时，显著提高了自我纠正（反思）的概率。

### **四、 实际价值与效果**
*   **性能卓越**：在六个视觉问答基准测试（包括in-domain和out-of-domain）上取得了**最先进的性能**，平均显著超越强基线模型（如MMSearch-R1）近20个百分点。
*   **行为可解释**：实验表明，SFT教会了模型基本的工具使用，而RL则显著提升了模型处理复杂查询时的**迭代推理能力和区域选择精度**。
*   **泛化性强**：在分布外数据集上的优异表现证明了GoG范式具有良好的泛化能力。
*   **方向引领**：为构建更自主、更智能的多模态搜索智能体提供了一个可行的框架，强调了**主动规划**和**基于复杂度的自适应学习**的重要性。

**结论**：GoG的创新不在于提出新的网络模块，而在于设计了一套完整的**“机制+训练”范式**，使LMMs能够像人类一样，在面对视觉问题时，动态地分配注意力，进行有计划的、反思性的信息搜集，从而更可靠地解决知识密集型任务。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

**核心问题**：现有的大型多模态模型（LMMs）在处理涉及长尾实体或动态更新信息的知识密集型视觉查询时，受限于其静态的、参数化的知识，表现不佳。现有的搜索增强方法存在两大缺陷：1) 检索策略低效且噪声大（如不加区分地处理整张图像）；2) 缺乏深度、迭代的跨模态反思能力，无法适应复杂的视觉问题。

**主要方法**：论文提出了 **Glance-or-Gaze (GoG)** 框架，旨在将LMMs从“被动感知”转变为“主动视觉规划”。其核心创新包括：
1.  **选择性凝视机制**：模型动态决定是“瞥一眼”全局上下文，还是“凝视”高价值区域进行精细搜索，从而在检索前主动过滤无关的视觉噪声。
2.  **双阶段训练策略**：
    *   **第一阶段（反思性GoG行为对齐）**：通过监督微调，利用精心构建的指令数据，让模型学会基本的主动选择和跨模态反思范式。
    *   **第二阶段（复杂度自适应强化学习）**：使用基于查询难度的分层数据，通过强化学习（GRPO算法）进一步优化模型的规划策略，使其能根据问题复杂度进行迭代推理和自适应搜索。

**主要效果**：在六个视觉问答基准测试（包括域内和域外）上的实验表明，GoG框架取得了最先进的性能。相比强基线（如强制全搜索流程、提示工程代理、MMSearch-R1等），GoG平均有显著提升（最高达+19.97分）。消融研究证实，选择性凝视机制和复杂度自适应强化学习都是提升模型有效性的关键。最终，GoG成功地将LMMs转变为能够自主规划、迭代反思的主动视觉搜索智能体。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning》的创新点分析

这篇论文针对大型多模态模型（LMMs）在知识密集型视觉问答任务中的局限性，提出了一个名为 **Glance-or-Gaze (GoG)** 的创新框架。其核心创新点在于将LMM从“被动感知者”转变为“主动视觉规划者”，通过强化学习激励模型自适应地聚焦搜索。以下是其相对于已有工作的明确创新点：

### 1. **提出了“选择性凝视”机制，实现了从粗粒度到细粒度的自适应视觉规划**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的搜索增强方法（如MMSearch-R1）通常采用“无差别全图检索”策略，即对整张图像或所有候选区域进行检索，这引入了大量视觉冗余和噪声。它们严重依赖将视觉细节转换为文本描述，造成了信息瓶颈。
     - **GoG的创新**：引入了 **Selective Gaze** 机制。该机制让模型在检索前，动态决策是“瞥一眼”全局上下文，还是“凝视”高价值区域。具体来说，模型会先通过视觉定位提出候选区域，然后评估并筛选出最相关的图像块，只对这些选定的区域进行精确搜索。
   - **解决的具体问题/带来的优势**：
     - **解决了视觉噪声和效率低下的问题**：通过主动过滤无关信息，大幅减少了检索引入的噪声，提升了搜索效率和精度。
     - **保留了原始视觉数据**：避免了将视觉信息完全转化为文本的信息损失，让模型能直接访问原始视觉证据进行推理。
     - **实现了物理锚定的推理**：迫使模型从“被动看”转变为“主动观察”特定细节，为推理提供了更细粒度的证据支撑。

### 2. **设计了一个双阶段训练策略，结合了模仿学习与复杂度自适应的强化学习**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的工具集成LMMs要么以单次传递的方式执行工具调用，要么将反思严格限制在文本模态，缺乏迭代的视觉验证和自我修正能力。它们的训练策略相对单一，无法根据查询复杂度自适应调整。
     - **GoG的创新**：
       1. **第一阶段：反射式GoG行为对齐**：通过监督微调，使用精心构建的 **GoG-Instruct** 数据集（经过不确定性感知过滤和人工验证）来灌输主动选择和跨模态反思的基本范式。
       2. **第二阶段：复杂度自适应强化学习**：采用 **Group Relative Policy Optimization** 算法，在一个按复杂度分层的数据集上进行训练。该数据集根据SFT模型的失败率构建，专注于那些对模型具有挑战性的查询。
   - **解决的具体问题/带来的优势**：
     - **解决了策略僵化和缺乏迭代反思的问题**：SFT阶段教会模型“何时”以及“如何”使用搜索工具，而RL阶段则显著增强了模型处理复杂查询时的**迭代推理和规划能力**。实验表明，RL训练后，模型进行多步骤混合搜索的比例大幅上升（从~30%增至~75%），展现了更复杂的、深思熟虑的信息收集过程。
     - **实现了对查询复杂度的自适应**：通过使用困难样本（Level 2数据）进行RL训练，模型学会了在面对真正具有决策挑战性的查询时，优化其搜索和反思策略，从而在复杂基准上取得了更大提升。

### 3. **构建了一个支持迭代、跨模态反思的完全自主视觉搜索框架**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：大多数搜索增强LMM的工作流程是线性的（检索->生成）或反思仅限于文本链。它们不支持在视觉模态上进行迭代验证。
     - **GoG的创新**：GoG框架将视觉定位、区域筛选、精确搜索和跨模态反思整合在一个**多步骤、迭代的循环**中。模型可以提出区域假设，执行搜索，根据返回的信息进行反思，然后决定是否需要修正其关注点并再次搜索。
   - **解决的具体问题/带来的优势**：
     - **解决了单次搜索可能导致的错误固化问题**：迭代反思机制允许模型进行自我纠正。手动分析表明，经过RL训练后，当初始“凝视”选择错误时，模型触发反思的比例从30%大幅提升至70%。
     - **提升了决策的校准性和鲁棒性**：模型不仅更频繁地使用工具，而且能更精准地**选择**搜索目标（裁剪选择准确率提升4.7%-6.8%），并能在发现证据不足或矛盾时调整策略，从而产生更可靠、更准确的答案。

### 总结：实际价值与优势
- **性能领先**：在六个不同的知识密集型VQA基准测试（包括in-domain和out-of-domain）上取得了最先进的性能，平均显著超越之前的搜索增强基线（如MMSearch-R1）近20个百分点。
- **通用性强**：框架不依赖于特定模型架构，在Qwen2.5-VL和Qwen3-VL两个不同骨干模型上都取得了成功验证，展示了良好的通用性。
- **指向新的研究方向**：该工作为构建**自主、能进行主动视觉规划和迭代推理的多模态智能体**开辟了道路，超越了传统的“检索-生成”范式，更贴近人类“观察-假设-验证”的认知过程。

**局限性**：论文也指出了当前框架的局限，如依赖的外部搜索基础设施可能存在网络不稳定导致检索失败（1-5%），以及实验主要集中于英文任务，多语言泛化能力尚未验证。这些为未来工作提供了明确的方向。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果分析

### 1. 评估数据集与指标
**数据集**：
- **In-Domain (IID)**：
  - **FVQA-test**：知识密集型视觉问答数据集，测试模型对事实性知识的检索与理解能力。
  - **InfoSeek**：信息寻求型视觉问答数据集，要求模型回答超出常识范围的问题。
- **Out-of-Domain (OOD)**：
  - **SimpleVQA**：评估多模态大模型事实知识边界的基准。
  - **MMSearch**：评估多模态搜索性能的综合性基准。
  - **DynVQA**：包含动态变化答案、多模态知识需求和多跳推理的挑战性数据集。
  - **LiveVQA-New**：测试模型对训练数据截止日期后新出现的视觉知识的处理能力。

**评价指标**：
- **主要指标**：**准确率（Accuracy）**，通过 **LLM-as-a-Judge**（使用 `gpt-oss-120b`）评估生成答案与标准答案的语义一致性。
- **辅助分析指标**：
  - **搜索行为分布**：分析模型在“不搜索”、“单次搜索”、“混合搜索”上的比例变化。
  - **Crop Selection Accuracy**：衡量模型选择与答案相关图像区域的能力。
  - **Gaze Correctness** 与 **Reflection Rate**：手动分析模型“凝视”操作的准确性和错误后自我纠正的触发率。

### 2. 对比的基线方法
论文将GoG框架与四类基线方法进行了全面对比：
1.  **直接回答（Direct Answer）**：模型仅基于图像和问题直接生成答案，不进行外部检索。
    - 包括开源模型（Qwen2.5-VL系列、Qwen3-VL系列）和闭源模型（GPT-4o）。
2.  **全搜索工作流（Full-Search Workflow）**：对每个查询都强制进行检索（图像搜索+文本搜索）。
    - 同样基于Qwen2.5-VL和Qwen3-VL模型。
3.  **基于提示的GoG智能体（Prompt-based GoG Agents）**：通过提示工程（如图思考Graph-of-Thought）引导模型进行搜索规划，不进行模型微调。
4.  **具备搜索能力的LMMs（Search-Equipped LMMs）**：
    - **MMSearch-R1**：当前最先进的、通过强化学习将搜索行为集成到训练循环中的多模态搜索模型。

### 3. 主要性能结果与结论
**总体性能**：
- **GoG-3-8B-Think-RL** 在六个基准测试的平均准确率达到 **56.88%**，实现了 **State-of-the-Art (SOTA)** 性能。
- 相比最强的 **全搜索工作流** 基线，平均提升 **+9.89** 个百分点。
- 相比 **基于提示的GoG智能体**，平均提升 **+15.18** 个百分点。
- 相比之前的SOTA搜索增强模型 **MMSearch-R1**，实现了显著的 **+19.97** 个百分点的巨大提升。

**关键观察与结论**：
1.  **双阶段训练的有效性**：
    - **SFT阶段**：成功教会模型 **何时** 以及 **如何** 使用搜索工具处理简单查询，建立了基础的“扫视-凝视”范式。
    - **RL阶段**：显著增强了模型的**迭代推理和复杂规划能力**。RL训练后，模型进行“混合搜索”（结合文本、图像、裁剪区域搜索）的比例从~30%激增至~75%，表明模型学会了进行多步骤、交叉验证的信息收集。

2.  **选择性凝视机制的优势**：
    - 消融实验表明，移除选择性凝视机制（SFT w/o SG）会导致平均性能下降（Qwen3-VL下降1.83分）。
    - 该机制通过动态裁剪和聚焦高价值区域，为推理提供了**细粒度视觉证据**，有效减少了噪声，在复杂场景（如DynVQA, MMSearch）中提升尤为明显（增益达4.25-7.60分）。

3.  **复杂度自适应RL训练的价值**：
    - 使用难度更高的 **Level 2数据**（SFT模型通过率<50%）进行RL训练，比使用较简单的Level 1数据效果更好。
    - 在Qwen2.5-VL-7B上，使用Level 2数据带来了 **+5.94** 的平均增益，证明了在具有挑战性的决策边界上优化策略能最大化学习收益。

4.  **RL提升了视觉聚焦精度**：
    - RL训练后，模型的 **Crop Selection Accuracy** 得到提升（Qwen3-VL-Think从42.1%提升至48.9%）。
    - 手动分析显示，RL模型不仅“凝视”操作的正确率更高（从59%提升至75%），而且在初始聚焦错误时，触发自我纠正（Reflection）的比例也大幅提高（从30%提升至70%），表明RL赋予了模型更好的**不确定性校准和主动纠错能力**。

**总结**：GoG框架通过**选择性凝视机制**和**双阶段（SFT+复杂度自适应RL）训练策略**，成功地将LMM从被动感知转变为主动视觉规划器。实验证明，该框架在多种知识密集型视觉问答任务上均取得了显著优于现有方法的性能，其核心创新在于实现了**自适应的、迭代的、噪声鲁棒的视觉搜索**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.13942v1)
- [HTML 版本](https://arxiv.org/html/2601.13942v1)
