# RoboBrain 2.5: Depth in Sight, Time in Mind

**相关性评分**: 6.0/10

**排名**: #41


---


## 基本信息

- **arXiv ID**: [2601.14352v1](https://arxiv.org/abs/2601.14352v1)
- **发布时间**: 2026-01-20T17:21:54Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Huajie Tan, Enshen Zhou, Zhiyu Li, Yijie Xu, Yuheng Ji, Xiansheng Chen, Cheng Chi, Pengwei Wang, Huizhu Jia, Yulong Ao, Mingyu Cao, Sixiang Chen, Zhe Li, Mengzhen Liu, Zixiao Wang, Shanyu Rong, Yaoxu Lyu, Zhongxia Zhao, Peterson Co, Yibo Li, Yi Han, Shaoxuan Xie, Guocai Yao, Songjing Wang, Leiduo Zhang, Xi Yang, Yance Jiao, Donghai Shi, Kunchang Xie, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

RoboBrain 2.5是一个下一代具身AI基础模型，通过精确的3D空间推理和密集时间价值估计，提升复杂精细操作的物理基础和执行感知能力。

## 摘要

We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io

## 详细分析

## 论文《RoboBrain 2.5: Depth in Sight, Time in Mind》详细摘要

### 1. 研究背景和动机
当前具身人工智能基础模型在将高层语义推理转化为物理世界精确操作方面存在显著差距。现有模型普遍面临两大核心局限：**空间维度上的“度量盲区”**（缺乏绝对深度和尺度信息，无法保证毫米级精度）和**时间维度上的“开环预测”**（缺乏对执行进度的密集监控，无法自适应恢复）。这导致模型在演示中表现良好，但在严苛的真实世界部署中可靠性不足。为弥合这一“可靠性鸿沟”，研究团队提出了下一代模型 **RoboBrain 2.5**，旨在推动具身智能从语义推理器向物理接地代理的范式转变。

### 2. 核心方法和技术创新
RoboBrain 2.5 在 RoboBrain 2.0 的基础上，引入了两大核心能力升级，构成了其技术创新主体：

- **精确三维空间推理**：将空间接口从二维像素相对定位，升级为**深度感知坐标预测和绝对度量约束理解**。模型采用解耦的 `(u, v, d)` 坐标表示，仅通过单目RGB输入，即可完成**三维空间指代、三维空间测量和三维空间轨迹生成**，输出满足物理约束的、有序的关键点序列作为完整操作轨迹。
- **密集时序价值估计**：建立了**密集、步骤感知的进度预测和执行状态理解**机制。通过创新的**基于跳数的相对进度归一化**标签策略，模型能够从多视角视觉观察中，稳健地估计任务进度、停滞、倒退或错误状态，为下游强化学习提供高保真、视角鲁棒的奖励信号。此外，通过**多视角进度融合**和**双向一致性检查**策略，进一步提升了估计的准确性和对分布外状态的鲁棒性。

### 3. 主要实验结果
研究在广泛的基准测试上进行了全面评估，结果证明了 RoboBrain 2.5 的卓越性能：
- **二维空间推理**：在 CV-Bench、CrossPoint 等五个基准上平均得分 **75.82**，显著超越通用及具身基线模型。
- **三维空间推理**：在 MSMU、TraceSpatial 等强调度量基础和轨迹感知的基准上取得最佳或接近最佳性能。例如，在 TraceSpatial 的轨迹生成成功率上达到 **44%**，远超通用大模型。
- **时序价值估计**：在涵盖真实机器人、仿真和人类视频的六个测试集上，其**前向/反向价值排序相关性**均表现优异且平衡，显著优于 GPT-5.2、Gemini 等通用大模型，证明了其提供双向一致、步骤感知反馈的能力。
- **基础设施**：模型成功在 **NVIDIA 和 摩尔线程** 两种异构硬件平台上完成训练，最终性能差距控制在 **0.62%** 以内，验证了其训练框架的跨平台鲁棒性和实用性。

### 4. 研究意义和价值
RoboBrain 2.5 通过赋予模型“**眼中见深度**”的空间精确性和“**心中有时间**”的执行稳健性，成功地将高层语义推理与低层物理交互桥接起来。其核心价值在于：
- **技术突破**：为解决具身AI的“度量盲区”和“开环预测”两大根本挑战提供了系统性的解决方案。
- **实用价值**：生成的精确三维轨迹和密集进度信号，可直接用于指导机器人执行和驱动强化学习，显著提升了零样本泛化能力和在接触丰富任务中的部署可靠性。
- **生态贡献**：开源代码与模型检查点，并展示了成熟的跨加速器训练能力，为社区提供了强大的基础模型和可复现的工程实践，有力推动了具身AI向物理接地、闭环可靠的方向发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：RoboBrain 2.5

### **一、 核心问题**
当前具身AI基础模型在从“语义推理器”向“物理实体智能体”演进时，存在两个关键缺陷：
1.  **空间维度上的“度量盲”**：现有模型通常基于2D像素坐标或弱拓扑关系进行物体定位，缺乏**绝对深度和尺度信息**。这导致模型无法保证毫米级的操作精度，也无法生成满足物理约束（如避障）的3D轨迹。
2.  **时间维度上的“开环预测”**：模型将动作生成视为静态序列预测任务，**缺乏对执行进度的内在监控机制**。它们依赖稀疏的外部成功标签，无法感知执行过程中的中间失败（如打滑、倒退），从而难以实现长时程任务中的自适应恢复。

### **二、 核心创新点**
RoboBrain 2.5 通过两大核心能力升级，旨在弥合上述“可靠性鸿沟”：

#### **1. 精确3D空间推理**
- **目标**：将空间接口从2D像素定位升级为**深度感知的坐标预测和完整操作轨迹生成**。
- **关键方法**：
    - **任务重构**：将3D空间追踪定义为预测一个有序的3D关键点序列 `τ = {p_t}`，其中每个点 `p_t = (u_t, v_t, d_t)` 包含图像平面坐标和绝对深度。
    - **解耦表示**：采用 `(u, v, d)` 表示法，而非直接预测相机/世界坐标系下的 `(x, y, z)`。这简化了训练，并可通过已知相机内参轻松转换为3D坐标。
    - **三项核心能力**：
        1.  **3D空间指代**：在复杂指令中精确定位物体。
        2.  **3D空间测量**：估计绝对度量值（如距离、间隙）。
        3.  **3D空间轨迹生成**：在物理约束下生成无碰撞的关键点轨迹。

#### **2. 密集时序价值估计**
- **目标**：建立一个**密集、步骤感知的进度预测**机制，为下游学习提供稳定的反馈信号。
- **关键方法**：
    - **基于“跳数”的相对进度归一化**：提出“跳数”标签 `ℋ(s_p, s_q)`，用于衡量从状态 `s_p` 到 `s_q` 的进度变化，并相对于到目标的剩余距离（或已走过的距离）进行归一化，确保监督信号稳定在 `[-1, 1]` 区间。
        ```python
        # 公式示意
        if q >= p: # 前进
            ℋ(s_p, s_q) = (Φ(s_q) - Φ(s_p)) / (Φ(s_M) - Φ(s_p))
        else: # 倒退
            ℋ(s_p, s_q) = (Φ(s_q) - Φ(s_p)) / (Φ(s_p) - Φ(s_0))
        ```
    - **多视角进度融合**：融合三种互补的进度估计视角，以抵抗误差累积：
        1.  **增量预测**：基于前一状态递归计算，捕捉局部动态。
        2.  **前向锚定预测**：相对于初始状态估计进度，提供全局稳定性。
        3.  **后向锚定预测**：相对于目标状态估计进度，在任务末期更敏感。
    - **双向一致性检查**：引入一致性权重 `w_t`，当来自不同视角的预测不一致时（可能处于分布外状态），降低更新置信度，防止策略利用错误的奖励信号。

### **三、 解决方案体系**
1.  **数据策略**：构建了约1240万高质量样本的统一数据集，涵盖**通用多模态理解**、**空间推理**（2D到3D）和**时序预测**（规划与密集价值估计）三大领域。
2.  **训练策略**：采用**两阶段渐进式训练**：
    - **阶段一（基础时空学习）**：在830万样本上训练，建立通用视觉感知、2D定位和开环规划能力。
    - **阶段二（特定时空增强）**：在410万样本上微调，专注于**度量感知的3D追踪**和**密集价值估计**，同时通过数据回放防止灾难性遗忘。
3.  **基础设施**：基于 **FlagScale** 框架，采用**混合并行策略**和**动态预分配内存**优化，成功在**NVIDIA和国产摩尔线程GPU**上千卡集群上完成训练，实现了**跨硬件平台的可靠训练与推理**。

### **四、 实际价值与评估结果**
- **性能提升**：在广泛的2D/3D空间推理和时序价值估计基准测试中，RoboBrain 2.5 均取得了**最先进（SOTA）或极具竞争力的性能**，显著超越了通用基线模型（如GPT-5.2, Gemini-3-Pro）和之前的具身模型（如RoboBrain 2.0）。
- **关键优势**：
    - **物理可执行性**：生成的3D轨迹可直接用于机器人运动规划，满足真实世界的度量约束。
    - **闭环鲁棒性**：密集的进度估计可作为**高保真的内在奖励信号**，驱动强化学习策略进行在线调整和错误恢复。
    - **强泛化能力**：模型在未见过的任务、视角变化和干扰下表现出色，验证了其从演示级成功向部署级可靠性的转化潜力。

**总结**：RoboBrain 2.5 通过赋予模型 **“眼中见深度”** 的空间精确性和 **“心中有时间”** 的执行状态感知，系统地解决了当前具身AI模型在物理落地中的核心瓶颈，为实现复杂、细粒度、鲁棒的机器人操作迈出了关键一步。其开源的代码和模型 checkpoint 将进一步推动该领域的研究与应用。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决当前具身人工智能基础模型在真实物理世界部署中存在的两大核心缺陷：**空间维度上的“度量盲区”**（模型缺乏对绝对深度和尺度的感知，无法生成满足物理约束的精确3D操作轨迹）和**时间维度上的“开环预测”**（模型缺乏对执行过程的密集监控，无法感知中间失败并进行自适应恢复）。

为此，论文提出了 **RoboBrain 2.5** 模型，其核心创新在于引入了两个关键能力升级：1）**精确3D空间推理**：通过采用解耦的 `(u, v, d)` 坐标表示法，直接从单目RGB图像预测深度感知的坐标和完整的3D操作轨迹（有序关键点序列），实现了从2D像素定位到度量约束理解的跨越。2）**密集时序价值估计**：提出了一种基于“跳数”的相对进度标注和预测方法，结合多视角融合与双向一致性检查，使模型能够提供稳健、步态感知的任务进度反馈信号。

最终，模型在广泛的2D/3D空间推理和时序价值估计基准测试中取得了最先进的性能，显著提升了模型在复杂、细粒度操作任务中的物理基础性和执行可靠性，成功地将演示级别的成功转化为部署级别的鲁棒性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《RoboBrain 2.5: Depth in Sight, Time in Mind》的创新点分析

本文在现有具身AI基础模型的基础上，提出了两项核心技术创新，旨在解决从高层语义推理到低层物理执行之间的“可靠性鸿沟”。以下是其明确的创新点、与以往方法的对比以及带来的具体优势：

### 1. **精确三维空间推理（Precise 3D Spatial Reasoning）**
   - **改进/不同之处**：
     - **从2D像素相对定位转向3D度量感知预测**：以往模型（如RoboBrain 2.0、通用VLM）的空间推理主要基于2D图像坐标或弱拓扑表示，缺乏绝对深度和尺度信息。RoboBrain 2.5引入了**解耦的 `(u, v, d)` 表示**（图像坐标+绝对深度），可直接通过相机内参转换为3D坐标。
     - **从单点预测转向完整操作轨迹生成**：模型不再仅预测单个目标点，而是输出**有序的关键点序列**，描述完整的操作过程（如抓取、移动、放置），自然编码了空间规划。
     - **三阶段能力课程**：通过**3D空间指代**（定位物体）、**3D空间测量**（估计绝对度量距离）和**3D空间轨迹生成**（生成无碰撞路径）的渐进式训练，实现度量感知的推理。
   - **解决的具体问题/优势**：
     - **解决“度量盲区”问题**：使模型能够理解毫米级间隙、绝对距离等物理约束，确保操作符合真实世界的度量要求。
     - **提升物理可行性**：生成的3D轨迹可直接用于机器人运动规划，支持精确、无碰撞的交互（如“将水壶悬停在花上方1-5厘米”）。
     - **数据兼容性与复用**：`(u, v, d)` 表示可灵活降维到2D，兼容现有2D数据集（如RefSpatial），促进多任务学习。

### 2. **密集时序价值估计（Dense Temporal Value Estimation）**
   - **改进/不同之处**：
     - **从稀疏开环预测转向密集闭环反馈**：以往模型依赖稀疏的成功/失败标签，缺乏对执行过程的中间状态监控。RoboBrain 2.5引入了**基于跳数（Hop）的归一化进度标签**，提供每一步的进度、停滞或回归估计。
     - **多视角融合与双向一致性检查**：通过**增量预测、前向锚定预测、后向锚定预测**三种视角的融合，并结合双向一致性权重（防止分布外幻觉），生成鲁棒的进度信号。
     - **理论保证的边界性**：论文证明了通过跳数迭代重建的全局进度值严格保持在 `[0, 1]` 区间内，避免了误差累积导致的数值溢出。
   - **解决的具体问题/优势**：
     - **解决“开环不可靠”问题**：使模型能够实时感知执行状态（如滑动、偏离），支持自适应恢复和闭环控制。
     - **提供高保真奖励信号**：密集进度估计可作为下游强化学习（RL）的奖励函数，显著提升策略学习的效率和稳定性。
     - **视角与采样鲁棒性**：模型在多种视角变化、不同时间采样间隔下均能保持一致的进度估计，适用于真实世界动态环境。

### 3. **大规模异构数据与训练策略的创新**
   - **改进/不同之处**：
     - **数据构成**：构建了约1240万样本的统一数据集，涵盖**通用MLLM数据**、**空间推理数据**（2D到3D度量）和**时序预测数据**（规划与密集价值估计），其中3D空间推理数据达174万样本，密集价值估计数据约350万样本。
     - **两阶段训练策略**：
       - **阶段一（基础时空学习）**：专注于通用感知、2D定位和开环规划，保留模型的一般推理能力。
       - **阶段二（特定时空增强）**：引入度量3D轨迹生成和密集价值估计任务，通过**数据回放（15%阶段一数据）** 防止灾难性遗忘。
   - **解决的具体问题/优势**：
     - **平衡通用性与专用性**：在提升物理推理能力的同时，不牺牲模型的通用对话和逻辑规划能力。
     - **支持跨平台训练**：通过**FlagScale框架**和**动态内存预分配策略**，实现在NVIDIA与Moore Threads GPU上的跨加速器训练，最终性能差距仅0.62%，证明了基础设施的成熟度。

### 4. **基础设施与跨平台训练创新**
   - **改进/不同之处**：
     - **混合并行与内存优化**：采用**非均匀流水线并行**平衡视觉编码器与语言解码器的计算负载，并设计**动态统一填充策略**，减少GPU内存碎片化，提升长序列训练效率。
     - **跨加速器训练与推理**：首次在千卡级非NVIDIA加速器集群上完成端到端训练，并实现与NVIDIA平台的无缝模型迁移与性能一致。
   - **解决的具体问题/优势**：
     - **提升训练效率与稳定性**：支持大规模多模态长序列训练，硬件利用率高。
     - **降低硬件依赖**：为国产芯片等异构计算平台的大模型训练提供了可行方案，增强技术生态的多样性。

### 5. **综合性能提升与零样本泛化能力**
   - **改进/不同之处**：
     - **在2D/3D空间推理和时序估计基准上全面达到SOTA**：如在CV-Bench（94.58）、CrossPoint（76.30）、TraceSpatial（3D轨迹成功率44%）、时序价值估计（VOC+/-均接近90+）等任务上超越GPT-5.2、Gemini-3-Pro等通用模型及RoboBrain 2.0等具身基线。
     - **零样本真实世界任务鲁棒性**：在接触式操作任务中展现出更强的零样本泛化能力，将演示级成功转化为部署级可靠性。
   - **解决的具体问题/优势**：
     - **验证技术有效性**：通过大量实验证明，两项核心创新确实解决了物理基础不足和时序监控缺失的瓶颈。
     - **推动具身AI落地**：模型生成的3D轨迹和密集进度信号可直接用于机器人控制与RL，加速真实世界应用部署。

---

**总结**：RoboBrain 2.5的核心创新在于**将空间推理从2D语义提升到3D度量层面**，同时**为时序建模引入密集、自监控的价值估计机制**。这两项升级共同解决了当前具身AI模型“物理基础薄弱”和“开环执行不可靠”的关键问题，并通过大规模数据、高效训练框架和跨平台支持，实现了性能与实用性的显著提升。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、整体评估效果
RoboBrain 2.5 在**空间推理**和**时序价值估计**两大核心能力上实现了显著提升，在多个基准测试中达到或超越了当前最先进的通用模型和具身模型，证明了其作为下一代具身AI基础模型的有效性。

### 二、使用的数据集与评价指标

#### 1. **空间推理能力评估**
- **2D空间推理**：使用5个基准数据集。
    - **CV-Bench**：评估视觉中心的空间理解和处理能力。
    - **CrossPoint**：评估跨视角点对应匹配能力。
    - **RoboSpatial**：评估机器人导向环境中的空间推理。
    - **RefSpatial**：评估复杂空间约束下的空间指代。
    - **EmbSpatial**：评估以自我为中心视角的具身空间理解。
    - **主要指标**：准确率（Accuracy, ↑）。

- **3D空间推理**：使用5个基准数据集。
    - **MSMU**：评估定量3D空间测量与理解。
    - **Q-Spatial**：评估图像中物体尺寸和距离的定量推理。
    - **TraceSpatial**：评估多步骤、度量基础的3D空间轨迹生成。
        - **细粒度指标**：`3D Start`（抓取成功率）、`3D End`（放置成功率）、`Success`（综合考虑抓取、放置和碰撞检测的轨迹成功率）。
    - **VABench-V**：评估从自然语言指令生成视觉轨迹。
    - **ShareRobot-T**：评估机器人交互相关的空间定位和轨迹预测。
    - **主要指标**：准确率（↑）或均方根误差（RMSE, ↓）。

#### 2. **时序价值估计评估**
- **评估范式**：遵循 **GPRM** 范式，模型根据任务指令、初始/目标状态的多视角图像，以及“之前/之后”状态的多视角观测，预测一个离散化的相对进度/退步“跳数”作为价值信号。
- **数据集**：涵盖6个不同来源的数据，以测试泛化性。
    - **Real-Bench**：真实机器人数据（AgiBot, DROID, Galaxea）。
    - **Sim-Bench**：仿真环境数据（LIBERO, RoboCasa）。
    - **Human-Bench**：人类操作视频（EgoDex）。
- **评价指标**：
    - **正向VOC (VOC+)**：在原始时间方向上计算的排序相关性。
    - **反向VOC (VOC-)**：将视频时间反转后重新评估模型预测的排序相关性。**两者均越高越好**，反向VOC是衡量时序鲁棒性的严格指标。

### 三、对比的基线方法

1.  **通用基线模型**：
    - **Gemini-3-Pro-Preview**
    - **GPT-5.2**
    - **Qwen3-VL-8B-Inst.**

2.  **具身基线模型**：
    - **RoboBrain-2.0 (7B)**：其前代模型。
    - **Mimo-Embodied (7B)**

### 四、关键性能提升与结论

#### 1. **2D空间推理能力**
- **整体表现**：RoboBrain 2.5 (NV/MTT) 在5个基准上的平均得分均为 **75.82**，显著优于所有基线。
- **关键亮点**：
    - **CV-Bench**：达到 **94.58** (NV)，超越了所有通用和具身基线，展现了强大的基础2D空间感知。
    - **CrossPoint**：达到 **76.30** (MTT)，**大幅领先**所有基线（第二名仅38.60），证明了其从粗粒度判断到精确坐标级对应的强大能力。

#### 2. **3D空间推理能力**
- **整体表现**：在需要**度量感知**和**轨迹生成**的任务上，RoboBrain 2.5 全面领先。
- **关键亮点**：
    - **MSMU**：达到 **64.17** (NV)，优于所有基线，证明了其度量基础感知的显著提升。
    - **TraceSpatial**：在最具挑战性的3D轨迹生成任务上，`3D Start` 达到 **83**，`Success` 达到 **44**，**远超**所有通用基线（Gemini最高仅7），体现了其**精确3D空间规划**的核心优势。
    - **VABench-V / ShareRobot-T**：均取得**最低误差**（0.1189 / 0.1164），在细粒度路径点生成和机器人交互空间输出上精度最高。

#### 3. **时序价值估计能力**
- **整体表现**：RoboBrain 2.5 在**所有6个测试集**上，其 **VOC-** 分数均**大幅、稳定地高于所有基线**，尤其是在反向评估中。
- **关键结论**：
    - **鲁棒性**：通用模型（如GPT-5.2）虽然在某些任务的VOC+上表现尚可，但其VOC-分数极低（常低于20），表明它们缺乏对时序方向的鲁棒理解，容易“记住”视频顺序而非理解进度语义。
    - **泛化性**：RoboBrain 2.5 在真实机器人、仿真和人类视频数据上均表现优异，证明了其**跨领域、跨 embodiment 的泛化能力**。
    - **核心价值**：其提供的密集、步进感知的价值信号，可以作为下游强化学习**高保真的奖励函数**，支持闭环可靠执行。

#### 4. **基础设施验证**
- **跨平台训练**：论文验证了在 **NVIDIA (NV)** 和 **Moore-Threads (MTT)** 两种GPU平台上训练的模型，性能高度一致（平均分相同，各项指标互有高低但差距极小），证明了其训练基础设施的**可靠性与平台无关性**。

### 总结
通过系统性的评估，论文证实了RoboBrain 2.5 的两大核心创新——**精确3D空间推理**和**密集时序价值估计**——有效解决了现有模型的“度量盲”和“开环预测”问题。其在空间任务上实现了从定性到定量、从2D到3D的飞跃，在时序任务上提供了鲁棒、可泛化的进度监控信号，为构建真正物理接地、执行感知的具身智能体奠定了坚实基础。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.14352v1)
- [HTML 版本](https://arxiv.org/html/2601.14352v1)
