# Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving

**相关性评分**: 7.0/10

**排名**: #23


---


## 基本信息

- **arXiv ID**: [2601.12142v1](https://arxiv.org/abs/2601.12142v1)
- **发布时间**: 2026-01-17T19:12:34Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao, Peng Lu, Zufeng Zhang, Sifa Zheng

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

这篇论文提出EchoVLA，一种结合音频指令的用户感知VLA模型，用于自动驾驶，通过情感语音-轨迹对微调多模态大模型，提升驾驶行为的语义和情感适应性。

## 摘要

Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\%$ and the collision rate by $74.4\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user's speech.

## 详细分析

## 论文摘要：《Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving》

**1. 研究背景和动机**
当前基于视觉-语言-动作（VLA）模型的自动驾驶系统，通常将语言指令视为静态的先验信息，在推理时无法实时更新。这导致模型必须仅从像素中持续推断不断变化的驾驶目标，容易产生**延迟或过于保守的驾驶行为**。为了提升自动驾驶系统的人机交互能力和对用户意图的实时响应性，本文提出需要为VLA模型引入一个**在线音频指令通道**，使用户能够通过语音实时、动态地影响驾驶决策。

**2. 核心方法和技术创新**
本文提出了 **EchoVLA**，一个用户感知的VLA模型，其核心创新在于**耦合视觉流与实时音频指令**。主要技术贡献包括：
- **多模态数据集构建**：基于nuScenes数据集，将自车运动描述转换为同步的合成语音指令，并进一步通过调整语速、音高等参数，合成了带有**紧急（urgent）和犹豫（hesitant）** 情感标签的音频-轨迹对。
- **情感引导的轨迹调制**：提出一种基于情感状态的轨迹速度剖面调制算法。通过计算音频的“唤醒度”（Arousal）来量化情感，并据此动态调整规划轨迹的速度（如紧急时加速、犹豫时减速或中途迟疑），而**不改变基本行驶方向**。
- **多模态思维链（CoT）微调**：构建了包含图像、音频、情感分析和调制后轨迹的多模态CoT数据，用于对**Qwen2.5-Omni**多模态大模型进行监督微调。该模型架构能有效处理并时序对齐高频率的音频和低频率的图像输入。

**3. 主要实验结果**
在nuScenes数据集的开环规划基准测试中，EchoVLA展现出显著优势：
- 与仅使用视觉感知的基线VLA模型相比，EchoVLA将**平均L2轨迹误差降低了59.4%**，将**碰撞率降低了74.4%**。
- 定性实验表明，模型能够准确理解音频的语义内容（如“左转”），并能根据语音中的**情感线索（如急促或犹豫的语气）** 相应地调整驾驶行为（如加速或减速）。
- 消融实验验证了，直接利用Qwen2.5-Omni原生多模态编码与CoT推理的方法，优于将音频单独编码（如HuBERT、VQ-VAE）后作为额外令牌输入的传统方法。

**4. 研究意义和价值**
本研究为自动驾驶VLA模型引入了**实时、动态的人机交互通道**，推动了自动驾驶系统从“单向感知”向“双向交互”的演进。其价值在于：
- **技术创新**：首次系统地将**语音情感分析**与自动驾驶轨迹规划相结合，实现了更细腻、人性化的驾驶行为调制。
- **实用价值**：提供了一种无需额外昂贵传感器、仅通过车载麦克风即可显著提升规划安全性和用户意图符合度的解决方案。
- **开源贡献**：构建并开源了首个大规模、包含情感标签的自动驾驶音频-视觉-轨迹数据集，为后续多模态交互研究奠定了基础。
- **局限性**：当前工作依赖于合成语音，未来需在真实嘈杂车载环境、多语言指令及流式生成延迟等方面进行进一步研究和完善。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
当前基于视觉-语言-动作（VLA）模型的自动驾驶系统存在一个关键缺陷：**语言指令在推理时是静态的、预先设定的**。这导致模型必须仅从连续的视觉像素流中推断用户不断变化的驾驶意图，结果往往产生**延迟的或过于保守的驾驶行为**，缺乏与用户的实时、动态交互能力。

### **核心创新点**
论文提出了 **EchoVLA**，一个用户感知的VLA模型，其核心创新在于**引入了实时的音频指令通道**，并实现了**基于语音情感的自适应驾驶行为调制**。具体创新如下：

1.  **多模态交互通道**：首次在自动驾驶VLA模型中**耦合摄像头视频流与在线音频指令**，为用户提供了一个实时影响车辆驾驶意图的通道。
2.  **情感感知驾驶**：不仅理解音频的**语义内容**（如“左转”），还通过分析语音的语调、音高、语速等特征，**检测用户的情绪状态**（如“紧急”或“犹豫”），并据此调制驾驶行为。
3.  **数据构建方法**：提出了一种创新的**多模态思维链数据集构建方法**：
    - **基础数据生成**：将nuScenes数据集中的自车运动轨迹描述转化为结构化的自然语言指令，再通过TTS模型合成同步的音频指令。
    - **情感数据增强**：通过调整基础音频的语速和音高，合成带有“紧急”和“犹豫”情感的音频变体。
    - **轨迹调制**：根据计算出的音频“唤醒度”（Arousal）标签，对原始轨迹进行**速度剖面重参数化**，生成与情感匹配的轨迹（紧急时加速，犹豫时减速或中途迟疑），形成“图像-音频-情感-轨迹”的思维链数据对。
4.  **模型与训练**：基于**Qwen2.5-Omni**多模态大模型进行监督微调。该模型本身的**时间对齐多模态RoPE**和**Thinker-Talker架构**能有效处理音频与视觉在时间尺度上的不对齐问题，并通过思维链推理输出情感分析和调制后的轨迹。

### **解决方案路径**
1.  **问题定义**：识别出现有VLA在自动驾驶中缺乏实时人机交互能力的问题。
2.  **数据构建**：扩展nuScenes数据集，创建包含对齐的音频指令、情感标签及对应调制轨迹的大规模多模态思维链数据集。
3.  **模型设计**：采用已具备强大多模态融合与时间对齐能力的Qwen2.5-Omni作为骨干网络。
4.  **训练方法**：使用构建的数据集对模型进行监督微调，使其学会从“视觉+音频”输入中，通过思维链推理，逐步分析音频内容、检测用户情绪，并最终生成用户意图感知的驾驶轨迹。
5.  **验证评估**：在nuScenes数据集上进行开环测试，以轨迹L2误差和碰撞率作为主要指标，与纯视觉感知的VLA基线模型对比，证明音频引入的有效性。

### **实际价值与技术贡献**
- **提升安全性与用户体验**：使自动驾驶系统能更及时、准确地响应用户的实时指令和情绪状态，减少因意图误解导致的延迟或保守操作，提升驾驶安全性和乘坐舒适度。
- **开辟新交互范式**：为自动驾驶的人机共驾提供了更自然、更丰富的**语音交互接口**，超越了传统按钮或触摸屏。
- **高效多模态融合**：展示了利用现有MLM（如Qwen2.5-Omni）的内在能力来处理自动驾驶中复杂时空对齐问题的有效性，避免了繁琐的手工设计。
- **显著性能提升**：实验表明，相比纯视觉感知基线，EchoVLA将**平均L2误差降低了59.4%，碰撞率降低了74.4%**，证明了音频信息对视觉感知的互补性而非竞争关系。

**总结**：这篇论文的核心是从 **“静态语言”** 到 **“动态音频交互”** 的范式转变，通过创新的**情感感知数据构建**和**多模态思维链微调**，赋予自动驾驶VLA模型实时理解并响应用户语音指令及情绪的能力，是迈向更人性化、更安全自动驾驶系统的重要一步。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对现有基于视觉-语言-动作（VLA）的自动驾驶系统将语言指令视为静态先验、无法实时响应用户动态意图的问题，提出了一个名为 **EchoVLA** 的用户感知VLA框架。其核心创新在于**引入实时音频指令作为在线交互通道**，使模型不仅能理解指令的语义内容，还能通过分析语音的语调、音高和语速来感知用户的情绪状态（如紧急或犹豫）。方法上，论文通过将nuScenes数据集中的自车运动描述转换为合成语音指令，并基于计算出的“唤醒度”为不同情绪配对相应的速度曲线，构建了一个包含多模态思维链（CoT）的数据集，用于对Qwen2.5-Omni多模态大模型进行监督微调。实验结果表明，该框架在nuScenes开环基准测试中，相比仅依赖视觉感知的基线模型，**平均L2误差降低了59.4%，碰撞率降低了74.4%**，成功实现了根据音频指令和用户情绪来调制驾驶行为（如调整速度曲线）的目标。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving》提出了一个名为 **EchoVLA** 的用户感知视觉-语言-动作模型。其核心创新在于为自动驾驶系统引入了一个**实时的、基于音频指令的在线交互通道**，并实现了对用户语音中**情感意图的感知与响应**。以下是其相对于已有工作的明确创新点：

---

### 1. **从静态语言先验到动态音频指令的在线耦合**
- **改进/不同之处**：
    - **以往方法**：现有的视觉-语言-动作模型通常将语言指令视为**静态的先验信息**（例如，在推理时固定的文本提示）。模型只能从连续的视觉像素中推断不断变化的目标，这常常导致动作延迟或过于保守。
    - **本文方法**：EchoVLA 引入了**实时的音频指令流**作为在线输入。模型在推理过程中可以持续接收用户的语音命令，从而动态地调整驾驶意图和目标。
- **解决的问题/带来的优势**：
    - **解决了意图推断延迟和歧义问题**。在驾驶场景存在多种可行选择时（例如，在岔路口），纯视觉模型可能无法准确判断用户意图。音频指令提供了明确的、语义化的即时目标，使模型能生成与用户指令一致的轨迹。
    - **实现了真正的人机协同驾驶**。为用户提供了一个自然、直观的交互通道，使其能在行驶过程中实时影响车辆决策，提升了系统的灵活性和用户友好性。

### 2. **从语义理解到情感感知的多模态指令理解**
- **改进/不同之处**：
    - **以往方法**：大多数多模态模型主要关注语音或文本的**语义内容**理解。
    - **本文方法**：EchoVLA 不仅理解音频指令的语义，还通过分析语音的**声学特征**（如语调、音高、语速）来检测用户的**情感状态**（如“紧急”或“犹豫”）。论文提出了一套量化的“唤醒度”计算方法和基于情感的轨迹速度调制算法。
- **解决的问题/带来的优势**：
    - **解决了用户状态感知缺失的问题**。相同的语义指令在不同情绪下可能对应不同的驾驶风格需求（例如，“加速”在紧急时需激进，在犹豫时需平缓）。
    - **实现了更细腻、人性化的驾驶行为调制**。模型能够根据用户情绪自适应调整轨迹的速度曲线（如图3所示），使自动驾驶行为更符合用户的即时心理状态，提升了乘坐体验和安全性。

### 3. **基于多模态思维链的数据集构建与模型微调范式**
- **改进/不同之处**：
    - **以往方法**：用于自动驾驶的VLA数据集构建多依赖于人工标注，过程耗时费力，且通常只包含视觉-文本-动作的对齐。
    - **本文方法**：
        1.  **数据集创新**：在 nuScenes 数据集基础上，**自动化合成了时序对齐的音频指令数据集**。将自车运动描述转换为结构化文本，再通过TTS生成语音。进一步，通过修改语音的节奏和音高，合成了带有不同情感标签（紧急/犹豫）的音频，并据此调制出对应的速度曲线轨迹。
        2.  **训练范式创新**：采用**多模态思维链**进行监督微调。模型被训练以“思维链”的方式，逐步输出：音频分析 -> 情感检测 -> 情感引导的轨迹路径点。这引导模型进行可解释的推理。
- **解决的问题/带来的优势**：
    - **解决了高质量音频-视觉-动作对齐数据稀缺的问题**。提供了一种可扩展的、低成本的数据合成方法。
    - **提升了模型推理的透明度和准确性**。CoT训练使模型学习到从感知到决策的中间推理步骤，有助于生成更可靠、符合逻辑的轨迹。

### 4. **利用原生支持多模态时序对齐的基础模型**
- **改进/不同之处**：
    - **以往方法**：将音频等新模态集成到VLM中，常采用外接编码器（如HuBERT、VQ-VAE）将音频转为特征或离散标记，然后拼接到模型中。这种方法可能面临模态对齐和融合效率的挑战。
    - **本文方法**：直接选用 **Qwen2.5-Omni** 作为基础模型进行微调。该模型原生支持音频、图像、文本的**统一编码**，并其 **Time-aligned Multimodal RoPE** 机制能保证跨模态的时序对齐。
- **解决的问题/带来的优势**：
    - **解决了跨模态（尤其是高时序分辨率的音频与图像）融合的固有难题**。避免了手工设计融合模块的复杂性。
    - **实现了更高效、更内聚的多模态理解**。如表II的消融实验所示，该方法在平均L2误差和碰撞率上均优于使用外部音频编码器的方法。

### 5. **在开放环评测中展现的显著性能提升**
- **改进/不同之处**：
    - **以往方法**：许多先进的VLA模型（如Qwen2-VL-7B, VLP-UniAD）仅依赖视觉感知进行规划。
    - **本文方法**：EchoVLA 在 **nuScenes 开放环评测**中，与纯视觉感知的基线模型相比，取得了显著提升。
- **解决的问题/带来的优势**：
    - **定量证明了音频模态的互补价值**：实验结果显示，EchoVLA 将**平均L2误差降低了59.4%**，将**碰撞率降低了74.4%**。这强有力地证明，音频指令并非与视觉感知竞争，而是提供了关键的补充信息，共同提升了轨迹预测的准确性和安全性。

---

**总结**：本文的核心创新在于**将自动驾驶VLA从被动的环境解释器，转变为能通过实时音频与用户进行主动、情感化交互的协同驾驶伙伴**。它通过**动态音频指令耦合**、**情感感知与响应**、**创新的数据合成与训练方法**，以及**利用先进的基础模型**，系统地解决了现有VLA在意图推断延迟、用户状态感知缺失和跨模态融合方面的局限性，为实现更安全、更人性化的自动驾驶系统提供了新的思路和实证基础。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 数据集与评价指标
- **数据集**：使用 **nuScenes** 数据集进行训练和评估。该数据集包含1000个驾驶场景，提供多模态传感器数据（摄像头、激光雷达、雷达等）。
- **评价指标**：
  - **L2误差（米）**：衡量预测轨迹与真实轨迹之间的平均欧氏距离，越低越好。
  - **碰撞率（%）**：通过检查预测轨迹中车辆边界框与其他物体的重叠情况计算，越低越好。
  - **时间范围**：评估1秒、2秒、3秒时间窗口下的性能。

### 基线方法对比
论文将提出的 **EchoVLA** 模型与以下基线方法进行对比：
1. **纯视觉感知的VLA模型**：
   - 基于 **Qwen2-VL-7B**（更大规模的视觉语言模型）。
   - 同样在nuScenes数据集上微调，但仅使用视觉输入。
2. **其他先进视觉驱动方法**（在消融实验中提及）：
   - **VLP-UniAD**、**RDA-Driver** 等纯视觉方法。

### 关键性能提升与结论
#### 1. **主要定量结果（与纯视觉基线对比）**
在nuScenes验证集上的开环规划任务中，EchoVLA显著优于纯视觉基线：

| 指标 | 提升幅度 | 具体数值对比 |
|------|----------|--------------|
| **平均L2误差** | **降低59.4%** | 从1.43米降至0.58米 |
| **平均碰撞率** | **降低74.4%** | 从0.43%降至0.11% |

- **分时间窗口表现**：
  - L2误差在3秒预测窗口改善最明显（从2.54米降至0.74米）。
  - 碰撞率在长时预测中大幅下降（3秒窗口从1.07%降至0.22%）。

#### 2. **情绪感知与行为调制验证**
- **定性实验**（图7）显示，模型能根据音频中的情绪（紧急/犹豫）调整速度曲线：
  - **紧急情绪**：生成更激进的速度曲线（加速更快）。
  - **犹豫情绪**：生成更保守的速度曲线（中途减速）。
- 证明模型不仅能理解语音语义，还能捕捉**声学特征（语调、语速）**中的情绪线索，实现更人性化的驾驶行为调制。

#### 3. **消融实验（音频编码方法对比）**
在nuScenes mini集上对比不同音频编码方法：

| 音频编码方法 | 平均L2误差 | 平均碰撞率 |
|--------------|------------|------------|
| HuBERT       | 0.45       | 0.33       |
| VQ-VAE       | 0.48       | 0.32       |
| STFT         | 0.60       | 0.46       |
| **Ours（Qwen2.5-Omni原生多模态对齐）** | **0.38** | **0.24** |

**结论**：论文提出的方法（利用Qwen2.5-Omni的**时间对齐多模态RoPE**和**CoT推理**）在音频编码效果上最优，误差和碰撞率最低。

#### 4. **场景适应性分析**
- **简单场景**（图4）：视觉输入已足够推断合理轨迹，音频指令与视觉意图一致时，EchoVLA与纯视觉模型表现相近。
- **复杂/歧义场景**（图5）：当音频指令与视觉推断意图冲突时，EchoVLA能**优先遵循音频指令**，而纯视觉模型无法响应实时用户意图，导致轨迹偏离。

### 总结
论文通过**音频增强的多模态VLA框架**，在nuScenes数据集上实现了：
- **显著降低轨迹误差和碰撞风险**（关键指标提升约60%-74%）。
- **验证了情绪感知对驾驶行为调制的有效性**。
- **证明了音频指令在歧义场景中的关键作用**，弥补了纯视觉感知的不足。

**局限与未来方向**：当前使用合成语音，未涵盖真实人声的多样性、噪声环境或多语言场景；推理延迟较高（非流式生成）。未来计划收集真实车内语音数据并增强噪声鲁棒性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.12142v1)
- [HTML 版本](https://arxiv.org/html/2601.12142v1)
