# FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions

**相关性评分**: 7.0/10

**排名**: #21


---


## 基本信息

- **arXiv ID**: [2601.12799v1](https://arxiv.org/abs/2601.12799v1)
- **发布时间**: 2026-01-19T07:59:32Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Peng Li, Zihan Zhuang, Yangfan Gao, Yi Dong, Sixian Li, Changhao Jiang, Shihan Dou, Zhiheng Xi, Enyu Zhou, Jixuan Huang, Hui Li, Jingjing Gong, Xingjun Ma, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, Xipeng Qiu

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, offline Reinforcement Learning, world model

## 一句话总结

FRoM-W1是一个开源框架，通过语言指令实现通用人形机器人全身运动控制，结合大规模语言驱动运动生成和强化学习微调，并在真实机器人上部署验证。

## 摘要

Humanoid robots are capable of performing various actions such as greeting, dancing and even backflipping. However, these motions are often hard-coded or specifically trained, which limits their versatility. In this work, we present FRoM-W1, an open-source framework designed to achieve general humanoid whole-body motion control using natural language. To universally understand natural language and generate corresponding motions, as well as enable various humanoid robots to stably execute these motions in the physical world under gravity, FRoM-W1 operates in two stages: (a) H-GPT: utilizing massive human data, a large-scale language-driven human whole-body motion generation model is trained to generate diverse natural behaviors. We further leverage the Chain-of-Thought technique to improve the model's generalization in instruction understanding. (b) H-ACT: After retargeting generated human whole-body motions into robot-specific actions, a motion controller that is pretrained and further fine-tuned through reinforcement learning in physical simulation enables humanoid robots to accurately and stably perform corresponding actions. It is then deployed on real robots via a modular simulation-to-reality module. We extensively evaluate FRoM-W1 on Unitree H1 and G1 robots. Results demonstrate superior performance on the HumanML3D-X benchmark for human whole-body motion generation, and our introduced reinforcement learning fine-tuning consistently improves both motion tracking accuracy and task success rates of these humanoid robots. We open-source the entire FRoM-W1 framework and hope it will advance the development of humanoid intelligence.

## 详细分析

## 论文《FRoM-W1：迈向基于语言指令的通用人形机器人全身控制》详细摘要

### 1. 研究背景和动机
当前人形机器人（如Unitree H1/G1）能够执行问候、舞蹈等高难度动作，但这些动作通常依赖于硬编码或针对特定任务训练的控制器，**泛化能力有限**，难以根据开放式的自然语言指令自主生成并执行多样化全身动作。本研究旨在解决这一核心挑战：**如何让机器人理解多样的语言指令，并在真实物理世界中稳定执行对应的全身运动**。

### 2. 核心方法和技术创新
论文提出了一个开源的两阶段框架 **FRoM-W1**：
- **H-GPT（语言到人体运动生成）**：利用大规模人体运动数据（HumanML3D-X, Motion-X），基于LLaMA-3.1大语言模型，训练了一个90亿参数的**语言驱动全身人体运动生成模型**。创新性地引入**思维链（CoT）技术**，将抽象指令分解为细粒度、有时序的动作描述，显著提升了模型对复杂和抽象指令的理解与生成泛化能力。
- **H-ACT（人体运动到机器人执行）**：包含**运动重定向**（将生成的人体SMPL-X动作适配到特定机器人形态）和**强化学习控制器**。控制器采用**预训练与微调（RPT+RFT）的两阶段策略**：先在大规模人体运动数据上进行通用控制器预训练，然后在推理时针对特定生成动作进行快速强化学习微调，从而在仿真和现实中实现**更精准、稳定的运动跟踪**。此外，设计了一个**模块化仿真到现实部署框架（RoboJudo）**，支持多种控制策略的即插即用。

### 3. 主要实验结果
- **运动生成**：在构建的HumanML3D-X基准测试上，**H-GPT** 在核心指标FID上比基线T2M-GPT提升了2.5倍。在测试泛化能力的δ-HumanML3D-X基准上，引入CoT的模型在指令改写和加噪场景下表现更优。
- **运动控制**：在Unitree H1和G1机器人上的仿真实验表明，提出的**强化学习微调（RFT）策略**能持续提升运动跟踪成功率，并将关节位置误差（MPJPE）的准确性指标提升了约15%。
- **系统集成**：整个框架成功在真实的H1和G1机器人上部署，能够执行由“扮演大象”、“拉小提琴”等复杂语言指令生成的动作，并验证了其部署框架对HugWBC、TWIST等其他先进控制器的兼容性。

### 4. 研究意义和价值
FRoM-W1是**首个完全开源**的、集成了语言理解、全身运动生成与机器人控制的全流程框架。其核心价值在于：
- **技术贡献**：通过 **“大语言模型+CoT”** 与 **“两阶段强化学习”** 的创新结合，为语言驱动的人形机器人控制提供了高效、可泛化的解决方案。
- **社区价值**：开源完整的框架、模型、代码和基准，旨在降低研究门槛，推动**通用人形机器人智能**的发展。
- **应用前景**：为实现更自然、灵活的人机交互，以及未来机器人在家庭、服务等非结构化环境中的自主任务执行奠定了基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：FRoM-W1

### **研究问题**
如何让**人形机器人**理解**开放式的自然语言指令**，并在**真实物理世界**中稳定地执行对应的**全身动作**。

### **核心挑战**
1.  **数据稀缺**：缺乏大规模“语言-机器人动作”配对数据。
2.  **执行困难**：即使能生成动作，双足人形机器人在重力、动力学不确定性下直接执行极易失稳摔倒。

### **解决方案与核心创新**
论文提出了 **FRoM-W1** 框架，其核心创新在于**两阶段、模块化的设计思路**，灵感来源于人类大脑的“大脑皮层”（语义理解与规划）和“小脑”（稳定运动执行）的分工协作。

#### **第一阶段：H-GPT（语言 → 人体动作）**
*   **目标**：将自然语言指令转化为高质量、语义对齐的**全身人体动作序列**（包含手部动作）。
*   **技术创新**：
    1.  **基于LLM的生成模型**：以 **LLaMA-3.1 (8B)** 为基础，通过LoRA微调，构建了一个90亿参数的生成模型。
    2.  **思维链增强**：利用GPT-4o为训练数据中的指令生成**细粒度、时序化的动作描述（CoT）**。这相当于让模型先“思考”如何分解复杂/抽象指令（如“扮演一头大象”）为具体的身体动作单元，再生成动作，显著提升了模型对复杂和抽象指令的**泛化理解能力**。
    3.  **全身动作分词器**：采用 **VQ-VAE** 技术，将连续的人体动作序列（SMPL-X格式）离散化为与语言Token同格式的“动作Token”，使LLM能够像处理文本一样自回归地生成动作序列。
*   **实际价值**：解决了从开放语言到动作的映射问题，生成了可用于后续机器人执行的、丰富的中间动作表示。

#### **第二阶段：H-ACT（人体动作 → 机器人执行）**
*   **目标**：将生成的人体动作**稳定、精确**地部署到不同形态的实体人形机器人上。
*   **技术创新**：
    1.  **两阶段强化学习控制策略**：
        *   **预训练**：在IsaacGym仿真器中，使用大规模人体动作数据集（AMASS）预训练一个通用的“教师-学生”全身运动跟踪控制器，获得基础的运动跟踪和平衡能力。
        *   **推理时微调**：针对**H-GPT生成的每一个特定目标动作**，在仿真中进行快速的**强化学习微调**。这使得控制器能专门优化对该动作的跟踪精度和稳定性，实现了**平均关节位置误差降低15%**、跟踪成功率持续提升的效果。
    2.  **模块化仿真到现实部署框架**：设计了一个名为 **RoboJudo** 的轻量级部署框架。它通过统一的接口，不仅支持其默认控制策略的部署，还能**无缝集成**其他主流运动模仿控制器（如HugWBC, TWIST），实现了**策略无关、平台通用**的快速Sim2Real迁移。

### **整体框架价值**
1.  **通用性与开放性**：首次提供了一个**完全开源**的、端到端的语言驱动人形机器人全身控制框架，包含模型、代码、基准测试和部署模块。
2.  **性能优越**：
    *   **H-GPT** 在新建的HumanML3D-X基准上，主要指标FID相比基线T2M-GPT提升了**2.5倍**；在测试泛化能力的δ-HumanML3D-X基准上，CoT技术展现出明显优势。
    *   **H-ACT** 的两阶段RL策略有效提升了动作跟踪的准确性和成功率。
3.  **实证验证**：在**Unitree H1和G1**两款真实机器人上进行了全面测试，成功演示了从理解抽象指令、生成包含手部的动作，到稳定执行的完整流程。

### **总结**
**FRoM-W1** 的核心创新在于**创造性地将大语言模型的语义理解与推理能力（通过CoT），与针对物理执行的、可快速定制的强化学习控制策略相结合**，并通过一个模块化的部署框架桥接了仿真与现实的鸿沟。它为解决“让机器人听懂人话并自然行动”这一核心难题提供了一个强大、可扩展且开放的工程与科研基础。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**人形机器人如何理解开放式的自然语言指令并稳定执行对应的全身动作**这一核心问题。为此，作者提出了一个名为 **FRoM-W1** 的两阶段开源框架：第一阶段 **H-GPT** 利用大规模人类动作数据，基于LLaMA大语言模型并引入思维链（CoT）技术，将语言指令生成全身人体动作；第二阶段 **H-ACT** 通过运动重定向和一种结合预训练与推理时微调的两阶段强化学习策略，训练出能精确、稳定跟踪并执行这些动作的人形机器人控制器。实验表明，该框架在HumanML3D-X基准上取得了显著的性能提升（FID指标提升2.5倍），其RL微调策略将动作跟踪成功率提升了15%，并成功在Unitree H1和G1机器人上实现了从抽象语言指令到真实世界稳定动作执行的闭环。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## FRoM-W1 论文创新点分析

这篇论文提出了一种基于语言指令的人形机器人全身控制框架 **FRoM-W1**。相对于已有工作，其创新点主要体现在以下几个方面：

---

### 1. **引入 Chain-of-Thought（CoT）增强的语言驱动人体运动生成模型（H-GPT）**
- **改进/不同之处**：  
  以往的语言-运动生成模型（如 T2M-GPT、MotionGPT）通常直接学习从文本到运动的映射，对复杂或抽象指令的泛化能力有限。本文在训练和推理阶段引入 **CoT 技术**，利用大语言模型（如 GPT-4o）将自然语言指令分解为细粒度、具有时间结构的动作描述（如“先站立，然后抬起右手”），作为中间表示来引导运动生成。
- **解决的问题/优势**：  
  - **提升泛化能力**：通过 CoT 将抽象指令（如“扮演大象”）转化为具体的动作序列，使模型能够理解并生成更复杂、多样化的运动。  
  - **缓解数据稀缺问题**：利用 CoT 对现有人体运动数据集（如 HumanML3D-X）进行语义增强，扩展了指令的覆盖范围，减少了对大规模机器人标注数据的依赖。  
  - **实验验证**：在提出的 δ-HumanML3D-X 基准测试中，CoT 版本在指令重述和噪声干扰场景下显著提升了生成质量（FID 指标提升约 20-30%）。

---

### 2. **提出两阶段强化学习控制策略：预训练 + 推理时微调（RPT + RFT）**
- **改进/不同之处**：  
  现有的人形机器人运动跟踪方法（如 OmniH2O、TWIST）通常仅进行大规模预训练，得到一个通用控制器。本文在 **推理阶段引入额外的强化学习微调（RFT）**，针对每个生成的运动进行短时域的策略优化，使控制器更精准地跟踪特定运动。
- **解决的问题/优势**：  
  - **提升跟踪精度与稳定性**：RFT 能够针对特定运动的动力学特性进行适配，减少跟踪误差（MPJPE 指标提升约 15%）并提高成功率（SR 指标显著改善）。  
  - **兼顾通用性与特异性**：预训练（RPT）提供通用运动基础，微调（RFT）实现针对性的运动优化，解决了通用控制器对生成运动适应性不足的问题。  
  - **效率高**：微调仅需少量步数（约 500 步），可在短时间内完成，适合实际部署。

---

### 3. **构建包含手部动作的全身运动基准测试与数据集扩展**
- **改进/不同之处**：  
  现有主流基准测试（如 HumanML3D）仅包含身体动作，缺乏手部运动。本文扩展了 **HumanML3D-X 和 δ-HumanML3D-X** 基准，将 SMPL 格式升级为包含手部模型的 SMPL-X 格式，并增加了指令重述和噪声干扰的测试场景。
- **解决的问题/优势**：  
  - **全面评估全身运动生成**：新基准能够评估包括手部在内的全身运动生成质量，更符合人形机器人实际任务需求。  
  - **推动领域标准化**：为后续研究提供了包含手部动作的评估标准，填补了现有基准的空白。  
  - **数据扩展尝试**：尝试利用大规模视频估计数据（Motion-X）进行训练，探索数据缩放对模型泛化的影响（尽管实验发现数据质量仍是瓶颈）。

---

### 4. **设计模块化、跨策略的仿真到现实部署框架（RoboJudo）**
- **改进/不同之处**：  
  现有部署框架通常针对特定控制器或机器人平台定制。本文提出了 **RoboJudo**，一个统一的模块化部署框架，将控制器、环境抽象层和策略层解耦，支持多种控制策略（如 HugWBC、TWIST）和机器人平台（Unitree H1/G1）的即插即用。
- **解决的问题/优势**：  
  - **提升部署灵活性**：同一生成运动可通过不同控制策略执行，支持策略间的快速切换与组合。  
  - **降低仿真到现实迁移成本**：通过统一接口和环境抽象，简化了从仿真到真实机器人的迁移过程。  
  - **促进算法比较与集成**：为不同控制方法提供了公平、便捷的部署和测试平台。

---

### 5. **全开源框架与完整系统整合**
- **改进/不同之处**：  
  许多相关工作仅开源部分代码或模型，且缺乏从语言理解到机器人执行的完整流水线。本文 **完全开源** 了整个框架，包括 H-GPT 和 H-ACT 的训练代码、模型检查点、评估基准以及部署模块。
- **解决的问题/优势**：  
  - **促进可重复性与社区发展**：提供了完整的工具链，降低了研究门槛，加速领域进步。  
  - **端到端系统验证**：首次实现了从语言指令到真实机器人全身控制的完整开源解决方案，并进行了全面评估（模拟+实物）。  
  - **强调实用性**：开源部署模块（RoboJudo）直接面向真实机器人应用，推动了研究成果向实际场景的转化。

---

### 总结：核心创新价值
FRoM-W1 的主要贡献在于 **系统性地整合了语言理解、运动生成和机器人控制**，并通过 **CoT 增强泛化、两阶段 RL 优化控制、模块化部署** 等创新点，解决了以下关键问题：
1. **语言-运动映射的泛化难题**（通过 CoT 桥接抽象指令与具体动作）。
2. **生成运动在物理机器人上的稳定执行问题**（通过 RFT 提升跟踪精度）。
3. **框架的通用性与可扩展性**（通过模块化设计支持多策略、多平台）。

这些创新使得 FRoM-W1 在语言驱动人形机器人控制领域向前迈进了一步，为构建通用的人形机器人智能体提供了可行的技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过一系列实验全面评估了 **FRoM-W1** 框架在三个核心环节的效果：语言到人体运动生成、仿真环境中的仿人机器人全身控制、以及真实世界部署。以下是详细的实验设置、指标、对比方法和关键结果。

### 1. 语言到全身人体运动生成 (H-GPT)

**目标**：评估 `H-GPT` 模型根据自然语言指令生成高质量、多样化全身人体运动（含手部动作）的能力。

#### 1.1 数据集
*   **HumanML3D-X**：论文扩展的基准数据集。在原始 **HumanML3D**（基于SMPL，无手部）的基础上，将运动数据升级为包含手部模型的 **SMPL-X** 格式，文本标签保持不变。用于训练和测试模型的基本生成能力。
*   **δ-HumanML3D-X**：为评估泛化能力构建的基准。在HumanML3D-X基础上进行两种变换：
    *   **指令重述**：使用GPT-4o改变原始文本的风格。
    *   **指令加噪**：在原始文本中加入拼写错误等噪声。
*   **Motion-X**：当前最大的公开全身人体运动数据集（含手部），包含大量从视频中估计的运动数据。用于探索数据规模扩展对模型性能的影响。

#### 1.2 评价指标
采用文本-运动生成领域的标准基于评分模型的指标：
*   **Fréchet Inception Distance (FID) ↓**：**主要指标**。衡量生成运动与真实运动特征分布之间的距离，值越低表示生成质量越高、越真实。
*   **检索精度 Top-k (R Top-k) ↑**：通过检索方法衡量文本与运动匹配的准确性。
*   **多模态距离 (MM Dist) ↓**：衡量文本与运动特征之间的距离。
*   **多样性 (DIV) →**：衡量生成运动本身的多样性，接近真实数据为佳。
*   **多模态性 (MModality) ↑**：衡量同一文本下生成不同运动的能力。

#### 1.3 基线方法
与四类主流文本-运动生成方法对比：
*   **T2M**：基于自编码器和循环神经网络的早期方法。
*   **MotionDiffuse**：基于线性注意力扩散模型的方法。
*   **MLD**：基于潜在扩散模型的方法。
*   **T2M-GPT**：基于独立Transformer解码器的自回归方法（与H-GPT架构思想最接近）。

#### 1.4 关键结果与性能提升
1.  **在HumanML3D-X基准上的卓越性能**：
    *   `H-GPT`（无论是否含CoT）在**核心指标FID上取得显著提升**。
    *   `H-GPT w.o. CoT` 的FID为 **0.229**，`H-GPT` 为 **0.255**，均远优于最佳基线 **T2M-GPT (0.677)**，实现了约 **2.5倍** 的改进。
    *   `H-GPT w.o. CoT` 在运动多样性 **(DIV: 9.674)** 上也达到了最佳。

2.  **在δ-HumanML3D-X基准上验证CoT的泛化能力**：
    *   在**指令重述**和**指令加噪**两种泛化场景下，引入 **Chain-of-Thought (CoT)** 的 `H-GPT` 模型在FID上均显著优于不含CoT的版本及所有基线。
    *   例如，在指令重述场景下，`H-GPT` (FID: 0.355) 优于 `H-GPT w.o. CoT` (FID: 0.455) 和 T2M-GPT (FID: 2.043)。**这证明了CoT技术有效提升了模型对复杂、抽象或噪声指令的理解和泛化能力。**

3.  **数据扩展实验 (Motion-X) 的发现**：
    *   使用更大规模的Motion-X数据训练得到的 `H-GPT++` 模型，在HumanML3D-X测试集上的表现**并未超越**仅用HumanML3D-X数据训练的模型。
    *   **结论**：从视频估计的大规模运动数据与高质量动捕数据存在分布和质量差异，直接使用可能无法带来预期提升。这指出了利用网络视频数据提升模型泛化能力仍需解决数据质量对齐问题。

4.  **对复杂和抽象指令的定性评估**：
    *   在50条复杂指令和50条抽象指令的人工评估中，引入CoT的模型（`H-GPT` 和 `H-GPT++`）在多数情况下生成的动作品质优于不含CoT的版本。
    *   例如，在抽象指令评估中，`H-GPT` 与 `H-GPT w.o. CoT` 的胜场比为 **30:23**，显示了CoT在理解抽象语义上的优势。

---

### 2. 仿真环境中的仿人机器人全身控制 (H-ACT)

**目标**：评估 `H-ACT` 模块在物理仿真中控制机器人稳定、精确地跟踪由 `H-GPT` 生成的运动的能力。

#### 2.1 数据集与平台
*   **训练数据**：使用 **AMASS** 人体运动数据集，经过运动重定向适配到Unitree **H1** (19自由度) 和 **G1** (21自由度) 机器人的骨骼结构。
*   **测试数据**：使用 `H-GPT` 生成的“复杂”和“抽象”指令对应的运动序列。
*   **仿真平台**：NVIDIA **IsaacGym**。

#### 2.2 评价指标
*   **平均每关节位置误差 (MPJPE) ↓**：机器人实际关节角度与参考运动之间的误差，主要精度指标。
*   **关节速度误差 (VEL) ↓**
*   **关节加速度误差 (ACCEL) ↓**
*   **成功率 (SR) ↑**：成功完成整个运动跟踪而不摔倒的比率。

#### 2.3 对比方法（训练策略）
*   **从头训练**：在目标运动上直接进行强化学习训练。
*   **预训练**：在AMASS大规模数据上预训练一个通用运动控制器。
*   **预训练+微调**：论文提出的**两阶段策略**。先预训练，然后在推理时针对特定目标运动进行短时间的强化学习微调。

#### 2.4 关键结果与结论
1.  **强化学习微调 (RFT) 的有效性**：
    *   与仅使用预训练策略相比，**RFT策略在MPJPE和SR上均取得一致且显著的提升**。
    *   对于H1机器人，RFT将MPJPE降低了约 **15%**，成功率也有提升。
    *   对于G1机器人，RFT在保持高成功率的同时，进一步提升了跟踪精度。
    *   **结论**：针对特定运动进行快速微调，可以显著提升控制器的跟踪精度和稳定性，使其更好地适应生成动作的细节。

2.  **不同机器人的性能差异**：
    *   G1机器人在所有指标上均优于H1机器人（例如，在AMASS数据上预训练的成功率：G1 ~80-90%， H1 ~40-50%）。
    *   **结论**：G1的机械结构设计使其能更稳定、更容易地执行多样化动作。

3.  **对生成运动的跟踪挑战**：
    *   在跟踪 `H-GPT` 生成的运动时，成功率低于跟踪AMASS中的高质量动捕数据。
    *   **结论**：生成的运动本身可能包含噪声或不稳定片段，对控制器的鲁棒性提出了更高要求。

---

### 3. 真实世界整体框架部署

**目标**：验证整个 `FRoM-W1` 流水线在真实仿人机器人平台上的端到端性能。

#### 3.1 部署平台
*   Unitree **H1** 和 **G1** 机器人。

#### 3.2 评估方式
*   主要以**定性演示和案例研究**为主，展示了框架的多种能力：
    *   **理解抽象指令**：如“扮演一头大象”。
    *   **生成并执行手部动作**：如“拉小提琴”。
    *   **组合动作**：如“先走几步，然后接电话”。
    *   **跨策略兼容性**：成功将 `H-GPT` 生成的运动，通过 `H-ACT` 的通用部署模块，应用于不同的控制策略（如 **HugWBC**, **TWIST**），并稳定执行。

#### 3.3 存在的挑战与失败案例（论文明确指出）
1.  **动作-指令错位**：模型有时会生成与指令语义不完全匹配的动作。
2.  **生成动作自身不稳定**：某些生成的动作序列在动力学上存在问题，导致机器人难以跟踪。
3.  **Sim2Real 差距**：即使在仿真中能成功跟踪的极端动作（如深蹲、快速奔跑），在真实世界中也会因模型误差、地面摩擦差异等导致失败或摔倒。
4.  **身体自碰撞**：生成的动作可能导致机器人肢体


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.12799v1)
- [HTML 版本](https://arxiv.org/html/2601.12799v1)
