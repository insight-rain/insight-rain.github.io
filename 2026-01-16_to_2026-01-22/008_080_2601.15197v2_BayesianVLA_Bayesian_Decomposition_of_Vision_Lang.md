# BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries

**相关性评分**: 8.0/10

**排名**: #8


---


## 基本信息

- **arXiv ID**: [2601.15197v2](https://arxiv.org/abs/2601.15197v2)
- **发布时间**: 2026-01-21T17:15:22Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune

## 一句话总结

该论文提出BayesianVLA框架，通过贝叶斯分解和潜在动作查询解决VLA模型在机器人操作中的信息崩溃问题，提升泛化能力，无需新数据即可微调。

## 摘要

Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.

## 详细分析

## 论文摘要：BayesianVLA

**1. 研究背景和动机**
视觉-语言-动作（VLA）模型在机器人操作中展现出潜力，但在面对新指令或复杂多任务场景时泛化能力不足。本文发现，当前基于目标驱动数据集的训练范式存在一个关键缺陷：数据集偏差导致语言指令仅从视觉观察即可高度预测，使得指令与动作之间的条件互信息趋于零，即发生“信息坍缩”。这导致模型退化为忽略语言约束的“纯视觉策略”，在分布外（OOD）场景中失败。

**2. 核心方法和技术创新**
为解决此问题，本文提出了 **BayesianVLA** 框架。其核心创新在于：
- **贝叶斯分解视角**：将最优策略分解为视觉先验 `p(a|v)` 和语言条件后验 `π(a|v,ℓ)`，并推导出最大化动作与指令间条件点互信息（PMI）的目标函数，即对数似然比（LLR）。
- **潜在动作查询**：引入一组可学习的潜在动作查询令牌，作为VLM与下游扩散Transformer策略之间的瓶颈接口。
- **双分支训练架构**：利用因果注意力掩码，通过调整查询令牌在输入序列中的位置，构建“先验分支”（仅视觉）和“后验分支”（视觉+语言），分别学习先验和后验策略，并联合优化LLR目标以惩罚视觉捷径。

**3. 主要实验结果**
在 SimplerEnv 和 RoboCasa 基准上的大量实验表明：
- **显著提升泛化能力**：在具有挑战性的 OOD SimplerEnv 基准上，相比强基线（QwenGR00T）实现了 **11.3%** 的绝对性能提升，平均成功率达到了 **66.5%**（SOTA）。
- **打破视觉捷径**：在 RoboCasa 上，BayesianVLA 取得了 **50.4%** 的平均成功率，超越了所有对比基线，并在视觉先验策略失效的任务上表现突出。
- **保留通用能力**：与基线模型在微调后出现文本对话能力退化不同，BayesianVLA 能更好地保留骨干VLM的纯文本推理和对话能力。

**4. 研究意义和价值**
本研究不仅诊断并实证了VLA训练中“视觉捷径”这一根本病理，更从信息论和贝叶斯理论出发，提出了一种结构化的解决方案。BayesianVLA 无需收集新数据，仅通过改进训练目标与架构，即可显著提升模型对语言指令的遵循能力和OOD泛化鲁棒性。这项工作为构建真正理解并可靠执行人类意图的具身智能体提供了新的理论见解和实践框架。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：BayesianVLA

### **一、 核心问题：视觉捷径与信息坍缩**

当前视觉-语言-动作模型在机器人操作中存在一个根本性缺陷：**模型倾向于忽略语言指令，仅依赖视觉观察来预测动作**。论文将这一现象称为 **“视觉捷径”**，其根源在于训练数据的**目标驱动偏差**。

- **问题本质**：在典型的机器人数据集中（如反复执行“打开柜子”），视觉场景 `v` 和语言指令 `ℓ` 之间存在近乎确定性的映射关系（`p(ℓ|v)` 非常尖锐）。这导致**动作 `a` 和指令 `ℓ` 之间的条件互信息 `I(ℓ; a|v)` 趋近于零**，即 **“信息坍缩”**。
- **严重后果**：模型退化为一个纯粹的视觉策略 `p(a|v)`，无法处理**模糊场景**（同一场景对应多个任务）或**分布外**环境，导致泛化能力极差。

### **二、 核心创新：贝叶斯分解与潜在动作查询**

论文提出了 **BayesianVLA** 框架，通过**贝叶斯分解**强制模型遵循语言指令。其创新点是一个**统一的双分支架构**，核心是引入 **“潜在动作查询”**。

#### **1. 理论创新：基于点互信息的目标函数**
- 从贝叶斯公式出发：`π(a|v,ℓ) = [p(ℓ|a, v) * p(a|v)] / p(ℓ|v)`
- **核心目标**：最大化动作 `a` 和指令 `ℓ` 之间的**条件点互信息**。这等价于最大化**对数似然比**：
  ```
  L_LLR = log p(ℓ|a, v) - log p(ℓ|v)
  ```
- **直观解释**：该目标惩罚仅从视觉 `v` 就能预测指令的行为，奖励那些能为指令 `ℓ` 提供**额外解释信息**的动作 `a`。

#### **2. 架构创新：潜在动作查询与双分支训练**
- **潜在动作查询**：在VLM词汇表中引入一组**可学习的令牌** `𝒬`（如64个）。它们作为VLM与下游扩散Transformer动作头之间的**瓶颈接口**。
- **双分支设计**：
  - **先验分支**：输入序列为 `[v, 𝒬, ℓ]`。由于因果掩码，`𝒬` 只能看到视觉 `v`，从而学习**视觉先验策略 `p(a|v)`**。
  - **后验分支**：输入序列为 `[v, ℓ, 𝒬]`。`𝒬` 能看到视觉和语言，从而学习**完整的条件策略 `π(a|v, ℓ)`**。
- **训练目标**：总损失结合了动作预测损失和对数似然比最大化项：
  ```
  L_total = (1-λ)*L_FM(后验) + λ*L_FM(先验) - β*L_LLR
  ```
- **推理效率**：仅使用后验分支，**不增加任何额外计算开销**。

### **三、 解决方案的价值与验证**

#### **1. 实际价值**
- **无需新数据**：直接从存在偏差的现有数据中恢复出真正的语言条件策略。
- **提升泛化能力**：强制模型利用语言信息来消歧，从而在OOD和模糊任务上表现更鲁棒。
- **保留通用能力**：LLR目标作为一种正则化，有助于保持VLM骨干的**文本对话能力**，缓解灾难性遗忘。

#### **2. 实验验证**
论文在**SimplerEnv**和**RoboCasa**基准上进行了广泛验证：
- **显著提升OOD性能**：在SimplerEnv上，相比基线QwenGR00T，**绝对性能提升11.3%**，达到SOTA的66.5%平均成功率。
- **打破视觉捷径**：在RoboCasa上，BayesianVLA（50.4%）超越了视觉仅用基线（44.7%）和标准VLA基线（47.8%），在视觉捷径失效的任务上（如“PnP Novel From Placemat To Plate”）提升显著。
- **消融实验**：证明了**贝叶斯分解目标**（+6.0%）是性能提升的主要来源，而**潜在动作查询**本身也带来了架构上的收益（+2.3%）。

### **四、 总结**

**BayesianVLA 的核心贡献在于**：
1.  **诊断**了一个被忽视但根本的VLA模型病理——由数据偏差引起的“信息坍缩”。
2.  **提出**了一个基于信息论的贝叶斯分解框架，通过最大化动作与指令的互信息来强制指令跟随。
3.  **设计**了一个高效、统一的架构（潜在动作查询+双分支），在不增加推理成本的情况下实现了上述目标。

这项工作为构建真正理解并遵循语言指令的、泛化能力强的机器人策略提供了新的理论视角和实用框架。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决当前视觉-语言-动作（VLA）模型在机器人操作任务中，因训练数据存在目标驱动偏差（即视觉场景与语言指令高度可预测地耦合）而导致的**“视觉捷径”问题**。该问题使得模型退化为忽略语言指令的纯视觉策略，在分布外（OOD）或模糊场景下泛化能力严重不足。为此，论文提出了 **BayesianVLA** 框架，其核心是通过引入**可学习的潜在动作查询**构建一个双分支架构，分别建模视觉先验策略和语言条件后验策略，并优化一个最大化动作与指令之间条件点互信息（PMI）的目标函数，从而强制模型依据语言指令生成动作。实验表明，该方法在不增加新数据的情况下，显著提升了模型在 SimplerEnv 和 RoboCasa 等基准上的泛化性能，例如在具有挑战性的 OOD SimplerEnv 基准上取得了 **11.3%** 的性能提升，有效缓解了视觉捷径问题并保留了基础模型的通用语言能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries》针对当前视觉-语言-动作（VLA）模型存在的核心问题，提出了一套系统性的创新解决方案。其创新点主要体现在**问题诊断、理论框架、方法设计和实际效果**四个层面。

---

### 1. **问题诊断的创新：识别并形式化“信息坍缩”与“视觉捷径”问题**
- **相比以往方法的改进/不同之处**：
    - 以往工作虽然观察到VLA模型在分布外（OOD）泛化能力差，但通常归因于数据量不足、模型容量不够或任务复杂性。本文**首次**从信息论和贝叶斯理论的角度，将问题根源诊断为**数据集偏差导致的条件互信息消失**，即“信息坍缩”。
    - 具体而言，论文指出，在目标驱动的数据集中，语言指令 `ℓ` 可以从视觉观察 `v` 中近乎确定性地预测（`p(ℓ|v)` 非常尖锐），导致动作 `a` 与指令 `ℓ` 之间的条件互信息 `I(ℓ; a|v)` 趋近于零。
- **解决的具体问题/带来的优势**：
    - **精准定位病根**：明确了模型退化成“视觉唯一”策略（`π(a|v, ℓ) ≈ p(a|v)`）的根本原因，而非笼统地归为“过拟合”或“泛化能力差”。
    - **提供理论指导**：为后续的解决方案设计（最大化PMI）提供了坚实的理论依据，使方法创新有的放矢。

### 2. **理论框架的创新：基于贝叶斯分解的条件点互信息最大化目标**
- **相比以往方法的改进/不同之处**：
    - 传统VLA训练直接最大化条件似然 `π(a|v, ℓ)`。本文则引入**贝叶斯分解视角**，将最优策略分解为先验 `p(a|v)` 和似然 `p(ℓ|a, v)`。
    - 基于此，论文提出了一个**全新的训练目标**：最大化动作与指令之间的**条件点互信息**，这等价于最大化对数似然比：`L_LLR = log p(ℓ|a, v) - log p(ℓ|v)`。
- **解决的具体问题/带来的优势**：
    - **强制模型关注语言**：该目标的核心是惩罚“视觉捷径”，奖励那些能为解释语言指令提供**额外信息**的动作。模型无法再仅凭视觉就获得高奖励，必须学会利用语言指令来消除歧义。
    - **从“拟合数据”到“提取信息”**：将训练目标从单纯的动作预测正确，转变为要求动作携带关于指令的增量信息，从而从根本上对抗信息坍缩。

### 3. **方法设计的创新：可学习的潜动作查询与双分支架构**
- **相比以往方法的改进/不同之处**：
    - **潜动作查询**：引入一组可学习的令牌作为VLM和动作解码器（DiT）之间的**瓶颈接口**。这与之前工作（如GR00T, π₀）将全部VLM隐藏状态输入动作头有本质不同。
    - **双分支训练策略**：利用因果掩码，通过简单地调整输入序列中 `[v, ℓ, Q]` 的顺序，**在同一套模型参数上**实例化先验分支和后验分支。
        - **先验分支**：输入为 `[v, Q, ℓ]`，`Q` 只能看到 `v`，用于学习 `p(a|v)`。
        - **后验分支**：输入为 `[v, ℓ, Q]`，`Q` 能看到全部，用于学习 `π(a|v, ℓ)`。
- **解决的具体问题/带来的优势**：
    - **高效实现理论框架**：双分支设计是实现在线估计 `p(a|v)` 和 `π(a|v, ℓ)` 并计算LLR损失的关键，且视觉特征可被两分支共享，计算开销增加有限。
    - **改善计算效率与泛化**：潜动作查询将动作解码器的条件输入从大量视觉语言令牌压缩为固定数量的查询令牌，降低了DiT的计算复杂度。更重要的是，它迫使VLM将任务相关信息**压缩和总结**到查询中，这可能作为一种有益的归纳偏置，提升泛化能力。
    - **推理零开销**：训练时虽为双分支，但推理时仅使用后验分支，与基线模型计算成本完全相同。

### 4. **实际效果的创新：显著提升OOD泛化并保留基础模型能力**
- **相比以往方法的改进/不同之处**：
    - **OOD泛化性能大幅提升**：在极具挑战性的SimplerEnv OOD基准上，相比同架构基线（QwenGR00T）取得了**11.3%** 的绝对性能提升，达到了SOTA水平。这直接验证了方法在解决“视觉捷径”问题上的有效性。
    - **缓解灾难性遗忘**：实验表明，标准的VLA微调会严重损害骨干VLM的文本对话能力（灾难性遗忘）。而BayesianVLA由于LLR目标强制模型保持对语言指令的依赖，**意外地起到了正则化作用**，较好地保留了骨干模型的纯文本推理和对话能力。
- **解决的具体问题/带来的优势**：
    - **实现可靠泛化**：使VLA模型不再依赖于训练集中的虚假视觉关联，能够真正响应语言指令，从而在陌生场景中依然有效。
    - **保护模型核心价值**：保留VLM的通用语言能力具有重要实用价值，意味着智能体在执行操控任务之余，仍能进行高层推理和理解新颖指令，这正是在机器人学中引入基础模型的初衷之一。

### 总结
| 创新点 | 核心改进 | 解决的问题/优势 |
| :--- | :--- | :--- |
| **1. 问题诊断** | 从信息论角度定义“信息坍缩” | 精准定位VLA泛化差的根本原因，为解决方案提供理论基石 |
| **2. 理论框架** | 提出最大化条件点互信息（PMI）的新目标 | 强制模型利用语言指令，从根本上规避视觉捷径 |
| **3. 方法设计** | 潜动作查询 + 双分支架构 | 高效、优雅地实现贝叶斯分解与PMI最大化，推理零开销 |
| **4. 实际效果** | OOD泛化SOTA + 保留语言能力 | 实现模型对指令的鲁棒响应，并保护了基础模型的通用性潜力 |

这些创新点环环相扣，从**发现问题** -> **理论建模** -> **方法实现** -> **效果验证**，形成了一个完整、严谨的研究闭环，显著推进了VLA模型在鲁棒性和泛化性方面的研究。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心评估结论
**BayesianVLA** 通过其提出的贝叶斯分解框架，**显著缓解了VLA模型中的“视觉捷径”问题**，从而在分布外（OOD）泛化和复杂多任务场景中实现了**显著的性能提升**。其核心效果是**强制模型真正依赖语言指令来生成动作**，而非仅从视觉观察中学习虚假关联。

### 二、 使用的数据集
论文在两个主流的机器人操作仿真基准上进行了评估：

1.  **SimplerEnv**：
    *   **训练数据**：使用Open X-Embodiment (OXE) 数据集的两个大规模子集：**BridgeDataV2**（多样化、真实世界场景）和 **Fractal**。
    *   **评估任务**：包含4个具体操作任务：“将勺子放在毛巾上”、“将胡萝卜放在盘子上”、“将绿色方块堆叠在黄色方块上”、“将茄子放入黄色篮子中”。

2.  **RoboCasa**：
    *   **训练数据**：使用 **PhysicalAI-Robotics-GR00T-X-Embodiment-Sim** 数据集中的人形机器人桌面操作子集。
    *   **评估任务**：包含 **24个** 多样化的桌面操作任务，涉及与铰接物体（如橱柜、抽屉）和电器（如微波炉）的复杂交互。

### 三、 评价指标
*   **主要指标**：**任务成功率**。
*   **统计方式**：
    *   **SimplerEnv**：每个任务运行 **480次** 独立试验，报告平均成功率（Avg@480）。
    *   **RoboCasa**：每个任务运行 **50次** 独立试验，报告平均成功率（Avg@50）。

### 四、 对比的基线方法
论文与当前最先进（SOTA）的多种VLA模型进行了广泛对比，主要包括：

*   **基于模仿学习的模型**：RT-1-X, Octo-Small
*   **基于VLM微调的模型**：OpenVLA, OpenVLA-OFT, RoboVLM, CogACT, SpatialVLA, TraceVLA, VideoVLA
*   **基于流匹配（Flow Matching）的模型**：`π0`, `π0.5`
*   **基于GR00T架构的模型**：Isaac-GR00T-N1.6-Bridge
*   **论文复现的直接基线**：**QwenGR00T**（基于相同的StarVLA框架），以及一个关键的诊断性基线——**VisionOnly**（训练时屏蔽语言指令，仅用视觉）。

### 五、 关键性能提升与结论

#### 1. **在SimplerEnv（OOD泛化）上的卓越表现**
*   **主要结果**：BayesianVLA取得了 **66.5%** 的平均成功率，达到SOTA。
*   **与直接基线对比**：相比同架构的QwenGR00T基线（55.2%），实现了 **11.3%** 的绝对性能提升。这证明了性能增益源于**提出的贝叶斯分解方法本身，而非基础架构**。
*   **任务级分析**：在需要精确物体识别和操作的任务上提升尤为显著，例如：
    *   “Put Carrot on Plate”: **+13.8%** (50.0% → 63.8%)
    *   “Put Eggplant in Yellow Basket”: **+15.0%** (64.2% → 79.2%)
*   **结论**：这验证了BayesianVLA能有效打破“视觉捷径”，迫使模型学习动作对语言指令的真实因果依赖，从而在OOD环境中实现鲁棒泛化。

#### 2. **在RoboCasa（复杂多任务）上的稳定提升**
*   **主要结果**：BayesianVLA取得了 **50.4%** 的平均成功率，超越了所有对比基线。
*   **揭示“视觉捷径”问题**：VisionOnly基线在RoboCasa上竟也达到了 **44.7%** 的成功率，仅略低于标准QwenGR00T（47.8%）。这**实证了当前数据集中视觉与任务高度耦合，导致模型严重依赖视觉线索**。
*   **突破性能天花板**：BayesianVLA成功打破了这一瓶颈。在VisionOnly基线表现较差的任务上（如“PnP Novel From Placemat To Plate”），BayesianVLA取得了 **70.0%** 的成功率，显著优于VisionOnly（34.0%）和标准基线（48.0%）。
*   **结论**：LLR目标成功地强制策略从语言中提取并利用任务指定信息，而不是满足于基于视觉捷径的局部最优解。

#### 3. **模型通用能力的保留**
*   **现象**：标准的VLA微调通常会导致VLM骨干网络的**灾难性遗忘**，丧失基本的文本对话和推理能力。
*   **BayesianVLA的效果**：如图4和图5所示，BayesianVLA**显著保留了骨干模型在纯文本输入上的对话和推理能力**，而基线模型则输出无意义的乱码。
*   **原因分析**：LLR目标强制模型建立动作与语言之间的强依赖关系，这作为一种正则化器，维持了指令令牌的功能效用，从而在视觉模态为控制而专门化的同时，保护了骨干网络的文本语言核心。
*   **实际价值**：这确保了智能体保留了进行高级推理和泛化到新指令的潜力，这对于在机器人中应用基础模型至关重要。

#### 4. **消融实验验证**
*   **Latent Action Queries的有效性**：仅添加潜在动作查询（无贝叶斯分解）即可将基线性能从55.2%提升至57.5%，证明了该架构作为高效信息瓶颈的潜力。
*   **贝叶斯分解的核心贡献**：完整的BayesianVLA框架（63.5%）相比仅使用动作查询的版本（57.5%）有 **+6.0%** 的显著提升，**确认了性能提升的核心源于双分支贝叶斯学习目标，而非单纯的架构变化**。

### 总结
论文通过系统的实验设计、与广泛基线的对比以及深入的消融分析，**强有力地证明了BayesianVLA框架的有效性**。它不仅**在关键基准上取得了SOTA性能**，更重要的是**从原理上解决了一个被广泛忽视但至关重要的问题（信息坍塌/视觉捷径）**，并**附带地缓解了VLA微调中的灾难性遗忘问题**，为构建真正理解并遵循语言指令的鲁棒机器人策略提供了新的方向。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.15197v2)
- [HTML 版本](https://arxiv.org/html/2601.15197v2)
