# Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization

**相关性评分**: 8.0/10

**排名**: #10


---


## 基本信息

- **arXiv ID**: [2601.12993v1](https://arxiv.org/abs/2601.12993v1)
- **发布时间**: 2026-01-19T12:20:38Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Hao Luo, Ye Wang, Wanpeng Zhang, Sipeng Zheng, Ziheng Xi, Chaoyi Xu, Haiweng Xu, Haoqi Yuan, Chi Zhang, Yiqing Wang, Yicheng Feng, Zongqing Lu

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

Being-H0.5是一个基于人类交互数据的基础视觉-语言-动作模型，通过统一动作空间和混合流框架实现跨机器人平台的泛化能力。

## 摘要

We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal "mother tongue" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.

## 详细分析

## 论文摘要：Being-H0.5：面向跨具身泛化的人本机器人学习规模化研究

### 1. 研究背景和动机
当前，视觉-语言-动作（VLA）模型在机器人领域展现出巨大潜力，但其发展面临两大核心瓶颈：**数据稀缺**与**具身异构性**。不同机器人平台（如机械臂、灵巧手、人形机器人）在形态、控制接口和动力学上差异巨大，导致现有VLA模型往往成为针对特定硬件的“专家”，难以实现跨平台的知识迁移与泛化。为解决此问题，本文提出一种**人本学习范式**，将人类交互数据视为物理世界的“通用母语”，并引入**Being-H0.5**这一基础VLA模型，旨在实现鲁棒的跨具身泛化。

### 2. 核心方法和技术创新
本文的核心贡献在于构建了一个从数据到模型再到部署的完整规模化体系：
- **超大规模数据集 UniHand-2.0**：构建了迄今最大的具身预训练数据集，包含超过35,000小时的多模态数据，涵盖30种不同的机器人具身、16,000小时的人类手部运动视频以及视觉-文本理解数据。
- **统一动作空间与序列建模**：提出了一个**统一状态-动作空间**，将人类手部运动和异构机器人控制信号映射到语义对齐的“槽位”中。在此基础上，将所有异构监督（视觉、文本、状态、动作）统一到一个序列建模框架下，使模型能够在一个框架内**感知、描述和行动**。
- **创新的模型架构**：采用**混合专家（MoT）** 设计，并引入了**混合流（Mixture of Flow, MoF）** 框架，将共享的运动基元与特定于具身的专家解耦，以提升模型容量和跨域迁移能力。
- **鲁棒的部署技术**：提出了**流形保持门控（MPG）** 以应对感知分布偏移，以及**通用异步分块（UAC）** 协议来统一处理不同延迟和控制频率的机器人平台，确保单一模型检查点能在真实世界中稳定、实时地控制多种机器人。

### 3. 主要实验结果
Being-H0.5在仿真和真实机器人实验中均取得了领先的性能：
- **仿真基准测试**：在LIBERO基准上达到**98.9%** 的成功率，在RoboCasa基准上达到**53.9%** 的成功率，均创下新纪录，且仅使用低分辨率RGB输入。
- **真实机器人跨平台部署**：将**单一**模型检查点成功部署到PND Adam-U、Franka+Inspire、Unitree G1等五种形态各异的机器人平台上，完成了空间排列、长时程、双手操作等多样化任务，证明了其卓越的跨具身泛化能力。
- **涌现的零样本迁移**：模型在未经目标机器人数据训练的情况下，对未见过的任务-具身组合表现出非零的成功率，显示了其从其他具身学习任务结构并迁移的能力。

### 4. 研究意义和价值
本研究标志着机器人学习向**通用、可扩展的具身智能**迈出了关键一步。其价值体现在：
- **范式创新**：成功验证了以人类数据作为通用物理先验来桥接异构机器人平台的可行性，为缓解机器人数据稀缺问题提供了新路径。
- **技术突破**：提出的统一动作空间、混合流架构及鲁棒部署技术，为构建大规模、可迁移的机器人基础模型提供了重要的架构和工程范例。
- **实用价值**：通过单一模型控制多种机器人的能力，极大地降低了为不同硬件开发和部署专用模型的成本与复杂性，为实现通用机器人智能奠定了坚实基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Being-H0.5

### **一、 核心要解决的问题**
论文旨在解决**机器人通用智能（General-Purpose Robotics）** 中的一个根本性挑战：**跨具身泛化**。具体表现为：
1.  **形态异构性**：不同机器人平台（如单臂、双臂、人形、灵巧手）具有截然不同的运动学结构、自由度、控制接口和延迟特性，导致为一个平台训练的模型无法直接用于另一个。
2.  **数据稀缺性**：针对特定机器人的高质量演示数据收集成本极高，规模有限，远不及自然语言处理等领域的数据规模，限制了模型的泛化能力。
3.  **“物理鸿沟”**：现有视觉-语言-动作模型通常是“单语者”，高度依赖特定硬件的数据，缺乏对底层物理交互常识的理解，导致在跨平台部署时产生不稳定、不可行的动作轨迹。

### **二、 核心创新点**
论文提出了一套完整的、以**人为中心**的解决方案，其创新是系统性的，涵盖数据、模型架构、训练范式和部署基础设施。

#### **1. 数据层面的创新：UniHand-2.0 大规模预训练配方**
- **规模与多样性**：构建了迄今为止最大的具身预训练数据集，包含 **35,000+ 小时**的多模态数据（16K小时人类视频、14K小时机器人数据、5K小时视觉-文本数据），覆盖**30种不同的机器人具身**。
- **核心理念**：将**人类手部运动**视为物理交互的“母语”和通用模板。人类视频提供了密集的、可转移的**行为先验**和物理常识，弥补了机器人数据的稀缺性。
- **数据收集系统**：开发了 **UniCraftor** 系统，用于便携、可扩展的高质量数据采集，提供原生深度、高精度外参和硬件同步的交互事件标注。

#### **2. 模型与算法层面的创新**
- **统一状态-动作空间**：
    - 提出一个**物理可解释的统一向量空间**，将人类手部轨迹（MANO参数）和异构机器人控制信号映射到语义对齐的“槽位”中。
    - 作用：充当不同硬件的“通用语法”，将功能意图（如“抓取”）与机械关节解耦，使模型学习底层交互物理而非平台特定命令。
- **混合流专家**：
    - 在混合Transformer架构中，为动作生成专家引入 **Mixture-of-Flow** 设计。
    - **基础专家层**：编码跨平台共享的通用运动基元（如接近、抓取动力学）。
    - **专用专家层**：通过可学习路由网络，动态激活针对特定具身或任务的专家，实现高效的专业化而不线性增加计算开销。
- **统一序列建模**：
    - 将**所有异构监督**（视觉、文本、状态、动作）序列化为单一的多模态token流。
    - 使模型能够在**同一个框架内感知、描述和行动**，通过统一的优化目标进行规模化预训练。
- **混合人类运动表征**：
    - 同时使用**连续流匹配**和**离散掩码运动token预测**来监督动作。
    - **连续流匹配**：确保高保真、平滑的动作生成。
    - **离散掩码预测**：学习稳定的、语言化的运动“语法”，过滤执行噪声，增强行为先验的鲁棒性。

#### **3. 后训练与部署层面的创新**
- **流形保持门控**：
    - 当感知上下文（如视觉特征）因分布偏移而不可靠时，MPG通过计算特征与动作先验的切片Wasserstein距离，生成一个门控值。
    - **机制**：门控值**缩放特征依赖的残差路径**，同时保留一个**无门控的偏置先验**作为稳定回退。这防止了在模糊感知下不稳定的修正被放大，确保了动作生成的平滑性和鲁棒性。
- **通用异步分块**：
    - 将实时分块控制扩展到**跨具身**场景。模型在训练时模拟不同平台特定的推理延迟和控制周期，学习生成连贯的动作后缀。
    - **部署协议**：采用**双线程环形缓冲区架构**，将推理与执行解耦，确保单个检查点能在具有不同延迟特性的异构机器人上实现流畅、连续的实时控制。
- **具身特定适配**：
    - 在统一动作空间的基础上，仅更新与目标具身活跃“槽位”相关的轻量级适配器参数，实现高效微调，同时保持共享主干网络的稳定性。

### **三、 解决方案的路径总结**
论文的解决路径可以概括为：**“用人类数据教通用物理，用统一接口对齐异构机器人，用创新架构保证稳定执行”**。

1.  **数据奠基**：利用海量、多样的人类交互视频（UniHand-2.0）为模型注入丰富的物理交互常识和行为先验。
2.  **表示统一**：通过**统一状态-动作空间**，为人类和所有机器人建立共同的“物理语言”，实现知识在语义层面的对齐与迁移。
3.  **架构支撑**：采用**MoT + MoF** 架构，分离理解与执行，并扩展动作专家的容量与专业性，以应对复杂、异构的控制任务。
4.  **鲁棒化与工程化**：通过 **MPG** 和 **UAC** 等后训练技术，解决实际部署中的感知分布偏移和实时性挑战，使理论模型能够稳定运行在真实的多样化机器人上。

### **四、 实际价值与验证**
- **卓越的性能**：在仿真基准测试（LIBERO: **98.9%**, RoboCasa: **53.9%**）和五个真实机器人平台（PND Adam-U, Franka+Inspire等）上实现了最先进或极具竞争力的结果。
- **真正的通用性**：**单个检查点**即可控制形态各异的机器人完成多样任务，证明了强大的跨具身泛化能力。
- **涌现的零样本迁移**：观察到了**具身级别的零样本任务完成**现象，即模型能将从未在目标机器人上演示过的任务结构进行实例化，这为通过增加数据多样性来提升智能体涌现能力指明了方向。
- **开源与可复现**：承诺开源模型权重、训练流水线和部署基础设施，推动社区发展。

**结论**：Being-H0.5 不仅仅是一个新的VLA模型，它代表了一种**范式转变**——从为特定硬件收集数据，转向以人类为中心的、旨在学习可转移物理常识的规模化学习范式。其系统性创新为解决机器人通用智能的规模化难题提供了切实可行的技术路线和强有力的实证支撑。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决机器人视觉-语言-动作（VLA）模型在**跨形态（Cross-Embodiment）泛化**方面的核心难题，即单一模型难以适配不同机械结构、控制接口的多样化机器人平台。为此，论文提出了一个**以人为中心（Human-Centric）** 的学习范式，其核心是构建了超大规模数据集 **UniHand-2.0**（超过3.5万小时）作为“物理世界的通用语料”，并设计了**统一动作空间**将人类手部运动与异构机器人控制信号映射到语义对齐的表示中。在此基础上，论文提出了 **Being-H0.5** 基础模型，它采用**混合流（Mixture-of-Flow）** 架构来解耦共享运动基元与特定形态专家，并引入了**流形保持门控（MPG）** 和**通用异步分块（UAC）** 等技术来确保实际部署的鲁棒性与实时性。最终，该模型在LIBERO（98.9%）和RoboCasa（53.9%）等模拟基准上取得了最先进的结果，并成功用一个检查点控制了五种物理形态迥异的真实机器人，完成了多样任务，甚至观察到了**跨形态的零样本任务迁移**现象，证明了其强大的泛化能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Being-H0.5》的核心创新点分析

这篇论文提出了一种名为**Being-H0.5**的视觉-语言-动作基础模型，旨在实现跨具身（Cross-Embodiment）的通用机器人控制。其核心创新点可以归纳为以下五个方面，每一项都针对现有VLA模型的瓶颈问题提出了系统性的解决方案。

---

### 1. **数据层面：构建超大规模、以人为中心的预训练数据集 UniHand-2.0**
*   **改进/不同之处**：
    *   **规模与多样性**：提出了迄今为止最大的具身预训练数据集，包含**超过35,000小时**的多模态数据（16,000小时人类手部视频、14,000小时机器人数据、5,000小时视觉-语言理解数据），覆盖**30种不同的机器人平台**。这比其前作（Being-H0）和当前主流数据集（如Open X-Embodiment）在规模上提升了**200倍**，在机器人形态多样性上也是最多的。
    *   **以人为中心**：将**人类手部运动数据**视为物理交互的“母语”和通用模板，而不仅仅是机器人数据的补充。通过大规模、多样化的第一人称人类视频，为模型注入了丰富的物理常识和交互先验。
*   **解决的问题/带来的优势**：
    *   **解决数据稀缺与形态单一问题**：传统机器人数据收集成本高昂、场景单一，且缺乏灵巧手数据。利用易于获取的人类视频，低成本地提供了海量的、涵盖真实世界复杂场景的交互数据。
    *   **提供可迁移的物理先验**：人类手部运动蕴含了通用的交互逻辑（如抓握、操作、因果推理），这些先验知识可以帮助模型快速适应新的、数据稀缺的机器人硬件，实现了从“高资源”形态（人类）向“低资源”形态（新机器人）的知识迁移。
    *   **平衡模态**：专门引入大规模视觉-语言理解数据，防止模型在预训练中因视觉-动作数据过多而“遗忘”高级语义推理和指令跟随能力。

### 2. **表示层面：提出统一的动作空间与序列建模范式**
*   **改进/不同之处**：
    *   **统一动作空间**：设计了一个**物理语义对齐的统一状态-动作空间**。它将人类手部轨迹（MANO参数）和不同机器人的异构控制信号（关节角、末端位姿、夹爪开合等）映射到同一个高维向量的不同“语义槽”中。所有动作都使用统一的物理单位（如弧度、米）和表示（如轴角法表示旋转）。
    *   **统一序列建模**：将所有异构数据（视觉、文本、状态、动作）序列化为一个单一的、多模态的token流。模型在一个框架内，通过统一的**问答格式**进行训练，使其能够**感知、描述和行动**。
*   **解决的问题/带来的优势**：
    *   **解决异构性冲突**：传统方法为不同机器人使用独立的动作头，导致模型参数割裂，无法学习跨平台的共享物理知识。统一空间充当了不同硬件间的“通用语法”，让模型学习底层的交互物理，而非特定硬件的控制命令。
    *   **实现高效预训练**：统一的序列化方法允许将海量、异构的数据源（人类视频、机器人轨迹、VQA数据）在一个连贯的课程中联合优化，极大简化了训练流程，并促进了跨模态的知识融合。
    *   **支持零样本迁移**：这种统一的表示是模型能够实现**跨具身零样本任务迁移**（即用在一个机器人上学到的任务结构，去控制另一个从未见过该任务的机器人）的关键基础。

### 3. **架构层面：引入混合流专家与解耦设计**
*   **改进/不同之处**：
    *   **混合流专家**：在动作生成专家中引入了 **“混合流”** 架构。它将动作专家分解为**共享的基础层**（编码通用运动基元，如接近、抓取）和**由路由网络激活的专用专家层**（处理特定形态或任务的精细动力学）。
    *   **解耦的混合Transformer**：整体采用**混合Transformer**架构，明确分离了**多模态理解专家**（负责高级语义推理和规划）和**动作生成专家**（负责低级运动控制），二者通过共享的注意力机制耦合。
*   **解决的问题/带来的优势**：
    *   **解决模型容量与泛化矛盾**：传统单一动作专家容量有限，难以同时掌握众多异构机器人的复杂技能。MoF通过稀疏激活机制，在不显著增加计算开销的前提下，大幅提升了模型容纳多样化技能的能力。
    *   **提升效率与可部署性**：MoF实现了总参数量与激活参数量的解耦。在推理时，只有少数相关专家被激活，这使得庞大的模型可以高效地部署在资源受限的边缘设备（如NVIDIA Orin）上。
    *   **明确功能分工**：理解与动作专家的解耦，使模型既能进行复杂的语言推理和长时程规划，又能生成高保真、平滑的运动轨迹，避免了单一网络在两者间的权衡。

### 4. **训练与部署层面：提出面向鲁棒性的后训练与实时控制技术**
*   **改进/不同之处**：
    *   **流形保持门控**：提出**MPG**，用于在感知上下文（如视觉特征）不可靠时（如光照变化、遮挡），动态降低其对动作生成的影响，并回退到一个稳健的、学习到的先验偏移上。这通过计算观测特征与动作先验锚点之间的**切片Wasserstein距离**来实现门控。
    *   **通用异步分块控制**：提出**UAC**，将训练时实时分块控制技术推广到**跨具身**场景。它根据每个机器人的特定控制频率和推理延迟，自适应地确定动作块的“已提交前缀”和“待预测后缀”，确保在不同延迟特性的硬件上都能实现平滑、连续的控制。
    *   **具身特定适配**：在统一动作空间的基础上，采用**槽位适配器**，仅更新与目标机器人相关的动作槽参数，实现高效、无冲突的跨平台微调。
*   **解决的问题/带来的优势**：
    *   **解决分布偏移与不稳定性**：MPG显著增强了模型在真实世界感知条件变化下的鲁棒性，防止因视觉噪声导致动作抖动或轨迹漂移，这是扩散/流匹配类VLA模型在实际部署中的关键挑战。
    *   **解决异构硬件实时控制难题**：UAC使得**同一个模型检查点**能够无缝适配从10Hz桌面机械臂到50Hz人形机器人的各种控制周期和延迟，实现了真正的跨平台实时部署。
    *   **实现高效精准适配**：ESA在保持主干网络稳定的同时，实现了对特定机器人动力学的快速、精准适应，避免了全参数微调可能导致的灾难性遗忘。

### 5. **系统层面：开发了端到端的、可扩展的数据收集与部署基础设施**
*   **改进/不同之处**：
    *   **UniCraftor数据收集系统**：设计了一个模块化、可扩展的系统，集成了**原生深度感知**、基于AprilTag的**高精度外参标定**和**硬件同步的交互事件记录**（如脚踏板），能够采集高质量、多视角、时间精确对齐的多模态人类演示数据。
    *   **双线程实时部署架构**：实现了一个解耦推理线程和控制线程的环形缓冲区架构，结合MPG和UAC，为多样化的机器人平台提供了低延迟、高鲁棒性的实时控制流水线。
*   **解决的问题/带来的优势**：
    *   **解决高质量人类数据获取难问题**：现有开源人类视频数据集缺乏深度、精确外参和细粒度时间标注。UniCraftor系统能生产研究级的高质量数据，为以人为中心的学习范式提供了可靠的数据基础。
    *   **验证了工业级部署可行性**：完整的软硬件基础设施表明，Being-H0.5不仅是一个学术模型，更是一套可实际部署到多种真实机器人上完成复杂任务的解决方案，推动了VLA模型从实验室走向实际应用。

---

**总结**：Being-H0.5的创新是一个**系统性工程**，它从**数据规模与范式**、**统一表示**、**可扩展架构**、**鲁棒性算法**到**部署基础设施**进行了全栈创新。其核心思想是**以人类交互为通用先验，通过统一的“物理语言”桥接异构机器人，并利用先进的模型设计和训练技术确保其在真实世界中的泛化能力和稳定性**。最终，**单个模型检查点**在模拟基准（LIBERO, RoboCasa）和五个真实机器人平台上均达到了领先性能，并首次展示了跨具身的零样本任务迁移能力，为通用机器人智能的规模化发展指明了方向。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

论文通过**大规模仿真基准测试**和**多平台真实机器人部署**，全面评估了Being-H0.5模型的性能，证明了其在**跨具身泛化**和**通用机器人控制**方面的卓越能力。

### 一、 使用的数据集与评价指标

#### 1. 主要数据集
*   **UniHand-2.0 (自建)**：核心预训练数据集，包含**35,000+小时**的多模态数据，分为：
    *   **人类演示数据**：16,000小时，来自Ego4D、EPIC-KITCHENS等公开数据集及自采数据。
    *   **机器人操作数据**：14,000小时，覆盖**30种不同的机器人平台**（如Franka、Unitree G1等），整合了Open X-Embodiment、AgiBot-World等多个来源。
    *   **视觉-文本理解数据**：5,000等效小时，用于保持模型的高层推理和指令跟随能力。
*   **仿真基准数据集**：
    *   **LIBERO**：用于评估知识迁移和终身学习能力的标准基准，包含空间、物体、目标和长时序四类任务。
    *   **RoboCasa**：用于评估长时序、多样化家庭任务（如拾放、开关抽屉）的基准，采用具有挑战性的 **`Human-50`** 少样本设置。

#### 2. 评价指标
*   **核心指标**：**任务成功率 (%)**。在仿真和真实实验中，均通过多次 rollout 计算平均成功率。
*   **辅助指标**：
    *   **Mean Wrist Displacement Similarity (MWDS)**：在人类数据消融实验中，用于衡量模型预测的腕部位移与真实意图的对齐程度。
    *   **零样本迁移成功率**：在真实机器人实验中，评估单一检查点在**未见过的任务-机器人组合**上的表现。

### 二、 对比的基线方法

论文与当前最先进的视觉-语言-动作模型及专用策略进行了广泛对比：

1.  **仿真基准对比**：
    *   **通用VLAs**：`π0`, `π0.5`, GR00T-N1, OpenVLA, InternVLA-M1, X-VLA, EO1, F1 等。
    *   **专用策略**：Diffusion Policy, 3D Diffuser Actor (3DA), DP3, GWM 等。

2.  **真实机器人对比**：
    *   **`π0.5`**：作为主要的开源VLA竞争对手，在**专家模式**下进行对比（因其原生不支持跨具身通用模式）。
    *   **消融模型**：
        *   **Being-H0.5-specialist**：在特定机器人数据上微调的专家模型。
        *   **Being-H0.5-generalist**：在**所有五个机器人平台数据上联合后训练**的单一通用检查点。
        *   **Being-H0.5-scratch**：**不使用UniHand-2.0预训练**的模型，用于隔离大规模预训练的价值。

### 三、 关键性能提升与结论

#### 1. 仿真基准结果：达到新的最先进水平
*   **LIBERO**：
    *   **Being-H0.5-specialist** 取得了 **98.9%** 的平均成功率，在LIBERO-Long（复杂多步推理）上达到 **97.4%**，**全面领先**所有对比方法。
    *   **Being-H0.5-generalist**（单一检查点）也达到了 **97.6%**，性能损失极小，证明了其强大的跨任务泛化能力。
*   **RoboCasa**：
    *   **Being-H0.5-specialist** 取得了 **53.9%** 的总平均成功率，**创下新的SOTA**。
    *   **Being-H0.5-generalist** 达到 **53.3%**，与专家模型几乎持平。
    *   **关键提升**：在**仅使用224x224 RGB输入**（无深度/点云）的情况下，在“拾放”任务上达到 **36-40%** 的成功率，显著超越了使用多模态输入的基线方法（如 `π0.5` 的21.5%），证明了人类中心学习带来的强大空间先验。

#### 2. 真实机器人结果：单一模型控制五种异构机器人
在五个形态各异的真实机器人平台（PND Adam-U, Unitree G1, FR3+Inspire Hand, BeingBeyond D1, LeRobot SO-101）上进行了10个任务的评估：
*   **通用 vs. 专家**：**Being-H0.5-generalist**（单一检查点）的性能与为每个机器人单独训练的 **Being-H0.5-specialist** **非常接近**，甚至在部分重叠任务上表现更优。这表明联合训练能提升对布局变化、遮挡等干扰的鲁棒性。
*   **显著超越基线**：Being-H0.5的两个变体均**大幅优于** `π0.5`，在**长时序**和**双手协调**任务上优势尤为明显。
*   **预训练的关键作用**：`scratch` 版本的通用模型性能显著下降，证明**UniHand-2.0的大规模人类中心预训练**是获得可迁移操作先验、实现稳定跨具身泛化的**必要条件**，而非简单的“更多数据”。
*   **涌现的零样本迁移**：一个**关键且新颖的发现**是，经过跨具身后训练的 **Being-H0.5-generalist** 检查点，在**完全没有目标机器人演示数据**的情况下，能在新机器人（如Adam-U）上对未见过的任务（如“翻转扫描”、“抽屉操作”）表现出**非零的成功率**和任务一致的多步执行。这为通过增加数据多样性来实现**组合式泛化**指明了方向。

#### 3. 消融研究结论
*   **人类中心学习**：在少样本（5-shot）适应设置下，使用预训练权重的模型相比从原生VLM初始化的模型，平均有 **+4.6% 到 +25.8%** 的性能提升，尤其在长时序任务上受益最大。
*   **掩码运动令牌预测**：移除该目标会导致在野外人类视频数据上的腕部位移相似性（MWDS）下降，证明其有助于学习**稳定、抽象的行为先验**。
*   **部署机制（MPG+UAC）**：移除流形保持门控和通用异步分块会导致**长时序和双手任务性能显著下降**，验证了它们对于处理感知分布偏移和硬件异构延迟至关重要。

### 总结
论文通过系统性的实验证明，**Being-H0.5不仅在主流仿真基准上达到了最先进的性能，更重要的是，其单一模型检查点能实际部署到五种形态迥异的真实机器人上，完成复杂的空间推理、长时序和双手任务，并首次观察到了跨具身的零样本任务迁移现象。** 这标志着向**真正通用、可移植的机器人基础模型**迈出了实质性的一步。所有评估均给出了明确的定量结果（成功率），支撑了其核心论点。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.12993v1)
- [HTML 版本](https://arxiv.org/html/2601.12993v1)
