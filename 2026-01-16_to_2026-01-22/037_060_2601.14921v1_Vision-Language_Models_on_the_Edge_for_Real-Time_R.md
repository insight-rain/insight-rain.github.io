# Vision-Language Models on the Edge for Real-Time Robotic Perception

**相关性评分**: 6.0/10

**排名**: #37


---


## 基本信息

- **arXiv ID**: [2601.14921v1](https://arxiv.org/abs/2601.14921v1)
- **发布时间**: 2026-01-21T12:09:48Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Sarat Ahmad, Maryam Hafeez, Syed Ali Raza Zaidi

## 关键词

Vision-Language-Action Model, VLA for Robotics, Edge Deployment, fine tune

## 一句话总结

这篇论文研究了在边缘计算环境下部署视觉语言模型（VLMs）以实现实时机器人感知，通过对比云端和边缘部署的性能，探讨了延迟、准确性和资源约束的权衡。

## 摘要

Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.

## 详细分析

## 论文详细摘要

**1. 研究背景和动机**
随着视觉-语言模型（VLMs）在机器人感知与交互中展现出强大的多模态推理能力，其在实际部署中面临延迟高、机载资源有限以及云卸载带来的隐私风险等挑战。6G网络中的边缘智能，特别是开放无线接入网（ORAN）和多接入边缘计算（MEC），通过将计算靠近数据源，为解决这些问题提供了新路径。然而，关于在真实机器人系统中将大型VLM部署于ORAN/MEC边缘基础设施的性能表现，尚缺乏实证研究。

**2. 核心方法和技术创新**
本研究设计并实现了一个面向实时机器人感知的边缘VLM部署系统。核心创新点包括：
- **系统集成**：以Unitree G1人形机器人为实体测试平台，构建了一个基于WebRTC的实时多模态数据流管道，将机器人传感器数据流式传输至边缘节点。
- **模型部署与对比**：在边缘节点上部署了大型模型**LLaMA-3.2-11B-Vision-Instruct**（应用4位量化以适配资源）和轻量级模型**Qwen2-VL-2B-Instruct**，并与相同的云部署（NVIDIA NIM API）进行对比。
- **评估框架**：结合标准化机器人视觉问答基准**Robo2VLM**和自建的机器人实时交互数据集，系统评估了不同部署方案下的**任务准确率**和**端到端延迟**。

**3. 主要实验结果**
- **精度与延迟权衡**：边缘部署的LLaMA-3.2在保持与云部署相近的准确率（约41%）的同时，将端到端延迟降低了约5%（从1685.20 ms降至1600.03 ms）。轻量级模型Qwen2-VL-2B实现了亚秒级响应（延迟降低超50%），但准确率显著下降至28.02%。
- **瓶颈分析**：延迟分解显示，**自回归文本生成**是推理过程的主要瓶颈，占总延迟的85%以上。
- **任务类别表现**：大型模型在各任务领域（如空间关系、社交导航）表现均衡，而轻量级模型在感知类任务（如人员检测）上表现尚可，在复杂推理任务上则显不足。

**4. 研究意义和价值**
本研究通过真实的系统集成与实证评估，首次量化了在ORAN/MEC边缘部署VLM用于机器人感知的可行性及性能权衡。其价值在于：
- **实践指导**：明确了“大型模型追求性能最优，轻量模型追求效率最优”的帕累托前沿，为不同延迟和精度要求的机器人应用场景提供了模型选型与部署策略依据。
- **系统优化方向**：指出未来优化应聚焦于缓解自回归解码瓶颈，例如通过推测解码、模型蒸馏等技术。
- **边缘计算优势**：论证了边缘智能在降低延迟、节省带宽、保护数据隐私方面的系统级优势，为6G时代实现可靠、实时的人机交互与自主机器人系统提供了关键技术支撑。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **一、 研究问题**
论文旨在解决**将大型视觉-语言模型应用于实时机器人感知时面临的核心挑战**：
1.  **高延迟问题**：云端推理因数据传输导致延迟过高，无法满足机器人交互所需的毫秒到秒级响应。
2.  **资源限制**：机器人本体计算、存储和功耗有限，难以部署大型VLM。
3.  **隐私与带宽风险**：将原始视频、音频等敏感多模态数据流传输至云端，存在隐私泄露风险并消耗大量带宽。
4.  **缺乏实证研究**：现有研究多在仿真或虚拟化环境中进行，缺乏在真实机器人系统和**6G边缘基础设施**上的实证评估，对延迟-精度权衡的理解不足。

### **二、 核心创新点**
1.  **系统集成创新**：首次设计并实现了**在真实的ORAN/MEC边缘节点上部署大型VLM的实时机器人感知全栈系统**。这并非单纯的算法改进，而是一个涵盖机器人、通信、边缘计算和模型的完整系统工程。
2.  **实证评估方法创新**：采用了**双重数据集评估策略**：
    - **标准化基准**：使用Robo2VLM基准测试，保证结果的可比性和可复现性。
    - **真实机器人采集数据集**：使用Unitree G1人形机器人在实验室环境中收集200个Q&A对，涵盖社交导航、手势识别等**人机交互场景**，使评估更贴近实际应用。
3.  **量化对比与深度分析**：对“边缘 vs. 云端”部署以及“大模型 vs. 轻量模型”进行了**系统性的量化对比**，不仅报告精度和延迟，还深入分析了：
    - **延迟构成**：发现文本生成（自回归解码）是主要瓶颈（占85%以上）。
    - **任务类别表现**：揭示了不同模型在不同交互任务（如感知 vs. 推理）上的优势差异。
    - **帕累托前沿**：明确指出了大模型（LLaMA）是“性能最优”，轻量模型（Qwen2）是“效率最优”，为系统设计提供了清晰的权衡依据。

### **三、 解决方案**
论文通过一个**三层系统架构**来解决问题：

1.  **机器人感知层**：
    - **硬件平台**：采用Unitree G1人形机器人作为具身化数据源，配备RealSense深度相机和麦克风阵列。
    - **功能**：采集RGB-D视频和语音，并通过ASR/TTS模块实现自然语音交互。

2.  **通信与控制层**：
    - **技术核心**：采用**WebRTC**框架建立实时通信管道。
    - **优势**：利用其UDP传输、自适应码率和低延迟特性，通过**数据通道**传输文本和控制信令，通过**视频轨道**传输实时视频流，有效降低了端到端延迟。

3.  **边缘推理层**（**核心解决方案**）：
    - **基础设施**：将VLM部署在**ORAN/MEC边缘节点**（配备NVIDIA L4 GPU），使计算紧靠数据源。
    - **模型策略**：
        - **大型模型**：部署**LLaMA-3.2-11B-Vision-Instruct**，并应用**4-bit NF4量化**以适配边缘资源，在保持精度的同时减少内存占用。
        - **轻量模型**：同时部署**Qwen2-VL-2B-Instruct**，作为低延迟、高效率的对比方案。
    - **服务化**：通过**FastAPI**构建推理服务，实现标准化的图像预处理、编码、生成和解码流程。

### **四、 实际价值与结论**
- **性能验证**：实验证明，边缘部署LLaMA-3.2能在**保持接近云端精度**的同时，**降低5%的端到端延迟**。轻量模型Qwen2能实现**亚秒级响应**（延迟降低一半以上），但精度有所下降。
- **系统级优势**：除了延迟优化，边缘部署**减少了带宽消耗**，并因数据在本地处理而**显著提升了隐私安全性**，这对于机器人（尤其是社交、家庭场景）至关重要。
- **指导意义**：研究为实时机器人系统的VLM部署提供了明确的决策框架：
    - **追求高性能推理**：选择大型模型边缘部署。
    - **追求极致响应速度**：选择轻量模型。
    - **未来方向**：提出**自适应混合部署策略**——轻量模型处理快速感知，大型模型按需调用进行复杂推理，以及通过**推测解码**等技术优化自回归解码瓶颈。

**总结**：该论文的核心创新在于**通过构建一个真实的、基于6G边缘计算（ORAN/MEC）的机器人-VLM集成系统，并进行了严谨的实证分析，量化了边缘部署的价值与模型选择的权衡，为下一代低延迟、高隐私、具身智能机器人系统提供了可行的工程路径和设计指南。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**将大型视觉语言模型（VLM）部署于实时机器人系统中时，因云端推理导致的延迟、带宽消耗和隐私风险等核心问题**。为此，研究提出了一个**基于6G ORAN/MEC边缘计算架构的系统框架**，该框架集成了Unitree G1人形机器人作为数据源，并利用WebRTC构建低延迟通信管道，将VLM（如LLaMA-3.2-11B-Vision-Instruct和轻量级Qwen2-VL-2B-Instruct）部署在边缘节点进行推理。最终实验表明，**边缘部署在保持与云端相近的推理准确性的同时，能有效降低端到端延迟（如LLaMA-3.2降低约5%），而轻量级模型更能实现亚秒级响应，但会牺牲部分准确性**，从而清晰揭示了在机器人实时感知任务中延迟与准确性之间的权衡关系。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文在“边缘部署视觉语言模型（VLM）用于实时机器人感知”这一领域，相对于已有工作提出了以下明确的创新点：

---

### 1. **在真实的ORAN/MEC边缘基础设施上进行实证系统集成与评估**
- **相比以往方法的改进/不同之处：**
    - **以往方法：** 多数相关研究要么在**模拟环境**中进行，要么使用**虚拟化的边缘节点**，或者仅评估轻量级模型，缺乏在真实、物理的6G边缘网络架构（如ORAN/MEC）上部署**大规模VLM**并进行端到端系统评估的实证研究。
    - **本文方法：** 论文设计并实现了一个完整的实时机器人感知系统，将**大规模VLM（LLaMA-3.2-11B）** 实际部署在**真实的ORAN/MEC边缘节点**（配备NVIDIA L4 GPU）上，并以**Unitree G1人形机器人**作为物理测试平台。
- **解决的具体问题/带来的优势：**
    - **解决了“仿真与现实差距”问题：** 提供了在真实边缘硬件和真实机器人交互场景下的**第一手性能数据**（如延迟、准确性），这些数据比仿真结果更具说服力和实际参考价值。
    - **验证了技术可行性：** 首次实证证明了在资源受限的边缘节点上部署和运行十亿参数级别VLM的**可行性**，为6G边缘智能支持高级多模态AI的应用铺平了道路。

### 2. **在统一框架下系统性地量化“部署位置”与“模型规模”对性能的影响**
- **相比以往方法的改进/不同之处：**
    - **以往方法：** 现有工作通常孤立地评估云部署或边缘部署，或者只比较不同模型，缺乏一个**控制变量的系统性对比**，以清晰剥离“部署位置”（云 vs. 边）和“模型架构”（大规模 vs. 轻量级）各自对性能（延迟、精度）的影响。
    - **本文方法：** 论文设计了严谨的对比实验：
        1. **同模型不同位置：** 将**同一个LLaMA-3.2模型**分别在边缘和云（NVIDIA NIM API）部署，使用相同的流水线和数据集进行评测。
        2. **同位置不同模型：** 在**同一个边缘节点**上，对比大规模模型（LLaMA-3.2-11B）和轻量级模型（Qwen2-VL-2B）。
- **解决的具体问题/带来的优势：**
    - **提供了明确的决策依据：** 通过数据清晰地揭示了两种维度的权衡：
        - **边缘 vs. 云：** 边缘部署在保持近乎云端精度的同时，实现了**约5%的端到端延迟降低**，并附带减少了带宽消耗和隐私风险。
        - **大模型 vs. 小模型：** 明确了 **“性能最优”**（LLaMA，高精度）和 **“效率最优”**（Qwen2，亚秒级响应）的帕累托边界，帮助系统设计者根据实时性和准确性需求选择模型。
    - **揭示了瓶颈所在：** 通过延迟分解，发现**自回归文本生成**是主要瓶颈（占85%以上），而非图像预处理，为未来优化指明了方向。

### 3. **构建并利用机器人实时交互数据集进行面向HRI的评估**
- **相比以往方法的改进/不同之处：**
    - **以往方法：** 评估多依赖于现有的静态基准数据集（如VQA数据集），这些数据集可能无法完全反映**实时、动态、具身的机器人交互场景**中的挑战（如光线变化、杂乱背景、即时语音查询）。
    - **本文方法：** 论文不仅使用了标准的机器人基准（Robo2VLM），还**自主构建了一个包含200个Q&A对的机器人采集数据集**。该数据集聚焦于**人机交互（HRI）领域**，涵盖社交导航、手势识别、人类存在检测等任务，数据来自G1机器人在真实实验室环境中的实时交互。
- **解决的具体问题/带来的优势：**
    - **评估更具现实相关性：** 该数据集能够评估VLM在**真实HRI任务**中的表现，弥补了标准基准在交互性和场景多样性上的不足。
    - **支持精确的端到端延迟测量：** 由于是实时交互采集，可以精确打点测量从用户查询开始到机器人给出响应之间的**真实端到端延迟**，这是评估交互式系统可用性和安全性的关键指标。
    - **揭示了模型的能力边界：** 通过分任务类别（如图4）的精度分析，发现轻量级模型在感知类任务上表现尚可，但在需要深度推理（如空间关系、社交导航）的任务上明显落后，这为**自适应混合部署策略**提供了实证依据。

### 4. **提出并实践了面向实时机器人交互的完整边缘系统架构**
- **相比以往方法的改进/不同之处：**
    - **以往方法：** 许多边缘计算研究侧重于单一技术层面（如模型压缩、通信协议），或仅针对特定任务（如SLAM），缺少一个**集成机器人、边缘AI、低延迟通信和用户交互界面**的完整系统范例。
    - **本文方法：** 论文提出了一个集成的系统设计，关键组件包括：
        - **具身机器人平台（Unitree G1）** 作为多模态数据源和执行器。
        - **基于WebRTC的低延迟通信层**，用于传输视频流和控制数据。
        - **边缘节点上的VLM推理服务（FastAPI）**。
        - **React Web应用**作为监控与控制界面。
- **解决的具体问题/带来的优势：**
    - **提供了一个可复现的蓝本：** 为后续研究如何在边缘智能体系中部署复杂AI模型支持机器人应用，提供了一个**详细的、模块化的系统架构参考**。
    - **确保了交互的实时性：** 采用WebRTC等旨在降低延迟的技术栈，满足了HRI中对**响应时间**的苛刻要求。
    - **增强了系统级优势：** 该架构天然实现了**数据本地化处理**，减少了向云端传输原始视频/音频流的需求，从而在**降低带宽、提升隐私安全性和网络鲁棒性**方面带来了云方案无法比拟的系统级优势。

---
**总结而言，** 本论文的核心创新在于**将前沿的大规模VLM、真实的6G边缘计算基础设施、以及具身机器人平台三者结合，在一个真实的端到端系统中进行实证研究**。它超越了单纯的算法或仿真实验，通过严谨的对比和面向实际场景的评估，量化了边缘部署的价值，明确了性能权衡，并为未来实时智能机器人的系统设计提供了重要的见解和可行的技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验评估效果分析

该论文通过一个完整的机器人-边缘计算系统，对视觉-语言模型（VLM）在边缘部署的性能进行了实证评估，揭示了在延迟、准确性和资源消耗之间的关键权衡。

### 一、 使用的数据集
论文使用了两个数据集进行综合评估：
1.  **标准化基准数据集**：**Robo2VLM**。这是一个大规模基准测试，旨在评估VLM在真实世界机器人操作场景中的表现。它从机器人交互数据集中构建视觉问答（VQA）任务，问题涵盖物体识别、空间推理、操作意图和结果预测等关键方面。
2.  **自建机器人采集数据集**：使用 **Unitree G1** 人形机器人在实验室环境中收集了 **200个问答对**。该数据集扩展了交互场景，包含了社交导航、手势识别、人员存在检测等以人为中心的交互任务，更贴近真实的人机交互环境。

### 二、 评价指标
论文采用两个核心指标进行量化评估：
1.  **准确性**：衡量任务有效性。对于是/否和多选题，采用精确匹配；对于自由回答的短文本，在比较前进行标准化处理（如小写化、去除标点）。
2.  **端到端延迟**：衡量系统响应性。定义为从机器人的摄像头捕获图像帧开始，到接收到VLM最终文本响应为止的总耗时。该指标包含了**帧传输、预处理、模型推理和响应解码**的全部时间。

### 三、 对比的基线方法
论文设置了两个维度的对比：
1.  **部署位置对比**：
    *   **边缘部署**：将 **LLaMA-3.2-11B-Vision-Instruct** 模型部署在ORAN/MEC边缘节点（配备NVIDIA L4 GPU）。
    *   **云端部署**：将**完全相同的LLaMA-3.2模型**通过NVIDIA NIM云API进行部署，作为传统云中心范式的基线。
2.  **模型架构对比**：
    *   **大型模型**：**LLaMA-3.2-11B-Vision-Instruct** (110亿参数)，代表“性能最优”模型。
    *   **紧凑模型**：**Qwen2-VL-2B-Instruct** (20亿参数)，代表“效率最优”模型，同样部署在边缘节点。

### 四、 关键性能结果与结论
实验得出了以下清晰的定量结果和定性结论：

#### 1. 延迟与准确性的权衡（核心结论）
*   **LLaMA-3.2：边缘 vs. 云端**
    *   **准确性**：边缘部署保持了与云端部署**相近甚至略高的准确性**（在机器人采集数据集上，边缘略优）。
    *   **延迟**：边缘部署实现了 **5%的端到端延迟降低**（从云端的1685.20毫秒降至边缘的1600.03毫秒）。虽然绝对增益不大，但对于要求毫秒级响应的人机交互场景意义显著。
    *   **结论**：边缘部署可以在**不牺牲模型核心推理能力**的前提下，有效降低延迟。

*   **Qwen2-VL-2B：边缘部署的另一种选择**
    *   **延迟**：凭借其轻量级架构，实现了**亚秒级响应**，延迟**比云端LLaMA基线降低了一半以上**。
    *   **准确性**：这是以**牺牲推理深度和准确性为代价**的。在Robo2VLM基准上，其准确率（28.02%）比边缘LLaMA（41%）低了约13个百分点。
    *   **结论**：**LLaMA-3.2是“性能最优”选择，而Qwen2是“效率最优”选择**。两者共同描绘了一个帕累托前沿，适用于不同的资源和延迟约束。

#### 2. 延迟瓶颈分析
*   对边缘部署的LLaMA-3.2进行延迟分解发现：**文本生成（自回归解码）占据了总推理时间的85%以上**，是主要的性能瓶颈。图像解码和预处理耗时可忽略不计。
*   **启示**：未来的系统级优化应重点针对生成阶段，例如采用量化、推测解码或模型蒸馏等技术。

#### 3. 任务类别性能差异
*   **LLaMA-3.2** 在所有评估的人机交互领域（如空间关系、社交导航）都表现出**均衡且较强的准确性**，显示了其强大的多模态推理能力。
*   **Qwen2-VL-2B** 在感知驱动任务（如人员检测、指令跟随）上表现尚可，但在需要复杂推理的领域（如空间关系、社交导航）上表现明显落后。
*   **启示**：这为**自适应混合部署策略**提供了依据：轻量级模型处理快速感知任务，大型模型仅在需要深度推理时被调用。

#### 4. 系统级优势
*   除了延迟和准确性，论文强调了边缘推理带来的**关键系统级收益**：
    *   **隐私保护**：避免了将原始视频/音频流传输到云端，降低了数据泄露风险。
    *   **带宽节省**：减少了网络回传负载。
    *   **鲁棒性提升**：降低了对稳定云端连接的依赖，更适合动态、不确定的环境。

### 总结
该论文通过严谨的实验设计，**定量地证明了在ORAN/MEC边缘部署VLM用于实时机器人感知的可行性**。其主要结论是：边缘部署能够以可接受的准确性损失（甚至无损）换取延迟的降低，而模型大小的选择（大型 vs. 紧凑）则是在**延迟**和**准确性**之间进行根本性权衡的关键。这些实证结果为未来在资源受限的嵌入式系统中部署复杂AI模型提供了宝贵的实践指导和设计思路。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.14921v1)
- [HTML 版本](https://arxiv.org/html/2601.14921v1)
