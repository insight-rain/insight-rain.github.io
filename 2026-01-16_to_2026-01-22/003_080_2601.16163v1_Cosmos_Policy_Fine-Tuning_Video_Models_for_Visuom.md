# Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning

**相关性评分**: 8.0/10

**排名**: #3


---


## 基本信息

- **arXiv ID**: [2601.16163v1](https://arxiv.org/abs/2601.16163v1)
- **发布时间**: 2026-01-22T18:09:30Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

Cosmos Policy通过微调预训练视频模型，利用其时空先验直接生成机器人动作和未来状态，实现视觉运动控制和规划，在仿真和真实任务中达到先进性能。

## 摘要

Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/

## 详细分析

## 论文摘要：Cosmos Policy：用于视觉运动控制和规划的视频模型微调

**1. 研究背景和动机**
近年来，大规模预训练视频生成模型在捕捉复杂物理交互和时空动态方面展现出卓越能力。然而，现有工作将这些模型用于机器人策略学习时，通常需要复杂的多阶段后训练或引入新的架构组件来生成动作，限制了其便捷性和性能。本研究旨在探索一种更简单、更有效的方法，直接利用预训练视频模型的时空先验知识，将其转化为高效的机器人策略。

**2. 核心方法和技术创新**
本文提出了 **Cosmos Policy**，一种新颖的机器人策略学习方法。其核心创新在于：
- **单阶段微调**：仅需在目标平台的机器人演示数据上进行一次后训练，无需对基础视频模型（Cosmos-Predict2）进行任何架构修改。
- **潜在帧注入**：将机器人动作、未来状态图像和状态价值（预期累积奖励）等新模态，通过归一化和复制，直接编码为视频模型潜在扩散序列中的“潜在帧”。这使得模型能够利用其预训练的学习机制来联合建模所有模态。
- **策略、世界模型与价值函数联合训练**：通过设计不同的条件生成目标，单个统一模型同时充当策略、世界模型和价值函数。这支持两种部署模式：作为直接策略执行，或利用其预测的未来状态和价值进行基于模型的规划（Best-of-N采样）。

**3. 主要实验结果**
Cosmos Policy 在多个基准测试中取得了最先进的性能：
- **仿真基准**：在 LIBERO 和 RoboCasa 基准上分别达到 **98.5%** 和 **67.1%** 的平均成功率，超越了从头训练的扩散策略、基于视频模型的策略以及在相同数据上微调的先进视觉-语言-动作模型。
- **真实世界任务**：在具有挑战性的双手操作任务中取得了最高的平均成功率（**93.6%**），尤其在处理高精度操作和多模态动作分布的任务上表现优异。
- **规划能力**：通过收集策略 rollout 数据微调世界模型和价值函数后，基于模型的规划在两项困难任务中进一步将平均成功率提升了 **12.5%**。

**4. 研究意义和价值**
本研究证明了大规模预训练视频模型所蕴含的丰富时空先验是机器人策略学习的强大基础。Cosmos Policy 提供了一种**简单、统一且高性能**的范式，仅通过单阶段微调即可将通用视频模型转化为具备规划能力的机器人策略。这项工作为利用日益强大的生成式基础模型进行机器人控制开辟了新途径，并展示了联合学习策略、模型与价值在提升决策可靠性方面的潜力。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Cosmos Policy

### **核心问题**
如何**高效地将预训练的大规模视频生成模型**（而非静态图像-语言模型）的**时空先验知识**，应用于机器人视觉运动控制与规划任务，同时避免复杂的多阶段训练和额外的网络架构设计。

### **核心创新点**
论文提出了 **“Cosmos Policy”** 方法，其核心创新在于一种**简单、统一且无需修改架构的适配策略**，具体体现在：

1.  **“潜在帧注入”机制**：
    - **问题**：预训练视频模型（如Cosmos-Predict2）的输入输出是图像序列，不支持机器人所需的**多模态输入**（如多视角图像、本体感知）和**多模态输出**（如动作、未来状态、价值函数）。
    - **解决方案**：不修改模型架构，而是将**动作、本体感知、价值等新模态数据，编码为与图像潜在表示形状相同的“潜在帧”**，直接插入到视频扩散模型的潜在序列中。模型通过微调学习在同一个扩散过程中联合生成这些“帧”。
    - **价值**：**极简设计**。完全复用预训练模型的核心学习算法（潜在扩散）和参数，无需设计单独的动作预测头、逆动力学模型或价值网络。

2.  **“策略-世界模型-价值函数”三位一体的联合训练**：
    - **问题**：传统方法通常需要分别训练策略、动态模型和价值函数，模块复杂且难以协同。
    - **解决方案**：通过设计不同的**条件生成掩码**，让同一个Cosmos Policy模型在单阶段微调中同时学习三个功能：
        - **策略**：给定当前状态 `s`，生成动作 `a`、未来状态 `s‘` 和价值 `V(s‘)`。
        - **世界模型**：给定当前状态 `s` 和动作 `a`，预测未来状态 `s‘` 和价值 `V(s‘)`。
        - **价值函数**：给定 `(s, a, s‘)`，预测价值 `V(s‘)`。
    - **价值**：**统一架构，协同优化**。共享的时空先验使模型能更一致地理解动作如何影响未来，辅助目标（预测未来状态和价值）也提升了策略本身的性能。

3.  **基于微调后模型的实时规划能力**：
    - **问题**：仅用演示数据训练的世界模型和价值函数，分布狭窄，无法应对规划时遇到的未见状态。
    - **解决方案**：
        1.  **从经验中学习**：收集策略 rollout 数据（包括失败经验），用其**重点微调世界模型和价值函数**，提升其在演示分布外的预测准确性。
        2.  **双模型部署与“最佳-N”采样规划**：
            - **策略模型**：使用初始微调 checkpoint，负责提出候选动作。
            - **规划模型**：使用经 rollout 数据精调的 checkpoint，负责评估候选动作的未来状态和价值。
            - **规划流程**：策略模型提出 N 个动作候选 → 规划模型为每个候选预测未来状态及价值 → 选择预测价值最高的动作执行。
    - **价值**：将**大型生成模型强大的“想象”能力**直接转化为**基于模型的序列决策能力**，在挑战性任务上显著提升成功率。

### **解决方案总结**
论文通过 **“潜在帧注入”** 这一巧妙的表示方法，将机器人控制的多模态问题“翻译”成预训练视频模型能理解的形式。然后，通过**精心设计的联合训练目标**，在**单阶段微调**中，将一个纯粹的视频生成模型，转化为一个集**策略、世界模型、价值函数**于一身的强大机器人控制模型。最后，利用模型自身的生成能力进行**基于模型的搜索规划**，实现更鲁棒、更成功的任务执行。

**简而言之，其核心是：利用预训练视频模型的强大生成能力作为统一的“大脑”，通过极简的适配方式，让其同时学会“执行”、“预测”和“评估”，从而解决复杂的机器人控制与规划问题。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决如何高效地将预训练的大规模视频生成模型适配为机器人策略，以利用其强大的时空先验知识进行视觉运动控制和规划。其核心方法是提出 **Cosmos Policy**，通过**潜在帧注入**技术，将机器人动作、未来状态图像和状态价值等新模态直接编码为视频扩散模型的潜在帧序列，从而仅需在目标平台的演示数据上进行单阶段微调，无需修改模型架构，即可将预训练视频模型（Cosmos-Predict2）转化为一个集策略、世界模型和价值函数于一体的统一模型。实验表明，该方法在LIBERO和RoboCasa仿真基准测试中取得了最先进的成功率（分别为98.5%和67.1%），并在真实世界复杂双臂操作任务中超越了从头训练的扩散策略、基于视频模型的策略以及微调后的视觉-语言-动作模型。此外，通过利用策略运行数据精炼其世界模型和价值函数，并结合基于模型的规划，能进一步提升在挑战性任务中的成功率。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning》的创新点分析

这篇论文提出了一种将预训练视频生成模型（Cosmos-Predict2）直接微调为机器人策略的新方法。其核心创新在于**简化了架构、统一了多模态学习，并实现了规划能力**，从而在多个基准测试中取得了最先进的性能。以下是其相对于已有工作的明确创新点：

---

### 1. **单阶段微调与零架构修改**
- **改进/不同之处**： 以往基于视频模型的机器人策略工作（如Video Policy、UVA等）通常需要**多阶段训练**（例如，先微调视频模型，再单独训练动作生成模块）并引入**新的架构组件**（如单独的动作扩散器或逆动力学模型）。而Cosmos Policy仅通过**单阶段**在机器人演示数据上进行微调，且**不对基础视频模型进行任何架构修改**。
- **解决的问题/优势**：
    - **极大简化了流程**： 减少了训练复杂性和工程开销。
    - **充分利用预训练先验**： 直接利用视频模型在数百万视频中学到的**时空先验**（如物理交互、运动模式），避免了因引入新模块而可能破坏这些宝贵先验的风险。
    - **证明了视频模型核心学习算法（潜扩散）本身就足以建模动作分布**，无需额外设计。

### 2. **潜帧注入：将多模态数据统一编码为潜帧序列**
- **改进/不同之处**： 为了在不修改架构的情况下处理机器人任务所需的多模态输入（多视角图像、本体感知）和输出（动作、未来状态、价值函数），论文提出了 **“潜帧注入”** 技术。它将**动作、本体感知、价值等非图像模态**，通过归一化和复制，直接编码成与图像潜帧**形状相同的张量**，并插入到视频模型的潜扩散序列中。
- **解决的问题/优势**：
    - **实现了真正的多模态统一建模**： 所有模态（图像、状态、动作、价值）都在同一个潜扩散框架下，使用**相同的训练目标**（去噪分数匹配）进行联合学习。
    - **保持了模型的简洁性**： 无需为不同模态设计独立的编码器、解码器或融合模块。
    - **提供了极大的灵活性**： 可以轻松适配不同机器人配置（如摄像头数量、传感器类型），只需调整潜帧序列的排列即可。

### 3. **策略、世界模型与价值函数的联合训练与统一架构**
- **改进/不同之处**： 传统方法通常使用**分离的模块**来分别实现策略、世界模型（动力学模型）和价值函数。Cosmos Policy使用**同一个模型**，通过**调整训练时的条件掩码**，来同时学习这三项功能：
    - **策略**： 给定当前状态 `s`，生成动作 `a`、未来状态 `s'` 和未来价值 `V(s')`。
    - **世界模型**： 给定当前状态 `s` 和动作 `a`，预测未来状态 `s'` 和价值 `V(s')`。
    - **价值函数**： 给定当前状态 `s`、动作 `a` 和未来状态 `s'`，预测价值 `V(s')`。
- **解决的问题/优势**：
    - **参数效率高**： 共享主干网络，减少了总参数量和训练数据需求。
    - **辅助监督提升性能**： 论文通过消融实验证明，让策略和世界模型**联合预测未来状态和价值**（作为辅助目标），比只预测主要目标（动作或未来状态）能带来性能提升。这提供了更丰富的学习信号。
    - **为规划奠定基础**： 一个模型同时具备“想象未来”和“评估未来”的能力，使得基于模型的规划成为可能。

### 4. **利用策略 rollout 数据精化世界模型与价值函数，实现有效规划**
- **改进/不同之处**： 许多模仿学习方法仅使用（成功的）演示数据训练，其世界模型在分布外泛化能力有限。Cosmos Policy 提出**两阶段部署**：
    1.  用演示数据训练基础策略模型。
    2.  收集该策略在环境中的 **rollout 数据**（包含成功与失败），用这些数据**重点微调世界模型和价值函数**，得到一个“规划模型”。
- **解决的问题/优势**：
    - **解决了演示数据分布狭窄的问题**： Rollout数据让模型见识到更广泛的状态-动作对，特别是失败案例，从而学习到更准确、更鲁棒的动力学和价值预测。
    - **实现了实用的模型预测规划**： 在测试时，使用基础策略模型提出多个候选动作，用精化后的规划模型预测每个动作导致的未来状态及其价值，然后选择**价值最高的动作**执行（Best-of-N采样）。
    - **显著提升挑战性任务性能**： 在真实世界双灵巧操作任务中，这种规划机制将平均成功率进一步提高了12.5%。

### 5. **在视频模型先验与数据效率方面超越视觉-语言-动作模型**
- **改进/不同之处**： 当前最先进的机器人策略多基于**视觉-语言-动作模型**（如RT-2, OpenVLA, π₀.₅），其视觉主干（如CLIP）主要在静态图像-文本对上进行预训练。Cosmos Policy 则基于在**海量视频数据**上预训练的视频扩散模型，其先验知识侧重于**时空动态和隐含物理规律**。
- **解决的问题/优势**：
    - **更擅长低层级、精密的运动控制**： 实验表明，在需要处理**高动作多模态性**（如从散落的糖果中抓取）和**高精度操作**（如操作自封袋拉链）的任务上，Cosmos Policy 显著优于微调后的VLA模型。VLA模型在这些任务上容易出现抓取不精确或动作平均化的问题。
    - **数据效率可能更高**： 在RoboCasa基准测试中，Cosmos Policy仅用**每任务50条**人类演示就达到了67.1%的平均成功率，而许多对比方法使用了300条甚至数千条数据。这表明视频时空先验为机器人控制提供了更有效的初始化。

---

### 总结
Cosmos Policy 的核心创新在于**“极简主义”**和**“统一性”**：
1.  **方法极简**： 通过潜帧注入，以最小改动将通用视频模型转化为机器人策略。
2.  **架构统一**： 一个模型同时承载策略、世界模型、价值函数三重功能。
3.  **流程统一**： 单阶段微调联合学习所有目标，并利用在线数据持续精化。

这些创新共同解决了以往方法中存在的**流程复杂、架构冗余、先验利用不充分、规划能力弱**等问题，最终在仿真和真实机器人任务中实现了**最先进的性能**，并展示了**强大的从经验中学习并规划**的能力。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

论文通过一系列仿真和真实世界实验，全面评估了Cosmos Policy方法的性能，证明了其在机器人视觉运动控制和规划任务中的卓越表现。

### 一、 使用的数据集与评价指标

#### 1. 数据集
- **LIBERO仿真基准**：包含四个任务套件（Spatial, Object, Goal, Long），评估策略处理不同空间布局、物体、语言指定目标和长时程任务的能力。每个套件包含10个任务，共500条专家演示。
- **RoboCasa仿真基准**：包含24个静态厨房操作任务，使用单个Franka机械臂。评估涉及**未见过的物体实例**和**未在训练数据中出现过的场景风格**。每个任务仅使用50条人类遥操作演示进行训练（远少于基线方法）。
- **真实世界ALOHA机器人任务**：使用双ViperX 300 S机械臂平台，配备三个摄像头（一个俯视，两个腕部）。包含四个具有挑战性的双手操作任务：
    - “put X on plate”：基于语言指令将物体放到盘子上（80条演示）。
    - “fold shirt”：多步骤折叠T恤（15条演示）。
    - “put candies in bowl”：收集散落的糖果（45条演示）。
    - “put candy in ziploc bag”：打开拉链袋并放入物品（45条演示）。

#### 2. 评价指标
- **主要指标**：**任务成功率**。
- **ALOHA任务补充指标**：**任务完成度分数**（0-100分），用于更精细地衡量多步骤任务的完成进度。
- **评估设置**：
    - **仿真**：每个任务进行大量重复试验（LIBERO: 10任务 × 50回合 × 3随机种子 = 1500回合/套件；RoboCasa: 24任务 × 50回合 × 3随机种子 = 3600回合）。
    - **真实世界**：每个方法在固定初始状态下进行总计101次试验，包含分布内和分布外测试条件。

### 二、 对比的基线方法

论文与三大类先进的模仿学习策略进行了全面对比：

1.  **从头训练的扩散策略**：
    - Diffusion Policy, Dita

2.  **基于视频模型的策略**：
    - UVA, UWM, Video Policy

3.  **微调的大规模视觉-语言-动作模型**：
    - `π0`, `π0.5`, OpenVLA-OFT, CogVLA, UniVLA, DP-VLA, GR00T-N1.5

### 三、 关键性能结果与结论

#### 1. 作为直接策略（无规划）的性能（回答Q1）
Cosmos Policy在三个领域均达到了**最先进的性能**。

- **LIBERO基准**：
    - **平均成功率：98.5%**，在四个套件上均名列前茅（Spatial: 98.1%, Object: 100%, Goal: 98.2%, Long: 97.6%）。
    - **结论**：显著超越了所有对比方法，包括微调的VLA模型（如`π0.5`: 96.9%, OpenVLA-OFT: 97.1%）。

- **RoboCasa基准**：
    - **平均成功率：67.1%**。
    - **关键优势**：在**仅使用50条演示/任务**的情况下，超越了所有使用**300条或更多演示**的基线方法（如Video Policy: 66.0%, FLARE: 66.4%）。这证明了其卓越的**数据效率**和从视频先验中学习的能力。

- **真实世界ALOHA任务**：
    - **平均任务完成度分数：93.6%**，在四个任务中的三个上表现最佳。
    - **定性分析**：在需要处理**高动作多模态性**（“put candies in bowl”）和**高精度操作**（“put candy in ziploc bag”）的任务中，Cosmos Policy比VLA模型（`π0.5`, OpenVLA-OFT+）表现**更可靠**。VLA模型在这些任务中常出现抓取失败或动作平均导致目标不明确的问题。

#### 2. 消融实验分析（回答Q2）
- **去除辅助学习目标**（策略同时预测未来状态和价值）：导致LIBERO平均成功率下降1.5%（从98.5%降至97.0%）。
- **不使用预训练视频模型（从头训练）**：导致LIBERO平均成功率大幅下降3.9%（至94.6%），且在真实任务中产生抖动运动。
- **逐步剥离联合训练组件**（见附录）：在RoboCasa上，当策略仅学习`p(a|s)`而不再联合预测未来状态时，性能**急剧下降**（从67.1%降至44.4%）。这表明**联合学习策略、世界模型和价值函数是Cosmos Policy高效性的核心**。

#### 3. 基于模型的规划性能（回答Q3 & Q4）
- **规划效果**：在最具挑战性的两个ALOHA任务（“put candies in bowl”和“put candy in ziploc bag”）上，使用收集的策略 rollout 数据微调世界模型和价值函数后，进行基于模型的规划。
- **性能提升**：与基础Cosmos Policy（无规划）相比，规划版本实现了**平均12.5分的任务完成度提升**。
- **规划机制对比**：
    - **模型基于规划**（使用世界模型预测未来状态`S'`，再评估`V(S')`）：表现最佳。
    - **模型无关规划**（直接学习并评估Q值`Q(S, A)`）：性能低于模型基于规划。
    - **结论**：在有限的rollout数据下，利用学习的环境动力学进行规划比直接学习高维的Q函数更有效、样本效率更高。

### 总结
论文通过详实的定量实验和定性分析表明，**Cosmos Policy通过简单的单阶段微调，成功将大规模预训练视频模型转化为高性能的机器人策略**。其核心优势在于：
1.  **卓越性能**：在多个仿真和真实基准上达到SOTA。
2.  **数据高效**：在RoboCasa上使用更少数据超越对手。
3.  **功能统一**：单一模型同时胜任策略、世界模型和价值函数角色。
4.  **可规划性**：能够利用经验数据改进模型，并通过规划进一步提升在困难任务上的成功率。

**局限性**：基于模型的规划导致推理速度较慢（约5秒生成一个动作块），可能限制在动态任务中的应用。未来工作可专注于优化推理速度。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.16163v1)
- [HTML 版本](https://arxiv.org/html/2601.16163v1)
