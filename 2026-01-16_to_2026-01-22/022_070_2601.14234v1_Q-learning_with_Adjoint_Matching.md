# Q-learning with Adjoint Matching

**相关性评分**: 7.0/10

**排名**: #22


---


## 基本信息

- **arXiv ID**: [2601.14234v1](https://arxiv.org/abs/2601.14234v1)
- **发布时间**: 2026-01-20T18:45:34Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Qiyang Li, Sergey Levine

## 关键词

Diffusion, Flow Matching, Inference Acceleration, offline Reinforcement Learning, fine tune

## 一句话总结

QAM是一种新颖的强化学习算法，通过伴随匹配技术优化扩散或流匹配策略，解决了连续动作RL中梯度不稳定的问题，在离线和离线到在线RL任务中表现优异。

## 摘要

We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.

## 详细分析

## 论文摘要：Q-learning with Adjoint Matching (QAM)

**1. 研究背景和动机**
在连续动作空间的强化学习中，一个长期存在的挑战是如何高效地优化一个表达能力强的扩散模型或流匹配策略，使其与参数化的Q函数（评论家）对齐。有效的优化需要利用评论家的一阶梯度信息，但对于流或扩散策略而言，直接通过其多步去噪过程进行基于梯度的反向传播优化，存在数值不稳定的问题。现有方法要么仅使用Q值而丢弃梯度信息，要么依赖近似方法牺牲策略的表达能力或引入偏差，均非理想解决方案。

**2. 核心方法和技术创新**
本文提出了**Q-learning with Adjoint Matching (QAM)**，一种新颖的基于时序差分（TD）的强化学习算法。其核心技术创新在于引入了**伴随匹配**这一来自生成建模领域的技术。QAM通过将评论家关于动作的梯度进行变换，构造出一个**分步的目标函数**。这种方法的关键优势在于：
- **避免了不稳定的反向传播**：无需通过策略的多步采样过程进行端到端反向传播。
- **保证了策略的最优性**：在最优解处，能提供一个**无偏且表达能力强**的策略。
QAM将此策略优化方法与用于评论家学习的时序差分更新相结合，形成了一个完整的算法框架。

**3. 主要实验结果**
在具有挑战性的**稀疏奖励任务**上，QAM在**离线强化学习**和**离线到在线强化学习**两种设置中，均**持续且稳定地超越了**先前的主流方法，验证了其有效性和优越性。

**4. 研究意义和价值**
QAM为解决连续动作RL中**复杂策略的稳定、高效优化**这一根本难题提供了新思路。它首次成功地将伴随匹配技术系统性地应用于RL策略优化，在**不牺牲策略表达能力或引入偏差的前提下**，充分利用了评论家的梯度信息。这项工作不仅提升了在困难任务上的性能，也为未来将更先进的生成模型与强化学习结合奠定了重要的方法论基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### 解决的问题
这篇论文旨在解决**连续动作强化学习**中的一个长期挑战：如何高效地优化一个**表达能力强的扩散模型或流匹配策略**，使其与参数化的Q函数（评论家）对齐。具体困难在于：
- 直接通过反向传播对多步去噪过程进行基于梯度的优化会导致**数值不稳定**。
- 现有方法要么**丢弃梯度信息**（仅使用价值），要么依赖**近似方法**，从而牺牲策略的表达能力或引入偏差。

### 核心创新点
提出 **Q-learning with Adjoint Matching (QAM)**，一种基于时序差分（TD）的新型强化学习算法，其创新在于：
- **引入伴随匹配（Adjoint Matching）**：利用生成建模中的最新技术，将评论家的动作梯度转换为**逐步目标函数**。
- **避免不稳定反向传播**：该目标函数无需通过多步去噪过程进行反向传播，从而解决了数值不稳定问题。
- **保持无偏性与表达性**：在最优解处提供**无偏且表达能力强的策略**，克服了现有方法的局限性。

### 解决方案
1. **算法设计**：
   - 结合**时序差分备份**进行评论家学习。
   - 通过伴随匹配技术，将Q函数的梯度信息**高效整合**到策略优化中，避免直接反向传播。

2. **技术优势**：
   - **稳定性**：避免了扩散/流策略多步去噪中的梯度不稳定问题。
   - **表达性**：保持生成式策略的完整表达能力，不引入近似偏差。
   - **效率**：利用一阶信息（梯度）进行优化，提升样本效率与收敛速度。

3. **实验验证**：
   - 在**离线RL**和**离线到在线RL**的**困难、稀疏奖励任务**中，QAM**持续优于**先前方法。

### 实际价值
- **推动连续控制RL发展**：为使用先进生成模型（扩散/流）作为策略提供了稳定、高效的优化框架。
- **提升复杂任务性能**：在稀疏奖励场景中表现优异，有助于解决更现实的机器人控制、自动驾驶等决策问题。
- **跨领域技术融合**：将生成建模中的伴随匹配引入RL，展示了跨学科方法解决核心挑战的潜力。

```plaintext
关键贡献总结：
1. 提出QAM算法，解决扩散/流策略优化中的梯度不稳定问题。
2. 通过伴随匹配利用评论家梯度，实现稳定、无偏的策略学习。
3. 在稀疏奖励任务中实现更优性能，适用于离线及离线到在线RL。
```


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决连续动作强化学习中一个长期存在的技术难题：如何高效地利用参数化Q函数的一阶梯度信息，来优化表达能力强大的扩散或流匹配策略，而避免因直接反向传播通过多步去噪过程导致的数值不稳定问题。为此，论文提出了**伴随匹配Q学习（QAM）**这一新颖的时序差分算法，其核心创新在于引入了生成建模中的伴随匹配技术，将评论家的动作梯度转化为一个无需不稳定反向传播、分步优化的目标函数，从而在最优解处获得无偏且高表达能力的策略。该方法结合评论家的时序差分更新，在离线以及离线到在线强化学习的困难稀疏奖励任务上，性能持续超越了现有方法。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

本文提出的 **Q-learning with Adjoint Matching (QAM)** 算法，在连续动作强化学习领域具有明确的创新性。其核心创新点可逐条归纳如下：

### 1. **引入伴随匹配技术，实现扩散/流匹配策略的高效、稳定优化**
   - **相比以往方法的改进/不同之处**：
     - **以往方法A（仅使用价值信息）**：如某些基于价值的算法或策略梯度变体，在优化扩散策略时，仅利用Q函数的**值**（value），而完全丢弃了其**梯度**（gradient）信息。这导致优化效率低下，收敛缓慢。
     - **以往方法B（近似梯度或简化策略）**：另一些方法试图利用梯度，但为了规避通过扩散模型多步去噪过程进行反向传播的**数值不稳定性**，不得不采用有偏的梯度近似，或者牺牲策略的表达能力（例如，使用更简单的策略参数化形式）。
     - **QAM的创新**：直接引入了生成建模领域最新的**伴随匹配**技术。该技术不通过不稳定的多步反向传播来直接计算梯度，而是将Q函数（评论家）关于动作的梯度信息，**转化**为一系列逐步骤的、稳定的目标函数。
   - **解决的具体问题/带来的优势**：
     - **解决了问题**：从根本上解决了**在利用表达力强的扩散/流匹配策略时，无法稳定、高效地利用评论家一阶信息（梯度）进行优化**的长期挑战。
     - **带来的优势**：
       1. **稳定性**：优化过程避免了不稳定的反向传播，训练更鲁棒。
       2. **无偏性**：在最优解处，能提供无偏的策略估计。
       3. **保持表达性**：无需为了稳定性而妥协，可以充分利用扩散/流模型强大的分布建模能力。

### 2. **首次将伴随匹配与时间差分学习框架系统结合，形成完整的RL算法**
   - **相比以往方法的改进/不同之处**：
     - 伴随匹配本身是一项**生成模型技术**。以往在RL中应用扩散策略的工作，要么没有利用梯度（如创新点1所述），要么在结合梯度信息时采用了不同的、有缺陷的优化路径。
     - QAM是**首个**将伴随匹配技术系统地整合到基于时间差分（Temporal-Difference, TD）的强化学习框架中的算法。它明确设计了**策略优化**（通过伴随匹配）与**评论家学习**（通过TD误差备份）两个模块的协同工作方式。
   - **解决的具体问题/带来的优势**：
     - **解决了问题**：填补了“先进的生成模型优化技术”与“经典的RL价值函数学习框架”之间的鸿沟，提供了一个**端到端**且**理论动机清晰**的解决方案。
     - **带来的优势**：形成了一个完整、自洽的算法。策略优化模块可以稳定地利用评论家梯度，而评论家又通过TD学习不断改进，两者相互促进，从而在具有挑战性的任务上实现更优的性能。

### 3. **在稀疏奖励的离线及离线到在线RL任务上实现显著性能提升**
   - **相比以往方法的改进/不同之处**：
     - 论文通过实验验证，QAM在**困难的、稀疏奖励的任务**上，**持续地**优于先前的方法。这包括了纯**离线RL**和更具挑战性的**离线到在线（Offline-to-Online）RL**两种设置。
     - 先前方法由于上述的优化效率低、有偏或表达性不足等问题，在稀疏奖励这种需要高效探索和精确策略建模的场景中，往往表现不佳。
   - **解决的具体问题/带来的优势**：
     - **解决了问题**：提升了RL算法在**最富挑战性的一类任务**上的实用性和性能上限。稀疏奖励和离线学习是RL走向实际应用的关键瓶颈。
     - **带来的优势**：
       1. **实证优势**：提供了坚实的实验证据，表明所提创新在实践中的有效性。
       2. **应用价值**：增强了复杂、高风险领域（如机器人控制、自动驾驶等，其中奖励信号常常是稀疏的，且在线试错成本高昂）应用连续动作RL算法的可行性。其优秀的离线到在线性能意味着算法可以从历史数据安全启动，并进一步通过环境交互高效提升。

---

**总结**：QAM的核心创新在于**方法论层面**的突破——**借用并融合了生成模型的伴随匹配技术，巧妙地化解了连续动作RL中表达性策略优化与梯度利用之间的矛盾**。这不仅带来了更稳定、无偏且表达力强的策略优化器，而且通过与TD学习的结合，在困难任务上实现了显著的性能提升，推动了连续动作RL，特别是基于扩散模型策略的RL，向更实用、更强大的方向发展。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

根据论文内容，Q-learning with Adjoint Matching (QAM) 在实验评估中取得了显著效果，其核心结论是：**在具有挑战性的稀疏奖励任务上，QAM 在离线强化学习（Offline RL）和离线到在线强化学习（Offline-to-Online RL）两种设定下，均持续优于现有基线方法。**

### 1. 评估任务与数据集
论文聚焦于**连续动作空间**的强化学习问题。虽然没有明确列出具体的数据集名称（如 D4RL），但从描述可以推断：
- **任务类型**：**“困难、稀疏奖励”** 的任务。这类任务通常指智能体仅在达成特定目标时才获得奖励信号，探索难度极大，是评估算法稳定性和效率的经典场景。
- **设定**：同时在 **Offline RL**（仅从静态数据集中学习）和 **Offline-to-Online RL**（先在静态数据集上预训练，再进行在线交互微调）两种模式下进行评估。这表明 QAM 具有良好的泛化性和实用性。

### 2. 对比的基线方法
论文将 QAM 与 **“现有方法”** 进行对比。虽然没有列举具体算法名称，但从技术背景描述可以推断，这些基线方法大致分为两类：
1.  **仅利用价值信息的方法**：在优化基于扩散模型或流匹配的表达性策略时，只使用 Q 函数的值，而丢弃其梯度信息。这可能导致学习效率低下。
2.  **依赖近似的方法**：为了使用梯度信息而对策略的表达能力做出妥协，或者引入了优化偏差的方法。这些方法虽然稳定，但可能无法逼近最优策略。

QAM 的核心创新正是为了**同时克服**这两类基线方法的局限性。

### 3. 关键性能提升与结论
- **主要性能提升**：QAM **“持续优于”** 现有方法。这表明：
    - **更高的样本效率**：在稀疏奖励环境下，能更有效地利用离线数据和在线交互，找到获得奖励的策略。
    - **更好的最终性能**：学得策略在任务上的表现更优。
    - **稳定性与表达性的兼得**：通过 **Adjoint Matching** 技术，QAM 成功利用了 Q 函数的梯度信息（一阶信息），从而实现了对高度表达性的扩散/流匹配策略的高效、稳定优化。这是其性能超越仅用价值或使用近似方法的基线的根本原因。

- **核心结论**：QAM 通过将 Adjoint Matching 与时间差分学习相结合，为解决连续动作 RL 中**表达性策略的稳定、无偏、高效优化**这一长期挑战，提供了一个有效的方案，并在最具挑战性的稀疏奖励场景中验证了其优越性。

### 总结
论文给出了明确的**定性结论**（“持续优于”）和**技术原理上的优势**（无偏、表达性强、优化稳定）。虽然没有展示具体的定量数据表格（如得分百分比提升），但其论述清晰地表明了 QAM 在关键技术指标（如学习稳定性、策略性能）上相比基线方法有显著提升，特别是在处理困难、稀疏奖励的连续控制任务时。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.14234v1)
- [HTML 版本](https://arxiv.org/html/2601.14234v1)
