# Behavior Knowledge Merge in Reinforced Agentic Models

**相关性评分**: 6.0/10

**排名**: #28


---


## 基本信息

- **arXiv ID**: [2601.13572v1](https://arxiv.org/abs/2601.13572v1)
- **发布时间**: 2026-01-20T03:56:53Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Xiangchi Yuan, Dachuan Shi, Chunhui Zhang, Zheyuan Liu, Shenglong Yao, Soroush Vosoughi, Wenke Lee

## 关键词

fine tune, offline Reinforcement Learning, world model

## 一句话总结

该论文提出了一种针对强化学习训练后的智能体模型的分布感知合并框架，以解决现有合并方法在保留任务特定能力方面的不足。

## 摘要

Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.

## 详细分析

## 论文摘要：强化智能体模型中的行为知识融合

**1. 研究背景和动机**
随着大语言模型（LLM）向智能体方向发展，强化学习（RL）已成为塑造其专业推理行为的关键后训练范式。在实际应用中，通常需要将多个在不同任务上通过RL训练得到的专家智能体合并为一个通用模型。然而，现有的模型融合方法（如Task Arithmetic、TIES、DARE）主要针对监督微调（SFT）场景设计，其假设任务向量（即参数更新）是**密集且全局可比**的。而RL训练产生的任务向量具有**高度稀疏和异构**的特性，直接应用现有方法会导致关键的任务特异性行为知识在全局平均过程中被稀释，造成性能下降。本文旨在解决这一根本性的**任务向量失配**问题。

**2. 核心方法和技术创新**
本文提出了**强化智能体融合（RAM）**方法，这是一种专为RL训练智能体设计的、**分布感知**的融合策略。其核心创新在于：
- **分布探测**：通过分析多个RL任务向量的分布，明确区分出**共享参数更新**（被多个任务修改）和**任务特异性唯一参数更新**（仅被单个任务修改）。
- **选择性融合**：对共享区域的参数进行平均，以保留通用能力；对唯一区域的参数则**完全保留其原始幅度**，避免信号稀释。
- **分布感知重缩放**：进一步提出RAM+，为那些共享区域占比较高的任务向量，按比例**放大其唯一区域参数**，以补偿共享区域因平均而造成的性能损失，实现功能等效。

**3. 主要实验结果**
在编码（CURE）、工具使用（ToolRL）、长上下文记忆（MemAgent）等多个智能体领域及Qwen、Llama不同架构上的实验表明：
- **性能优越**：RAM/RAM+在12个任务中的9个上达到SOTA，平均性能显著超越所有基线融合方法。
- **产生协同效应**：融合后的通用模型（RAM+）在多数任务上甚至**超越了原始的领域专家模型**，显示出智能体间知识产生了正向协同。
- **高效稳健**：RAM在保持高性能的同时，融合速度远快于TIES、DARE等复杂基线，并且在指令跟随等通用能力上遗忘更少。

**4. 研究意义和价值**
本研究首次系统揭示了RL任务向量的稀疏异构特性与现有SFT融合方法之间的根本矛盾，并提出了针对性的解决方案。RAM方法具有重要的理论价值和实际意义：
- **理论层面**：深化了对不同训练范式（RL vs. SFT）如何影响模型参数空间的理解，为后续研究提供了新视角。
- **实践层面**：为工业界高效构建多功能通用智能体提供了切实可行的技术路径，无需昂贵的联合训练即可集成多个RL专家，在保证性能的同时极大提升了部署效率。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
现有模型合并方法（如Task Arithmetic、TIES、DARE）是为**监督微调（SFT）**设计的，其假设任务向量（task vectors）**稠密且全局可比**。然而，**强化学习（RL）微调的智能体模型**产生的任务向量具有**高度稀疏性和异质性**。当使用标准全局平均合并时，RL中编码关键任务特定行为的**非重叠任务向量会被稀释**，导致**性能下降**。论文称之为 **“信号稀释（Signal Dilution）”** 问题。

### **核心创新点**
提出了 **RAM（Reinforced Agent Merging）**，一种**专为RL训练智能体设计**的分布感知合并方法。其核心创新在于：

1.  **首次系统分析了RL任务向量的独特行为**：
    - **稀疏性异质**：不同RL智能体的参数更新稀疏度差异极大（例如，编码智能体仅更新3.2%参数，而记忆智能体更新54.3%）。
    - **分布异质**：参数更新集中在不同的、非重叠的区域（“独特区域”），这些区域对各自领域性能至关重要且几乎不干扰其他任务。

2.  **提出“分布感知选择性合并”框架**：
    - **探测向量分布**：根据参数被多个智能体更新的情况，将参数划分为**共享区域**（`c_i ≥ 2`）和**独特区域**（`c_i = 1`）。
    - **选择性合并策略**：
        - **共享区域**：执行平均，以保留多任务共识能力。
        - **独特区域**：**完全保留其原始幅度**，并引入一个**基于分布的重缩放因子（λ_t）** 进行补偿性放大，以对抗因共享区域被平均而可能造成的性能损失。
    - **重缩放机制**：缩放因子 `λ_t` 与**重叠-独特比（ρ_t）** 成正比。`ρ_t` 高的任务（即其更新更多位于共享区域）其独特区域会获得更大程度的放大，以在功能上等价于原始任务向量。

### **解决方案流程**
```
1. 输入：多个RL智能体的任务向量 {τ_t}。
2. 分布探测：
   - 根据阈值 (ε=10^-5) 生成二进制掩码 m_t，标识每个智能体更新的参数。
   - 计算重叠计数向量 c = Σ m_t。
   - 根据 c_i 的值，将每个参数分类为：未更新 (0)、独特更新 (1)、共享更新 (≥2)。
3. 计算重缩放因子：
   - 对每个任务 t，计算 ρ_t = (共享参数数量) / (独特参数数量)。
   - λ_t = 1 + r · clip(ρ_t, 0, α)，其中 r 和 α 是超参数。
4. 选择性合并生成最终任务向量 τ_merged：
   - 若 |𝒯_i| = 0 (无更新): τ_merged,i = 0。
   - 若 |𝒯_i| = 1 (独特更新): τ_merged,i = λ_t · τ_t,i。
   - 若 |𝒯_i| ≥ 2 (共享更新): τ_merged,i = (1/|𝒯_i|) Σ τ_t,i。
5. 输出：合并后的模型 θ_merged = θ_pre + τ_merged。
```

### **实际价值**
1.  **性能提升**：在编码（LiveBench）、工具使用（BFCL）、长上下文记忆（RULER）等多个智能体领域和Qwen、Llama两种架构上，RAM/RAM+均显著优于现有合并基线，平均性能达到SOTA。
2.  **解锁协同潜力**：合并后的通用模型在**多数任务上超越了原始领域专家模型**的性能，表明RAM能有效融合并增强不同智能体的能力，产生“1+1>2”的效果。
3.  **高效实用**：RAM在保持高性能的同时，计算效率高于TIES、DARE等复杂基线，更适用于实际部署。
4.  **推动工业应用范式**：为像UI-TARS2这样“先RL训练垂直智能体，后合并为通用智能体”的工业范式提供了理论基础和高效工具，解决了该范式中关键的性能下降瓶颈。

**总结**：本文的核心贡献是**揭示了RL与SFT任务向量的根本差异**，并据此设计了**首个针对RL智能体的、分布感知的模型合并方法RAM**。它通过**区分并差异化处理共享与独特参数更新**，有效解决了信号稀释问题，不仅提升了合并性能，更解锁了智能体间的协同效应，具有重要的理论和实践意义。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**强化学习（RL）微调后的智能体模型在合并时性能下降**的核心问题展开研究。作者指出，现有为监督微调（SFT）设计的模型合并方法（如全局平均）与RL产生的**稀疏、异构的任务向量**不匹配，导致关键的任务特定行为知识在合并中被稀释，即“信号稀释”问题。

为解决此问题，论文提出了**强化智能体合并（RAM）方法**。该方法的核心创新在于**基于分布感知的差异化合并策略**：首先探测并区分任务向量中的共享参数区域和任务独有参数区域；然后对共享区域进行平均以整合通用能力，同时对独有区域进行选择性保留和基于重叠-独有比率的重新缩放，以补偿信号稀释。

实验结果表明，RAM方法在多个智能体领域和模型架构上均显著优于现有合并基线。更重要的是，它能够解锁智能体间的协同潜力，使得合并后的通用模型在多数任务上的性能**甚至超过了原始的、专门化的单个智能体**，实现了性能的“正向协同”。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Behavior Knowledge Merge in Reinforced Agentic Models》针对**强化学习（RL）训练的智能体模型**的合并问题，提出了明确的创新方法（RAM）。其核心创新点在于**首次系统性地识别并解决了现有模型合并方法与RL训练模式之间的根本性不匹配问题**。具体创新点逐条分析如下：

---

### 1. **识别并形式化了“RL任务向量”与“SFT任务向量”的根本性差异**
   - **改进/不同之处**：
     - **以往方法**：现有的模型合并方法（如Task Arithmetic、TIES、DARE）均基于**监督微调（SFT）** 的假设设计。SFT产生的任务向量（Task Vectors）通常是**密集（Dense）且全局可比（Globally Comparable）** 的，即参数更新广泛分布在网络中且在不同任务间有大量重叠。
     - **本文发现**：论文通过实证分析发现，**基于策略的RL训练**产生的任务向量具有**高度稀疏（Sparse）和异质（Heterogeneous）** 的特性。不同RL智能体（如编码、工具使用、记忆）更新的参数区域重叠度很低，且稀疏程度差异巨大（例如，编码智能体仅更新3.2%的参数，而记忆智能体更新54.3%）。
   - **解决的具体问题/带来的优势**：
     - **揭示了性能下降的根本原因**：当将适用于SFT的全局平均合并方法直接用于RL智能体时，会导致**信号稀释（Signal Dilution）**。具体来说，对于某个智能体独有的关键参数更新（Unique Regions），与其它智能体的零更新进行平均后，其幅度会被除以模型总数（1/N），从而严重削弱了该智能体的专项能力。
     - **为针对性方法设计奠定了基础**：这一发现是提出RAM方法的前提，明确了不能对RL任务向量“一视同仁”地进行处理。

### 2. **提出了首个面向RL智能体的分布感知合并方法：RAM**
   - **改进/不同之处**：
     - **以往方法**：采用统一的策略处理所有参数更新（如全局平均、基于Fisher信息加权、修剪后共识平均）。
     - **RAM方法**：核心创新在于**显式地解耦（Disentangle）任务向量中的共享区域（Shared Regions）和独有区域（Unique Regions）**，并对它们实施**选择性合并策略**：
       1. **共享区域**：进行平均，以保留和平衡多任务通用能力。
       2. **独有区域**：**完全保留其原始幅度**，并引入一个**基于分布的重缩放机制**进行增强，以补偿因共享区域被平均而可能带来的性能损失。
   - **解决的具体问题/带来的优势**：
     - **直接解决了信号稀释问题**：通过保护并增强独有参数更新，确保了每个RL智能体的核心行为知识在合并后不被削弱。
     - **实现了功能等价（Functional Equivalence）**：通过重缩放因子 λ_t，旨在使合并后模型在特定任务上的性能增益与原始专项智能体相当。这使合并后的通用模型不仅能保留，甚至能超越原始专项智能体的性能（产生协同效应）。

### 3. **设计了基于“重叠-独有比率”的自适应重缩放机制**
   - **改进/不同之处**：
     - **以往方法**：DARE等方法也有重缩放，但其目的是补偿随机丢弃参数带来的期望偏差，且是全局性的，与参数更新的分布无关。
     - **RAM的重缩放**：缩放因子 λ_t 是**任务特定的、自适应的**。它基于每个任务向量的**重叠-独有比率（Overlap-Unique Ratio, ρ_t）** 计算得出。ρ_t 越高，说明该任务向量的更新更多位于共享区域，在合并中因平均遭受的损失可能更大，因此其独有区域需要更强的放大来进行补偿。
   - **解决的具体问题/带来的优势**：
     - **实现了精细化的性能补偿**：不是对所有独有区域进行无差别的放大，而是根据每个任务向量自身的结构分布进行有区别的增强，使合并过程更加合理和高效。
     - **提高了方法的鲁棒性和泛化性**：通过裁剪（Clipping）操作防止在 ρ_t 极大时产生不稳定的过度放大，使方法在不同架构（Qwen, Llama）和不同领域（编码、工具、记忆、数学、搜索）的智能体上都能稳定生效。

### 4. **实证验证了方法在多个维度的优越性，并揭示了“协同效应”**
   - **改进/不同之处**：
     - **以往工作**：模型合并的目标通常是尽可能减少性能损失，达到与专项模型相当或略低的水平。
     - **本文结果**：RAM/RAM+ 不仅在平均性能上**显著超越所有现有合并基线**，更关键的是，在多个任务上，**合并后的通用模型性能甚至超过了原始的、独立的专项RL智能体**（如表1中红色高亮部分）。
   - **解决的具体问题/带来的优势**：
     - **解锁了智能体间的协同潜力**：这表明RAM不仅成功保留了知识，还通过整合不同智能体的推理模式，产生了“1+1>2”的效果。例如，工具使用智能体的性能可能因为融合了编码智能体的逻辑严谨性而得到提升。
     - **提供了高效的实用方案**：RAM在计算效率上也优于TIES、DARE等复杂基线（如图9），实现了性能与效率的帕累托前沿，使其更具实际部署价值。

---

## 总结
本文的核心创新是一个**从问题诊断到方案设计的完整体系**：
1.  **理论创新**：首次明确指出并量化了RL与SFT任务向量的分布差异及其对合并的影响（信号稀释）。
2.  **方法创新**：提出了RAM，一种基于分布感知、区分对待共享/独有知识的选择性合并框架。
3.  **机制创新**：引入了基于重叠统计的自适应重缩放机制，实现精准性能补偿。
4.  **价值创新**：通过实验证明，该方法不仅能有效合并，更能激发模型潜能，产生协同效应，为构建高性能、多能力的通用RL智能体提供了切实可行的技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心实验效果
论文提出的 **RAM/RAM+** 方法在合并多个通过强化学习（RL）训练的任务特定智能体模型时，实现了以下关键效果：
1.  **超越现有合并方法**：在多个智能体领域和模型架构上，RAM/RAM+ 的平均性能显著优于所有基线合并方法。
2.  **解锁协同潜力**：合并后的通用模型（RAM+）在多数任务上的性能**甚至超过了原始的、专门针对该任务训练的专家模型**，表明不同智能体的知识产生了正向协同效应。
3.  **有效缓解信号稀释**：通过显式区分并差异化处理共享参数更新和任务特有参数更新，RAM 成功解决了 RL 任务向量稀疏、异构导致的“信号稀释”问题。

### 二、 使用的数据集与评价指标
实验在三个核心的智能体领域进行，使用了领域内权威的基准测试：

| 领域 | 数据集 | 主要评价指标 |
| :--- | :--- | :--- |
| **代码生成** | **LiveBench** (White et al., 2025) <br> **LiveCodeBench** (Jain et al., 2024) <br> MBPP、CodeContests、CodeForces (补充实验) | **ACC**: 代码通过率 (Pass@1) <br> **UT**: 单元测试通过率 (Pass@1) <br> **BoN ACC/UT**: Best-of-N (N=4) 采样下的通过率 |
| **工具使用** | **Berkeley Function Calling Leaderboard (BFCL)** (Patil et al., 2025) <br> - Live 数据集 (真实世界) <br> - Non-Live 数据集 (合成) | **准确率**：基于抽象语法树（AST）匹配，评估函数调用的正确性。重点关注 **Parallel** (并行) 和 **Parallel Multiple** (并行多函数) 等复杂场景。 |
| **长上下文记忆** | **RULER** 基准 (Hsieh et al., 2024) <br> - RULER-HotpotQA (多跳推理) <br> - RULER-SQuAD (事实检索) | **子字符串精确匹配 (sub_em)**：评估模型在超长上下文（7K 至 896K tokens）中准确回忆和推理信息的能力。 |
| **通用指令跟随** | **IFEval** (Zhou et al., 2023) | **指令准确率** 与 **提示准确率**，分 **宽松 (Loose)** 和 **严格 (Strict)** 两种标准。用于评估合并后模型是否遗忘基础能力。 |
| **数学与搜索** | **GSM8K, MATH500** (数学) <br> **NQ, 2WikiMultiHopQA** (搜索) | 用于在 **Llama-3.2-3B** 架构上的扩展实验，评估相应领域的性能。 |

### 三、 对比的基线方法
论文与两大类基线进行了全面对比：

1.  **原始专家模型**：作为性能上限的参考。
    - **CURE** (编码专家)
    - **ToolRL** (工具使用专家)
    - **MemAgent** (记忆专家)

2.  **主流模型合并方法**：作为技术对比基线。
    - **Task Arithmetic (TA)**：任务向量线性平均。
    - **Fisher Merging**：基于Fisher信息加权平均。
    - **TIES-Merging**：通过修剪和符号一致性减少干扰。
    - **DARE**：随机丢弃并重缩放参数，常与TA或TIES结合使用 (DARE+TA, DARE+TIES)。
    - **WUDI Merging** (补充实验)：一种需要优化步骤的复杂合并方法。

### 四、 关键性能提升与结论
#### 1. 主实验（三智能体合并，Qwen2.5-7B基座）
- **整体性能**：RAM+ 在12个任务上的**平均得分达到66.55**，显著优于最佳基线 DARE+TA (63.33) 和原始专家模型的平均表现。
- **超越专家**：RAM+ 在 **9/12** 的任务上取得了最佳性能，甚至在编码（LiveBench）、工具使用（Live P_Mul）和记忆（SQuAD 64K）等任务上**超越了对应的原始专家模型**，证明了其协同效应。
- **分领域表现**：
    - **编码**：RAM+ 在 LiveBench (ACC: 40.23) 和 LiveCodeBench (ACC: 31.60) 上均表现最佳。
    - **工具使用**：RAM+ 在复杂的 Live Parallel Multiple 任务上达到 **70.83**，大幅领先工具专家 ToolRL (58.33)。
    - **记忆**：RAM+ 在 SQuAD 64K 任务上取得 **82.03** 的全局最佳成绩，优于记忆专家 MemAgent (77.34)。

#### 2. 扩展与鲁棒性实验
- **两两智能体合并**：在 Coding+Tool, Tool+Memory, Coding+Memory 三种组合下，RAM/RAM+ 均取得最高平均分，证明了方法的普适性。
- **不同模型架构**：在 **Llama-3.2-3B** 基座模型上合并数学、搜索、工具智能体，RAM/RAM+ 同样全面领先基线，并在数学和工具领域实现超越专家的协同。
- **指令跟随能力**：在 IFEval 测试中，RAM 能很好地保持基座模型的指令跟随能力，而 TIES、DARE 等方法在小模型（Llama-3.2-3B）上出现了严重的性能崩溃（下降超10%）。
- **合并效率**：RAM 在取得高性能的同时，**合并耗时仅75.4秒**，远低于 TIES 和 DARE 等复杂方法（>400秒），实现了效率与性能的最佳权衡。

#### 3. 消融实验
- **缩放因子 `r`**：实验表明，适度的缩放（`r=0.10`）能带来最佳性能（RAM+, 66.55），完全不放缩（`r=0.00`，即RAM）或过度放缩（`r=0.20`）都会导致性能下降，验证了补偿机制的必要性和敏感性。

### 五、 核心结论
论文通过系统的实验证明：
- **问题存在**：直接将为SFT设计的模型合并方法用于RL训练的智能体会导致严重的性能下降（信号稀释）。
- **方法有效**：提出的 **RAM** 方法通过**分布感知的合并策略**（区分共享/独特参数，并对独特参数进行补偿性重缩放），成功解决了上述问题。
- **效果卓越**：RAM 不仅在合并性能上实现了 **SOTA**，更关键的是，它能够**融合并提升**不同专家的能力，使合并后的通用模型在多数任务上**优于原有的单一专家**，为构建高性能、多功能的通用智能体提供了一种高效实用的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.13572v1)
- [HTML 版本](https://arxiv.org/html/2601.13572v1)
