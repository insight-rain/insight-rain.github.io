# Behavior Knowledge Merge in Reinforced Agentic Models

**相关性评分**: 6.0/10

**排名**: #24


---


## 基本信息

- **arXiv ID**: [2601.13572v1](https://arxiv.org/abs/2601.13572v1)
- **发布时间**: 2026-01-20T03:56:53Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Xiangchi Yuan, Dachuan Shi, Chunhui Zhang, Zheyuan Liu, Shenglong Yao, Soroush Vosoughi, Wenke Lee

## 关键词

fine tune, offline Reinforcement Learning, world model

## 一句话总结

该论文提出了一种针对强化学习训练后的智能体模型进行模型合并的新框架，以解决现有方法在保留任务特定能力方面的不足。

## 摘要

Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.

## 详细分析

## 论文摘要：强化智能体模型中的行为知识融合

### 1. 研究背景和动机
随着大语言模型（LLM）在智能体应用中的普及，**强化学习（RL）** 已成为提升模型推理能力的关键后训练方法。然而，现实应用通常需要一个通用模型具备多种任务能力。**模型融合** 提供了一种无需重新训练、即可整合多个任务专用模型的实用方案。现有融合方法（如Task Arithmetic、TIES、DARE）主要针对监督微调（SFT）场景设计，其假设任务向量（参数更新）是**密集且全局可比**的。但研究发现，**基于策略的RL产生的任务向量具有高度稀疏性和异质性**。当将标准的全局平均融合应用于RL训练的智能体时，那些编码了关键任务特定行为的、非重叠的“独特”参数更新会被稀释，导致**信号稀释**问题，严重损害融合后模型的性能。

### 2. 核心方法和技术创新
为解决上述问题，本文提出了 **RAM（强化智能体融合）** ，一种专为RL训练智能体设计的、**分布感知**的融合方法。其核心创新在于：
- **任务向量分布探测**：通过阈值分析，将每个RL任务向量划分为**共享参数**（被多个任务更新）和**独特参数**（仅被单个任务更新）两个区域。
- **选择性融合策略**：对共享区域的参数进行平均，以整合通用能力；对独特区域的参数则**完全保留其原始幅度**，避免信号稀释。
- **分布感知的重新缩放**：引入**重叠-独特比率**概念，对独特参数进行**任务特定的幅度放大**，以补偿共享区域因平均化带来的性能损失，实现功能等效。

### 3. 主要实验结果
在编码（CURE）、工具使用（ToolRL）、长上下文记忆（MemAgent）等多个智能体领域和Qwen、Llama等不同架构上的实验表明：
- **性能领先**：RAM及其增强版RAM+在12个任务中的9个上达到了最先进性能，平均得分显著超越所有基线融合方法。
- **超越专家**：融合后的通用模型在多数任务上**性能甚至超过了原始的领域专家模型**，显示出智能体间的协同潜力。
- **高效稳健**：RAM在保持高性能的同时，融合速度远快于TIES、DARE等复杂方法，并且在指令跟随等通用能力上遗忘更少，表现出色。

### 4. 研究意义和价值
本研究首次系统揭示了RL任务向量的稀疏异质特性与现有SFT融合方法之间的根本性**不匹配**，并提出了针对性的解决方案。RAM方法为**高效集成多个RL训练的专业智能体**提供了一个强大、实用的工具，使得构建高性能通用智能体成为可能，避免了昂贵的联合训练或维护多个独立模型的开销。这项工作推动了模型融合技术向强化学习领域的发展，对工业界构建复杂多任务智能体系统具有重要的实际应用价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文标题**
**《强化智能体模型中的行为知识融合》**

### **核心问题**
现有模型融合方法（如Task Arithmetic、TIES、DARE）是为**监督微调**模型设计的，它们假设任务向量（Task Vectors）**密集且全局可比**。然而，**基于强化学习训练的智能体模型**产生的任务向量具有**高度稀疏性和异质性**。当使用标准的全局平均融合方法时，RL模型中那些编码关键任务特定行为的、**非重叠的独特参数更新会被稀释**，导致融合后模型性能下降。这种现象被称为 **“信号稀释”**。

### **核心创新点：RAM方法**
论文提出了 **“强化智能体融合”** 方法，这是一种**分布感知**的融合方法，专门为RL训练的智能体模型设计。

**RAM的核心思想**：
1.  **显式解耦**： 将RL任务向量中的参数更新明确区分为**共享区域**和**任务特定独特区域**。
2.  **差异化处理**：
    *   **共享区域**： 进行平均化，以保留多任务通用能力。
    *   **独特区域**： **完全保留其原始幅度**，并引入一个**分布感知的重缩放机制**进行补偿性放大，以对抗因共享区域被平均而导致的性能损失。
3.  **功能等价目标**： 通过理论推导，将重缩放因子 `λ_t` 与任务的“重叠-独特比率” `ρ_t` 关联起来，旨在使融合后模型在特定任务上的性能增益与原始专家模型相当。

### **解决方案的技术步骤**
1.  **探测向量分布**： 通过阈值判断每个任务向量的活跃更新参数，生成二进制掩码 `m_t`，并计算重叠计数向量 `c`，从而区分出共享参数和独特参数。
2.  **重缩放独特区域**： 为每个任务计算一个任务特定的缩放因子 `λ_t`。该因子基于 `ρ_t`（共享参数数量 / 独特参数数量），采用裁剪线性规则 `λ_t = 1 + r · clip(ρ_t, 0, α)`。`ρ_t` 越高，说明该任务更新更多位于共享子空间，在融合中损失更大，因此需要对其独特区域进行更强补偿。
3.  **选择性融合**： 根据每个参数的重叠情况，应用不同的融合策略：
    *   未被任何任务更新：置零。
    *   仅被一个任务更新：应用 `λ_t` 进行缩放。
    *   被多个任务更新：直接取平均。

### **实际价值与实验验证**
*   **性能超越**： 在编码、工具使用、长上下文记忆等多个智能体领域和Qwen、Llama等多种模型架构上的实验表明，RAM/RAM+不仅显著优于所有现有融合基线，其融合后的**通用模型在多数任务上甚至超越了原始的领域专家模型**，实现了**性能协同**。
*   **效率优势**： RAM在取得最优性能的同时，计算效率远高于TIES、DARE等复杂方法，实现了性能-效率帕累托前沿的突破。
*   **泛化性强**： 该方法被证明在不同模型架构、不同智能体组合（两两融合、三融合）以及不同领域上均有效，表明其对RL任务向量异质性的处理具有普适性。
*   **缓解灾难性遗忘**： 在指令跟随评估中，RAM相比其他融合方法（尤其是TIES/DARE）能更好地保持基础模型的通用能力，特别是在小模型上表现更稳定。

### **总结**
**RAM的创新本质在于，它首次系统性地识别并解决了RL智能体融合中的“信号稀释”根本问题。** 它没有将RL任务向量视为同质整体进行粗暴平均，而是通过分布感知的解耦与重缩放，**像保护“珍贵标本”一样保护了RL学习到的稀疏但关键的行为知识**，从而实现了从“多个专家”到“一个更强通才”的高效、无损融合。这项工作为工业界构建统一、强大的多任务智能体提供了一种实用且高效的后处理方案。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对强化学习（RL）训练的智能体模型在合并时性能下降的核心问题，指出其根源在于现有为监督微调（SFT）设计的模型合并方法（如全局平均）与RL产生的稀疏、异构的任务向量不匹配，导致关键任务特定行为知识被稀释。为解决此问题，论文提出了**强化智能体合并（RAM）**方法，该方法通过分析任务向量的分布，显式地区分并差异化处理参数更新中的共享区域和任务独有区域：对共享区域进行平均以整合通用能力，同时对独有区域进行选择性保留和基于分布的重缩放，以防止信号稀释。实验结果表明，RAM不仅在多个智能体领域和模型架构上超越了现有合并基线，而且能够解锁智能体间的协同潜力，使得合并后的通用模型在其各自领域任务上的性能甚至超越了原始的专用智能体。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Behavior Knowledge Merge in Reinforced Agentic Models》针对**强化学习（RL）训练的智能体模型**的合并问题，提出了名为**RAM（Reinforced Agent Merging）**的新方法。其核心创新在于首次系统性地识别并解决了将传统为监督微调（SFT）设计的模型合并方法直接应用于RL智能体时存在的根本性不匹配问题。

以下是其明确的创新点，逐条列出并说明：

---

### **1. 识别并形式化了“RL任务向量”与“SFT任务向量”的根本性差异**
- **改进/不同之处**：
    - **以往方法**：现有的模型合并方法（如Task Arithmetic, TIES-Merging, DARE）都基于一个隐含假设——从同一个基础模型微调得到的**任务向量**（即参数更新量 `τ = θ_finetuned - θ_base`）是**稠密、冗余且全局可比**的。这使得全局平均等操作在SFT场景下有效。
    - **本文发现**：论文通过实证分析首次明确指出，**由On-policy RL训练产生的任务向量具有高度稀疏性和异质性**。例如，文中编码智能体仅更新了3.2%的参数，而工具使用和记忆智能体则更新了约50%的参数，且这些更新分布在参数空间的不同区域。
- **解决的具体问题/带来的优势**：
    - **揭示了性能下降的根源**：直接应用SFT的合并方法会导致**信号稀释**——对于RL智能体独有的、关键的任务特异性参数更新，与多个零更新（来自其他未更新该参数的智能体）进行平均，会将其幅度除以智能体数量（`1/N`），从而严重削弱其功能。
    - **为针对性解决方案提供了理论基础**：这一发现是设计RAM方法的前提，明确了不能“一刀切”地处理所有参数更新。

### **2. 提出了“分布感知”的合并框架RAM，明确区分并差异化处理“共享”与“独特”参数更新**
- **改进/不同之处**：
    - **以往方法**：对所有参数更新采用统一的处理策略（如全局平均、基于幅度的修剪、随机丢弃后重缩放）。
    - **本文方法**：RAM首先**探测任务向量的分布**，根据每个参数被多少个智能体更新过，将其划分为三类：
        1.  **共享区域**：被两个及以上智能体更新的参数。
        2.  **独特区域**：仅被一个智能体更新的参数。
        3.  **未改变区域**：所有智能体均未显著更新的参数。
    - 随后，RAM对这三类区域实施**选择性合并策略**：
        - **共享区域**：执行平均，以融合多任务共识能力。
        - **独特区域**：**完全保留其原始更新幅度**，并引入一个基于分布的重缩放因子 `λ_t` 进行**放大**，以补偿共享区域平均化带来的性能损失。
        - **未改变区域**：置零，保持基础模型的通用能力不受干扰。
- **解决的具体问题/带来的优势**：
    - **直接解决了信号稀释问题**：通过保护并放大独特区域的更新，RAM确保了每个智能体的核心专业技能在合并后的模型中得以完整保留。
    - **实现了功能等价性**：论文从理论上推导了重缩放因子 `λ_t`，旨在使合并后模型在特定任务上的性能增益与原始专家模型相当，这为方法提供了理论依据。
    - **保持了多任务平衡**：对共享区域进行平均，避免了不同任务知识之间的冲突，维持了模型的通用性和稳定性。

### **3. 引入了基于“重叠-独特比率”的自适应重缩放机制**
- **改进/不同之处**：
    - **以往方法**：DARE等方法也使用重缩放，但其缩放因子是为了补偿随机丢弃操作，是**全局且与任务无关**的。
    - **本文方法**：RAM的重缩放因子 `λ_t` 是**任务特定且自适应的**。它基于每个智能体任务向量的 **“重叠-独特比率”** `ρ_t`（共享参数数量与独特参数数量之比）进行计算。
    - 公式为：`λ_t = 1 + r · clip(ρ_t, 0, α)`。`ρ_t` 越高的智能体，其更新中共享部分占比越大，在合并中因平均化损失也越大，因此需要对其独特部分进行更强的放大以补偿。
- **解决的具体问题/带来的优势**：
    - **实现了精准补偿**：不是对所有独特更新进行无差别放大，而是根据每个智能体在合并中“受损”的风险程度进行差异化补偿，使合并过程更精细、更有效。
    - **解锁了协同潜力**：实验表明，带有重缩放的 **RAM+** 不仅恢复了专家性能，甚至能在多个领域**超越原始专家模型**。这表明，妥善处理共享和独特知识后，不同领域的推理能力可以产生积极的协同效应，使合并后的通才模型比单一领域的专家更强大。

### **4. 在RL智能体合并的特定问题上实现了显著的性能提升与泛化验证**
- **改进/不同之处**：
    - **以往工作**：如UI-TARS2等工业实践尝试合并RL智能体，但仅使用简单的权重插值，效果次优。学术界缺乏针对此问题的系统性研究和专用方法。
    - **本文工作**：
        1.  **全面实验**：在编码（CURE）、工具使用（ToolRL）、长上下文记忆（MemAgent）等多个关键智能体领域，以及Qwen和Llama两种不同架构的模型上进行了广泛验证。
        2.  **显著优势**：RAM/RAM+在几乎所有任务和模型组合上都显著超越了所有基线合并方法（TA, Fisher, TIES, DARE），平均性能最佳。
        3.  **超越专家**：RAM+合并后的模型在多数任务上表现**优于原始的单一领域专家模型**，证明了其“1+1>2”的协同能力。
- **解决的具体问题/带来的优势**：
    - **提供了RL时代模型合并的实用解决方案**：为实际应用中先训练垂直领域RL专家、再合并为通用智能体的范式（如UI-TARS2）提供了高效、高性能的工具。
    - **证明了方法的通用性**：成功在不同模型架构（Qwen, Llama）和不同领域组合上验证了RAM的有效性，表明其对RL任务向量异质性的分析具有普适性。
    - **高效性**：RAM在取得最优性能的同时，计算效率高于TIES、DARE等复杂基线，更具实用价值。

---

## 总结
本文的核心创新在于**范式转换**：它指出RL训练产生的模型更新具有与SFT本质不同的统计特性，因此需要全新的合并哲学。RAM方法通过**分布感知、分区处理、自适应补偿**三大核心设计，成功解决了**信号稀释**这一关键瓶颈，不仅保护了专业知识，更激发了跨任务的协同效应，为构建高性能、多能力的通用RL智能体提供了关键技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

论文通过系统性的实验，证明了所提出的 **RAM (Reinforced Agent Merging)** 方法在合并多个经过强化学习（RL）微调的智能体模型时，显著优于现有基线方法，并展现出**协同效应**——合并后的通用模型在多个领域上的性能甚至超过了原始的、专门化的单个智能体。

### 一、 使用的数据集与评价指标
实验在三个核心的智能体领域进行，使用了业界公认的基准数据集：

| 领域 | 数据集 | 评价指标 |
| :--- | :--- | :--- |
| **代码生成** | LiveBench, LiveCodeBench, MBPP, CodeContests, CodeForces | **ACC (代码通过率)** 和 **UT (单元测试通过率)**， 包括 Pass@1 和 Best-of-N (N=4)。 |
| **工具使用** | Berkeley Function Call Leaderboard (BFCL) 的 **Live** 和 **Non-Live** 子集 | **准确率**，重点关注 **Parallel (并行)** 和 **Parallel Multiple (并行多工具)** 等复杂场景。 |
| **长上下文记忆** | RULER 基准 (基于 HotpotQA 和 SQuAD 合成) | **子串精确匹配 (sub_em)**，在多种上下文长度（7K, 14K, 32K, 64K tokens等）下进行评估。 |
| **通用指令跟随** | IFEval | **指令准确率** 和 **提示准确率**，分 **宽松 (Loose)** 和 **严格 (Strict)** 标准，用于评估合并后模型是否遗忘基础能力。 |

### 二、 对比的基线方法
论文与两大类基线进行了全面对比：

1.  **原始专家模型**：作为性能上限参考。
    *   **CURE** (代码专家)
    *   **ToolRL** (工具使用专家)
    *   **MemAgent** (记忆专家)

2.  **主流模型合并方法**：作为性能下限和对比基准。
    *   **Task Arithmetic (TA)**：任务向量线性平均。
    *   **Fisher Merging**：基于Fisher信息加权平均。
    *   **TIES-Merging**：通过修剪和符号共识减少干扰。
    *   **DARE**：随机丢弃并重缩放参数，常与TA或TIES结合使用 (DARE+TA, DARE+TIES)。
    *   **WUDI Merging**：一种需要额外优化步骤的复杂方法。

### 三、 关键性能提升与结论

#### 1. **整体性能超越所有基线**
*   **主要结果 (表1)**：在合并三个智能体（代码、工具、记忆）的任务中，**RAM+** 取得了 **66.55** 的平均分，显著优于最强的基线 **DARE+TA (63.33)** 和 **DARE+TIES (62.47)**。**RAM** 本身也以 **64.82** 分领先。
*   **结论**：RAM/RAM+ 在所有三个领域的平均性能上均排名第一或第二，在12个具体任务中的9个上达到了最优性能。

#### 2. **实现“协同效应”，超越专家模型**
这是论文最核心的发现之一。合并后的通用模型不仅保留了各领域能力，还通过知识融合实现了**性能提升**。
*   **代码领域**：RAM+ 在 LiveBench 和 LiveCodeBench 上的 ACC/UT 超过了专门的代码专家 **CURE**。这表明来自其他任务（如工具使用、记忆）的推理信号增强了对代码的精确性。
*   **工具使用领域**：RAM+ 在复杂的 **Live Parallel Multiple** 任务上取得了 **70.83** 分，远高于工具专家 **ToolRL** 的 **58.33** 分。
*   **记忆领域**：RAM+ 在 **Ruler-SQuAD 64K** 任务上取得了 **82.03** 分，超过了记忆专家 **MemAgent** 的 **77.34** 分。

#### 3. **在多种合并组合中保持鲁棒性**
*   **两两合并实验 (图5， 表4-6)**：在 **代码+工具**、**工具+记忆**、**代码+记忆** 三种两两组合中，RAM/RAM+ 的平均性能始终最高。例如在代码+工具组合中，RAM+ (60.04) 大幅领先最强基线 DARE+TIES (56.74)。

#### 4. **有效缓解基础能力遗忘**
*   **指令跟随评估 (表3)**：在 IFEval 基准上，基于 Qwen 的 RAM 模型在指令准确率上甚至**优于基础模型**。在较小的 Llama 模型上，虽然所有合并方法都有所遗忘，但 **RAM 的遗忘程度显著小于 TIES 和 DARE 等基线**，后者出现了超过10个百分点的性能崩溃。

#### 5. **高效的计算开销**
*   **效率对比 (图9)**：RAM 在仅用 **75.4 秒** 的合并时间内达到了 64.82 的高性能，在 **效率-性能帕累托前沿** 上表现最优。相比需要400秒以上的 TIES 和 DARE 变体，RAM 实现了数倍的加速。

#### 6. **跨架构和领域的泛化性**
*   **Llama 架构实验 (图6)**：将实验扩展到基于 **Llama3.2-3B** 的数学、搜索、工具使用智能体，RAM/RAM+ 同样在所有领域上超越基线，并在数学和工具领域实现了协同效应，验证了方法的普适性。

### 总结
论文通过详实的实验证明，**RAM 方法成功解决了将 RL 微调智能体合并时的“信号稀释”问题**。其核心优势在于：
1.  **性能更优**：在多个基准上全面超越现有 SOTA 合并方法。
2.  **效果协同**：合并后的模型性能可超越原始专家模型，实现了“1+1+1 > 3”。
3.  **稳健高效**：在不同模型组合、架构上表现鲁棒，且计算效率高。
4.  **保留基础能力**：能较好地维持模型原有的通用指令跟随能力。

这些定量结果强有力地支撑了论文的核心论点：针对 RL 任务向量的稀疏、异质特性进行**分布感知的差异化合并**，是提升智能体模型合并效果的关键。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.13572v1)
- [HTML 版本](https://arxiv.org/html/2601.13572v1)
