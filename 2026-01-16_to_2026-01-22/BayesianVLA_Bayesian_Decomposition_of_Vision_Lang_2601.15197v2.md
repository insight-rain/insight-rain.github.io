# BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries

**相关性评分**: 8.0/10

**排名**: #6


---


## 基本信息

- **arXiv ID**: [2601.15197v2](https://arxiv.org/abs/2601.15197v2)
- **发布时间**: 2026-01-21T17:15:22Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune

## 一句话总结

这篇论文提出BayesianVLA框架，通过贝叶斯分解和潜在动作查询解决VLA模型在机器人操作中的泛化问题，无需新数据即可提升性能。

## 摘要

Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.

## 详细分析

## 论文摘要：BayesianVLA: 通过潜在动作查询对视觉-语言-动作模型进行贝叶斯分解

**1. 研究背景和动机**
视觉-语言-动作（VLA）模型在机器人操作中展现出潜力，但在面对新指令或复杂多任务场景时泛化能力不足。本文发现，当前基于目标驱动数据集的训练范式存在一个关键缺陷：**信息坍缩**。在这种数据集中，仅凭视觉观察就能高度预测语言指令，导致指令与动作之间的条件互信息消失。这使得模型退化为**仅依赖视觉的策略**，忽略了语言约束，在分布外（OOD）场景中失败。

**2. 核心方法和技术创新**
为解决此问题，本文提出了 **BayesianVLA** 框架，通过贝叶斯分解强制模型遵循指令。其核心创新包括：
- **潜在动作查询**：引入一组可学习的查询令牌，作为VLM与下游扩散Transformer策略之间的瓶颈接口。
- **双分支架构**：利用因果注意力机制，通过调整查询令牌在输入序列中的位置，构建**先验分支**（仅视觉，学习 `p(a|v)`）和**后验分支**（视觉+语言，学习 `π(a|v, ℓ)`）。
- **贝叶斯目标函数**：优化策略以最大化动作与指令之间的**条件逐点互信息**，等价于最大化后验与先验的**对数似然比**。该目标惩罚了“视觉捷径”，奖励那些能明确解释语言指令的动作。

**3. 主要实验结果**
在 SimplerEnv 和 RoboCasa 基准上的大量实验验证了方法的有效性：
- 在具有挑战性的 OOD SimplerEnv 基准上，相比基线（QwenGR00T）取得了 **11.3%** 的显著提升。
- 在 RoboCasa 桌面操作基准上，达到了 **50.4%** 的平均成功率，超越了包括 Vision-Only 基线在内的多种先进方法。
- 消融实验表明，贝叶斯分解是性能提升的核心，而潜在动作查询本身也带来了效率优势和性能增益。
- 此外，该方法还有助于**保持VLM骨干的通用文本对话能力**，缓解了VLA微调中常见的灾难性遗忘问题。

**4. 研究意义和价值**
本研究从信息论角度揭示了VLA训练中的根本性病理（视觉捷径），并提出了一种结构化的贝叶斯解决方案。**BayesianVLA** 在不需新数据的情况下，显著提升了模型的指令遵循能力和OOD泛化鲁棒性。这项工作强调了在具身AI中引入结构性归纳偏置的重要性，为构建真正理解并可靠执行人类意图的机器人策略提供了新的理论基础和实践路径。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题：视觉捷径（Vision Shortcut）与信息坍缩（Information Collapse）**
当前视觉-语言-动作（VLA）模型在机器人操作任务中，**在训练数据分布内表现良好，但泛化到新指令或复杂多任务场景时表现脆弱**。论文指出根本原因在于**数据集偏差**：大多数机器人数据集以目标驱动方式收集，导致**语言指令 ℓ 仅从视觉观察 v 就能高度预测**。这使得动作 a 和指令 ℓ 之间的条件互信息 **I(ℓ; a | v)** 趋近于零，即发生 **“信息坍缩”**。模型因此退化为一个**仅依赖视觉的先验策略 p(a | v)**，而忽略了语言指令的约束，在分布外（OOD）或模糊场景中完全失败。

### **核心创新点：BayesianVLA 框架**
论文提出 **BayesianVLA**，一个通过贝叶斯分解和潜在动作查询来强制模型遵循指令的新框架。

#### **1. 理论创新：基于信息论的训练目标**
- **贝叶斯分解视角**：将最优策略分解为 **π(a | v, ℓ) = [p(ℓ | a, v) * p(a | v)] / p(ℓ | v)**。
- **关键目标**：最大化动作与指令之间的**条件逐点互信息（PMI）**，等价于最大化**对数似然比（LLR）**：
  **ℒ_LLR = log p(ℓ | a, v) - log p(ℓ | v)**
- **作用**：该目标**惩罚视觉捷径**，奖励那些能**显式解释语言指令**的动作。它要求动作 a 提供**超出仅从视觉 v 所能推断的、关于指令 ℓ 的额外信息**。

#### **2. 架构创新：潜在动作查询与双分支设计**
- **潜在动作查询（Latent Action Queries）**：引入一组可学习的令牌 `𝒬`，作为 VLM 和下游扩散变换器（DiT）策略之间的**瓶颈接口**。这迫使 VLM 将任务相关信息压缩到这些查询中，并**将 DiT 的条件处理复杂度从 O(N²) 降低到 O(K²)**（N 为视觉语言令牌数，K 为固定的小查询数）。
- **双分支训练框架**：
  - **先验分支（Priori Branch）**：输入序列为 `[v, 𝒬, ℓ]`。利用因果掩码，`𝒬` 只能看到视觉 v，从而学习**视觉先验 p(a | v)**。
  - **后验分支（Posteriori Branch）**：输入序列为 `[v, ℓ, 𝒬]`。`𝒬` 能看到视觉和语言，从而学习**完整的语言条件策略 π(a | v, ℓ)**。
- **推理无开销**：训练时计算双分支，但**推理时仅使用后验分支**，与基线模型相比没有增加任何计算成本。

#### **3. 方法创新：联合优化目标**
总训练损失结合了动作预测损失和 LLR 最大化目标：
```math
ℒ_total = (1-λ)ℒ_FM(ψ; 𝐇_𝒬_post) + λℒ_FM(ψ; 𝐇_𝒬_prior) - βℒ_LLR
```
其中 `ℒ_FM` 是整流流匹配损失，`λ` 和 `β` 是平衡超参数。**-βℒ_LLR** 意味着我们**最大化 LLR**。

### **解决方案总结**
1.  **诊断问题**：通过实验验证了标准 VLA 模型存在“指令遵循的幻觉”，在模糊或 OOD 场景中严重依赖视觉捷径。
2.  **理论重构**：从贝叶斯和信息论角度，将问题形式化为最大化动作与指令的条件互信息。
3.  **架构实现**：设计**潜在动作查询**作为信息瓶颈，并利用 VLM 的因果注意力机制，通过**简单的输入序列重排**实现双分支（先验/后验）建模。
4.  **优化驱动**：使用**对数似然比（LLR）目标**作为核心正则化项，直接激励模型挖掘语言指令中无法从视觉中推断的信息，从而打破视觉捷径。

### **实际价值**
- **显著提升泛化能力**：在 SimplerEnv (OOD) 和 RoboCasa 基准测试上取得 SOTA 性能，特别是在 **SimplerEnv 的 OOD 设置上相对基线提升 11.3%**。
- **保持通用能力**：与基线相比，**BayesianVLA 能更好地保持 VLM 主干网络的文本对话和推理能力**，避免了灾难性遗忘。这是因为 LLR 目标强制模型保持对语言指令的功能性依赖，起到了正则化作用。
- **无需新数据**：该方法**直接从存在偏差的现有数据中恢复出真正的语言条件策略**，降低了数据收集成本。
- **提供新视角**：为社区指出了数据集偏差（`H(ℓ|v) ≈ 0`）的根本问题，并启发了未来关于数据收集策略（如增加模糊性）和替代架构（如世界模型）的研究方向。

**总之，BayesianVLA 的核心贡献在于通过一个优雅的贝叶斯分解框架和信息论目标，从结构和算法层面解决了 VLA 模型中的视觉捷径问题，从而实现了更鲁棒、更真正理解语言的机器人策略。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决当前视觉-语言-动作（VLA）模型在机器人操作任务中，因训练数据存在目标驱动偏差（即视觉场景与语言指令高度可预测地耦合）而导致的“视觉捷径”问题。该问题使得模型退化为忽略语言指令的纯视觉策略，在分布外（OOD）或模糊场景下泛化能力严重不足。为此，论文提出了 **BayesianVLA** 框架，其核心是通过引入**潜在动作查询**构建双分支架构，分别建模视觉先验策略和语言后验策略，并优化一个基于贝叶斯分解的对数似然比目标，以最大化动作与指令之间的条件互信息，从而强制模型依据语言指令生成动作。实验表明，该方法在不增加新数据的情况下，显著提升了模型在 SimplerEnv 和 RoboCasa 等基准上的泛化性能，例如在具有挑战性的 OOD SimplerEnv 基准上取得了 11.3% 的性能提升，有效缓解了视觉捷径问题并保留了基础模型的通用语言能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries》针对当前视觉-语言-动作（VLA）模型存在的核心问题，提出了一套系统性的创新解决方案。以下是其相对于已有工作的明确创新点：

### 1. **问题诊断：识别并形式化“视觉捷径”与“信息坍缩”问题**
   - **相比以往方法的改进/不同之处**：
     - 以往工作主要关注于**扩大模型规模**、**改进架构**（如专用扩散头、双系统设计）或**增加数据多样性**，但未深入分析训练数据固有偏差导致的根本性失效模式。
     - 本文首次通过**信息论视角**，明确指出在目标驱动型数据集中，视觉观测 `v` 和语言指令 `ℓ` 之间存在**近乎确定性的映射**（即 `p(ℓ|v)` 非常尖锐），导致条件互信息 `I(ℓ; a|v)` 坍缩至零。
   - **解决的具体问题/带来的优势**：
     - **精准定位了VLA模型泛化能力差的根源**：模型退化为仅依赖视觉的策略 `p(a|v)`，而忽略了语言指令，因此在分布外（OOD）或模糊场景下完全失效。
     - 为后续的算法设计提供了清晰的理论靶点，即必须**显式地最大化动作与指令之间的条件互信息**。

### 2. **方法创新：基于贝叶斯分解的双分支训练框架**
   - **相比以往方法的改进/不同之处**：
     - 主流VLA模型（如GR00T、π₀系列）通常采用**单一策略网络**直接建模 `π(a|v, ℓ)`，其训练目标（如流匹配损失）容易与有偏数据中的视觉捷径过拟合。
     - 本文提出**显式地分解并建模两个分布**：
       1. **视觉先验分支（Priori Branch）**：学习 `p(a|v)`，捕捉数据中固有的动作偏差。
       2. **语言后验分支（Posteriori Branch）**：学习目标策略 `π(a|v, ℓ)`。
     - 通过**最大化两者的对数似然比（LLR）** 来优化策略，该目标等价于最大化动作与指令之间的**条件点互信息（PMI）**。
   - **解决的具体问题/带来的优势**：
     - **强制模型利用语言信息**：LLR目标惩罚了仅从视觉就能预测动作的行为，奖励那些能为解释语言指令提供额外信息的动作。
     - **从有偏数据中恢复真实的语言条件策略**：即使训练数据存在强视觉-语言相关性，该框架也能迫使模型关注指令，从而显著提升在OOD和模糊任务上的泛化能力（如在SimplerEnv上带来11.3%的绝对提升）。

### 3. **架构创新：引入“潜在动作查询”作为瓶颈接口**
   - **相比以往方法的改进/不同之处**：
     - 现有架构（如OpenVLA、GR00T）通常将**所有视觉和语言标记的隐藏状态**输入到动作解码器（如DiT），导致计算复杂度和模型容量需求高，且视觉与语言信息耦合紧密。
     - 本文在VLM词表中引入了一组**可学习的“潜在动作查询”标记** `𝒬`，并将其**附加在输入序列末尾**。**仅使用这组查询对应的隐藏状态** `𝐇_𝒬` 来条件化下游的扩散变换器（DiT）。
   - **解决的具体问题/带来的优势**：
     - **实现高效的双分支建模**：利用解码器VLM的**因果注意力掩码**，仅通过改变 `𝒬` 在输入序列中的位置（`[v, 𝒬, ℓ]` vs `[v, ℓ, 𝒬]`），即可严格控制其能关注到的上下文（仅视觉 vs 视觉+语言），从而优雅地实例化了先验和后验分支。
     - **降低计算复杂度**：将DiT条件处理的计算复杂度从 `O(N²)`（N为大量视觉-语言标记数）降低到 `O(K²)`（K为少量固定查询标记数，如64），**推理时无额外开销**。
     - **提供有益的归纳偏置**：迫使VLM将任务相关信息压缩并总结到一组紧凑的潜在标记中，这可能有助于学习更鲁棒和可转移的动作表示。

### 4. **副产物优势：缓解灾难性遗忘，保留VLM的通用对话能力**
   - **相比以往方法的改进/不同之处**：
     - 标准的VLA微调通常会导致**灾难性遗忘**，严重损害骨干VLM原有的多模态理解和纯文本对话能力（如图4所示，基线模型输出无意义的乱码）。
     - 本文的LLR目标作为一种**正则化器**，强制模型保持对语言指令的强依赖性，这间接地**维护了骨干VLM处理语言的功能**。
   - **解决的具体问题/带来的优势**：
     - **保留了智能体的高级推理潜力**：模型在微调后仍能进行连贯的纯文本对话（如图4、5所示），这意味着其语言核心未被破坏，为处理新颖指令和进行高层规划保留了基础。
     - **提升了模型的实用价值**：一个既能执行物理动作又能进行自然对话的智能体，更符合通用具身AI的愿景。

### 总结
**BayesianVLA** 的核心创新在于**从“治本”的角度出发**，而非单纯进行规模扩展或架构修补。它通过**信息论诊断问题**，利用**贝叶斯分解和双分支训练框架**设计解决方案，并辅以**潜在动作查询**这一精巧的架构实现。这套组合拳**系统性地解决了VLA模型因数据偏差而忽视语言指令的根本缺陷**，从而在保持高效推理的同时，显著提升了在分布外、模糊场景下的泛化性能，并意外地保留了骨干模型的通用语言能力。这为构建真正理解并遵循人类意图的鲁棒机器人策略提供了新的理论洞察和技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心实验效果
**BayesianVLA** 在多个机器人操作基准测试中显著提升了**指令跟随的鲁棒性和泛化能力**，特别是在**分布外（OOD）和模糊场景**下表现突出。其核心贡献在于通过贝叶斯分解和最大化条件互信息，有效缓解了现有VLA模型因数据集偏差而依赖“视觉捷径”、忽略语言指令的问题。

### 二、 使用的数据集
论文在两个主要仿真基准上进行了评估：
1.  **训练数据集**：
    *   **BridgeDataV2**：大规模、多样化的真实世界机器人操作数据。
    *   **Fractal**：来自Open X-Embodiment (OXE) 的大规模数据集。
    *   **PhysicalAI-Robotics-GR00T-X-Embodiment-Sim**：用于RoboCasa实验的人形机器人桌面操作数据子集。
    *   **LIBERO**：用于动机实验（验证视觉捷径问题）的基准。

2.  **评估基准/数据集**：
    *   **SimplerEnv**：一个模拟评估环境，用于关联真实世界性能，包含4个具体操作任务（如“将勺子放在毛巾上”、“将胡萝卜放在盘子上”）。
    *   **RoboCasa**：大规模仿真厨房环境，包含24个多样化的桌面操作任务（如打开抽屉、使用微波炉等），涉及复杂的关节物体交互。

### 三、 评价指标
*   **主要指标**：**任务成功率**。
    *   **SimplerEnv**：每个任务进行480次独立试验，报告平均成功率（Avg@480）。
    *   **RoboCasa**：每个任务进行50次独立试验，报告平均成功率（Avg@50）。
*   **辅助指标**：
    *   **动作损失**：在训练过程中，比较仅视觉模型与完整VLA模型的损失，以证明视觉捷径的存在。
    *   **定性分析**：展示模型在**保留通用多模态推理能力**（如解决数学问题）方面的表现，与基线模型进行对比。

### 四、 对比的基线方法
论文与当前多种先进的VLA模型和机器人策略进行了广泛对比，主要包括：
*   **基于模仿学习的策略**：RT-1-X, Octo-Small。
*   **开源VLA模型**：OpenVLA, OpenVLA-OFT, RoboVLM, CogACT, SpatialVLA, TraceVLA, VideoVLA。
*   **基于流匹配的模型**：`π0`, `π0.5`。
*   **GR00T系列模型**：Isaac-GR00T-N1.6-Bridge。
*   **同框架下的强基线**：**QwenGR00T**（基于相同的StarVLA框架），这是最直接、最重要的对比基线。
*   **消融实验对照**：
    *   **VisionOnly**：仅使用视觉输入（掩码语言指令）训练的模型，用于实证“视觉捷径”问题。
    *   **QwenGR00T + Action Query**：仅添加潜在动作查询（Latent Action Queries）但不使用贝叶斯分解目标的模型，用于分离架构改进的贡献。

### 五、 关键性能提升与结论
1.  **在SimplerEnv上的OOD泛化能力（核心突破）**：
    *   **BayesianVLA** 取得了 **66.5%** 的平均成功率，达到了**最先进水平**。
    *   相较于同框架基线 **QwenGR00T (55.2%)**，实现了 **+11.3%** 的绝对性能提升。这直接证明了贝叶斯分解目标的有效性，而非架构优势。
    *   在需要精确识别的任务上提升显著，例如“Put Carrot on Plate” (+13.6%) 和 “Put Eggplant in Yellow Basket” (+15.0%)。
    *   **结论**：该方法成功迫使模型学习动作对语言指令的真实因果依赖，而非退化为仅依赖视觉先验的策略。

2.  **在RoboCasa上的表现**：
    *   **BayesianVLA** 取得了 **50.4%** 的平均成功率，**超越了所有对比基线**。
    *   仅视觉基线（VisionOnly）也达到了44.7%的高成功率，证实了该基准中存在严重的视觉捷径问题。
    *   BayesianVLA在视觉捷径失效的任务上表现尤为突出，例如“PnP Novel From Placemat To Plate”任务上达到70.0%，远超VisionOnly基线(34.0%)和标准QwenGR00T(48.0%)。
    *   **结论**：LLR目标成功迫使策略从语言中提取并利用任务指定信息，突破了视觉捷径造成的性能天花板。

3.  **保留通用能力（重要附加价值）**：
    *   **定性实验表明**，标准的QwenGR00T基线在经过VLA训练后，其骨干VLM的**纯文本对话能力出现严重退化**（产生无意义的重复输出）。
    *   相比之下，**BayesianVLA 显著保留了骨干VLM的文本推理和生成能力**。论文分析认为，LLR目标通过强化模型对语言的依赖，起到了正则化作用，防止了语言处理功能的崩溃。
    *   **结论**：这不仅具有实用价值（智能体仍可进行高级推理），也表明该方法更健康地利用了基础模型，而非将其退化为简单的特征提取器。

4.  **消融研究结果**：
    *   **“QwenGR00T + Action Query” (57.5%) vs “QwenGR00T” (55.2%)**：仅引入潜在动作查询带来了+2.3%的提升，证明了该架构作为高效瓶颈接口的价值。
    *   **“BayesianVLA” (63.5%) vs “QwenGR00T + Action Query” (57.5%)**：进一步引入双分支贝叶斯目标带来了+6.0%的显著提升，**证明性能提升的核心源于所提出的贝叶斯分解学习目标，而非单纯的架构变化**。

### 总结
论文通过系统的实验设计，定量和定性地证明了 **BayesianVLA 框架的有效性**。其主要优势体现在：
*   **显著提升OOD泛化能力**（在SimplerEnv上+11.3%），解决了VLA模型的核心痛点。
*   **在存在视觉捷径的基准上实现SOTA**（RoboCasa）。
*   **意外地保留了骨干VLM的通用语言能力**，避免了灾难性遗忘。
*   消融实验明确了**贝叶斯分解目标是性能提升的关键驱动力**。

这些结果共同验证了论文的核心论点：通过显式地建模和最大化动作与指令之间的条件互信息，可以迫使VLA模型真正“理解”并遵循语言指令，从而学习到更鲁棒、更通用的操作策略。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.15197v2)
- [HTML 版本](https://arxiv.org/html/2601.15197v2)
