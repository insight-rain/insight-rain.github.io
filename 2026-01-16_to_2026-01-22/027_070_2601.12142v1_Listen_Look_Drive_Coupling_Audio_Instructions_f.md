# Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving

**相关性评分**: 7.0/10

**排名**: #27


---


## 基本信息

- **arXiv ID**: [2601.12142v1](https://arxiv.org/abs/2601.12142v1)
- **发布时间**: 2026-01-17T19:12:34Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao, Peng Lu, Zufeng Zhang, Sifa Zheng

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune

## 一句话总结

这篇论文提出EchoVLA，一种结合音频指令的用户感知VLA模型，用于自动驾驶，通过情感语音-轨迹对微调多模态大模型，提升驾驶行为的适应性和安全性。

## 摘要

Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\%$ and the collision rate by $74.4\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user's speech.

## 详细分析

## 论文摘要：《Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving》

**1. 研究背景和动机**
当前基于视觉-语言-动作（VLA）模型的自动驾驶系统，通常将语言指令视为静态的先验信息，在推理时无法更新。这导致模型只能从连续的视觉像素中推断不断变化的驾驶目标，常常产生**延迟或过于保守的驾驶行为**。为了让人能够实时、动态地影响自动驾驶决策，本文提出需要为VLA模型引入一个**在线的音频指令通道**，以传达用户的特定意图和情感状态。

**2. 核心方法和技术创新**
本文提出了 **EchoVLA**，一个用户感知的VLA模型。其核心创新在于：
- **多模态数据集构建**：基于nuScenes数据集，将自车运动描述转换为**同步的合成语音指令**，并进一步通过调整语速、音高等合成带有**紧急**或**犹豫**情感的音频。
- **情感引导轨迹调制**：提出一种算法，根据检测到的用户情感（通过计算音频的“唤醒度”`Arousal`），对原始轨迹进行**速度剖面重参数化**，生成情感适配的轨迹（如紧急时加速、犹豫时减速）。
- **多模态思维链（CoT）微调**：构建“图像-音频-情感-轨迹”的多模态CoT数据，对**Qwen2.5-Omni**多模态大模型进行监督微调。该模型能同时处理视觉和音频输入，并通过CoT推理输出情感分析和用户感知的轨迹。

**3. 主要实验结果**
在nuScenes数据集的开环评估中，EchoVLA展现出显著优势：
- 与仅使用视觉感知的基线VLA模型相比，**平均L2误差降低了59.4%，碰撞率降低了74.4%**。
- 定性实验表明，模型能成功**根据音频指令的语义内容调整轨迹**，并能**依据语音中的情感线索（如紧急/犹豫）调制驾驶行为**（如改变速度）。
- 消融实验证明，利用Qwen2.5-Omni原生多模态编码与CoT推理的方法，优于将音频单独编码为特征再融合的其他方案。

**4. 研究意义和价值**
本研究为自动驾驶的VLA模型引入了**实时、动态的人机交互通道**，使系统不仅能理解用户的显式指令，还能感知其情感状态，从而实现**更人性化、更安全**的驾驶决策。它证明了音频信息是对视觉感知的**有效补充而非竞争**，为构建下一代可交互、用户感知的自动驾驶系统奠定了重要基础。未来工作可集中于处理真实环境噪音、多语言指令及降低系统延迟。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
当前基于视觉-语言-动作（VLA）的自动驾驶系统存在一个关键缺陷：**语言指令在推理时是静态的、预先设定的**。这导致模型必须仅从像素中持续推断不断变化的驾驶目标，常常产生**延迟或过于保守的驾驶行为**，无法实时响应用户的动态意图。

### **核心创新点**
论文提出了 **EchoVLA**，一个用户感知的VLA模型，其核心创新在于：
- **引入实时音频指令通道**：将摄像头数据与**现场音频指令**耦合，为用户提供了一个在线的、可实时影响驾驶决策的交互通道。
- **融合用户情感理解**：不仅理解音频的**语义内容**，还通过分析语调、音高、语速中的情感线索（如紧急、犹豫），来**检测用户情绪状态**，并据此调制驾驶行为。
- **构建多模态思维链数据集**：通过数据合成方法，将情感化的语音-轨迹对构建成**多模态思维链**，用于监督微调，使模型学会“听音识意、察情驾车”的推理过程。

### **解决方案**
1.  **数据集构建与增强**：
    - 基于nuScenes数据集，将自车运动描述转换为结构化的自然语言，再通过TTS模型合成同步的音频指令。
    - **情感增强**：通过调整基础音频的语速和音高，合成带有“紧急”和“犹豫”情感的音频指令。使用一个基于音频特征（RMS能量、基频、节奏、频谱质心）的**唤醒度计算公式**来量化情感强度。
    - **轨迹调制**：根据情感标签，对原始轨迹进行**速度剖面重参数化**。紧急情绪对应更激进（更快）的速度曲线，犹豫情绪则对应更保守（更慢，甚至在中点犹豫）的速度曲线，从而生成情感-行为对齐的训练数据。

2.  **模型架构与训练**：
    - **骨干模型**：采用**Qwen2.5-Omni**作为多模态大模型基础。其**时间对齐的多模态RoPE**和**Thinker-Talker架构**能有效处理音频与视觉信号之间的时间对齐问题，并支持高效的流式处理。
    - **监督微调**：使用构建的多模态思维链数据集对模型进行微调。模型输入为前视图像和用户音频，通过思维链推理，输出**音频分析、情感检测和调制后的轨迹航点**。

3.  **验证与结果**：
    - **定量结果**：在nuScenes开环评测中，相比仅使用视觉感知的基线VLA，EchoVLA将**平均L2误差降低了59.4%**，**碰撞率降低了74.4%**。
    - **定性结果**：可视化表明，在驾驶意图明确时，EchoVLA与纯视觉模型表现相当；但当音频指令表达的意图与视觉理解不一致时，EchoVLA能成功遵循音频指令。同时，模型能根据情感调整速度剖面，实现更细腻的驾驶行为。

### **实际价值**
- **提升人车交互与安全性**：为自动驾驶提供了直观、自然的实时指令接口，使系统能更及时、准确地理解并执行用户意图，减少因意图不明确导致的保守或危险行为。
- **实现情感自适应驾驶**：首次在VLA框架中系统性地整合了情感理解，使自动驾驶行为能适应用户的心理状态，提升乘坐体验和信任度。
- **高效的多模态融合范式**：展示了利用现有数据集合成多模态对齐数据、并通过思维链微调赋能模型复杂推理能力的有效路径，为后续多模态自动驾驶研究提供了新思路。

**总结**：EchoVLA的核心创新在于**为自动驾驶VLA模型增加了一个实时、动态且富含情感的“听觉”维度**，通过创新的数据合成方法和模型设计，解决了静态语言指令无法适应动态驾驶意图的问题，显著提升了规划性能和人机协同能力。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决当前基于视觉-语言-动作（VLA）模型的自动驾驶系统存在的一个核心问题：**语言指令在推理时是静态且固定的，导致模型只能从模糊的视觉像素中持续推断用户意图，从而产生延迟或过于保守的驾驶行为**。为此，论文提出了 **EchoVLA** 框架，其核心创新在于**为VLA模型引入了一个在线的音频指令通道**，使用户能够通过实时语音表达具体意图和情绪状态，从而实现对驾驶行为的动态、个性化引导。

该方法的主要技术路径是：首先，**构建一个多模态思维链（CoT）数据集**，通过将nuScenes数据集中的自车运动描述转换为合成语音指令，并进一步合成带有不同情绪（如紧急、犹豫）的语音，同时根据情绪标签调制对应的轨迹速度剖面。然后，**基于Qwen2.5-Omni多模态大模型进行监督微调**，使其能够同时处理视觉图像和音频输入，通过CoT推理输出用户情绪分析以及符合语义和情绪上下文的轨迹。

实验结果表明，该方法取得了显著效果：在nuScenes开环基准测试中，与仅依赖视觉感知的基线VLA模型相比，EchoVLA将**平均L2轨迹误差降低了59.4%，碰撞率降低了74.4%**。这证明了音频指令的引入能够有效补充视觉感知，使模型不仅能遵循语音的语义内容，还能根据语音中的情绪线索（如语调、语速）调整驾驶行为（如加速或减速），从而实现更符合用户实时意图的、更安全的自动驾驶。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving》提出了一个名为 **EchoVLA** 的用户感知视觉-语言-动作模型。其核心创新在于将**实时音频指令**与**用户情感分析**耦合到自动驾驶的决策过程中，以解决现有VLA模型在动态意图理解和交互方面的局限性。

以下是其相对于已有工作的明确创新点：

---

### 1. **引入实时、在线的音频指令通道**
- **改进/不同之处**：
    - **以往方法**：大多数现有的自动驾驶VLA模型将语言输入（如导航指令）视为**静态先验**，在推理时固定不变。模型只能从像素中持续推断可能变化的目标，导致决策延迟或过于保守。
    - **本文方法**：提出了一个**在线音频通道**，允许用户在推理过程中实时输入语音指令，动态影响驾驶行为。这使模型具备了接收**情境相关（in situ）** 指令的能力。
- **解决的问题/带来的优势**：
    - **解决了“意图推断延迟”问题**：当用户意图突然改变（如临时要求变道或加速）时，模型无需仅从模糊的视觉场景中猜测，可直接根据明确的音频指令做出响应，减少了决策延迟。
    - **提升了交互性与可控性**：为自动驾驶系统提供了一个自然、直观的人机交互接口，使用户能够像与人类司机沟通一样，实时表达具体驾驶意图。

### 2. **融合用户情感分析进行轨迹调制**
- **改进/不同之处**：
    - **以往方法**：现有的多模态交互研究主要关注指令的**语义内容**，极少考虑语音中蕴含的**副语言信息**（如语调、语速、音高），即用户的情感状态（如急切、犹豫）。
    - **本文方法**：
        1. 提出了一套计算语音**唤醒度（Arousal）** 的量化方法（公式1-3），用于自动识别“急切”或“犹豫”情感。
        2. 根据识别出的情感，设计了一套**情感条件速度剖面**算法（公式9-11），对规划的轨迹进行速度调制（如急切时加速，犹豫时减速或在中点迟疑）。
- **解决的问题/带来的优势**：
    - **实现了更细腻、人性化的驾驶行为**：系统不仅能理解用户“说什么”，还能感知用户“怎么说”，从而调整驾驶风格以匹配用户的情绪状态，提升乘坐体验和安全性。
    - **解决了单一语义指令的歧义性**：例如，一句“超车”指令，用急切的语气说出和用犹豫的语气说出，应对应不同的执行策略（激进或保守）。情感分析为此提供了额外的决策依据。

### 3. **构建多模态思维链数据集与训练范式**
- **改进/不同之处**：
    - **以往方法**：用于训练自动驾驶VLMs的数据集构建多依赖于人工标注，过程耗时费力，且侧重于视觉问答（VQA）或静态描述，缺乏**指令-动作-情感**的连贯对齐。
    - **本文方法**：
        1. **自动化数据合成**：基于nuScenes数据集，将自车运动描述转化为结构化文本，再使用TTS模型生成同步的音频指令。
        2. **情感增强**：通过修改基音频的语速和音高，合成带有不同情感色彩的同一语义指令。
        3. **轨迹调制**：根据合成音频的情感标签，对对应的真实轨迹进行速度剖面调制，生成“情感-行为”配对数据。
        4. **多模态CoT格式**：将图像、音频、情感分析、调制后的轨迹组织成**多模态思维链**数据，用于监督微调。
- **解决的问题/带来的优势**：
    - **低成本构建大规模对齐数据**：避免了为“音频-情感-驾驶行为”三元组进行海量人工标注，提供了一种可扩展的数据合成方法。
    - **端到端学习对齐**：通过CoT格式，模型被训练成能够进行连贯推理：接收多模态输入 → 分析音频语义与情感 → 输出符合用户意图和状态的轨迹。这提升了模型决策的可解释性和一致性。

### 4. **利用原生支持多模态时序对齐的基础模型**
- **改进/不同之处**：
    - **以往方法**：在处理音频和视觉融合时，常采用外部分离的编码器（如HuBERT、VQ-VAE）提取音频特征，再与视觉特征拼接，面临**跨模态时序对齐**的挑战。
    - **本文方法**：直接选用 **Qwen2.5-Omni** 作为基础模型进行微调。该模型原生具备两大优势：
        1. **时间对齐的多模态RoPE**：通过创新的位置编码，在特征层面保证了音频帧与图像帧的时序对齐。
        2. **Thinker-Talker架构**：解耦了高层理解（Thinker）与序列生成（Talker），适合处理长序列音频并实现高效流式处理。
- **解决的问题/带来的优势**：
    - **解决了跨模态时序不对称问题**：确保了音频指令与视觉场景在时间轴上的精确关联，这对于理解如“现在左转”这类即时指令至关重要。
    - **实现了更高效的融合**：避免了手工设计融合模块，利用了基础模型强大的原生多模态理解与生成能力，取得了更好的性能（如消融实验所示，该方法优于其他音频编码方案）。

---

### **总结与核心价值**
本文的核心创新在于将自动驾驶VLA从**开环、静态、仅语义**的范式，推进到了**闭环、动态、多维度感知用户状态**的新阶段。它通过**实时音频通道**解决了意图输入的动态性问题，通过**情感分析**解决了指令执行的细腻度问题，并通过一套**创新的数据合成与模型训练方法**实现了这一目标。最终，在nuScenes开环评测中，相比纯视觉感知基线，**平均L2误差降低59.4%，碰撞率降低74.4%**，显著提升了轨迹预测的准确性和安全性，为构建更智能、更人性化、更安全的交互式自动驾驶系统奠定了重要基础。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果总结

### 数据集与评价指标
- **数据集**：使用 **nuScenes** 数据集进行训练和评估。这是一个大规模自动驾驶数据集，包含1000个场景，提供多传感器（摄像头、激光雷达、雷达）同步数据。
- **评价指标**：
    - **平均L2误差（米）**：衡量预测轨迹与真实轨迹之间的逐点欧氏距离。
    - **碰撞率（百分比）**：通过将自车边界框沿预测轨迹放置，检查与环境中其他物体边界框的重叠情况。

### 对比基线方法
论文将提出的 **EchoVLA** 模型与以下 **仅视觉感知的VLA基线模型** 进行了对比：
1. **Qwen2-VL-7B**：一个更大的（7B参数）视觉语言模型，使用Multimodal Rotary Position Embedding处理多模态输入。
2. **其他视觉模型**：在消融实验的表格中还提及了 **VLP-UniAD** 和 **RDA-Driver** 作为视觉感知的对比基准。

### 关键性能提升与结论
在nuScenes验证集上的开环规划实验表明，EchoVLA相比仅视觉感知的基线模型取得了显著提升：

| 性能指标 | 提升幅度 | 具体数值对比（EchoVLA vs. Qwen2-VL-7B基线） |
| :--- | :--- | :--- |
| **平均L2误差** | **降低59.4%** | 从 **1.43米** 降至 **0.58米** |
| **平均碰撞率** | **降低74.4%** | 从 **0.43%** 降至 **0.11%** |

**主要结论**：
1. **音频增强的有效性**：引入在线音频指令作为用户意图的显式通道，显著提升了轨迹预测的准确性和安全性。音频信息与视觉感知是**互补而非竞争**的关系。
2. **情感理解与行为调制**：模型能够根据音频中的情感线索（如急促或犹豫的语调）调整驾驶行为。例如，检测到“紧急”情绪时，轨迹的速度配置文件会加快；检测到“犹豫”情绪时则会减速。
3. **技术优势**：通过**多模态思维链**微调 **Qwen2.5-Omni** 模型，有效解决了音频与视觉信号之间的时间对齐问题，实现了基于语义和情感上下文的用户感知驾驶。

### 其他实验发现
- **消融实验**：对比了不同音频编码方法（HuBERT、VQ-VAE、STFT），结果表明，直接利用Qwen2.5-Omni内置的多模态编码器与思维链推理的方法效果最佳（平均L2误差0.38米，碰撞率0.24%）。
- **定性结果**：在驾驶意图存在歧义（视觉观察提示多种可行选项）的场景下，仅视觉模型无法遵循用户音频指令，而EchoVLA可以成功输出符合用户意图的轨迹。

**总结**：论文通过系统的实验验证了EchoVLA框架的有效性。其核心价值在于为自动驾驶VLA模型提供了一个**实时、可交互的用户意图输入通道**，并创新性地利用**音频中的情感信息**来微调驾驶行为，从而在标准基准上大幅提升了规划性能和安全指标。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.12142v1)
- [HTML 版本](https://arxiv.org/html/2601.12142v1)
