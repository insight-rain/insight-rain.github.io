# Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning

**相关性评分**: 8.0/10

**排名**: #4


---


## 基本信息

- **arXiv ID**: [2601.16163v1](https://arxiv.org/abs/2601.16163v1)
- **发布时间**: 2026-01-22T18:09:30Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

Cosmos Policy通过单阶段微调预训练视频模型，直接生成机器人动作和未来状态，实现视觉运动控制和规划，在仿真和真实任务中达到先进性能。

## 摘要

Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/

## 详细分析

## 论文摘要：Cosmos Policy: 基于视频模型微调的视觉运动控制与规划策略

**1. 研究背景和动机**
近年来，大规模预训练视频生成模型在捕捉复杂物理交互和时空演化方面展现出卓越能力。然而，现有工作将这些模型用于机器人策略学习时，通常需要复杂的多阶段后训练或引入新的架构组件来生成动作，过程繁琐。本研究旨在探索一种更简单、高效的方法，直接利用预训练视频模型强大的时空先验知识，将其转化为高效的机器人策略。

**2. 核心方法和技术创新**
本文提出了 **Cosmos Policy**，一种新颖的机器人策略微调方法。其核心创新在于：
- **潜在帧注入**：无需修改预训练视频模型（Cosmos-Predict2）的架构，而是将机器人动作、未来状态图像和状态价值等新模态，通过归一化和复制，直接编码为模型潜在扩散序列中的“潜在帧”。
- **联合训练**：通过单阶段后训练，使模型同时学习策略（生成动作）、世界模型（预测未来状态）和价值函数（预测未来回报），所有目标均通过视频扩散的学习机制统一建模。
- **基于模型的规划**：利用预测的未来状态和价值，通过“最佳N采样”在测试时规划更可能成功的动作轨迹。模型还能利用策略 rollout 数据进一步精炼其世界模型和价值函数。

**3. 主要实验结果**
Cosmos Policy 在多个基准测试中取得了最先进的性能：
- **仿真任务**：在 LIBERO 和 RoboCasa 基准测试中，平均成功率分别达到 **98.5%** 和 **67.1%**，超越了从头训练的扩散策略、基于视频模型的策略以及在同一演示数据上微调的先进视觉-语言-动作模型。
- **真实世界任务**：在具有挑战性的双手操作任务中，取得了 **93.6%** 的最高平均成功率。当启用基于模型的规划后，在两个困难任务中的平均完成率进一步提升了 **12.5%**。
- **消融实验**：验证了预训练模型先验、联合训练目标（预测未来状态和价值）对最终性能均有重要贡献。

**4. 研究意义和价值**
本研究证明了大规模预训练视频模型作为机器人策略强大基础模型的潜力。Cosmos Policy 提供了一种**简单、统一且高效**的微调范式，成功地将视频生成能力转化为机器人控制与规划能力。该方法在数据效率、任务泛化性和复杂操作精度方面均表现出显著优势，为利用基础模型推进机器人学习提供了新的思路。同时，其集成的世界模型与价值函数为实现更智能的模型规划奠定了基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Cosmos Policy

### **核心问题**
如何**高效地将预训练的大规模视频生成模型**（而非静态图像-语言模型）的**时空先验知识**，应用于机器人视觉运动控制与规划任务，同时避免复杂的多阶段训练和额外的架构修改。

### **核心创新点**
论文提出了 **“Cosmos Policy”** 方法，其核心创新在于一种**简单、统一且高效的适配范式**，具体体现在：

1.  **“潜在帧注入”机制**：
    - **问题**：预训练视频模型（如Cosmos-Predict2）原生不支持机器人专有模态（如关节状态、动作、价值函数）。
    - **解决方案**：不修改模型架构，而是将机器人**动作、未来状态观测、状态价值**等新模态，通过归一化和复制，**编码为与视频帧维度相同的潜在帧**，直接插入到模型原有的潜在扩散序列中。
    - **价值**：利用视频模型已有的、强大的处理高维时空序列数据的能力，统一建模所有机器人相关模态，无需设计单独的动作预测头或逆动力学模型。

2.  **单阶段微调与联合训练**：
    - **问题**：先前工作通常需要多阶段训练（如先微调视频模型，再训练独立动作模块），流程复杂。
    - **解决方案**：仅通过**单阶段**在机器人演示数据上进行微调，同时学习**策略、世界模型和价值函数**。通过调整训练批次中数据的条件掩码，在同一架构和训练流程中优化这三个功能。
    - **价值**：极大简化了训练流程，并利用辅助目标（如让策略同时预测动作和未来状态）提升了策略性能，实现了数据高效的学习。

3.  **基于模型的规划能力**：
    - **问题**：仅凭演示数据训练的世界模型和价值函数，难以对分布外的情况进行有效规划。
    - **解决方案**：
        - 收集**策略 rollout 数据**，并利用这些经验数据进一步微调世界模型和价值函数，提升其预测准确性。
        - 采用 **“双模型部署”**：原始模型作为策略（提议动作），微调后的模型作为规划模型（预测未来状态和价值）。
        - 实施 **Best-of-N 采样规划**：并行生成多个候选动作，用规划模型预测其导致的未来状态及价值，选择价值最高的动作执行。
    - **价值**：使策略具备了在复杂、多模态任务中进行前瞻性决策的能力，显著提升了在挑战性任务上的成功率。

### **技术路径总结**
1.  **初始化**：使用预训练的 **Cosmos-Predict2-2B**（一个大规模视频潜在扩散模型）作为基础。
2.  **数据表示**：通过“潜在帧注入”，将机器人多视角图像、本体感知、动作块、未来状态、价值统一编码到模型的潜在序列中。
3.  **训练**：在机器人演示数据上单阶段微调，联合训练策略、世界模型、价值函数。
4.  **部署与规划**：
    - **直接策略**：并行解码生成动作，用于控制。
    - **规划策略**：收集策略经验数据，精炼世界模型和价值函数；通过Best-of-N采样进行基于模型的规划。

### **实际价值与效果**
- **性能领先**：在LIBERO（98.5%）、RoboCasa（67.1%）仿真基准和真实世界双灵巧操作任务（平均93.6分）上均达到**最先进水平**，超越了从头训练的扩散策略、其他视频策略以及微调的大型视觉-语言-动作模型。
- **数据高效**：在RoboCasa上仅用**每任务50条**人类演示数据就达到了SOTA，而对比方法大多需要300条以上。
- **能力全面**：统一框架同时提供了**开环控制、状态预测、价值估计和序列规划**的能力，为机器人学习提供了一个强大且简洁的基础模型适配方案。

**简而言之，Cosmos Policy 的核心创新是提出了一种“即插即用”式的范式，将预训练视频生成模型的核心能力——对复杂时空动态的建模——直接、高效地迁移到了机器人控制与规划领域，并通过巧妙的表示学习和训练策略，实现了卓越的性能和丰富的功能。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决如何高效地将预训练的大规模视频生成模型适配为机器人策略，以利用其强大的时空先验知识进行视觉运动控制和规划。其核心方法是提出 **Cosmos Policy**，通过一种称为“潜在帧注入”的简单技术，在**不修改预训练视频模型架构**的前提下，仅通过单阶段的后训练，将机器人动作、未来状态图像和状态价值等新模态编码为视频扩散过程中的潜在帧，从而将模型统一转化为兼具策略、世界模型和价值函数功能的智能体。该方法在LIBERO、RoboCasa仿真基准和真实世界双臂操作任务中均取得了**最先进的性能**，显著超越了从头训练的扩散策略、基于视频模型的策略以及经过微调的视觉-语言-动作模型，并证明了其能够利用策略经验数据改进世界模型，通过基于模型的规划进一步提升在复杂任务中的成功率。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning》提出了一种将预训练视频生成模型直接适配为机器人策略的新方法。其核心创新点在于**简化了架构、统一了学习目标，并充分利用了预训练模型的时空先验**。以下是其相对于已有工作的明确创新点：

---

### 1. **单阶段微调与零架构修改**
- **改进/不同之处**：
    - **以往方法**：大多数基于视频模型的机器人策略工作（如Video Policy、UVA等）需要多阶段训练（例如，先微调视频模型，再训练独立的动作模块）和引入新的架构组件（如单独的动作扩散器或逆动力学模型）。
    - **Cosmos Policy**：仅通过**单阶段**在机器人演示数据上进行后训练，**无需任何架构修改**。它直接利用预训练视频模型（Cosmos-Predict2）的核心扩散学习机制。
- **解决的问题/优势**：
    - **简化流程**：消除了复杂的多阶段训练和定制化模块设计，降低了实现复杂度和计算成本。
    - **保持先验**：完整保留了预训练视频模型学到的**时空先验**（如物理交互、运动模式），避免了因架构改动而可能损失的有用知识。

---

### 2. **潜在帧注入：统一多模态表示**
- **改进/不同之处**：
    - **以往方法**：通常需要为不同模态（图像、动作、状态值）设计独立的编码器或处理流，或将动作作为条件输入而非生成目标。
    - **Cosmos Policy**：提出**潜在帧注入**技术，将机器人动作、未来状态图像、状态值等非图像模态**编码为潜在扩散序列中的潜在帧**，与图像帧共同处理。
- **解决的问题/优势**：
    - **统一建模**：所有模态（多视角图像、本体感知、动作、值函数）在同一个扩散框架内联合建模，实现了**策略、世界模型和价值函数的统一学习**。
    - **灵活性**：支持任意机器人配置（如不同数量的摄像头），只需调整潜在帧序列的排列即可，无需重新设计模型。

---

### 3. **联合训练策略、世界模型与价值函数**
- **改进/不同之处**：
    - **以往方法**：策略、世界模型和价值函数通常是分开训练的独立模块（如Dreamer、TD-MPC），需要复杂的协调和优化。
    - **Cosmos Policy**：在单模型中通过**条件掩码机制**，在同一训练批次中联合优化三个目标：
        - **策略**：学习 `p(a, s', V(s') | s)`。
        - **世界模型**：学习 `p(s', V(s') | s, a)`。
        - **价值函数**：学习 `p(V(s') | s, a, s')`。
- **解决的问题/优势**：
    - **端到端学习**：共享表征学习提高了样本效率，并利用辅助监督（如预测未来状态）提升了策略的准确性。
    - **简化系统**：无需维护多个独立模型，降低了部署和调参复杂度。

---

### 4. **基于模型的规划与经验利用**
- **改进/不同之处**：
    - **以往方法**：基于模型的规划通常需要从零开始训练世界模型，且难以利用预训练知识；或依赖模型预测控制（MPC）但计算开销大。
    - **Cosmos Policy**：
        1. **利用策略 rollout 数据**：收集策略执行的经验（包括失败数据），微调世界模型和价值函数，使其能预测更广分布的状态和结果。
        2. **双模型部署**：使用原始策略模型生成动作候选，使用微调后的规划模型预测未来状态和价值，进行**最佳N采样**。
- **解决的问题/优势**：
    - **提升泛化**：通过经验学习，世界模型能更准确地预测**分布外情况**（如任务失败），从而规划出更鲁棒的动作。
    - **提高成功率**：在挑战性任务中（如高精度操作），规划版本比直接策略平均成功率提升**12.5%**。

---

### 5. **噪声分布调整以提高动作生成精度**
- **改进/不同之处**：
    - **以往方法**：视频生成模型的噪声分布（对数正态）侧重于视觉质量，但对动作生成可能不够精确。
    - **Cosmos Policy**：将噪声分布调整为**混合对数正态-均匀分布**，增加对高噪声水平的采样权重，并在推理时提高噪声下限（`σ_min = 4`）。
- **解决的问题/优势**：
    - **提升动作精度**：确保模型在去噪过程的早期（高噪声阶段）就能获得足够信号，生成更精确的动作序列，减少机器人控制中的累积误差。
    - **针对机器人任务优化**：针对控制任务对**精确性**的高要求，定制了扩散生成过程。

---

### 6. **在多个基准上实现SOTA性能**
- **改进/不同之处**：
    - **以往方法**：在LIBERO、RoboCasa等基准上，现有方法（如VLA模型、扩散策略）在数据效率、泛化能力或任务成功率上存在瓶颈。
    - **Cosmos Policy**：
        - **LIBERO**：平均成功率 **98.5%**（SOTA）。
        - **RoboCasa**：仅用**50条演示/任务**达到 **67.1%** 平均成功率（SOTA，且数据效率显著更高）。
        - **真实世界ALOHA任务**：平均得分 **93.6%**，优于微调后的VLA模型（如π₀.₅、OpenVLA）。
- **解决的问题/优势**：
    - **数据效率**：在少量演示数据下实现高性能，降低了机器人数据收集成本。
    - **泛化能力**：在未见过的物体、场景和任务中表现鲁棒，证明了视频时空先验对机器人控制的强大迁移能力。

---

## 总结
Cosmos Policy的核心创新在于**“简单而有效”**：通过**潜在帧注入**和**单阶段微调**，将大规模预训练视频生成模型直接转化为强大的机器人策略，同时**统一了策略学习、世界建模和规划能力**。相比以往工作，它避免了复杂的多阶段流水线和定制架构，在保持高性能的同时显著简化了系统设计，并为利用视频基础模型进行机器人控制提供了一个通用且高效的框架。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过广泛的仿真和真实世界实验，全面评估了Cosmos Policy的性能，证明了其在机器人视觉运动控制和规划任务上的卓越表现。

### 一、 使用的数据集与评价指标

#### 1. 数据集
- **LIBERO仿真基准**：包含四个任务套件（Spatial, Object, Goal, Long），用于评估策略处理不同空间布局、物体、语言指定目标和长时程任务的能力。每个套件提供500条专家演示（10个任务 × 50条/任务）。
- **RoboCasa仿真基准**：包含24个静态厨房操作任务，评估在**未见过的物体实例**和**全新场景风格**下的泛化能力。论文仅使用每个任务50条人类遥操作演示进行训练，以评估数据效率。
- **真实世界ALOHA机器人任务**：包含四个具有挑战性的双手操作任务：
    1.  **“将X放在盘子上”**：基于语言指令放置物体（80条演示）。
    2.  **“折叠T恤”**：多步骤、接触丰富的长时程操作（15条演示）。
    3.  **“将糖果放入碗中”**：处理高度多模态的抓取序列（45条演示）。
    4.  **“将糖果放入密封袋”**：毫米级精度的高精度操作（45条演示）。
    评估包含分布内和分布外测试条件，总计101次试验。

#### 2. 评价指标
- **主要指标**：**任务成功率**。
- **ALOHA任务补充指标**：**任务完成度分数**（0-100分），用于更精细地衡量多步骤任务的完成进度，而非简单的二元成功/失败。
- **规划实验指标**：比较**使用规划**与**不使用规划（直接策略）** 时的成功率/分数提升。

### 二、 对比的基线方法

论文与三大类最先进的模仿学习策略进行了全面对比：

1.  **从头训练的扩散策略**：
    - Diffusion Policy, Dita

2.  **基于视频模型的策略**：
    - UVA, UWM, Video Policy

3.  **微调后的视觉-语言-动作模型**：
    - `π0`, `π0.5`, OpenVLA-OFT, CogVLA, UniVLA, DP-VLA, GR00T-N1.5

### 三、 关键性能结果与结论

#### 1. 作为直接策略（无规划）的性能（回答Q1）
Cosmos Policy在三个评估领域均达到了**最先进的性能**。

| 评估领域 | Cosmos Policy 平均成功率 | 对比的最佳基线方法及成功率 | 主要结论与提升 |
| :--- | :--- | :--- | :--- |
| **LIBERO** | **98.5%** | CogVLA (97.4%) | 在最具挑战性的长时程任务套件上提升显著（97.6% vs. 95.4%），**全面超越所有微调的VLA模型**。 |
| **RoboCasa** | **67.1%** | Video Policy (66.0%) / FLARE (66.4%) | **使用更少的演示数据（50条 vs. 基线通常300+条）** 取得了最高成功率，证明了卓越的**数据效率**和泛化能力。 |
| **真实世界ALOHA** | **平均分数 93.6** | `π0.5` (88.6分) | 在需要处理**高动作多模态性**（“放糖果入碗”）和**高精度操作**（“放糖果入密封袋”）的任务上优势明显，可靠性显著更高。 |

**定性分析**：论文指出，微调的VLA模型（如`π0.5`和OpenVLA-OFT+）在后两个高难度ALOHA任务上常出现失败，例如抓取精度不足或无法建模多模态动作分布（见图5）。而Cosmos Policy凭借其视频模型预训练获得的时空先验，能更可靠地处理这些挑战。

#### 2. 消融实验分析（回答Q2）
论文通过消融实验验证了核心组件的必要性：

| 消融项 | LIBERO平均成功率 | 结论 |
| :--- | :--- | :--- |
| **完整Cosmos Policy** | 98.5% | - |
| **移除辅助监督**（策略不联合预测未来状态和值） | 97.0% (-1.5%) | 联合训练目标（策略预测`(a, s’, V(s’))`，世界模型预测`(s’, V(s’))`）对提升性能有积极作用。 |
| **移除预训练模型**（从头训练） | 94.6% (-3.9%) | **视频模型的时空先验至关重要**。从头训练的版本动作生硬，在真实世界任务中得分大幅下降（“折叠T恤”任务得分80.8 vs. 完整版99.5）。 |
| **在RoboCasa上逐步移除所有联合训练目标**，最终变为仅预测动作的“简陋”策略 | 44.4% (vs. 完整版67.1%) | 训练策略时**同时预测未来状态**是Cosmos Policy有效的关键。仅预测动作会导致性能严重下降。 |

#### 3. 基于模型的规划效果（回答Q3 & Q4）
- **规划数据**：收集了648条策略 rollout 数据（包括成功和失败经验），用于微调世界模型和价值函数，得到“规划模型”。
- **规划方法**：采用 **best-of-N采样**（N=8）。使用“策略模型”提出候选动作，用“规划模型”预测未来状态和状态价值`V(s’)`，选择价值最高的动作执行。
- **性能提升**：在最具挑战性的两个ALOHA任务（“放糖果入碗”、“放糖果入密封袋”）上，**模型规划版本比直接策略版本平均分数提升了12.5分**（见图7）。
- **规划机制对比**：论文比较了两种价值函数形式：
    - **模型规划**：预测`V(s’)`，需要世界模型先预测`s’`。
    - **无模型规划**：直接预测`Q(s, a)`。
    - **结论**：**模型规划（`V(s’)`）效果更好**。作者认为在有限的rollout数据下，学习准确的Q函数更困难，且容易过拟合。

### 总结
论文给出了**明确且强有力的定量结果**。Cosmos Policy的核心贡献在于：
1.  **性能领先**：在主流仿真基准和真实机器人任务上均达到SOTA。
2.  **设计简洁**：通过**潜在帧注入**的单阶段微调，无需修改架构或多阶段训练，即统一了策略、世界模型和价值函数。
3.  **数据高效**：在RoboCasa上仅用少量演示即超越使用更多数据的方法。
4.  **可规划性**：能够利用在线经验微调其世界模型，并通过基于模型的规划进一步提升在困难任务上的成功率。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.16163v1)
- [HTML 版本](https://arxiv.org/html/2601.16163v1)
