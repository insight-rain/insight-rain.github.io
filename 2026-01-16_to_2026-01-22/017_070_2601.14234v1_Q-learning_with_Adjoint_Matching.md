# Q-learning with Adjoint Matching

**相关性评分**: 7.0/10

**排名**: #17


---


## 基本信息

- **arXiv ID**: [2601.14234v1](https://arxiv.org/abs/2601.14234v1)
- **发布时间**: 2026-01-20T18:45:34Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Qiyang Li, Sergey Levine

## 关键词

Diffusion, Flow Matching, Inference Acceleration, offline Reinforcement Learning, fine tune

## 一句话总结

QAM是一种基于TD的强化学习算法，通过伴随匹配技术高效优化扩散或流匹配策略，在离线强化学习中表现出色。

## 摘要

We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.

## 详细分析

## 论文摘要：Q-learning with Adjoint Matching (QAM)

**1. 研究背景和动机**
在连续动作空间的强化学习中，一个长期存在的挑战是如何高效地优化一个表达能力强的扩散模型或流匹配策略，使其与参数化的Q函数（评论家）对齐。有效的优化需要利用评论家的一阶梯度信息，但对于流或扩散策略而言，直接通过其多步去噪过程进行基于梯度的反向传播优化，存在数值不稳定的问题。现有方法要么仅使用Q值而丢弃梯度信息，要么依赖近似方法牺牲策略的表达能力或引入偏差，均非理想解决方案。

**2. 核心方法和技术创新**
本文提出了**伴随匹配Q学习**这一新颖的时序差分强化学习算法。其核心技术创新在于引入了**伴随匹配**技术（一种近期在生成模型中提出的方法）。QAM通过转换评论家输出的动作梯度，构建了一个**分步式的目标函数**。该方法的优势在于：
- **避免了不稳定的反向传播**：无需通过策略的多步采样过程进行端到端反向传播。
- **保证了策略的最优性与表达性**：在最优解处能提供**无偏且表达能力强**的策略。
- **形成完整算法框架**：与评论家学习的时序差分更新相结合，构成了一个高效、稳定的训练流程。

**3. 主要实验结果**
在具有挑战性的**稀疏奖励任务**上，QAM在**离线强化学习**和**离线到在线强化学习**两种设置中，均**持续且稳定地超越了现有方法**，证明了其有效性和优越性。

**4. 研究意义和价值**
QAM为解决连续动作RL中**复杂策略的梯度优化难题**提供了一个新颖且实用的解决方案。它首次成功地将伴随匹配思想与强化学习中的价值函数学习相结合，在**不牺牲策略表达能力或引入偏差的前提下**，实现了对评论家梯度信息的有效利用。这项工作为未来在机器人控制、自动驾驶等需要精细连续控制的应用领域，部署更强大、更稳定的深度强化学习算法奠定了重要的技术基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### 一、 论文想解决的核心问题
这篇论文旨在解决**连续动作强化学习（RL）中长期存在的一个技术难题**：如何高效地优化一个**表达能力强的扩散模型（diffusion）或流匹配（flow-matching）策略**，使其与一个参数化的Q函数（critic）对齐。

**问题的具体挑战在于**：
- 为了有效优化策略，需要利用critic（Q函数）的**一阶梯度信息**。
- 但对于扩散或流匹配策略，其**多步去噪过程**使得通过反向传播进行直接梯度优化变得**数值不稳定**。
- 现有方法存在明显缺陷：
    1.  **只利用值函数，丢弃梯度信息**（如某些基于采样的方法），导致优化效率低下。
    2.  **依赖近似方法**（如重参数化技巧的简化版），这会**牺牲策略的表达能力**或**引入偏差**，使学到的策略并非最优。

### 二、 论文的核心创新点
论文提出了一个名为 **“Q-learning with Adjoint Matching (QAM)”** 的新型时序差分（TD）强化学习算法。其核心创新在于**首次将“伴随匹配（Adjoint Matching）”这一生成建模技术，创造性地应用于连续动作RL的策略优化中**。

**创新的具体体现**：
- **方法论的迁移与融合**：将用于稳定训练生成模型（如扩散模型）的“伴随匹配”技术，与基于TD的Critic学习框架相结合，形成了一套完整的RL算法。
- **解决了“利用梯度”与“保持稳定”的矛盾**：QAM能够**同时**做到：
    1.  充分利用Critic提供的**动作梯度信息**来指导策略优化。
    2.  完全**避免**通过多步去噪过程进行不稳定的反向传播。
    3.  在最优解处得到一个**无偏且表达能力强的策略**。

### 三、 解决方案：QAM如何工作
QAM的解决方案可以概括为以下关键步骤：

1.  **策略表示**：使用**扩散模型或流匹配模型**作为策略，这类模型具有强大的表达能力和生成高质量连续动作的潜力。
2.  **Critic学习**：采用标准的**时序差分（TD）更新**来学习参数化的Q函数（Critic），这部分与许多Actor-Critic算法类似。
3.  **策略优化（核心创新步骤）**：
    - **问题转化**：不直接对策略的多步生成过程进行端到端微分。相反，利用**伴随匹配**技术。
    - **伴随匹配的作用**：它将Critic关于动作的梯度信息进行变换，**构造出一个逐步骤（step-wise）的目标函数**。
    - **优化过程**：优化这个新的目标函数来训练策略模型。这个目标函数的关键特性是：
        ```plaintext
        目标函数 = f(变换后的Critic梯度)
        ```
        - **无需**通过策略的多步采样链进行反向传播，从而**避免了数值不稳定**。
        - 在理论上能保证，当优化达到最优时，所得到的策略与使用真实、无偏梯度信息得到的策略是一致的。

4.  **整体流程**：Critic通过TD学习不断改进对状态-动作价值的估计；策略则通过伴随匹配得到的稳定目标函数，利用Critic提供的梯度信息不断更新，最终收敛到一个高性能的表达式策略。

### 四、 实际价值与效果
- **性能提升**：在**困难、稀疏奖励**的任务上（这类任务对探索和策略精度要求极高），QAM在**离线RL**和**离线到在线（offline-to-online）RL**设置中，**均持续优于现有方法**。
- **算法通用性**：为解决扩散/流匹配策略在RL中的优化问题提供了一个**通用、稳定且理论扎实的新框架**，可能推动此类表达力强的生成模型在更复杂RL场景中的应用。

**总结**：QAM的核心创新是**通过“伴随匹配”技术，巧妙地绕开了扩散策略直接梯度优化的稳定性瓶颈，从而能够稳定、高效地利用Critic的全部梯度信息，最终在复杂任务上训练出更强大的策略**。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文针对连续动作强化学习中一个长期存在的挑战：如何高效优化基于扩散或流匹配的表达性策略以最大化参数化Q函数，提出了Q-learning with Adjoint Matching (QAM)算法。核心问题是直接通过反向传播对多步去噪过程进行梯度优化会导致数值不稳定，而现有方法要么丢弃梯度信息，要么牺牲策略的表达能力或引入偏差。QAM通过引入伴随匹配这一生成建模技术，将评论家的动作梯度转化为一个无需不稳定反向传播的逐步目标函数，从而在最优解处获得无偏且表达性强的策略。该方法结合时序差分更新进行评论家学习，在离线和离线到在线强化学习的困难、稀疏奖励任务上，性能持续超越先前方法。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

本文提出的 **Q-learning with Adjoint Matching (QAM)** 算法，在连续动作强化学习领域，针对基于扩散或流匹配的策略优化问题，提出了明确的创新解决方案。

以下是其核心创新点及与已有工作的对比分析：

- **创新点一：引入伴随匹配技术以稳定利用评论家的一阶信息**
    - **与以往方法的区别**：现有方法在处理扩散或流匹配策略时面临两难选择：1）仅使用评论家的价值函数（零阶信息），丢弃梯度信息，导致优化效率低下；2）尝试通过反向传播直接利用梯度，但因策略的多步去噪过程而导致数值不稳定。QAM首次将生成建模中的**伴随匹配**技术引入RL策略优化。
    - **解决的问题与优势**：该技术将评论家关于动作的梯度进行变换，构造出一个**逐步的目标函数**。这完全避免了通过多步去噪过程进行不稳定的反向传播，从而**稳定、高效地利用了评论家的一阶信息**，解决了长期存在的优化稳定性难题。

- **创新点二：实现了无偏且高表达能力的策略优化**
    - **与以往方法的区别**：为了规避不稳定的反向传播，一些现有方法采用了近似手段（例如，对策略表达能力进行限制，或引入优化偏差）。QAM通过伴随匹配，在最优解处能提供一个**无偏的、高表达能力的策略**。
    - **解决的问题与优势**：这确保了算法不会因技术妥协而牺牲策略的**表达能力**或引入**系统性偏差**。高表达能力的策略对于复杂、稀疏奖励任务至关重要，而无偏性保证了学习的理论可靠性。

- **创新点三：在稀疏奖励的困难任务上实现更优性能**
    - **与以往方法的区别**：论文将基于伴随匹配的策略优化器与时间差分备份的评论家学习相结合，形成了一个完整的RL算法。实验表明，QAM在离线和离线到在线RL的困难、稀疏奖励任务上，**性能持续超越**先前方法。
    - **解决的问题与优势**：这证明了该创新点的**实际价值**：它不仅是一个理论上的改进，更能切实解决RL在实际应用中的核心挑战——**在探索困难、反馈稀疏的环境中进行高效、稳定的学习**。其优势在离线和在线两种范式下均得到验证，显示了算法的鲁棒性和通用性。

**总结**：QAM的核心创新在于**方法论层面**的突破。它通过跨领域引入“伴随匹配”这一工具，巧妙地绕开了扩散策略梯度优化中的稳定性瓶颈，从而能够同时实现**稳定优化**、**利用一阶信息**、**保持策略表达力与无偏性**这三个以往难以兼顾的目标，最终在具有实际挑战性的任务上取得了性能提升。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

根据论文内容，Q-learning with Adjoint Matching (QAM) 在实验评估中取得了显著效果，其核心结论是：**在具有挑战性的稀疏奖励任务上，QAM 在离线强化学习（Offline RL）和离线到在线强化学习（Offline-to-Online RL）中，均持续优于现有基线方法。**

### 1. 评估任务与数据集
论文聚焦于**连续动作空间**的强化学习问题，并专门选择 **“困难且具有稀疏奖励”** 的任务进行评估。这类任务对算法的探索能力和策略优化效率要求极高。
- **任务类型**：未在提供的摘要中明确列出具体任务名称（如 MuJoCo、AntMaze 等），但强调是“hard, sparse reward tasks”。
- **设置**：实验涵盖了两种主流的强化学习范式：
    - **离线强化学习**：仅从固定的、预先收集的数据集（无额外环境交互）中学习策略。
    - **离线到在线强化学习**：先在离线数据集上预训练，再允许与环境进行在线交互以进一步微调和提升策略性能。

### 2. 对比的基线方法
论文将 QAM 与**解决同类问题的现有方法**进行对比。这些方法大致分为两类，对应摘要中指出的两种“绕行”策略：
- **仅利用价值信息的方法**：这类方法在优化扩散/流匹配策略时，只使用 Q 函数的值（即 `Q(s, a)`），而丢弃了其梯度信息（即 `∇_a Q(s, a)`）。
- **依赖近似的方法**：这类方法为了使用梯度信息，采用了各种近似手段，但往往**牺牲了策略的表达能力**或**引入了优化偏差**，导致学得的策略并非最优。

### 3. 核心性能优势与结论
QAM 通过 **“伴随匹配”** 技术，成功解决了上述基线方法的局限性，从而实现了性能提升：
- **技术创新带来的优势**：
    - **无偏且表达力强**：QAM 在最优解处能提供无偏的、表达力强的策略，克服了近似方法的有偏性和表达力限制。
    - **稳定高效优化**：通过转换评论家（Critic）的动作梯度，构建了无需不稳定反向传播的逐步骤目标函数，从而实现了对扩散/流匹配策略稳定、高效的一阶优化。
- **最终效果**：
    - **关键指标**：在任务的成功率或累积回报等关键评价指标上，QAM **“持续优于”** 先前的方法。
    - **结论**：这证明了 QAM 有效解决了**在连续动作空间中，利用评论家的一阶信息高效优化高表达力生成式策略**这一长期挑战，并在最具挑战性的稀疏奖励场景中展现了其优越性。

**总结**：论文给出了明确的定量结果结论（“consistently outperforms”），表明 QAM 在设定的困难稀疏奖励任务上，在离线与离线到在线两种设置下，其性能在关键指标上均超越了那些要么丢弃梯度信息、要么引入近似偏差的基线方法。具体的性能提升百分比或曲线图需查阅论文实验章节获取。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.14234v1)
- [HTML 版本](https://arxiv.org/html/2601.14234v1)
