# Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning

**相关性评分**: 6.0/10

**排名**: #32


---


## 基本信息

- **arXiv ID**: [2601.13942v1](https://arxiv.org/abs/2601.13942v1)
- **发布时间**: 2026-01-20T13:18:18Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, Yike Guo

## 关键词

fine tune, Reinforcement Learning

## 一句话总结

该论文提出Glance-or-Gaze框架，通过强化学习激励大型多模态模型自适应聚焦视觉搜索，以解决静态知识导致的查询限制。

## 摘要

Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.

## 详细分析

## 论文《Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning》详细摘要

### 1. 研究背景和动机
大型多模态模型（LMMs）在视觉理解方面取得了显著成功，但其静态参数化知识难以应对涉及长尾实体或动态更新信息的复杂视觉查询。现有的搜索增强方法通常采用被动、无差别的全图像检索，引入了大量视觉冗余和噪声，且缺乏深度迭代反思，限制了其在复杂视觉问答任务中的有效性。因此，本文旨在解决现有方法效率低下、噪声过多以及缺乏自适应规划能力的问题。

### 2. 核心方法和技术创新
本文提出了 **Glance-or-Gaze（GoG）** 框架，将LMM从被动感知转变为主动视觉规划。其核心创新包括：
- **选择性凝视机制**：模型动态决定是“瞥视”全局上下文，还是“凝视”高价值区域，在检索前主动过滤无关视觉信息，减少噪声。
- **双阶段训练策略**：
    - **第一阶段：反思性GoG行为对齐**：通过监督微调，利用精心构建的GoG-Instruct数据集，让模型学会主动选择与跨模态反思的基本范式。
    - **第二阶段：复杂度自适应强化学习**：基于查询难度分层数据，采用分组相对策略优化（GRPO），进一步优化模型的规划策略，使其能根据查询复杂度进行迭代推理和自适应搜索。

### 3. 主要实验结果
在六个基准测试（包括FVQA、InfoSeek、SimpleVQA、MMSearch、LiveVQA、DynVQA）上的实验表明：
- **性能领先**：GoG-3-8B-Think-RL模型取得了最先进的性能，平均表现显著优于全搜索工作流、提示式GoG代理以及之前的搜索增强基线（如MMSearch-R1），提升幅度最高达+19.97分。
- **训练阶段有效性**：监督微调使模型掌握了基本的搜索触发能力，而强化学习则显著提升了模型进行复杂、多步骤混合搜索和迭代反思的比例。
- **消融研究验证**：选择性凝视机制和基于高难度数据的复杂度自适应强化学习对最终性能均有关键贡献，特别是在需要精细视觉验证的复杂场景中。

### 4. 研究意义和价值
本研究具有重要的理论意义和实际价值：
- **范式转变**：提出了从“被动感知”到“主动规划”的新范式，为构建更自主、高效的多模态搜索智能体指明了方向。
- **技术创新**：选择性凝视机制有效解决了视觉检索中的噪声问题，双阶段训练策略（特别是复杂度自适应RL）为训练具备复杂推理能力的LMM提供了可复现的框架。
- **应用前景**：该框架显著提升了LMM在知识密集型视觉问答任务上的准确性和可靠性，可广泛应用于需要实时、精确外部知识验证的领域，如事实核查、产品识别、动态信息查询等。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Glance-or-Gaze (GoG)

### **一、 研究问题**
论文旨在解决**大型多模态模型在知识密集型视觉问答任务中的根本性缺陷**：
1.  **静态知识局限**：LMMs 的参数量化知识是静态的，无法处理涉及**长尾实体**、**实时/演化信息**的查询，导致“知识截止”和“长尾遗忘”。
2.  **现有检索增强方法的低效与噪声**：
    *   **被动、无差别检索**：现有方法通常对整个图像进行检索，引入大量无关的视觉冗余和噪声。
    *   **信息瓶颈**：依赖将视觉细节转换为文本描述，丢失了原始视觉信息。
    *   **缺乏深度迭代反思**：工具调用多为单次、单向，缺乏基于视觉反馈的自我修正和策略调整能力。

### **二、 核心创新点**
论文提出了 **Glance-or-Gaze (GoG)** 框架，其核心创新在于**从被动感知到主动视觉规划的范式转变**。

1.  **“选择性凝视”机制**
    *   **核心理念**：模型不再被动接收整个图像信息，而是**主动决策**是“瞥一眼”全局上下文，还是“凝视”高价值区域进行精细搜索。
    *   **实现方式**：在检索前，通过视觉定位提出候选区域，并动态评估和筛选最相关的图像块，过滤无关信息。这相当于为模型装上了“物理锚点”，强制其从“看到”过渡到“观察”特定细节。

2.  **双阶段训练策略**
    这是一个渐进式、互补的学习架构：

    *   **阶段一：反射式GoG行为对齐**
        *   **目标**：通过监督微调，让模型学会GoG的基本范式（何时及如何调用搜索工具）。
        *   **关键步骤**：
            *   **数据构建**：通过不确定性感知过滤（保留模型回答不一致的难题）和人工验证的轨迹合成，构建 **GoG-Instruct** 数据集。
            *   **训练**：训练模型主动触发 `[Glance]` 或 `[Gaze]` 动作，并进行跨模态反思。

    *   **阶段二：复杂度自适应强化学习**
        *   **目标**：超越SFT学到的固定模式，增强模型处理复杂查询的**规划与迭代推理能力**。
        *   **关键设计**：
            *   **数据分层**：根据SFT模型的失败率，构建不同难度级别（Level 1, Level 2）的RL训练数据，**专注于挑战性样本**以最大化策略优化收益。
            *   **优化算法**：采用**分组相对策略优化**，优化模型在复杂场景下的多步探索性搜索策略。
            *   **奖励函数**：结合答案准确性 (`r_acc`) 和格式合规性 (`r_fmt`)，引导模型产生既正确又结构化的推理轨迹。

### **三、 解决方案与实现路径**
1.  **框架工作流**：`视觉定位提出候选区域` -> `选择性凝视筛选相关区域` -> `仅在选定区域进行精确搜索` -> `迭代的跨模态反思`。
2.  **技术实现**：
    *   **基础模型**：基于 Qwen2.5-VL-7B-Instruct 和 Qwen3-VL-8B-Think。
    *   **高效训练**：使用 LoRA 进行参数高效微调。
    *   **工具集成**：整合了基于 SerpAPI 的图文搜索工具和基于 Grounding DINO 的视觉定位工具。
3.  **评估验证**：
    *   **广泛基准测试**：在6个数据集（FVQA, InfoSeek, SimpleVQA, MMSearch, LiveVQA, DynVQA）上进行了评估，涵盖分布内和分布外场景。
    *   **对比基线**：对比了直接回答、全搜索工作流、基于提示的GoG智能体以及现有搜索增强模型。

### **四、 实际价值与效果**
1.  **卓越性能**：GoG 在多个基准上实现了**最先进的性能**。例如，GoG-3-8B-Think-RL 相比最强的全搜索工作流平均提升 **+9.89**，相比之前的搜索增强基线 MMSearch-R1 提升 **+19.97**。
2.  **行为进化**：
    *   **SFT** 教会模型**基本能力**（何时搜索）。
    *   **RL** 教会模型**高级策略**（如何复杂搜索）：RL训练后，模型进行混合多步搜索的比例大幅上升（从~30%升至~75%），表明其学会了更深入、迭代的信息收集策略。
3.  **关键能力提升**：
    *   **区域选择精度**：RL训练显著提升了模型选择答案相关区域的能力（Crop Selection Accuracy 提升 4.7%-6.8%）。
    *   **错误感知与修正**：RL模型在初始“凝视”错误时，触发反思和自我纠正的比例从30%大幅提升至70%，显示出更好的不确定性校准和主动纠错能力。
4.  **消融实验验证**：
    *   **选择性凝视机制**：移除该机制导致性能显著下降，证明了其必要性，尤其在复杂任务上。
    *   **复杂度自适应RL**：使用困难样本（Level 2）进行RL训练，相比简单样本带来显著且一致的性能提升，验证了该策略的有效性。

### **五、 总结**
**Glance-or-Gaze (GoG)** 通过引入 **“主动视觉规划”** 的新范式，结合 **“选择性凝视”机制** 和 **“双阶段复杂度自适应训练”策略**，有效解决了现有搜索增强LMMs在**效率、噪声和深度推理**方面的瓶颈。它不仅提升了模型在知识密集型视觉问答任务上的准确率，更重要的是赋予了模型**像人类一样“先观察，再聚焦，后验证”的智能搜索行为**，为构建更自主、更可靠的多模态搜索智能体指明了方向。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

**核心问题**：现有的大型多模态模型（LMMs）在处理涉及长尾实体或动态更新信息的知识密集型视觉查询时，受限于其静态的、参数化的知识，表现不佳。现有的搜索增强方法存在两大缺陷：1）检索过程粗放，对整张图像进行检索会引入大量视觉冗余和噪声；2）缺乏深度、迭代的跨模态反思，难以应对复杂的视觉问题。

**主要方法**：论文提出了 **Glance-or-Gaze (GoG)** 框架，旨在将 LMMs 从“被动感知”转变为“主动视觉规划”。其核心创新包括：
1.  **选择性凝视机制**：模型动态决策是“瞥一眼”全局上下文，还是“凝视”高价值区域，在检索前主动过滤无关信息。
2.  **双阶段训练策略**：
    *   **第一阶段（有监督微调）**：通过构建 **GoG-Instruct** 数据集，教导模型掌握主动选择（Glance/Gaze）和跨模态反思的基本范式。
    *   **第二阶段（强化学习）**：采用 **复杂度自适应强化学习**，在更具挑战性的查询上优化模型的规划策略，使其能根据问题复杂度进行迭代推理和自适应搜索。

**主要效果**：在六个视觉问答基准测试（包括域内和域外）上的实验表明，GoG 框架取得了最先进的性能。相比强基线（如强制全搜索流程、提示工程代理、MMSearch-R1等），GoG 实现了显著提升（平均提升高达+19.97分）。消融研究证实，选择性凝视机制和复杂度自适应强化学习对于提升模型在复杂查询上的表现都至关重要。该框架成功地将 LMMs 转变为能够自主规划、迭代验证的主动视觉搜索智能体。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning》的创新点分析

这篇论文针对大型多模态模型（LMMs）在知识密集型视觉问答任务中的局限性，提出了一个名为 **Glance-or-Gaze (GoG)** 的创新框架。其核心创新点在于将LMM从“被动感知者”转变为“主动视觉规划者”，通过动态的视觉搜索策略和强化学习优化，显著提升了处理复杂、长尾或动态信息查询的能力。

以下是论文相对于已有工作的明确创新点，逐条列出并分析其改进之处和带来的优势：

### 1. **提出了“选择性凝视”机制，实现了从粗粒度到细粒度的自适应视觉聚焦**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的搜索增强方法（如MMSearch-R1）通常采用“无差别全图检索”策略，即对整张图像或所有候选区域进行检索。这会导致大量**视觉冗余和噪声**，因为图像中许多区域与问题无关。同时，这些方法严重依赖将视觉细节**转换为文本描述**，造成了信息瓶颈，模型无法直接访问原始视觉数据。
     - **GoG的创新**：引入了 **“Glance”（一瞥）** 和 **“Gaze”（凝视）** 两种动作模式。模型首先“一瞥”全局图像以获取上下文，然后**动态决策**是直接回答，还是需要“凝视”某个特定高价值区域进行精细搜索。这本质上是一个**主动过滤**过程，在检索前就筛除了不相关信息。
   - **解决的具体问题/带来的优势**：
     - **解决了视觉噪声和效率低下问题**：通过先评估后聚焦，大幅减少了传递给检索模块的无关视觉信息，降低了计算开销并提升了检索质量。
     - **保留了原始视觉信息**：直接对选定的图像区域进行搜索，避免了“视觉->文本”转换造成的信息损失，使模型能基于更丰富的视觉证据进行推理。
     - **实现了物理锚定的推理**：强制模型将注意力锚定在特定的候选区域上，促使其从“看见”过渡到“观察”，为推理提供了更细粒度的证据支撑，减少了在噪声环境中的幻觉。

### 2. **设计了“复杂度自适应的强化学习”训练策略，优化了迭代推理和规划能力**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：大多数工具集成式LMMs以**单次通过**的方式执行工具调用，缺乏深度的、迭代式的自我反思和验证。它们的反思通常仅限于文本模态，不支持**跨模态的迭代验证**（例如，根据搜索到的文本信息重新审视图像）。
     - **GoG的创新**：采用**双阶段训练架构**：
       1.  **反射式GoG行为对齐**：通过监督微调，使用精心构建的 `GoG-Instruct` 数据集，教会模型基本的“一瞥/凝视”范式和跨模态反思。
       2.  **复杂度自适应强化学习**：使用**分组相对策略优化**，并在**按难度分层的数据**上进行训练。特别关注那些SFT模型失败率高的“困难样本”，迫使模型学习超越标准模式的、更鲁棒的推理策略。
   - **解决的具体问题/带来的优势**：
     - **解决了复杂查询处理能力不足的问题**：RL训练鼓励模型进行**多步骤、混合类型**的探索性搜索（结合文本搜索、图像搜索和图像裁剪），而不是依赖单一搜索动作。如图3所示，RL后模型进行“混合搜索”的比例从~30%激增至~75%。
     - **增强了错误感知和自我纠正能力**：如表3所示，RL训练不仅将“凝视”正确率从59%提升至75%，更重要的是，当初始聚焦错误时，模型触发反思（自我纠正）的比例从30%大幅提升至70%。这表明RL赋予了模型更好的不确定性校准能力。
     - **实现了基于查询复杂度的自适应规划**：模型能够根据问题的难易程度，动态调整其搜索策略的深度和广度，而不是对所有问题采用固定模式。

### 3. **构建了一个完全自主、端到端的主动视觉规划框架**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有方法可大致分为三类：(1) 基于RAG的静态知识库检索（灵活性差）；(2) 基于提示工程的搜索代理（“即插即用”，模型内在搜索能力未优化）；(3) 工具集成式LMMs（虽将搜索纳入训练，但视觉策略低效且缺乏迭代）。
     - **GoG的创新**：GoG是第一个**完全自主的框架**，将**视觉定位、区域选择、检索执行和跨模态反思**无缝集成在一个统一的、可训练的框架内。它不再是简单地“调用搜索工具”，而是进行“视觉规划”。
   - **解决的具体问题/带来的优势**：
     - **实现了范式转变**：从“被动图像感知”转向“动态、复杂度自适应的视觉规划”。模型主动决定**看哪里、何时看、以及如何结合多模态信息进行迭代推理**。
     - **带来了显著的性能提升**：如表2所示，GoG-3-8B-Think-RL在六个基准测试上取得了最先进的性能，平均分大幅超越最强的全搜索工作流（+9.89）、提示式GoG代理（+15.18）以及之前的搜索增强基线MMSearch-R1（+19.97）。这证明了其框架设计的优越性和强大的泛化能力。

### 总结
该论文的核心创新在于**系统性地解决了现有搜索增强LMMs的两大痛点**：**低效嘈杂的视觉策略**和**缺乏深度迭代反思**。通过 **“选择性凝视”机制** 和 **“复杂度自适应RL”训练策略** 这两个关键技术贡献，GoG框架使LMMs具备了像人类一样“先扫视全局，再聚焦细节，并不断反思验证”的主动视觉信息寻求能力，从而在知识密集型视觉问答任务上实现了质的飞跃。其实验设计充分，消融研究（表4，表5）也有力证明了每个创新组件的必要性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果分析

### 数据集与评价指标
**数据集**：实验在六个知识密集型视觉问答（VQA）基准上进行，分为**领域内（IID）**和**领域外（OOD）**两类：
- **领域内（IID）**：
  - **FVQA-test**：事实性视觉问答。
  - **InfoSeek**：信息寻求型视觉问答（从测试集中随机采样2,000个样本）。
- **领域外（OOD）**：
  - **SimpleVQA**：评估多模态大模型的事实知识边界。
  - **MMSearch**：评估多模态搜索性能。
  - **DynVQA**：动态、多跳视觉问答。
  - **LiveVQA-New**：评估对最新视觉知识的处理能力（采样2,000个英文样本）。

**评价指标**：
- **主要指标**：答案准确率（Accuracy）。
- **评估方法**：采用 **LLM-as-a-Judge**（使用 `gpt-oss-120b` 作为评判模型），通过语义等价性判断预测答案与标准答案是否一致，而非严格字符串匹配。

### 基线方法对比
论文将 GoG 框架与四类基线方法进行了全面对比：
1.  **直接回答（Direct Answer）**：模型仅基于图像和问题直接生成答案，不进行外部检索。
    - 包括开源模型（Qwen2.5-VL 和 Qwen3-VL 系列）和闭源模型（GPT-4o）。
2.  **全搜索工作流（Full-Search Workflow）**：对每个查询都强制进行检索（图像搜索+文本搜索）。
3.  **基于提示的GoG智能体（Prompt-based GoG Agents）**：通过思维图（Graph-of-Thought）提示工程，让基础LMM模拟GoG的搜索规划。
4.  **配备搜索的LMM（Search-Equipped LMMs）**：与专门为搜索优化的模型 **MMSearch-R1** 进行对比。

### 主要性能结果与结论
根据论文表2和后续分析，GoG 框架取得了显著的性能提升：

1.  **整体性能达到最先进水平（SOTA）**：
    - **GoG-3-8B-Think-RL** 模型在六个数据集上的**平均准确率达到56.88%**，显著超越所有基线。
    - 相比最强的 **全搜索工作流** 基线，平均提升 **+9.89** 个百分点。
    - 相比 **基于提示的GoG智能体**，平均提升 **+15.18** 个百分点。
    - 相比之前的搜索增强基线 **MMSearch-R1**，实现了 **+19.97** 个百分点的巨大提升。

2.  **强大的泛化能力**：
    - 性能优势在**领域内**和**领域外**数据集上均保持一致，证明了GoG框架的**强适应性**。例如，在复杂的OOD数据集MMSearch和LiveVQA上，GoG-3-8B-Think-RL相比全搜索工作流分别提升了约1.7和20.3个百分点。

3.  **双阶段训练的有效性**：
    - **监督微调（SFT）**：成功教会模型**何时**以及**如何**使用搜索工具。SFT后的模型（GoG-SFT）已能大幅超越直接回答和MMSearch-R1基线。
    - **强化学习（RL）**：进一步显著提升了模型性能。RL训练鼓励模型进行**更复杂、多步骤的迭代搜索**（见图3，“混合搜索”比例从~30%激增至~75%），并**精细化区域选择能力**（见图4，裁剪选择准确率提升4.7%-6.8%）。

4.  **关键机制的有效性验证（消融实验）**：
    - **选择性凝视（Selective Gaze）机制**：移除该机制（SFT w/o SG）导致模型性能平均下降1.5-1.8个百分点，在复杂任务（如DynVQA）上下降尤为明显（-4.25），证明了其过滤视觉噪声、进行细粒度验证的必要性。
    - **复杂度自适应RL训练**：使用难度更高的 **Level 2 数据**（SFT模型通过率<50%）进行RL训练，相比使用较简单的Level 1数据，能带来更显著的性能提升（对Qwen2.5-VL-7B，平均提升+5.94），验证了针对困难样本优化策略的有效性。

### 核心结论
- **技术创新转化为了实际性能优势**：GoG框架通过 **“选择性凝视”** 机制和 **“复杂度自适应强化学习”** 双阶段训练，成功将LMM从被动感知转变为**主动视觉规划者**。
- **效率与精度兼得**：GoG不仅通过动态决策（Glance-or-Gaze）避免了不必要的搜索开销，还通过迭代反思和区域聚焦，在需要搜索时获得了更精准的信息，从而在多项基准上实现了SOTA性能。
- **该框架为构建更自主、更强大的多模态搜索智能体指明了一个有前景的方向**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.13942v1)
- [HTML 版本](https://arxiv.org/html/2601.13942v1)
