# Point Bridge: 3D Representations for Cross Domain Policy Learning

**相关性评分**: 6.0/10

**排名**: #27


---


## 基本信息

- **arXiv ID**: [2601.16212v1](https://arxiv.org/abs/2601.16212v1)
- **发布时间**: 2026-01-22T18:59:24Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Siddhant Haldar, Lars Johannsmeier, Lerrel Pinto, Abhishek Gupta, Dieter Fox, Yashraj Narang, Ajay Mandlekar

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

Point Bridge 是一个利用点云表示和视觉语言模型实现零样本仿真到现实策略迁移的框架，通过合成数据训练机器人操作代理。

## 摘要

Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/

## 详细分析

## 论文摘要：Point Bridge: 3D Representations for Cross Domain Policy Learning

**1. 研究背景和动机**
构建通用机器人智能体需要大规模的真实世界交互数据，但此类数据的采集成本高昂、难以扩展。仿真和合成数据生成提供了一种可扩展的替代方案，但其有效性受到仿真与现实之间**视觉域差异**的严重制约。现有方法通常需要精确的视觉对齐或大量真实数据，限制了其广泛应用。因此，本研究旨在开发一种能够**最小化对齐需求**、实现**零样本仿真到现实策略迁移**的框架。

**2. 核心方法和技术创新**
本文提出了 **Point Bridge** 框架，其核心是使用**统一、领域无关的基于点的表示**来桥接仿真与现实之间的鸿沟。主要技术创新包括：
- **自动化点表示提取流水线**：利用视觉语言模型（如Gemini）根据任务描述识别相关物体，结合SAM-2进行分割与跟踪，并通过Foundation Stereo等深度估计方法将2D点提升为3D关键点云。
- **统一的策略学习**：将场景（物体关键点）和机器人（末端执行器关键点）统一表示为点云，使用**PointNet编码器**和**基于Transformer的策略网络**进行策略学习。该方法支持单任务和多任务（通过语言指令条件化）训练。
- **灵活的推理时流水线**：支持多种3D感知策略（立体视觉、RGB-D、多视图三角测量），以适应不同的性能与吞吐量需求，并专门处理了仿真中点采样（全表面）与现实（仅可见表面）的视角差异。

**3. 主要实验结果**
在6个真实世界操作任务上的实验表明：
- **零样本仿真到现实迁移**：在单任务和多任务设置下，分别实现了**39%** 和**44%** 的性能提升。
- **与少量真实数据协同训练**：性能进一步提升，在单任务和多任务设置下分别以**61%** 和**66%** 的优势超越之前的视觉基线方法。
- **泛化能力**：方法能够处理透明、反光物体，并成功扩展到涉及软体（折叠毛巾）和关节物体（关闭抽屉）的任务。
- **系统分析**：验证了基于立体视觉的深度估计、仿真中相机视角对齐、以及VLM过滤管道对背景干扰物的鲁棒性等关键设计选择的有效性。

**4. 研究意义和价值**
Point Bridge 通过引入**领域无关的3D点表示**，为利用大规模合成数据训练现实世界机器人策略提供了一条有效途径。其价值在于：
- **实际价值**：显著降低了对昂贵、大规模真实机器人数据集的依赖，并减少了对仿真与现实之间精细视觉对齐的需求，使基于仿真的机器人学习更加实用和可扩展。
- **技术贡献**：提出了一套完整的、自动化的从感知到控制的流水线，创新性地将VLM用于跨域场景理解，为跨域策略学习提供了新的表征范式。
- **启发性**：展示了**结构化、抽象的场景表示**在克服视觉域差异、实现高效策略迁移方面的巨大潜力，为未来机器人基础模型的研究指明了方向。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Point Bridge

### **一、 研究背景与核心问题**

**核心问题**：如何利用**大规模合成仿真数据**来训练机器人策略，并实现**零样本（zero-shot）的仿真到现实（sim-to-real）策略迁移**，从而克服现实世界机器人数据稀缺、采集成本高昂的瓶颈。

**具体挑战**：
1.  **视觉域鸿沟**：仿真环境与真实世界在纹理、光照、背景等方面存在巨大差异，导致基于原始图像（RGB）训练的模型难以直接迁移。
2.  **对齐成本高**：现有的仿真-现实协同训练方法通常需要精心对齐两个域的视觉外观或物体实例，限制了可扩展性。
3.  **表征不统一**：仿真和现实数据使用不同的表征形式（如网格 vs. 图像），阻碍了跨域的有效知识迁移。

### **二、 核心创新点**

Point Bridge 的核心创新在于提出并实现了一套 **“基于统一、领域无关的点云表征”** 的框架，以弥合仿真与现实的鸿沟。其创新性主要体现在以下三个层面：

#### **1. 表征层面的创新：统一且领域无关的点云**
*   **核心理念**：将仿真和现实中的场景都抽象为**任务相关物体的3D关键点云**。这种表征剥离了无关的视觉外观（颜色、纹理）和具体的物体实例，只保留对任务执行至关重要的几何和空间关系信息。
*   **关键优势**：
    *   **领域不变性**：仿真中的网格采样点和现实中的图像投影点，在3D空间中是同一种数据结构，天然对齐。
    *   **任务聚焦**：通过视觉语言模型（VLM）自动过滤，只提取与任务指令相关的物体点云，排除背景干扰。
    *   **支持多任务**：统一的点云输入格式，结合语言指令嵌入，使得单一策略架构能够处理多种任务。

#### **2. 技术流程的创新：自动化、可扩展的流水线**
*   **自动化点云提取**：
    *   **仿真端**：直接从物体网格采样，并模拟真实相机视角和噪声进行投影，以匹配现实分布。
    *   **现实端**：构建了一个**全自动的VLM引导流水线**（Gemini → Molmo → SAM-2 → Foundation Stereo），无需人工标注，即可从RGB图像中识别、分割并重建出任务物体的3D点云。
*   **灵活的推理时流水线**：支持多种3D感知方案（立体视觉Foundation Stereo、商用RGB-D相机、多视图三角定位），让用户可以根据对性能和吞吐量的需求进行选择，增强了框架的实用性。

#### **3. 方法整合的创新：系统性解决跨域策略学习**
Point Bridge 不是单一技术的改进，而是一个**完整的框架**，它系统性地整合了：
*   **合成数据生成**：利用 MimicGen 等工具，将少量人工示教扩展为大规模仿真数据集。
*   **统一表征学习**：通过上述点云表征作为桥梁。
*   **策略架构**：采用基于Transformer的多任务策略（受BAKU启发），以点云和语言指令为输入，预测动作。
*   **协同训练范式**：框架天然支持在合成数据基础上，混合少量真实机器人数据进行协同训练，以进一步提升性能。

### **三、 解决方案概述**

Point Bridge 通过一个**三阶段流程**解决问题：

1.  **数据准备与表征提取**：
    *   在仿真中，利用 MimicGen 生成大规模示教数据。
    *   将**所有观测**（无论仿真或现实）转换为**任务相关物体的3D点云**和**机器人末端关键点**的集合。

2.  **策略学习**：
    *   使用 **PointNet 编码器**处理点云，与语言指令嵌入一起，输入到 **Transformer 策略网络**中进行训练。
    *   通过行为克隆（BC）学习预测机器人的末端位姿和夹爪状态。

3.  **部署与推理**：
    *   在真实机器人上，运行**轻量级的VLM感知流水线**，实时提取当前场景的任务点云。
    *   将点云输入已训练的策略网络，得到动作指令，实现零样本控制。

### **四、 实际价值与性能表现**

*   **卓越的零样本迁移能力**：在单任务和多任务设置下，**零样本 sim-to-real 迁移成功率分别提升39%和44%**，显著优于依赖视觉对齐的基线方法。
*   **高效利用真实数据**：当加入少量（45条）真实示教进行协同训练时，性能进一步提升，**相比之前的视觉方法提升高达61%（单任务）和66%（多任务）**。
*   **强大的泛化性**：
    *   **跨物体实例**：能够处理仿真中未见过、视觉外观差异大的真实物体。
    *   **跨任务类型**：成功应用于刚性物体摆放、软体物体（折叠毛巾）和关节物体（关闭抽屉）操作。
    *   **抗背景干扰**：VLM过滤机制使其对背景干扰物具有鲁棒性。
*   **为机器人基础模型提供新思路**：提供了一条不依赖海量真实机器人数据，而是利用“**合成数据 + 统一抽象表征**”来训练通用机器人智能体的可行路径。

**总结**：Point Bridge 的核心贡献是提出了一种**以几何为中心、任务为驱动**的抽象表征，并构建了自动化工具链来实现这种表征，从而巧妙地绕过了仿真与现实之间难以克服的**视觉域鸿沟**，为实现数据高效的通用机器人学习提供了强有力的新范式。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决机器人策略学习中大规模真实世界数据稀缺，以及仿真与真实环境间视觉域差异（sim-to-real gap）阻碍合成数据有效利用的核心问题。为此，论文提出了 **Point Bridge** 框架，其核心方法是利用**统一、领域无关的基于点的表示**来桥接仿真与现实：通过视觉语言模型（VLM）自动提取任务相关物体的3D关键点，构建与原始视觉外观解耦的场景表示，并基于此训练Transformer策略。该方法最终实现了**无需显式视觉或物体对齐的零样本仿真到现实策略迁移**，在多个真实世界操作任务上，零样本迁移性能提升高达44%，结合少量真实数据协同训练后性能提升高达66%，显著优于之前的视觉基线方法。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Point Bridge: 3D Representations for Cross Domain Policy Learning》的创新点分析

这篇论文针对机器人仿真到现实（sim-to-real）策略迁移中的核心瓶颈——视觉域差异和数据稀缺问题，提出了一个名为 **Point Bridge** 的创新框架。其核心创新点在于**利用统一的、与领域无关的基于点的表示法**，来桥接仿真与现实之间的鸿沟。以下是其相对于已有工作的明确创新点，以及每一点带来的改进和优势：

---

### 1. **统一的、领域无关的基于点的表示法**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：主流的策略学习通常依赖原始的图像观测（如RGB图像），这导致策略对视觉外观（如纹理、光照、背景）高度敏感，仿真与现实之间的视觉差异会严重阻碍零样本迁移。另一些工作使用结构化表示（如边界框、物体姿态、关键点），但这些方法要么需要大量人工标注（如关键点），要么难以跨域泛化（如姿态估计需要针对每个物体的单独模型），要么专注于解决形态差异（如人机迁移）而非视觉差异。
     - **Point Bridge 的做法**：提出将场景（包括机器人和任务相关物体）统一抽象为**三维点云**。在仿真中，点云直接从物体网格采样获得；在现实中，通过一个自动化的、基于视觉语言模型（VLM）的流水线从图像中提取任务相关物体的三维点。这种表示法**丢弃了原始视觉外观信息**，只保留几何和空间关系。
   - **解决的具体问题/带来的优势**：
     - **解决了视觉域差异问题**：由于表示法不包含颜色、纹理等域特异性信息，策略对仿真和现实之间巨大的视觉差异（如不同的桌子、背景、光照）变得不敏感。
     - **实现了零样本仿真到现实迁移**：这是论文最核心的贡献。仅使用仿真数据训练的策略，无需在现实中进行任何微调或显式的视觉/物体级对齐，就能直接在现实世界中执行任务。实验显示，在单任务和多任务设置下，零样本迁移成功率分别提升了39%和44%。
     - **提供了跨域统一的策略输入**：为仿真和现实数据提供了一个共同的“语言”，使得策略可以无缝地在两个域上训练和部署。

### 2. **自动化、可扩展的VLM引导三维点提取流水线**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：许多基于关键点的方法（如Point Policy）依赖于**人工标注**来定义任务相关的语义关键点，这严重限制了可扩展性，无法快速适应新任务或新物体。
     - **Point Bridge 的做法**：设计了一个全自动的流水线，利用现成的视觉语言模型（VLM）和视觉基础模型（VFM）来提取点：
       1. **任务理解与物体识别**：使用 **Gemini** 根据自然语言任务描述识别场景中相关的物体类别。
       2. **物体定位与分割**：使用 **Molmo** 进行像素级定位，再用 **SAM-2** 生成并跟踪物体的2D分割掩码。
       3. **三维点云生成**：从掩码内均匀采样2D点，利用**Foundation Stereo**（或RGB-D、多视图三角化）估计深度，结合相机参数将其提升到3D空间，最终转换到机器人基坐标系。
   - **解决的具体问题/带来的优势**：
     - **解决了人工标注的瓶颈**：完全消除了对每项任务进行耗时、昂贵的人工关键点标注的需求，使方法能够快速扩展到新任务和新物体。
     - **提高了鲁棒性和通用性**：流水线结合了多个先进模型，能够处理具有挑战性的物体（如透明、反光物体），并且SAM-2的内存模块有助于在部署期间处理遮挡。
     - **支持多任务学习**：由于流水线是任务条件化的（通过语言指令），同一套感知系统可以服务于不同的任务，无需为每个任务重新设计。

### 3. **支持多种深度感知策略的灵活推理时流水线**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：通常绑定于单一的感知模态（如特定RGB-D相机），限制了在不同硬件设置下的部署灵活性。
     - **Point Bridge 的做法**：在推理时，提供了**三种可选的深度感知策略**来将2D关键点提升到3D：
       1. **立体视觉与Foundation Stereo**：提供高质量、对反光表面鲁棒的深度，但速度较慢（~5 Hz）。
       2. **商用RGB-D传感器**：速度快（~15 Hz），但对噪声和特定材质敏感。
       3. **双RGB相机与点跟踪三角化**：无需深度传感器，但计算更复杂、速度最慢。
   - **解决的具体问题/带来的优势**：
     - **提高了部署的灵活性和实用性**：用户可以根据对**性能（精度）** 和**吞吐量（速度）** 的不同需求，选择最适合其硬件和任务的感知策略。这使得同一个训练好的策略可以适配多样化的真实机器人平台。
     - **平衡了精度与速度**：论文通过系统分析（见表4）为不同场景提供了指导，例如，**Foundation Stereo**在需要高精度处理复杂物体时是最佳选择。

### 4. **针对仿真-现实点分布不匹配的“相机对齐”数据增强**
   - **相比以往方法的改进/不同之处**：
     - **以往问题**：在仿真中，可以从物体整个表面均匀采样3D点；而在现实中，相机只能看到物体的可见表面，这导致了仿真和现实点云分布的**不匹配**。
     - **Point Bridge 的做法**：在生成仿真训练数据时，**不是**从物体网格上均匀采样，而是**模拟真实相机的视角**。具体步骤是：将网格点投影到虚拟相机图像平面，再利用仿真的真实深度图将其“反投影”回3D，从而生成与真实世界观测更为一致的、**视角依赖的**点云。
   - **解决的具体问题/带来的优势**：
     - **显著缩小了仿真与现实的表示差距**：这种“相机对齐”的采样方式，使得策略在训练时看到的点云分布更接近部署时从真实相机获得的分布。
     - **直接提升了零样本迁移性能**：实验表明（表4），使用相机对齐点进行训练，比使用均匀采样点，在零样本迁移任务上取得了**显著更好的成功率**。这解决了基于仿真的策略训练中一个微妙但关键的表征对齐问题。

### 5. **将点云表示与Transformer策略架构结合，支持高效的多任务学习**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：一些基于点的策略使用Transformer将每个点的轨迹历史作为独立token处理（如Point Policy），这种方式在点数增多时计算量较大，且通常局限于单任务。
     - **Point Bridge 的做法**：
       - **编码器**：使用 **PointNet** 将整个3D点云（包含机器人点和物体点）编码为一个紧凑的向量表示。这类似于CNN处理图像，比基于点的ViT式架构更高效、数据需求更少。
       - **策略架构**：采用基于 **BAKU** 的Decoder-Only Transformer架构，并加入**语言嵌入**（通过Sentence Transformer编码）作为条件。
       - **动作表示**：输出机器人末端执行器的位姿和夹爪状态，并采用动作分块和平滑技术。
   - **解决的具体问题/带来的优势**：
     - **实现了高效的多任务策略**：统一的点云输入和语言条件化，使得单个策略可以学习并执行多个任务。实验证明，多任务策略的性能**不低于甚至优于**单任务策略，展示了出色的可扩展性。
     - **计算更高效**：PointNet编码器比将每个点作为Transformer token处理的方式更轻量，有利于处理更密集的点云（如每物体128个点）。

### 6. **在“仿真-现实协同训练”范式中展现卓越性能**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的仿真-现实协同训练方法（co-training）通常严重依赖仿真与现实场景的**视觉对齐**（如使用数字孪生环境），当视觉差异较大时，协同训练的增益有限甚至为负。
     - **Point Bridge 的做法**：由于使用了域不变的点表示，**Point Bridge 可以无缝地将大规模仿真数据与小规模真实示教数据混合训练**，而无需关心它们的视觉外观是否一致。
   - **解决的具体问题/带来的优势**：
     - **放大了小规模真实数据的价值**：仅需少量（如45条）真实机器人示教，与仿真数据协同训练，就能将策略性能提升至接近完美（98%-100%成功率）。
     - **大幅超越基于图像的协同训练**：在视觉未对齐的设置下，Point Bridge 的协同训练比基于图像的方法在单任务和多任务上分别取得了**61%和66%** 的性能提升。这为解决机器人数据稀缺问题提供了一个更有效的路径：用大量廉价仿真数据辅以极少量的昂贵真实数据。

---

**总结**：`Point Bridge` 的核心创新在于通过 **“领域无关的3D点表示”** 这一抽象层，从根本上


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验效果概述
论文提出的 **Point Bridge** 框架在**零样本仿真到现实策略迁移**和**结合少量真实数据的协同训练**两个核心场景下，均取得了显著优于现有基线方法的性能。其核心贡献在于，通过**领域无关的点云表示**，有效弥合了仿真与现实之间的视觉域差距，实现了高性能的机器人操作策略。

### 二、 使用的数据集
1.  **仿真数据集**：
    *   **来源**：使用 MimicLabs 任务套件构建了3个原子任务：`碗放盘上`、`杯子放盘上`、`叠碗`。
    *   **生成方式**：每个任务收集少量（5个）人工示教，然后利用 **MimicGen** 合成数据生成工具，将每个任务的演示数据扩展到 **1200条**，总计3600条仿真演示。
    *   **目的**：用于训练策略，并评估零样本仿真到现实迁移。

2.  **真实世界数据集**：
    *   **协同训练数据**：为上述3个任务，额外收集了 **45条** 真实机器人遥操作演示（每个任务15条），用于与仿真数据混合训练。
    *   **纯真实数据任务**：为评估框架在更广泛任务上的能力，额外收集了3个仅使用真实数据的任务（`叠毛巾`、`关抽屉`、`碗放入烤箱`），每个任务 **20条** 演示。

### 三、 评价指标
*   **主要指标**：**任务成功率**。在真实机器人上执行策略，统计成功完成指定任务的次数与总尝试次数的比例。
*   **评估规模**：总计进行了 **1410次** 真实世界评估。
*   **具体设置**：对于每个任务配置，通常在 **3个不同的物体实例对上** 各进行 **10次** 策略 rollout，总计 **30次评估**，报告成功次数（如 `23/30`）。

### 四、 对比的基线方法
论文主要与基于**图像表示**的方法进行对比，这是当前机器人策略学习的主流范式。
1.  **图像基线**：使用原始图像作为观测输入，训练行为克隆策略。此基线在两种数据配置下评估：
    *   **纯真实数据训练** (`Real`)
    *   **仿真与真实数据协同训练** (`Co-Train Sim`)
2.  **点云与点轨迹基线**（在附录中）：
    *   **BAKU-PCD**：使用未经筛选的完整场景点云作为输入（需手动排除工作台等干扰物）。
    *   **Point Policy**：一种依赖人工标注关键点并使用点跟踪的方法。

### 五、 关键性能提升与结论
#### 1. 零样本仿真到现实迁移（无需任何真实数据）
*   **单任务设置**：Point Bridge 在三个任务上的平均成功率显著提升。例如，在 `碗放盘上` 任务中，从图像基线的 `9/30` 提升至 `23/30`。**整体相比图像基线提升了39%**（图像基线在零样本设置下完全失败，未在表中显示具体数值，但文中指出其失败）。
*   **多任务设置**：Point Bridge 支持语言条件化的多任务策略，其零样本迁移性能**相比图像基线提升了44%**，且多任务策略性能与单任务相当甚至更好，证明了其可扩展性。
*   **结论**：Point Bridge 利用统一的点云表示，**无需显式的视觉或物体级对齐**，即可实现有效的跨域策略迁移，显著优于依赖像素级一致性的图像方法。

#### 2. 结合少量真实数据的协同训练
*   **单任务设置**：在加入45条真实演示进行协同训练后，Point Bridge 在三个任务上几乎达到完美性能（如 `29/30`, `30/30`），**相比图像基线的协同训练版本，性能提升高达61%**。
*   **多任务设置**：协同训练后的多任务 Point Bridge 策略达到了 **100% 的成功率**（`30/30`），**相比图像基线提升66%**。
*   **结论**：Point Bridge 能够高效地利用小规模真实数据与大规模仿真数据进行协同训练，大幅提升在现实世界中的鲁棒性和性能。

#### 3. 扩展到新任务与物体
*   **新物体类型**：在仅用真实数据训练的 `叠毛巾`、`关抽屉`、`碗放入烤箱` 任务上，Point Bridge 平均成功率达到 **85%**，证明了其表示方法对**非刚性物体和关节物体**也有效。
*   **未见过的物体实例**：即使对训练集中未出现过的全新物体实例，协同训练后的多任务 Point Bridge 策略仍能保持 **97%** 的高成功率，展示了强大的泛化能力。

#### 4. 系统组件分析（关键结论）
*   **深度感知方式**：使用 **Foundation Stereo**（立体视觉）进行深度估计效果最佳，在反射/透明物体上更鲁棒，是性能与速度的最佳权衡（5 Hz）。RGB-D相机因噪声大、对反射表面处理差而性能较低。
*   **相机视角对齐**：在仿真中生成点云时，**模拟真实相机的视角**（而非均匀采样物体全部表面）对缩小仿真与现实分布差距、实现成功迁移**至关重要**。
*   **场景过滤的价值**：与使用未过滤全场景点云的 BAKU-PCD 基线相比，Point Bridge 的 VLM 引导场景过滤管道能自动聚焦于任务相关物体，**对背景干扰物具有极强的鲁棒性**，在存在干扰物时性能几乎不下降，而 BAKU-PCD 则失败。

### 六、 总结
论文通过大量系统的实验证明，**Point Bridge 框架在零样本仿真到现实迁移和仿真-现实协同训练两个核心问题上，均取得了突破性的性能提升**（最高达66%）。其成功关键在于**采用任务相关的3D点云作为统一、领域无关的表示**，并结合**自动化的VLM引导场景理解管道**，从而绕过了对仿真与现实之间像素级对齐的依赖，为利用海量仿真数据训练通用机器人策略提供了高效、可扩展的新路径。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.16212v1)
- [HTML 版本](https://arxiv.org/html/2601.16212v1)
