# HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation

**相关性评分**: 6.0/10

**排名**: #40


---


## 基本信息

- **arXiv ID**: [2601.14874v1](https://arxiv.org/abs/2601.14874v1)
- **发布时间**: 2026-01-21T11:04:19Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Yara Mahmoud, Yasheerah Yaqoot, Miguel Altamirano Cabrera, Dzmitry Tsetserukou

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, Edge Deployment

## 一句话总结

HumanoidVLM 是一个基于视觉-语言模型的检索框架，用于人形机器人通过语义感知自适应选择阻抗参数和抓取配置，实现接触丰富的操作任务。

## 摘要

Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.

## 详细分析

## 论文摘要：HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation

- **研究背景和动机**：人形机器人需要在非结构化环境中执行丰富的接触式操作任务（如放置、抓取、施力），这要求机器人能根据任务语义自适应地调整其交互行为。然而，现有控制器通常依赖**固定、手动调优的阻抗参数和抓取配置**，无法根据场景和任务意图进行动态调整。同时，尽管视觉语言模型（VLM）在语义理解方面表现出色，但其与决定机器人物理交互行为的底层控制参数之间仍存在**语义鸿沟**。本研究旨在**弥合高层语义感知与底层合规控制之间的差距**。

- **核心方法和技术创新**：本文提出了 **HumanoidVLM** 框架，这是一个**基于检索增强的视觉语言驱动系统**，用于Unitree G1人形机器人。其核心创新在于：
    1.  **VLM任务推理**：利用Molmo VLM处理机器人头戴式摄像头的**第一视角RGB图像**，通过结构化视觉查询推断当前操作任务。
    2.  **RAG参数检索**：将推断出的任务标签嵌入后，使用**FAISS相似性搜索**从两个自定义数据库中检索控制参数：
        - **笛卡尔阻抗数据库**：存储任务特定的末端执行器刚度（`K`）和阻尼（`D`）系数。
        - **夹爪角度数据库**：存储针对不同物体类别的最优抓取配置（`γ`）。
    3.  **任务空间阻抗控制**：将检索到的参数发送至机器人板载控制器，驱动一个**无外力传感器的笛卡尔阻抗控制器**，生成顺应性的末端轨迹，并通过逆运动学转换为关节指令执行。整个流程实现了**从单张图像到自适应合规控制参数的端到端自动选择**。

- **主要实验结果**：
    1.  **检索精度**：在涵盖9种任务的14个测试场景中，VLM-RAG系统实现了**93%的参数检索准确率**（13/14），失败案例主要由物体遮挡导致。
    2.  **控制性能**：真实世界实验（包括表面跟随、施压、双物体放置、工具交互、抓取等任务）表明，系统能根据任务选择恰当的阻抗参数。
        - **Z轴跟踪误差**通常保持在 **1-3.5 cm** 以内。
        - 产生的**虚拟力**大小与所选阻抗增益一致，例如软刚度（`Kz=2.0 N/m`）用于顺应性表面跟踪，高刚度（`Kz=6.0 N/m`）用于需要施压的任务，验证了参数检索的有效性和控制的稳定性。

- **研究意义和价值**：
    1.  **技术贡献**：首次将VLM语义推理与基于检索的阻抗控制相结合，应用于全尺寸人形机器人，为**自适应、任务感知的合规操作**提供了一种可解释的新范式。
    2.  **实际价值**：该方法减少了对固定、手工调参的依赖，使机器人能更智能、更安全地适应不同的物体和交互任务，提升了在**人机协作**等非结构化环境中的实用性和安全性。
    3.  **启发性**：研究证明了即使使用**小型、经过实验验证的数据库**，也能通过语义检索有效桥接感知与控制，为未来结合连续参数映射、力反馈闭环调优等工作奠定了基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
论文旨在解决**人形机器人在非结构化环境中进行接触式操作时，缺乏自适应能力**的问题。传统方法依赖**固定、手动调优的阻抗参数和抓取配置**，无法根据不同的物体、任务和场景语义进行动态调整，限制了机器人的适应性、安全性和执行成功率。

### **核心创新点**
论文提出了一个名为 **HumanoidVLM** 的集成框架，其创新性主要体现在以下三个层面的结合：

1.  **语义感知与底层控制的直接桥梁**：首次将**视觉-语言模型**的高层语义推理能力，与决定机器人物理交互行为的**底层笛卡尔阻抗控制参数**直接连接起来。这解决了当前VLM在机器人领域“只懂语义，不懂控制”的脱节问题。
2.  **检索增强生成用于机器人控制**：创新性地将 **FAISS向量数据库检索技术** 应用于机器人控制参数的选择。系统不依赖VLM直接生成控制参数（这不可靠且难以验证），而是通过VLM理解任务后，从一个**经过实验验证的小型数据库**中检索出最优的、已知可行的参数组合。这保证了**控制的安全性和可解释性**。
3.  **面向人形机器人的全系统集成与验证**：在**Unitree G1人形机器人**上实现了从视觉输入到关节执行的完整闭环系统。特别针对人形机器人双足站立、双臂操作的特点，设计了任务空间阻抗控制器，并同时管理**双臂的阻抗参数**和**手爪的开合角度**，完成了多任务的实际验证。

### **解决方案：HumanoidVLM框架**
系统通过一个清晰的流水线解决上述问题：

```
[视觉输入] -> [语义理解] -> [参数检索] -> [阻抗控制] -> [物理执行]
```

1.  **感知与推理层**：
    - **输入**：机器人头部摄像头获取的**第一视角RGB图像**。
    - **处理**：使用 **Molmo VLM** 对图像进行结构化视觉问答，推断出当前的**操作任务**（例如，“表面跟随”、“施加压力”、“双手放置”）。

2.  **参数检索层**：
    - **嵌入与检索**：将VLM输出的任务描述，通过句子嵌入模型转换为向量，使用 **FAISS** 进行相似性搜索。
    - **双数据库**：
        - **笛卡尔阻抗数据库**：存储9种任务对应的、经过实验验证的**刚度(K)和阻尼(D)** 参数（尤其是Z轴方向）。
        - **手爪角度数据库**：存储不同物体类别对应的**最优抓取角度**。
    - **输出**：检索得到针对当前任务和物体的 **`[K, D, γ]`** 三元组。

3.  **控制与执行层**：
    - **任务空间阻抗控制器**：在机器人机载电脑上运行。利用检索到的 `K` 和 `D`，根据公式 **`Më + Dė + Ke = 0`** 计算虚拟力，生成柔顺的末端执行器轨迹。**在没有力传感器的情况下，虚拟力 `F_virt = Ke + Dė` 作为接触力的有效代理**。
    - **逆运动学与轨迹规划**：将柔顺的笛卡尔位姿转换为关节角度指令，发送给机器人的底层位置控制器执行。
    - **手爪控制**：同步执行检索到的手爪开合指令。

### **实际价值与验证**
- **有效性**：在14个视觉场景测试中，系统参数检索准确率达到 **93%**。
- **稳定性**：实际操控中，Z轴跟踪误差保持在 **1-3.5 cm** 以内，虚拟力大小与任务所需的阻抗设置一致（如放置鸡蛋时刚度低、力小；按压时刚度高、力大）。
- **意义**：这项工作为**自适应人形机器人操控**提供了一条**可解释、数据驱动且易于扩展**的技术路径。它表明，结合**大模型的语义理解**与**基于检索的可靠先验知识**，可以安全、有效地实现高层指令到低层物理行为的转化，是迈向更智能、更灵巧人形机器人的重要一步。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决人形机器人在非结构化环境中执行接触式操作任务时，**无法根据语义场景自适应调整底层控制参数（如阻抗和抓取配置）** 的核心问题。为此，作者提出了 **HumanoidVLM** 框架，该方法**将视觉语言模型（VLM）的语义任务推理与基于FAISS的检索增强生成（RAG）模块相结合**，通过单目第一视角图像，从两个预构建的、经过实验验证的数据库中，检索出任务相关的笛卡尔阻抗参数（刚度、阻尼）和最优夹爪角度，并交由任务空间阻抗控制器执行。实验结果表明，该框架在14个视觉场景下的**参数检索准确率达到93%**，并在真实机器人（Unitree G1）上实现了稳定、合规的交互行为，**证明了将高层语义感知与基于检索的底层控制相连接的可行性**，为人形机器人的自适应操作提供了一条可解释的路径。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation》针对人形机器人接触式操作，提出了一个结合视觉语言模型（VLM）与检索增强生成（RAG）的控制框架。其相对于已有工作的明确创新点如下：

---

### 1. **首次将VLM语义推理与任务空间阻抗控制结合，应用于人形机器人平台**
   - **相比以往方法的改进/不同之处：**
     - **先前工作：** 现有的VLM-机器人系统（如RT-2, SayCan, PG-VLM）主要关注**策略生成或末端位姿规划**，且通常使用**固定的阻抗增益**。另一些专注于可变阻抗控制的工作（如OmniVIC, ImpedanceGPT）则主要面向**工业机械臂**或**无人机集群**，未涉及人形机器人及其特有的双足、双操作臂、共享工作空间等挑战。
     - **本文方法：** 首次在**人形机器人（Unitree G1）** 上，构建了一个端到端管道，**直接从单目视觉图像中推断语义任务，并据此检索并执行任务相关的阻抗参数和手爪配置**。
   - **解决的具体问题/带来的优势：**
     - 解决了人形机器人在非结构化环境中，**无法根据语义场景自适应调整物理交互行为（如柔顺性）** 的问题。
     - 使得机器人能够根据具体任务（如“表面跟随”、“施加压力”、“放置易碎物”）和物体特性，**自主选择刚度和阻尼**，实现了从高级语义理解到低级控制参数的直接映射，提升了操作的**适应性、安全性和任务成功率**。

### 2. **提出基于RAG的双数据库检索机制，用于映射语义到控制参数**
   - **相比以往方法的改进/不同之处：**
     - **先前工作：** 可变阻抗参数的调整通常依赖于**学习复杂策略、专家经验手动调试或基于力/触觉传感器的闭环调整**。这些方法要么数据需求大、泛化性有限，要么需要额外的传感器硬件。
     - **本文方法：** 构建了两个**小型、经过实验验证的定制化数据库**（笛卡尔阻抗数据库和手爪角度数据库），并采用**FAISS向量相似性搜索**进行快速检索。这是一个**基于知识的、可解释的检索方法**，而非端到端的黑箱模型。
   - **解决的具体问题/带来的优势：**
     - 解决了**数据效率**和**可解释性**问题。系统不需要海量数据训练复杂的策略网络，仅通过小型数据库即可实现可靠的任务适配。
     - 提供了**明确的决策路径**：VLM识别任务 -> 检索对应阻抗参数 -> 执行。这比端到端模型更易于调试和验证。
     - 在**缺乏腕部力传感器**的硬件限制下，通过预定义的、经过验证的参数库，实现了安全的柔顺控制，是一种实用的工程解决方案。

### 3. **设计了针对人形机器人且无需力传感器的任务空间阻抗控制器**
   - **相比以往方法的改进/不同之处：**
     - **先前工作：** 经典的阻抗控制通常依赖**力/力矩传感器**来测量实际接触力，以实现力/位混合控制或阻抗调节。
     - **本文方法：** 针对Unitree G1**没有腕部力传感器和手爪力反馈**的硬件约束，采用了**基于位置的阻抗控制（或称为“虚拟阻抗”控制）**。控制器通过调节参考轨迹的刚度(K)和阻尼(D)来模拟质量-弹簧-阻尼系统行为，并计算**虚拟力**作为交互响应的代理指标。
   - **解决的具体问题/带来的优势：**
     - 解决了在**低成本、传感器配置有限的人形机器人**上实现柔顺控制的难题。
     - 通过**虚拟力**的分析，可以定量评估不同阻抗参数下的交互行为（如表1所示），证明了即使没有真实力反馈，通过精心选择的参数也能实现稳定且任务适宜的接触。这为资源受限的机器人平台提供了可行的柔顺控制方案。

### 4. **实现了面向人机交互（HRI）场景的、基于视觉的全身人形机器人操作框架**
   - **相比以往方法的改进/不同之处：**
     - **先前工作：** 许多基于VLM的操纵研究集中于**桌面机械臂**的单臂操作，或专注于导航、抓取等单一任务，较少考虑**人形机器人的全身姿态、双操作臂协同、以及以自我为中心（ego-centric）的视觉视角**带来的复杂性。
     - **本文方法：** 整个系统以**头戴式摄像头**的自我中心视角为输入，考虑了**双操作臂任务**（如双手放置不同物体），并在真实的Unitree G1人形机器人上进行了集成与验证。
   - **解决的具体问题/带来的优势：**
     - 强调了系统在**共享工作空间**和**人类环境**中应用的潜力。自我中心视觉更贴近人类操作视角，使任务推断更自然。
     - 通过**双操作臂非对称阻抗参数**的检索（例如，放置鸡蛋的手用低刚度，放置瓶子的手用较高刚度），展示了框架对复杂、精细化HRI任务的支持能力，向真正适应人类环境迈出了一步。

---

**总结：** 本文的核心创新在于**架构层面的整合**——它将前沿的视觉语言理解能力与经典的阻抗控制理论，通过一个可解释的检索机制，巧妙地结合在了一个完整的人形机器人系统上。它没有追求在任何一个子模块（如VLM或控制算法）上的颠覆性突破，而是**务实且创新地解决了“如何让机器人看懂并安全地做”这个实际问题**，特别针对硬件受限的实用人形机器人平台，具有明确的工程和学术价值。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验效果概述
论文通过提出的 **HumanoidVLM** 框架，成功实现了 **基于视觉-语言理解的、任务自适应的仿人机器人阻抗控制与抓取**。核心效果是：机器人能够仅通过一个第一视角（egocentric）的RGB图像，自动推理当前任务，并检索出合适的末端阻抗参数（刚度、阻尼）和夹爪角度，从而在真实物理交互中完成稳定、柔顺的操控。

### 二、 使用的数据集与评价指标

#### 1. 数据集
- **自定义任务数据库**：论文构建了两个小型但经过实验验证的数据库，作为检索的知识库：
    - **笛卡尔阻抗数据库**：包含 **9种** 操控任务（如表面跟随、施压、双物体放置等）对应的最优刚度 `K=[Kx, Ky, Kz]` 和阻尼 `D=[Dx, Dy, Dz]` 参数。这些参数通过在实际的Unitree G1机器人上进行多次试验确定。
    - **夹爪角度数据库**：包含 **9种** 物体类别对应的最优夹爪闭合角度 `γ`，用于安全、可靠地抓取不同物体（刚性、柔软、易碎等）。
- **测试图像集**：收集了 **14张** 第一视角测试图像，覆盖了数据库中9种任务类型，但在**相机视角、物体摆放、机械臂姿态**上有所变化，用于评估系统的泛化能力而非记忆。

#### 2. 评价指标
- **VLM-RAG检索准确率**：核心评价指标。衡量系统从单张图像中**正确推理任务并检索出对应阻抗和夹爪参数**的能力。判定正确的条件包括：
    1.  VLM通过视觉问答正确分类任务。
    2.  基于VLM输出，检索到正确的阻抗参数条目。
    3.  基于任务标签和阻抗场景，检索到正确的夹爪配置。
- **控制性能指标**：用于验证检索出的参数在实际执行中的有效性。
    - **`z`轴跟踪误差**：末端执行器在垂直方向（主要交互方向）的位置误差均值（`|e_z|¯`）和最大值（`max|e_z|`）。
    - **最大虚拟法向力**（`max|F_z_virt|`）：根据阻抗模型（公式4）计算出的虚拟接触力幅值，作为实际接触力的**代理指标**（因机器人无腕部力传感器）。

### 三、 基线方法对比与性能结论

#### 1. 对比基线
- **论文未进行与传统或学习型控制器在相同任务上的直接性能对比（如跟踪误差、成功率等）**。
- 主要的对比体现在 **框架设计理念** 上，与相关工作进行**定性比较**：
    - **OmniVIC, ImpedanceGPT**：同属VLM+检索+阻抗控制的范式，但前者针对工业机械臂，未考虑仿人机器人形态、共享工作空间和人机交互安全；后者用于无人机群，而非接触式操作。
    - **PG-VLM, SayCan, RT-2, Bi-VLA, SwarmVLM**：这些视觉-语言-动作模型主要在**策略或位姿层面**进行泛化，**底层阻抗增益通常是固定不变的**，缺乏根据任务语义进行连续阻抗调制的机制。
- **结论**：HumanoidVLM的创新点在于**首次将视觉-语言模型的语义理解能力，通过检索增强生成（RAG）技术，与仿人机器人任务空间的连续阻抗控制直接连接**，实现了**基于任务语义的自适应柔顺操控**。

#### 2. 关键性能结果与结论
- **检索准确率**：在14个测试场景中，**13次检索正确，准确率达到93%**。唯一失败案例是由于主要物体被部分遮挡，揭示了当前纯视觉任务推理的局限性。
- **控制性能**：如表1所示，所有任务均实现了稳定、有界的交互行为。
    - **`z`轴跟踪误差**：通常保持在 **1-3.5厘米** 的范围内，证明控制器具有良好的跟踪性能。
    - **虚拟力与阻抗参数的关联性**：结果明确显示，**检索出的刚度/阻尼参数直接且合理地影响了虚拟接触力的大小**。例如：
        - **表面跟随**（低刚度）：产生很小的虚拟力（0.103 arb.），实现柔顺接触。
        - **施压按摩**（高刚度）：产生较大的虚拟力（0.334 arb.），实现有力交互。
        - **双物体放置**：为易碎的鸡蛋（右臂）设置低刚度（2.0 N/m），为坚固的瓶子（左臂）设置高刚度（6.0 N/m），虚拟力响应与之匹配（0.089 vs 0.176 arb.），体现了**任务和物体感知的适应性**。
- **核心结论**：
    1.  **可行性验证**：该研究成功证明了 **“语义感知 -> 检索 -> 阻抗控制” 这一路径的可行性**，为仿人机器人的自适应操控提供了一种**可解释**的方法。
    2.  **开环控制的成功**：在没有力传感器反馈的**开环**情况下，通过预定义的任务-参数映射，能够实现多种接触式任务的稳定执行。
    3.  **虚拟力作为有效代理**：实验结果证实，基于阻抗模型计算的虚拟力与实际交互意图一致，未来可作为闭环控制的重要参考。

### 四、 未提供明确定量对比的原因
论文的核心贡献在于提出并验证一个**新的框架范式**，而非在特定任务指标上超越现有控制器。其比较对象是 **“固定阻抗参数”的传统控制方式**。因此，评估的重点在于：
1.  **框架本身是否工作**（检索是否准确）。
2.  **检索出的参数是否有效**（控制是否稳定，虚拟力是否合理）。
3.  **是否实现了语义到控制的衔接**（能否根据不同任务自动选择不同参数）。

论文作者也明确指出，当前结果应视为 **“概念可行性证明”**，而非全面的鲁棒性评估。未来的工作将向更连续的学习映射和闭环力适应扩展。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.14874v1)
- [HTML 版本](https://arxiv.org/html/2601.14874v1)
