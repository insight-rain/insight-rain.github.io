# CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation

**相关性评分**: 6.0/10

**排名**: #38


---


## 基本信息

- **arXiv ID**: [2601.08010v1](https://arxiv.org/abs/2601.08010v1)
- **发布时间**: 2026-01-12T21:24:45Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Chaoyu Li, Deeparghya Dutta Barua, Fei Tao, Pooyan Fazli

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

这篇论文提出CASHEW框架，通过迭代轨迹聚合稳定多模态推理，但未直接涉及机器人应用、轻量架构或边缘部署，主要关注推理稳定性和性能提升。

## 摘要

Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.

## 详细分析

## 论文《CASHEW：通过迭代轨迹聚合稳定多模态推理》详细摘要

### 1. 研究背景和动机
当前，视觉-语言模型在多模态理解和推理任务上表现出色，但其多步推理过程存在**不稳定**的问题。对同一输入进行多次采样，常常会产生**发散**的推理轨迹和**不一致**的最终预测。这种不稳定性在复杂推理场景中尤为突出，微小的感知或解释错误会在思维链中传播放大。现有的“测试时扩展”方法多采用“采样-选择”范式，丢弃了被拒绝轨迹中的部分洞见，且缺乏显式的视觉证据验证，可能导致模型“想得更久”而非“想得更好”。

### 2. 核心方法和技术创新
本文提出了两种互补的方法来稳定多模态推理：

- **Cashew（推理时框架）**：这是一个即插即用的测试时扩展框架。其核心是**迭代轨迹聚合**：将推理视为一个进化过程，在每次迭代中，从候选轨迹群体中采样子集，并利用**显式视觉验证**（通过Grounding DINO模型）过滤掉幻觉的物体/属性声明，仅保留有视觉证据支持的“关键视觉对象”。模型基于这些经过验证的证据，对多个候选轨迹进行合成，生成更高质量的聚合轨迹。该过程重复数次，最终产生一个稳定、基于证据的共识推理轨迹。

- **Cashew-RL（学习型变体）**：通过**后训练**将上述聚合行为内化到单一模型中。训练分为两阶段：
    1.  **监督微调**：学习标准化的聚合输出格式（`<think>`推理链、`<visual_keys>`关键视觉对象、`<answer>`最终答案）。
    2.  **强化学习**：采用**组序列策略优化**，并设计了一个**复合奖励函数**，该函数同时优化**答案正确性**、**证据选择质量**（平衡精确率与召回率）以及**难度感知的长度惩罚**（鼓励对困难问题分配更多推理努力，对简单问题保持简洁）。这使得模型能够学会自适应地整合和提炼多个推理轨迹。

### 3. 主要实验结果
在涵盖图像理解、视频理解和视频推理的**13个基准测试**上进行了广泛评估，结果显著：

- **Cashew**：作为推理时方法，能稳定提升多种骨干VLM（如Qwen3-VL、InternVL）的性能。例如，在ScienceQA上，Qwen3-VL-4B的准确率从69.5%提升至**93.1%（+23.6个百分点）**；在EgoSchema上，Qwen2.5-VL-7B提升**+8.1个百分点**。
- **Cashew-RL**：经过训练后，即使仅进行一次聚合（`T=1`），也显著优于基线。进行三次迭代（`T=3`）时，性能进一步提升，在多个任务上甚至超过了使用相同骨干的Cashew，证明了其内化聚合策略的有效性。例如，Qwen3-VL-8B在ScienceQA上达到**97.8%**。
- **对比优势**：Cashew显著优于其他测试时扩展基线（如Self-Consistency, Self-Synthesizer），证明了**迭代聚合**与**视觉验证**相结合的策略比单纯的轨迹选择或单次合成更为有效。

### 4. 研究意义和价值
- **方法论贡献**：提出了一种新颖的“**迭代聚合**”范式，超越了传统的“采样-选择”方法，能够综合利用多个推理路径中的部分正确信息，并通过视觉验证确保推理的**可解释性与可靠性**。
- **实际应用价值**：显著提升了多模态模型在复杂、长序列推理任务（如科学问答、长视频理解）上的**稳定性和准确性**，为将VLM更可靠地部署到需要严谨推理的现实场景（如教育、医疗、自动驾驶）提供了技术支撑。
- **技术启发性**：Cashew-RL展示了如何通过精心设计的奖励函数和课程学习策略，将复杂的多步、多轨迹聚合能力**内化**到单一模型中，为减少对昂贵测试时计算的依赖提供了可行路径。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：CASHEW

### **一、 核心问题**
当前视觉语言模型在多步推理任务中存在**不稳定性**：
- **推理轨迹发散**：对同一输入多次采样，会产生不一致的推理路径和最终答案。
- **视觉幻觉**：模型容易生成缺乏视觉证据支持的虚假陈述。
- **“想得更久，而非更好”**：现有测试时扩展方法（如增加采样次数或推理长度）可能放大早期感知错误，而非纠正它们。

### **二、 核心创新点**

#### **1. 方法论创新：迭代轨迹聚合**
- **核心理念**：将推理视为一个**进化过程**，而非简单的“采样-选择”。通过多轮迭代，将一组候选推理轨迹**合成**为更高质量的统一轨迹，而非丢弃“失败”的轨迹。
- **关键技术**：**显式视觉验证**。使用预训练的 Grounding DINO 模型，在聚合前验证推理轨迹中提到的物体是否真实存在于图像/视频中，以此过滤幻觉，确保推理基于视觉证据。

#### **2. 模型创新：Cashew-RL（可学习的聚合器）**
- **目标**：将上述迭代聚合行为**内化**到单一模型中，减少推理时对大量采样的依赖。
- **训练方法**：采用两阶段后训练。
    1.  **监督微调**：学习**结构化聚合格式**（`<think>`推理链、`<visual_keys>`视觉关键物体、`<answer>`最终答案），建立参考策略。
    2.  **强化学习**：使用**分组序列策略优化** 和一个**复合奖励函数**来优化聚合策略。
- **奖励函数设计**：
    - **答案正确性**：鼓励给出正确答案。
    - **证据选择质量**：平衡精确率与召回率，鼓励选择相关且充分的视觉证据。
    - **难度感知长度惩罚**：根据任务难度自适应分配推理努力。简单任务鼓励简洁，困难任务允许更长推理。

### **三、 解决方案架构**

#### **方案A：Cashew（推理时框架）**
```
1. 种群初始化：冻结的VLM生成N个初始推理轨迹。
2. 迭代聚合（T轮）：
   a. 子集采样：从当前种群中随机采样M个轨迹作为参考。
   b. 视觉验证：用Grounding DINO验证子集中每个轨迹提到的物体。
   c. 聚合生成：VLM基于输入、子集轨迹及已验证的视觉证据，生成新的、更优的轨迹。
3. 最终聚合：将所有最终轨迹合并，输出一个统一的答案和推理链。
```

#### **方案B：Cashew-RL（训练后模型）**
- **推理时**：只需单模型前向传播，即可完成对多个候选轨迹的内部聚合。
- **关键训练技巧**：课程式教师-策略轨迹混合。在RL训练中，候选池混合了高质量的教师轨迹和当前策略生成的有噪声轨迹，逐步增加后者比例，提升模型对真实推理噪声的鲁棒性。

### **四、 实际价值与实验效果**

#### **1. 性能显著提升**
在13个图像、视频理解和推理基准测试上均取得显著改进：
- **图像理解**：在ScienceQA上，Qwen3-VL-4B模型性能提升**+23.6个百分点**（69.5% → 93.1%）。
- **视频推理**：在EgoSchema上，Qwen2.5-VL-7B模型提升**+8.1个百分点**。
- **广泛适用性**：方法对多种不同规模、架构的VLM骨干网络（如InternVL, Qwen-VL系列）均有效。

#### **2. 超越现有测试时扩展方法**
与Self-Consistency（多数投票）、Self-Selector（模型选择）、Self-Synthesizer（单轮合成）等方法相比，**Cashew的迭代聚合策略效果最佳**。例如在MME基准上，其感知和认知分数均大幅领先。

#### **3. 实现稳定、可解释的推理**
- **稳定性**：通过聚合多轨迹共识，减少了单次采样的随机性。
- **可解释性**：输出的结构化格式（推理链+视觉关键物体）使模型的决策过程更加透明和可验证。
- **效率与效果的平衡**：Cashew-RL在保持高性能的同时，降低了推理时计算开销。

### **五、 总结**
**CASHEW** 通过引入**迭代轨迹聚合**与**显式视觉验证**，系统性地解决了多模态推理中的不稳定性问题。其提出的 **Cashew（即插即用的推理框架）** 和 **Cashew-RL（内化聚合能力的可学习模型）** 构成了一个完整的技术方案，不仅在多项任务上实现了性能的飞跃，更重要的是为构建**更可靠、更 grounded 的多模态AI系统**提供了新的思路和可复现的范式。其核心价值在于让模型学会“**一起思考**”和“**用证据思考**”，而不仅仅是“思考更久”。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

**核心问题**：当前视觉语言模型（VLMs）在多步推理中存在**不稳定性**，即对同一输入进行多次采样会产生发散且不一致的推理轨迹和最终预测，且容易产生视觉幻觉。

**主要方法**：论文提出了两种互补的方法：
1.  **Cashew**：一个**推理时框架**，通过迭代地聚合多个候选推理轨迹来稳定推理。其核心创新在于引入了**显式的视觉验证**（使用Grounding DINO等模型），过滤掉幻觉步骤，确保推理过程基于视觉证据。
2.  **Cashew-RL**：一个**学习型变体**，通过强化学习（使用组序列策略优化GSPO）将上述聚合行为内化到单一模型中。它使用一个复合奖励函数，鼓励模型基于最少但充分的视觉证据得出正确答案，并根据任务难度自适应地分配推理努力。

**主要效果**：在13个图像理解、视频理解和视频推理基准测试上的广泛实验表明，该方法能显著提升性能。例如，在ScienceQA上最高提升23.6个百分点，在EgoSchema上提升8.1个百分点。Cashew-RL在单次推理中即可实现稳健的自我聚合，性能优于仅进行测试时聚合的Cashew和其他先进的测试时扩展方法（如Self-Consistency、Self-Synthesizer），证明了其学习到的聚合策略的有效性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

本文提出了 **CASHEW** 框架，旨在解决多模态大模型（VLMs）多步推理不稳定、易产生幻觉和结果不一致的问题。其核心创新点如下：

---

### 1. **推理范式创新：从“采样-选择”到“迭代-聚合”**
- **改进/不同之处**： 现有测试时扩展方法（如 Self-Consistency, Self-Selector）主要遵循 **“采样-选择”** 范式，即生成多个独立的推理轨迹，然后选择“最佳”的一个，丢弃其他轨迹中的部分信息。CASHEW 则提出 **“迭代-聚合”** 范式，将推理视为一个进化过程：在每一轮迭代中，它**综合**一组候选轨迹的信息，生成质量更高的新轨迹，而非简单地选择。
- **解决的问题/优势**：
    - **充分利用信息**：避免了丢弃部分正确或互补的推理片段，能整合多个不完美轨迹中的有效信息。
    - **提升稳定性**：通过多轮迭代合成，使最终推理轨迹对初始采样噪声的敏感性降低，输出更稳定、一致。
    - **思考更协同**：鼓励模型“一起思考”（think together），而非仅仅“思考更久”（think longer）。

### 2. **显式的视觉验证与证据 grounding**
- **改进/不同之处**： 在每次聚合步骤前，CASHEW 引入了一个**显式的视觉验证模块**。它使用预训练的 Grounding DINO 模型，对候选推理轨迹中提及的物体和属性进行验证，只保留在图像/视频中有视觉证据支持的 claims。
- **解决的问题/优势**：
    - **直接对抗幻觉**：有效过滤掉基于文本统计规律而非真实视觉证据产生的“幻觉”内容，将推理过程锚定在视觉事实上。
    - **提升可解释性与可靠性**：使模型的推理步骤有据可依，提高了最终答案的可信度和可解释性。
    - **区别于纯文本聚合**：这是多模态场景下的关键创新，确保了“聚合”是基于视觉证据的融合，而非无根据的文本合成。

### 3. **可学习的内部聚合策略：Cashew-RL**
- **改进/不同之处**： 在推理时框架（Cashew）的基础上，进一步提出了 **Cashew-RL**。它通过**强化学习（RL）**，具体是**组序列策略优化（GSPO）**，将“多轨迹聚合”的能力**内化**到单个模型的参数中。
- **解决的问题/优势**：
    - **降低推理成本**：Cashew 需要在测试时对每个输入进行多次采样和迭代，计算开销大。Cashew-RL 训练后，在推理时仅需单次前向传播即可表现出聚合行为，效率显著提升。
    - **实现自适应推理**：通过精心设计的复合奖励函数，模型能学会根据任务难度**自适应地分配推理努力**（如调整推理链长度），在简单任务上保持简洁，在复杂任务上进行深入思考。
    - **提升鲁棒性**：通过课程学习（混合教师轨迹和策略自身轨迹进行训练），使模型能处理更嘈杂、真实的推理轨迹输入，增强了泛化能力。

### 4. **针对聚合任务设计的复合奖励函数**
- **改进/不同之处**： 为 Cashew-RL 的 GSPO 训练设计了一个包含三部分的奖励函数：
    1.  **答案正确性奖励**：鼓励最终答案正确。
    2.  **证据选择质量奖励**：采用平衡精确率与召回率的公式，鼓励模型识别出**既相关又精确**的视觉关键物体，避免生成无关或过多的“视觉关键词”。
    3.  **难度感知的长度惩罚**：根据当前问题在训练中的预估解决率，动态调整对长推理链的惩罚力度。难题允许更长思考，易题鼓励简洁。
- **解决的问题/优势**：
    - **多目标优化**：同时优化答案正确性、视觉忠实性和推理效率，引导模型学习高质量、高效率的聚合策略。
    - **避免冗长与退化**：长度惩罚防止模型为获取高正确性奖励而生成无意义的冗长文本，或为逃避惩罚而退化到极简回答。

### 5. **统一的、结构化的聚合输出格式**
- **改进/不同之处**： 为 SFT 和 RL 阶段设计了一个统一的输出模板：`<think>推理链</think> <visual_keys>关键物体列表</visual_keys> <answer>最终答案</answer>`。
- **解决的问题/优势**：
    - **稳定训练**：为模型提供了清晰、可解析的输出结构，使监督学习和强化学习的信号传递更稳定。
    - **便于奖励计算**：结构化输出使得奖励函数中的各个部分（如视觉关键词匹配、推理链长度）可以方便地被提取和计算。
    - **提升可解释性**：强制模型分离推理过程、证据和结论，增强了结果的可解释性。

---

## 总结
**CASHEW 的核心价值**在于，它系统性地将多模态推理从脆弱的单路径生成，升级为**基于视觉证据的、协同的、可迭代优化的群体智慧过程**。其创新点环环相扣：
1.  **迭代聚合范式**提供了方法论框架。
2.  **显式视觉验证**确保了框架的可靠性根基。
3.  **Cashew-RL 与复合奖励**实现了该框架的高效内化与自适应执行。

这最终在包括 ScienceQA、EgoSchema 在内的 13 个多模态基准上带来了显著且一致的性能提升（最高达 +23.6%），**实质性地推进了多模态大模型在复杂推理任务上的稳定性、忠实性和效率**。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

本文通过广泛的实验验证了 **CASHEW** 和 **CASHEW-RL** 在提升多模态推理稳定性和准确性方面的有效性。以下是详细的实验设置、结果和结论。

### 一、 评估数据集与指标

论文在 **13个** 图像和视频理解与推理基准上进行了全面评估，涵盖三大类任务：

| 类别 | 具体数据集 | 主要评估指标 |
| :--- | :--- | :--- |
| **图像理解** | ScienceQA, MME, POPE, SEED-Bench (图像子集) | **准确率 (Accuracy)**、**感知/认知分数 (Perception/Cognition Score)**、**幻觉检测准确率 (Hallucination Detection Accuracy)** |
| **视频理解** | Video-MME, LongVideoBench, EgoSchema, MVBench, NExT-QA | **准确率 (Accuracy)** |
| **视频推理** | VideoMMMU, VSI-Bench, Video-TT, TOMATO | **准确率 (Accuracy)** |

### 二、 对比的基线方法

论文与以下类型的基线方法进行了对比：

1.  **基础视觉语言模型 (VLMs)**： 包括 LLaVA-1.5-7B, Qwen-VL-Chat-7B, VILA1.5-13B, LLaVA-Next-7B, LLaVA-OneVision-7B, Qwen3-VL-4B/8B, Qwen2.5-VL-7B, InternVL3.5-8B, VideoLLaMA3-7B 等。这些模型代表当前多模态推理的“单轨迹推理”标准范式。
2.  **测试时扩展 (Test-Time Scaling) 方法**： 这是论文重点对比的方向，包括：
    - **Self-Consistency**： 对多个采样结果进行多数投票。
    - **Self-Selector**： 使用模型自身判断来选择最佳轨迹。
    - **Self-Synthesizer**： 生成一个新响应来整合多个候选轨迹的信息（最接近的基线）。

### 三、 主要性能提升与结论

#### 1. **CASHEW (推理时框架) 的效果**
- **显著提升**： CASHEW 作为即插即用的推理时方法，在**所有测试的骨干模型**上均带来了**一致且显著的性能提升**。
- **关键结果示例**：
    - **ScienceQA (图像)**： 在 Qwen3-VL-4B 上，准确率从 **69.5%** 提升至 **93.1%**，绝对增益高达 **+23.6个百分点**。
    - **MME (图像)**： 在 Qwen3-VL-8B 上，感知和认知分数分别提升了 **+95.0** 和 **+51.7** 分。
    - **EgoSchema (长视频推理)**： 在 Qwen2.5-VL-7B 上，准确率从 57.7% 提升至 65.8%，增益 **+8.1个百分点**。
    - **NExT-QA (视频因果推理)**： 在多个模型上获得约 **+4.0 至 +5.0个百分点** 的提升。
- **结论**： 迭代轨迹聚合与显式视觉验证能有效**稳定多步推理**，减少幻觉，整合分散的线索，从而在图像和视频的复杂理解与推理任务上实现大幅性能飞跃。

#### 2. **CASHEW-RL (学习型变体) 的效果**
- **超越CASHEW**： 经过 GSPO 训练后，**Cashew-RL** 能够将聚合行为内化到模型参数中。在相同迭代次数下，其性能通常**优于或匹配**需要大量采样的 **Cashew**。
- **关键结果示例**：
    - 在 Qwen3-VL-8B 上，**Cashew-RL (T=3)** 在 ScienceQA 上达到 **97.8%** 的准确率，优于基础模型和 Cashew。
    - 在视频任务上，如 EgoSchema 和 TOMATO，**Cashew-RL** 也实现了比基础模型和 Cashew 更大的提升（例如 TOMATO +2.5个百分点）。
- **结论**： 通过强化学习，模型学会了**更智能地聚合证据**，能够自适应地根据任务难度分配推理精力，并在单次推理中实现高质量的聚合，**降低了对昂贵测试时多采样的依赖**。

#### 3. **与测试时扩展基线的对比**
- **全面领先**： 如表3所示，**Cashew** 在多个基准上均**优于所有对比的测试时扩展方法**（Self-Consistency, Self-Selector, Self-Synthesizer）。
- **关键优势**： 与简单的“采样-选择”或单轮合成相比，**Cashew 的迭代聚合和视觉验证机制**能更有效地融合多轨迹信息并确保视觉基础，从而产生更可靠的结果。例如，在 MME 上，Cashew 比次优的 Self-Synthesizer 高出 **+48.2/+82.6** 分。

#### 4. **消融实验的关键结论**
- **视觉验证至关重要**： 移除 Grounding DINO 验证步骤会导致性能下降，尤其在需要细粒度物体推理的任务上，证明了**显式视觉 grounding 对抑制幻觉的核心作用**。
- **两阶段训练缺一不可**：
    - **SFT阶段**： 主要提供**结构化的输出格式**，但单独使用对性能提升不稳定。
    - **RL (GSPO)阶段**： 是获得**稳健、泛化性强的聚合策略**的关键，带来了跨任务的**一致提升**。
- **超参数敏感性**： 实验表明，**N=8（种群大小）** 和 **T=3（迭代次数）** 是一个在性能和计算成本之间较好的平衡点。

### 四、 总结

论文通过系统性的实验证明了其核心价值：
- **技术创新有效**： 提出的**迭代轨迹聚合**和**基于GSPO的视觉基础强化学习**是解决多模态推理不稳定性问题的有效途径。
- **实际价值显著**： 方法具有**普适性**（适用于多种骨干模型和任务）、**可扩展性**（可作为推理时插件或通过训练内化），并能带来**显著的性能提升**（在 ScienceQA 等关键基准上提升超过20个百分点）。这为构建更可靠、更鲁棒的多模态AI系统提供了重要的技术思路。

**所有报告的提升均经过自助法计算95%置信区间检验，大部分结果具有统计显著性**，确保了结论的可靠性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.08010v1)
- [HTML 版本](https://arxiv.org/html/2601.08010v1)
