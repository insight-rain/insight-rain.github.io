# FlowAct-R1: Towards Interactive Humanoid Video Generation

**相关性评分**: 6.0/10

**排名**: #53


---


## 基本信息

- **arXiv ID**: [2601.10103v1](https://arxiv.org/abs/2601.10103v1)
- **发布时间**: 2026-01-15T06:16:22Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao Liang, Weifeng Chen, Xing Wang, Yuan Zhang, Mingyuan Gao

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

FlowAct-R1是一个基于MMDiT架构的实时交互式人形视频生成框架，通过分块扩散强制策略和高效蒸馏优化，在480p分辨率下实现25fps的稳定帧率和约1.5秒的首帧时间，专注于高保真合成与实时交互的平衡。

## 摘要

Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.

## 详细分析

## 论文摘要：FlowAct-R1: Towards Interactive Humanoid Video Generation

**1. 研究背景和动机**
实现能与人类进行实时、自然交互的视觉人形代理是研究界长期追求的目标。现有方法在**高保真合成**与**实时交互需求**之间存在权衡：扩散模型质量高但计算慢、缺乏流式生成能力；而支持实时流式生成的方法又往往局限于肖像裁剪或行为生动性不足。因此，亟需一个能同时满足**流式、实时、全身控制、高保真**的解决方案。

**2. 核心方法和技术创新**
本文提出 **FlowAct-R1**，一个专为实时交互式人形视频生成设计的框架。其核心创新包括：
- **流式与无限长生成**：基于MMDiT架构，采用**分块扩散强制**策略及**自强制变体**，结合结构化记忆库（参考/长/短期记忆），缓解误差累积，确保长期时序一致性。
- **实时低延迟推理**：通过多阶段**高效蒸馏**将去噪步骤压缩至仅3步，并结合FP8量化、算子融合、异步流水线等系统级优化，在480p分辨率下实现**25fps稳定生成**，首帧延迟仅约**1.5秒**。
- **生动性与可控性**：利用多模态大语言模型进行**动作规划**，指导智能体在说话、倾听、思考等多种交互状态间自然过渡。框架支持通过音频和文本对唇形、表情、全身姿态进行细粒度控制。

**3. 主要实验结果**
与KlingAvatar 2.0、LiveAvatar、Omnihuman-1.5等SOTA方法进行用户研究（GSB指标）对比。FlowAct-R1在**运动自然度、唇形同步准确性、帧稳定性、运动丰富性**方面均显著优于基线模型，同时是唯一能兼顾**长时流式、实时响应与高行为生动性**的方法。

**4. 研究意义和价值**
FlowAct-R1为实现**沉浸式实时交互**迈出了关键一步，其低延迟、高保真的特性为**直播、虚拟陪伴、视频会议**等应用场景提供了可行的技术基础。该框架在算法与系统层面的协同优化范式，对推动交互式生成模型的实用化部署具有重要参考价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：FlowAct-R1

### **一、 拟解决的核心问题**
该论文旨在解决**实时交互式人形视频生成**领域的三个关键挑战：
1.  **实时性与流式生成的矛盾**：现有方法难以在保证高保真视频质量的同时，实现低延迟的实时流式生成。
2.  **长视频的时序一致性**：在持续多轮交互的长视频生成中，如何避免错误累积、保持动作连贯和身份一致。
3.  **行为的生动性与自然度**：如何使生成的虚拟人像能在说话、倾听、思考、空闲等多种交互状态间进行自然、生动、符合逻辑的过渡。

### **二、 核心创新点**
FlowAct-R1 的创新是一个系统性工程，涵盖了算法、训练策略和系统优化多个层面。

#### **1. 算法与框架创新**
- **基于MMDiT的流式生成架构**：以 **Seedance (MMDiT)** 为骨干网络，通过**分块扩散强制**策略将其改造为支持**无限长度流式生成**的自回归模型。
- **创新的训练策略**：
    - **自强制变体**：在训练中引入由中间模型生成的“伪真实潜变量”作为记忆组件，模拟推理阶段的记忆误差，有效**缓解了长序列生成中的错误累积问题**。
    - **课程训练**：采用三阶段训练（自回归适应、音视频联合微调、蒸馏），逐步将通用模型转化为适合实时交互的专用模型。
- **结构化记忆银行**：设计了包含**参考潜变量、长时记忆队列、短时记忆潜变量、去噪流**的四组件流缓冲区，确保了生成过程的时序锚定与连贯性。
- **多模态动作规划**：引入**多模态大语言模型**，根据最新音频和参考图像预测合理的后续动作，作为先验指导视频生成，显著提升了行为转换的自然度和逻辑性。

#### **2. 系统级性能优化**
- **高效蒸馏流水线**：通过多阶段蒸馏（去除分类器引导、朴素步长蒸馏、改进的DMD蒸馏），将去噪步数大幅压缩至**仅需3步**，实现了约8倍的加速。
- **全面的系统优化**：
    - **计算优化**：采用FP8量化、算子融合、异步流水线（解耦DiT去噪与VAE解码）等技术。
    - **并行策略**：从令牌级序列并行转向**帧级混合并行**，大幅降低了通信开销。
- **内存精炼策略**：定期对短时记忆帧进行噪声注入与修复，以纠正流式生成中累积的伪影，保障长视频的质量。

### **三、 解决方案与成果**
通过上述技术创新，FlowAct-R1 成功地将**高质量生成**、**实时流式输出**和**全身精细控制**统一在一个框架内：

- **性能指标**：在NVIDIA A100上，稳定实现 **480p分辨率、25fps** 的实时视频生成，**首帧延迟仅约1.5秒**。
- **综合能力**：如表1所示，FlowAct-R1是当时唯一能同时实现**流式、实时、全身控制、强泛化性和高生动性**的方法。
- **评估结果**：用户研究表明，其在动作自然度、口型同步准确性、运动丰富度等方面均优于KlingAvatar 2.0、LiveAvatar等前沿模型。

### **总结**
**FlowAct-R1的核心价值**在于，它并非单一技术的突破，而是通过一套**紧密耦合的算法-系统协同设计**，系统性解决了交互式数字人生成中的“不可能三角”（质量、速度、长度），为虚拟直播、视频会议、虚拟陪伴等需要**低延迟、高拟真、长时交互**的应用场景提供了可行的技术方案。其**分块强制流式生成**与**面向推理的蒸馏优化**思路，对后续的实时生成模型设计具有重要的借鉴意义。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**实时交互式数字人视频生成**中**高保真合成**与**低延迟响应**难以兼顾的核心问题。为此，作者提出了 **FlowAct-R1** 框架，其核心是基于 MMDiT 架构，通过**分块扩散强制**与**自强制变体**策略实现任意时长的流式生成，并利用**高效蒸馏**与**系统级优化**（如FP8量化、异步流水线）来加速推理。最终，该方法在保持对全身姿态、口型、表情的细粒度控制下，实现了 **480p分辨率、25fps的实时生成**，首帧延迟仅约1.5秒，在行为生动性、时间一致性和跨角色泛化能力上均优于现有先进方法。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## FlowAct-R1 论文创新点分析

这篇论文针对**实时交互式人形视频生成**这一核心任务，提出了一套名为 **FlowAct-R1** 的完整框架。其创新点并非单一技术，而是一系列协同工作的算法与系统设计，旨在同时解决高保真度、实时性、长时一致性和行为自然性等多个挑战。以下是其相对于已有工作的明确创新点：

### 1. **面向无限长流的“分块扩散强制”与“自强制”策略**
- **改进/不同之处**：
    - **分块扩散强制**：在基于 MMDiT 的骨干网络上，将长视频流切分为固定大小的“块”进行自回归式生成，实现了**无限长度**的视频流式合成。这与以往多数扩散模型（如 OmniHuman-1.5, KlingAvatar 2.0）仅限于生成短片段（如30秒或5分钟）有本质区别。
    - **自强制变体**：在训练中，**有概率地使用由中间模型生成的“伪真实潜变量”** 作为记忆组件，而非纯粹的真实数据。这模拟了推理阶段记忆组件中可能存在的累积误差。
- **解决的问题/带来的优势**：
    - **解决了长视频生成的连续性问题**：支持与交互时长同步的、任意长度的视频流输出。
    - **缓解了误差累积**：通过“自强制”策略，缩小了训练（使用完美记忆）与推理（使用含噪记忆）之间的差距，有效减轻了在长时流式生成中因错误传播导致的质量退化问题，**确保了长时程的时间一致性**。

### 2. **为实时交互量身定制的系统级优化与高效蒸馏流水线**
- **改进/不同之处**：
    - **多阶段定制化蒸馏**：设计了专门的蒸馏流程，包括：1) 消除分类器无关引导（CFG）开销；2) 朴素步数蒸馏；3) 基于 DMD 的少步分数蒸馏。最终将去噪过程压缩至仅 **3 次网络函数评估（NFEs）**，实现了 **8倍加速**。
    - **全面的系统级优化**：融合了 **FP8 量化**、**算子融合**、**从令牌级到帧级的混合并行策略**，以及**DiT去噪与VAE解码的异步流水线**。
- **解决的问题/带来的优势**：
    - **解决了实时性与高保真度的矛盾**：以往高质量模型（如基于扩散的模型）计算开销大，无法实时；而实时模型（如 LiveAvatar）则在生动性上存在差距。FlowAct-R1 通过软硬件协同优化，在 NVIDIA A100 上实现了 **480p分辨率、25fps 的稳定实时生成**。
    - **降低了交互延迟**：**首帧时间（TTFF）仅约1.5秒**，这对于需要即时反馈的对话式交互至关重要，提升了用户体验。

### 3. **结合结构化记忆库与记忆精炼机制的推理框架**
- **改进/不同之处**：
    - **结构化记忆库**：设计了包含**参考潜变量、长期记忆队列、短期记忆潜变量、去噪流**四个组件的固定大小流缓冲区。这种结构明确区分了不同时间跨度的信息锚点。
    - **记忆精炼策略**：定期对**短期记忆潜变量**进行“加噪-去噪”修复操作，使用参考和长期记忆作为稳定约束。
- **解决的问题/带来的优势**：
    - **解决了长时生成中的身份漂移和运动抖动**：结构化记忆为生成过程提供了稳定的上下文锚点，而记忆精炼则主动修正了在连续流中累积最显著的短期记忆误差。
    - **确保了运动平滑性与身份一致性**：使得系统在长达数十分钟的交互中，能保持人物外观和动作的连贯自然，避免了同类流式方法（如 LiveAvatar）中出现的运动重复或不自然问题。

### 4. **融合多模态大语言模型（MLLM）的行为规划**
- **改进/不同之处**：
    - 在传统的音频/文本驱动之上，引入了 **MLLM 作为“动作规划器”**。MLLM 根据最新的音频片段（语音内容）和参考图像，预测符合上下文和视觉约束的后续合理动作。
- **解决的问题/带来的优势**：
    - **解决了行为状态间过渡的生硬问题**：在交互中，人物需要在说话、倾听、思考、空闲等多种状态间自然切换。纯数据驱动的模型容易产生重复或不合逻辑的动作序列。
    - **提升了行为的生动性与合理性**：MLLM 提供了高层语义规划，引导 MMDiT 骨干网络生成更符合常识、过渡更平滑的全身动作，这是实现“类人”自然交互的关键。用户研究也证实其在“行为自然度”上优于竞争对手。

### 5. **支持细粒度全身控制与强泛化性的统一框架**
- **改进/不同之处**：
    - **全身控制**：通过精细标注的行为数据集和多模态融合（音频、文本），实现了对**唇部同步、面部表情、身体姿态、物体交互**的**细粒度、全方位控制**。这与早期只关注唇同步（如 Heygen）或近期仍局限于肖像裁剪（如 INFP, ARIG）的方法形成鲜明对比。
    - **强泛化性**：继承了基础模型（Seedance）的强大泛化能力，**仅凭单张参考图像**即可在不同角色风格上生成高质量视频。
- **解决的问题/带来的优势**：
    - **解决了交互表达力不足的问题**：全身姿态和表情是传达情感和意图的重要组成部分。FlowAct-R1 使得生成的数字人能够进行丰富的非语言交流，更贴近真实人类互动。
    - **提升了实用性与可扩展性**：单一模型即可适用于多种角色和风格，降低了部署成本，为直播、虚拟陪伴、视频会议等应用提供了可行方案。

### **总结**
FlowAct-R1 的核心创新在于**系统性**地整合了从**算法策略**（分块自回归、自强制、MLLM规划）、**模型优化**（定制蒸馏）到**系统工程**（内存管理、并行、量化）的全栈解决方案。它并非在单一指标上突破，而是**在“流式”、“实时”、“高保真”、“全身可控”、“长时一致”和“行为自然”这六个交互式人形视频生成的关键维度上同时取得了显著进展**，从而在整体能力上超越了现有的 SOTA 方法（如表1所示）。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

### 一、 核心实现效果
FlowAct-R1 在**实时交互式人形视频生成**任务上实现了以下关键效果：
- **实时性能**：在 NVIDIA A100 平台上，稳定生成 **480p 分辨率、25 FPS** 的视频流。
- **低延迟**：**首帧生成时间（TTFF）仅约 1.5 秒**，确保了交互的即时响应性。
- **无限时长**：支持**任意时长**的流式视频生成，突破了传统扩散模型在生成长度上的限制。
- **高保真与强泛化**：从单张参考图像出发，能生成行为生动、感知真实的人形视频，并**泛化到多种角色风格**。
- **细粒度控制**：实现了对**口型同步、面部表情、身体姿态和物体交互**的全面、精细控制。

### 二、 数据集与评价方法
- **数据集**：论文使用了**复合数据集**进行训练，包括通用视频样本和对话视频。所有数据都被预处理为带有**密集行为标注**的短片片段，以支持细粒度的流式控制。
- **评价指标**：主要采用**主观用户研究（User Study）** 进行评估。
    - **评价方法**：使用 **GSB（Good-Same-Bad）度量**，即让用户比较两个系统生成的视频，判断哪个更好、相同或更差。
    - **评价维度**：重点关注以下四个方面的感知质量：
        1.  **动作自然度** (Motion Naturalness)
        2.  **口型同步准确度** (Lip-sync Accuracy)
        3.  **帧结构稳定性** (Frame Structure Stability)
        4.  **动作丰富度** (Motion Richness)

### 三、 基线方法对比
论文选取了三个代表性的先进方法作为基线进行对比：
1.  **KlingAvatar 2.0**：能生成5分钟长视频且视觉质量高，但**不支持流式生成**，且存在动作重复问题。
2.  **LiveAvatar**：基于Wan2.2模型，支持**实时流式生成**，但同样面临动作重复问题，影响自然度。
3.  **Omnihuman-1.5**：与FlowAct-R1网络结构相似，但**仅支持最长30秒视频**，且**不具备实时流式能力**。

### 四、 主要性能提升与结论
- **综合性能领先**：如图3的用户研究结果所示，FlowAct-R1在**所有对比维度上均显著优于基线方法**。用户投票显示，FlowAct-R1在与其他方法对比时，获得“更好”评价的比例（图中橙色部分）最高。
- **关键优势**：
    - **同时实现了流式、实时、高保真**：如表1所示，FlowAct-R1是唯一一个在“流式”、“实时”、“全身控制”、“泛化性”和“生动性”五个关键属性上全部达标的模型。
    - **解决了动作重复问题**：通过**分块扩散强制策略**和**MLLM引导的动作规划**，有效缓解了长视频生成中常见的动作重复问题，从而提升了行为的自然度和多样性。
    - **在实时约束下保持了高质量**：通过高效的3步去噪（3 NFE）蒸馏和系统级优化，在满足25fps实时生成的同时，并未牺牲视觉保真度和行为生动性。

**结论**：实验表明，FlowAct-R1成功地在**实时交互的严格延迟要求**与**高质量、长时长视频生成**之间取得了卓越的平衡，其综合性能超越了当前最先进的方法，为实现真正生动、自然的数字人交互提供了可行的技术方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.10103v1)
- [HTML 版本](https://arxiv.org/html/2601.10103v1)
