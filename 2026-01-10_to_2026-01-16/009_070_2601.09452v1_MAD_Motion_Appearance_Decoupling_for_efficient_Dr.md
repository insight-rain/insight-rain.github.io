# MAD: Motion Appearance Decoupling for efficient Driving World Models

**相关性评分**: 7.0/10

**排名**: #9


---


## 基本信息

- **arXiv ID**: [2601.09452v1](https://arxiv.org/abs/2601.09452v1)
- **发布时间**: 2026-01-14T12:52:23Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Ahmad Rahimi, Valentin Gerard, Eloi Zablocki, Matthieu Cord, Alexandre Alahi

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

MAD提出一种高效的两阶段运动-外观解耦框架，将通用视频扩散模型转化为可控的自动驾驶世界模型，显著提升推理效率并支持轻量化部署。

## 摘要

Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively "dressing" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/

## 详细分析

## 论文摘要：MAD: Motion Appearance Decoupling for efficient Driving World Models

### 1. 研究背景和动机
当前通用的视频扩散模型虽能生成逼真视频，但难以作为自动驾驶领域可靠的**世界模型**，因其缺乏对结构化运动与物理一致交互的建模。将通用视频模型适配到驾驶领域通常需要海量领域数据和昂贵的微调计算（如VISTA和GEM分别消耗25,000和50,000 GPU小时）。本研究旨在提出一种**高效适配框架**，以极低的计算成本将通用视频模型转化为可控的驾驶世界模型。

### 2. 核心方法和技术创新
本文提出 **MAD（运动-外观解耦）** 方法，其核心创新在于**两阶段生成流程**，模仿动画师的“动画草稿-最终渲染”工作流：
- **运动预测器**：首先，使用轻量级LoRA适配器微调基础视频模型，使其生成抽象的**骨架化姿态视频**，专注于学习场景中智能体（车辆、行人）和车道线的物理与社会合理性运动。
- **外观合成器**：随后，复用同一模型主干，以生成的姿态序列为条件，合成最终的**逼真RGB视频**，为运动“穿上”纹理和光照。

**关键技术点**：
- **高效适配**：仅需微调两个轻量LoRA，最大化复用基础模型（如SVD、LTX）的预训练知识。
- **统一视觉潜空间条件**：将所有控制信号（首帧、文本、新颖的**视觉化自车运动表示**、物体轨迹）通过模型自带的VAE编码到其原生潜空间，使模型能以熟悉的“视觉语言”学习新任务。
- **针对性噪声注入**：在训练外观合成器时，对姿态潜特征施加针对性噪声，以模拟预测姿态的瑕疵，提升模型鲁棒性。

### 3. 主要实验结果
- **极高效率**：基于SVD的MAD-SVD模型，仅用**不足6%的计算和数据**（1,500 GPU小时 vs. 25,000+），即达到与VISTA、GEM相当的性能。
- **领先性能**：基于更强LTX模型的MAD-LTX（2B & 13B）在人类偏好研究中**超越所有开源竞品**，生成质量与顶级闭源模型Cosmos Predict 2相当，且**推理速度快达3.6倍**。
- **有效解耦**：消融实验证明，两阶段解耦方法显著优于直接端到端微调，能生成**更多样、更准确的轨迹**，并避免模式坍塌。
- **全面可控性**：模型支持**文本、自车运动、物体运动**三种控制模态，验证了其作为可控世界模型的有效性。

### 4. 研究意义和价值
本研究提出了一种**革命性的高效适配范式**，通过解耦运动与外观学习，极大地降低了创建专用、可控驾驶世界模型的计算与数据门槛。这使得研究社区能够快速利用通用视频生成模型的进展，加速自动驾驶仿真、规划与合成数据生成等下游应用的发展。MAD框架具有通用性，其核心思想可扩展至**人体动作生成、机器人操作**等其他需要物理合理性建模的领域。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：MAD: Motion Appearance Decoupling for efficient Driving World Models

### **一、 论文旨在解决的核心问题**

当前通用的视频扩散模型（如SVD、LTX）虽然能生成逼真的视频，但**无法直接作为可靠的自动驾驶世界模型**。主要矛盾在于：
- **通用视频模型**：擅长视觉真实感，但缺乏对**物理规律、多智能体结构化交互和社交合理性**的建模能力。
- **专用驾驶世界模型**：需要上述能力，但传统适配方法（如大规模领域微调）**计算成本和数据需求极高**（例如VISTA需25,000 GPU小时），阻碍了研究社区利用快速发展的通用视频模型进展。

**核心挑战**：驾驶世界模型必须**同时掌握**逼真的外观渲染和复杂的物理/社会动力学，这两者紧密耦合，使得端到端学习异常困难且昂贵。

### **二、 核心创新点：运动-外观解耦（MAD）方法论**

论文提出了一种**高效适配框架**，将通用视频生成模型转化为可控的驾驶世界模型，其核心创新是 **“运动-外观解耦”** 的两阶段范式，灵感来源于专业动画师的“动画草稿-最终渲染”工作流。

#### **1. 核心思想：两阶段生成模仿“推理-渲染”**
- **第一阶段：运动预测器**
    - **任务**：生成抽象的**中间运动表示**（即“姿态视频”），仅包含场景中智能体（车辆、行人）和关键静态元素（车道线）的骨架化表示。
    - **目标**：专注于学习**物理和社会合理的动力学**，剥离了纹理、光照等外观细节的干扰。
- **第二阶段：外观合成器**
    - **任务**：以第一阶段生成的姿态视频为条件，**“渲染”出最终的光照真实RGB视频**。
    - **目标**：专注于**外观合成**，将运动“骨架”赋予真实的纹理和光照。

#### **2. 关键技术实现**
- **单一骨干模型 + 轻量级适配**：**复用同一个预训练的视频扩散模型骨干**（如SVD或LTX），通过两个独立的**轻量级LoRA适配器**分别实现运动预测和外观合成。这最大程度保留了骨干模型的先验知识。
- **创新的中间表示**：采用**骨架姿态视频**作为中间表示。相比HD地图（过于抽象）和全景分割（缺乏3D感知），姿态表示在可扩展性、3D感知和对象中心结构之间取得了最佳平衡，便于模型理解和生成。
- **高效的视觉条件化**：**所有条件信号**（第一帧RGB、姿态视频、甚至新颖的自我运动表示）都通过模型**自带的预训练VAE编码到其本征视觉潜在空间**。这使得模型能用其已经理解的“视觉语言”来学习新任务，无需设计复杂的适配网络。
- **针对性的噪声注入**：在训练外观合成器时，对输入的条件姿态潜在特征施加**针对性噪声**，以模拟运动预测器输出可能存在的瑕疵（如模糊、弯曲），提升了合成器对不完美运动输入的鲁棒性。
- **新颖的控制信号**：
    - **自我运动控制**：设计了一种**视觉化的自我运动表示**（在静态纹理球体和尘埃粒子环境中的相机运动视频），使模型能直观理解转向、加速等指令。
    - **对象运动控制**：允许用户通过稀疏的2D边界框轨迹视频来控制特定智能体的运动。

### **三、 解决方案的实际价值与效果**

#### **1. 极高的效率提升**
- **MAD-SVD**：在达到与VISTA、GEM等SOTA模型竞争的质量时，**仅使用了不到6%的计算资源和数据**（1，500 GPU小时 vs. 25，000+ GPU小时）。
- **MAD-LTX**：创建了一个开源SOTA驾驶世界模型，其训练成本（2B模型128 GPU小时，13B模型700 GPU小时）远低于从头训练或大规模微调的竞争对手。

#### **2. 卓越的性能表现**
- **生成质量**：在大规模人工评估中，MAD-LTX**超越了所有开源竞争对手**，并且生成质量与顶级的闭源模型Cosmos Predict 2相当。
- **规划能力**：在开环运动规划评估中，MAD-LTX在轨迹预测准确性（minADE）和多样性（APD）上均优于直接微调的基线，**证明了其运动预测器能更好地捕捉底层交互和长期动力学**，避免了直接视频微调容易导致的模式坍塌。
- **综合可控性**：支持**文本、自我运动和对象运动**的全面控制套件，为场景创建和规划测试提供了灵活性。
- **推理速度**：得益于高效的架构，MAD-LTX的推理速度比同类性能的模型**快达3.6倍**。

### **四、 总结**

MAD方法论的核心贡献在于**通过“运动-外观解耦”的思想，将复杂的世界建模任务分解为两个更易学习、更高效的子任务**。它**不是提出一个全新的模型架构**，而是**提供了一种高效的“适配器”范式**，能够将任何强大的通用视频生成模型快速、低成本地转化为高性能、高可控性的专用驾驶世界模型。这项工作**极大地降低了驾驶世界模型的开发门槛和计算成本**，为研究社区快速利用通用视频模型的进展铺平了道路。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决将通用视频生成模型高效适配为自动驾驶世界模型时面临的计算和数据成本过高的问题。为此，论文提出了 **MAD（Motion Appearance Decoupling）框架**，其核心思想是将复杂的驾驶场景生成任务解耦为两个阶段：首先，利用一个**运动预测器**在抽象的骨架化姿态表示空间中学习物理和社交上合理的结构化运动；然后，利用同一个模型骨干，通过**外观合成器**将预测的运动序列“渲染”成逼真的RGB视频。这种方法极大地复用了预训练模型的先验知识，仅需添加轻量级的LoRA适配器进行微调。实验表明，该方法能以极低的成本（例如，仅需基线模型约6%的计算量）将SVD等通用模型转化为高质量的驾驶世界模型，其最终实现的MAD-LTX模型在生成质量上超越了所有开源竞争对手，并与顶尖的闭源模型性能相当，同时支持文本、自车运动和物体运动等多种控制方式。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《MAD: Motion Appearance Decoupling for efficient Driving World Models》的创新点分析

这篇论文提出了一种高效地将通用视频生成模型（VGMs）适配为自动驾驶世界模型的新方法。其核心创新在于**将复杂的驾驶场景生成任务解耦为“运动预测”和“外观合成”两个阶段**。以下是其相对于已有工作的明确创新点：

---

### 1. **核心方法论创新：运动与外观的解耦学习**
- **改进/不同之处**：
    - **以往方法**：大多数驾驶世界模型（如VISTA、GEM）或通用视频模型（如SVD）采用端到端方式，直接学习从条件（如第一帧）到逼真RGB视频的映射。这要求模型同时学习复杂的物理/社会动力学和逼真的外观渲染，导致训练数据量和计算成本极高。
    - **MAD方法**：受动画师工作流程（先画草图分镜，再渲染细节）启发，提出两阶段流水线：1) **运动预测器** 先生成抽象的“姿态视频”（骨架化的智能体和场景元素），专注于学习物理和社会合理性；2) **外观合成器** 再以该姿态视频为条件，渲染出逼真的RGB视频。
- **解决的问题/带来的优势**：
    - **大幅提升训练效率**：将复杂任务分解为两个更专注的子任务，降低了同时学习动力学和外观的难度。实验证明，基于SVD的MAD-SVD仅需**不到6%的计算资源**即可达到与VISTA、GEM相当的性能。
    - **更好地捕获结构化运动**：迫使模型在抽象层面（无纹理干扰）学习驾驶场景中多智能体交互和物理一致的运动规律，提升了生成视频的动态合理性。

### 2. **高效适配框架：单一骨干模型 + 轻量级LoRA适配器**
- **改进/不同之处**：
    - **以往方法**：适配通用VGM到驾驶领域通常需要**大规模全参数微调**（如VISTA消耗25k GPU小时）或从头开始训练专用模型（如Cosmos），计算成本令人望而却步。
    - **MAD方法**：使用**同一个**预训练VGM骨干（如SVD或LTX），通过训练两个轻量级的**LoRA适配器**分别实现运动预测和外观合成。所有条件信号（第一帧RGB、姿态视频、自车运动等）都通过模型自带的预训练VAE编码到其固有的视觉潜在空间。
- **解决的问题/带来的优势**：
    - **极低的适配成本**：避免了训练新模型或复杂适配网络的开销。MAD-LTX（13B）的总微调时间仅需700 GPU小时，远低于竞争对手。
    - **最大化知识复用**：模型在“说”它已经理解的视觉语言，从而快速学习新控制任务。条件信号在潜在空间中的投影比设计新的跨模态融合机制更高效。

### 3. **新颖的中间运动表示：可扩展的3D感知姿态视频**
- **改进/不同之处**：
    - **以往方法**：相关工作使用的中间表示各有局限：
        - **HDMap（3D包围盒）**：过于抽象，不利于VGM利用其视觉先验，且依赖标注数据，可扩展性差。
        - **全景分割**：虽是像素级，但本质是2D的，难以捕获3D方向或行人细节。
    - **MAD方法**：提出使用**骨架姿态视频**作为中间表示。通过现成的姿态提取器（如OpenPifPaf、DWPose）对大规模驾驶视频生成伪标签，得到包含车辆、行人、车道线骨架的序列。
- **解决的问题/带来的优势**：
    - **理想平衡点**：该表示是**可扩展的**（伪标签）、**3D感知的**（能表达方向）、**目标中心的**（便于VGM理解）。消融实验证明，它比HDMap和全景分割都更有效。
    - **促进两阶段任务**：对运动预测器而言，它足够结构化以学习动力学；对外观合成器而言，它又足够具体以指导纹理渲染。

### 4. **创新的可控条件信号：视觉化自车运动表示**
- **改进/不同之处**：
    - **以往方法**：控制自车运动通常使用数值化的轨迹或位姿参数，需要模型学习这些抽象参数与视觉场景变化之间的关联。
    - **MAD方法**：设计了一种**视觉化的自车运动表示**：渲染一段从自车视角观看一个带有纹理的静态球体和尘埃粒子的合成视频。旋转通过背景纹理的视运动体现，平移通过尘埃粒子的视差体现。
- **解决的问题/带来的优势**：
    - **更直观的条件融合**：这种表示是一种**视频**，可以通过VAE编码为潜在特征并输入模型。由于VGM对视频有强大的先验，它能更容易地将这种控制输入与期望的场景演变关联起来，从而更高效、更准确地实现自车运动控制。

### 5. **针对性的训练策略：外观合成器的“定向噪声注入”**
- **改进/不同之处**：
    - **常见做法**：在训练外观合成器时，通常直接使用干净的真实姿态视频作为条件。
    - **MAD方法**：为了弥合**训练时使用真实姿态**与**推理时使用预测的（有瑕疵的）姿态**之间的域差距，在训练外观合成器时，对作为条件的姿态潜在特征施加**定向高斯噪声**。噪声只添加到对应骨架部分的潜在特征上，背景部分保持干净。
- **解决的问题/带来的优势**：
    - **提升模型鲁棒性**：迫使外观合成器学会处理不完美的、模糊的或扭曲的运动输入，从而在推理时即使运动预测有瑕疵，也能生成更清晰、更稳定的最终视频。消融实验证实了该策略的有效性。

### 6. **系统性验证与开源贡献：MAD-LTX模型**
- **改进/不同之处**：
    - **现状**：领域内高性能模型（如Cosmos）多为闭源或需要巨大算力。开源模型在质量和可控性上存在差距。
    - **MAD贡献**：成功将MAD方法论扩展到更强的LTX骨干上，发布了**MAD-LTX**（2B和13B版本）——一个开源的、支持全面控制（文本、自车运动、目标运动）的SOTA驾驶世界模型。
- **解决的问题/带来的优势**：
    - **实现SOTA性能**：人类偏好研究表明，MAD-LTX在生成质量上超越了所有开源竞争对手，并与顶级闭源模型（Cosmos Predict 2）质量相当。
    - **高效与快速**：在达到高性能的同时，所需计算资源和数据量远少于竞争对手，且**推理速度快3.6倍**。
    - **促进社区发展**：提供了一个高效、高性能、可复现的开源基准，有望加速视频生成模型在驾驶领域的研究与应用。

---

**总结**：MAD的核心创新在于通过**“运动-外观解耦”** 这一范式性转变，将通用视频模型的强大先验以极高效的方式引导至高度专业化的自动驾驶世界建模任务中。它不仅在**计算效率**上实现了数量级的提升，还通过新颖的中间表示、条件设计和训练策略，在**生成质量、运动合理性和可控性**方面达到了新的高度。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过一系列实验验证了所提出的 **MAD（Motion-Appearance Decoupling）方法** 在高效构建自动驾驶世界模型方面的有效性。核心结论是：**MAD方法能以极低的计算和数据成本，将通用视频生成模型（VGM）高效地适配为高质量、可控的自动驾驶世界模型，其性能达到甚至超越了需要大量资源训练的现有最佳模型。**

### 一、 使用的数据集
1.  **主要数据集：OpenDV**
    *   **内容**： 从YouTube收集的1700小时驾驶视频。
    *   **预处理**： 视频被处理为1056×704分辨率、24fps，并分割成5秒（120帧）的片段。使用**OpenPifPaf**和**DWPose**等现成姿态提取器，从视频中生成伪标签，得到论文的核心中间表示——**姿态视频**。
    *   **训练/验证集**： 从训练视频中采样10万个片段用于训练，从验证视频中采样5000个片段用于评估，确保无数据泄露。
2.  **辅助数据集：Waymo Open Dataset**
    *   **用途**： 主要用于**控制能力评估**的消融实验，以验证模型对ego-motion（自车运动）、object（物体运动）和text（文本）等控制信号的响应 fidelity。

### 二、 评价指标
论文认为在复杂的驾驶场景中，传统的分布级指标与人类感知质量相关性较差，因此采用了**以人类偏好研究为主，传统指标为辅**的评估策略。

1.  **核心指标：人类偏好研究**
    *   **方法**： 进行大规模的双盲“A/B”测试。参与者（领域专家）观看由不同模型从同一初始帧生成的视频对，并从三个维度进行选择：
        *   **整体质量**： 哪个视频更难以与真实视频区分？
        *   **运动质量**： 哪个视频的运动更真实、流畅、物理和社会层面更合理？（例如，避免无故急刹、碰撞）
        *   **视觉质量**： 哪个视频更清晰、伪影更少、视觉效果更好？
    *   **优势**： 直接衡量人类对生成视频真实感和可用性的主观判断，被认为是该任务最可靠的指标。

2.  **辅助指标**
    *   **开环运动规划指标**： 用于专门评估**运动预测器**的规划能力。
        *   **minADE@6**： 在生成的6条轨迹中，与真实轨迹相比的**最小平均位移误差**（越低越好）。衡量**规划准确性**。
        *   **APD@6**： 生成的6条轨迹之间的**平均成对距离**（越高越好）。衡量**轨迹多样性**，避免模式坍塌。
    *   **传统视频生成指标**： 为完整性而报告，但论文指出其与人类偏好相关性弱。
        *   **FID (Fréchet Inception Distance)**： 衡量生成图像与真实图像分布的距离。
        *   **FVD (Fréchet Video Distance)**： 衡量生成视频与真实视频分布的距离。
    *   **控制能力指标**：
        *   **Ego-Motion控制**： 使用**ADE**衡量生成视频中提取的自车轨迹与给定控制轨迹的误差。
        *   **Object控制**： 使用**IoU**衡量生成视频中目标物体的检测框与给定控制框的重叠度。
        *   **Text控制**： 使用**成功率**，通过VQA模型判断生成视频是否符合文本描述（如“左转”、“有自行车出现”）。

### 三、 对比的基线方法
论文在两个层次上进行了对比：

1.  **效率验证层 (基于SVD模型)**：
    *   **MAD-SVD**： 本文方法适配Stable Video Diffusion (SVD) 的产物。
    *   **VISTA** 和 **GEM**： 两个同样基于SVD进行全模型微调（fine-tuning）的SOTA自动驾驶世界模型，但需要海量计算资源（25k和50k GPU小时）。

2.  **性能SOTA验证层 (基于LTX模型)**：
    *   **MAD-LTX (2B & 13B)**： 本文方法适配更强大的LTX视频生成基础模型的产物，是论文的主要贡献。
    *   **Fine-tuned LTX**： **关键对比基线**。使用与MAD-LTX**相同的计算预算和数据**，但对LTX进行端到端的LoRA微调（**不采用**运动-外观解耦的两阶段方法）。此对比用于**孤立地证明MAD方法论本身的有效性**。
    *   **Base LTX**： 未经过驾驶数据微调的原始LTX模型。
    *   **开源竞品**： GEM, VISTA等。
    *   **闭源SOTA模型**： **Cosmos Predict 1 & 2**。这是需要私有数据和巨大计算资源训练的工业级模型，作为性能上限参考。

### 四、 关键性能提升与结论

1.  **惊人的训练效率（核心贡献）**：
    *   **MAD-SVD** 在达到与**VISTA**相近性能的同时，仅使用了后者 **6%的计算资源**（1.5k vs 25k GPU小时）和 **8%的数据量**（139 vs 1700小时）。
    *   **MAD-LTX** 在达到顶尖性能的同时，其训练成本远低于从头训练或全模型微调的竞品。

2.  **生成质量达到SOTA水平**：
    *   人类偏好研究显示（见图5），**MAD-LTX-13B在整体质量上超越了所有开源竞品**（GEM, VISTA, Cosmos Predict 1）。
    *   **MAD-LTX-13B与最强的闭源模型Cosmos Predict 2 (14B) 质量相当**，但推理速度更快（见图7）。

3.  **方法论有效性验证**：
    *   **MAD-LTX 显著优于 Fine-tuned LTX**： 在相同计算和数据下，MAD-LTX在人类偏好中大幅领先（例如13B模型，33 vs 15）。这**强有力地证明了解耦两阶段方法是性能提升的关键**，而非仅仅是使用了更好的基础模型或数据。
    *   **避免了模式坍塌**： 在开环规划评估中（表1），Fine-tuned LTX的轨迹多样性（APD）大幅下降（-37.8%），而**MAD-LTX保持了与Base LTX相近的高多样性**。这表明在抽象姿态空间学习运动，迫使模型学习底层物理规律，而非记忆数据中的表面纹理关联。

4.  **卓越的运动规划能力**：
    *   在开环规划指标上（表1），**MAD-LTX取得了最佳的 minADE**（即最准确的轨迹预测），证明了其运动预测器能更好地捕捉场景交互和长期动力学。

5.  **全面且有效的可控性**：
    *   控制能力评估（表3）表明，MAD-LTX能有效响应**ego-motion、object-motion和text**三种控制信号。与无条件生成相比，在给定具体控制信号时，生成结果能更准确地贴合控制目标（如ego轨迹误差ADE从3.4降至1.5）。

6.  **更快的推理速度**：
    *   如图7所示，在相同硬件上，**MAD-LTX的推理速度比同等性能的竞品快最多3.6倍**，这得益于其高效的两阶段生成流程。

### 总结
论文通过严谨的实验设计，特别是引入 **“Fine-tuned LTX”** 这一关键基线，令人信服地证明了**MAD方法在效率、生成质量、规划能力和可控性方面的全面优势**。其核心价值在于**提供了一条低成本的路径，能将快速发展的通用视频生成模型的进展，迅速转化为专精、可控的领域世界模型（如自动驾驶）**，极大降低了相关研究和应用的门槛。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.09452v1)
- [HTML 版本](https://arxiv.org/html/2601.09452v1)
