# RelayLLM: Efficient Reasoning via Collaborative Decoding

**相关性评分**: 8.0/10

**排名**: #7


---


## 基本信息

- **arXiv ID**: [2601.05167v1](https://arxiv.org/abs/2601.05167v1)
- **发布时间**: 2026-01-08T17:56:16Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Chengsong Huang, Tong Zheng, Langlin Huang, Jinyuan Li, Haolin Liu, Jiaxin Huang

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

RelayLLM 提出了一种通过令牌级协作解码实现高效推理的框架，显著降低计算成本，适用于资源受限环境。

## 摘要

Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

## 详细分析

## 论文《RelayLLM: Efficient Reasoning via Collaborative Decoding》详细摘要

### 1. 研究背景和动机
大型语言模型（LLMs）在复杂推理任务上表现出色，但其高昂的计算成本和延迟限制了实际部署。小型语言模型（SLMs）虽然高效，但推理能力有限。现有的协作方法（如级联或路由）通常在查询级别进行粗粒度卸载，一旦判定查询困难，便将整个生成任务交给LLM，这导致了显著的**计算浪费**，因为SLM通常有能力处理大部分推理步骤，仅在**关键位置**需要专家干预。

### 2. 核心方法和技术创新
本文提出了 **RelayLLM**，一个通过**令牌级协作解码**实现高效推理的新框架。其核心创新在于：
- **动态令牌级接力**：不同于静态路由器，RelayLLM将SLM训练为**主动控制器**。在生成过程中，SLM可以通过生成一个特殊的 `<call> n </call>` 命令来暂停自身生成，并动态调用LLM生成指定的 `n` 个令牌，随后SLM接收专家的指导并恢复推理，实现模型间的“接力”。
- **两阶段训练框架**：
    1. **监督预热**：使用合成数据教会模型生成调用命令的语法结构。
    2. **强化学习精炼**：采用**组相对策略优化（GRPO）**，并设计了**难度感知的奖励函数**。该奖励根据查询难度（分为“学生可解”、“依赖教师”、“教师无解”三类场景）动态调整，引导模型在独立求解与必要求助之间取得最优平衡，同时惩罚不必要的调用和可避免的错误。

### 3. 主要实验结果
在六个数学推理基准测试（如MATH500、GSM8K、AIME等）上的实验表明：
- **性能显著提升**：使用Qwen3-1.7B作为学生模型时，RelayLLM将平均准确率从基线的42.5%提升至**49.52%**，恢复了学生模型与教师模型（Qwen3-8B）之间约60%的性能差距。
- **极致效率**：上述性能提升仅通过调用LLM生成**总令牌数的1.07%** 实现。与性能匹配的随机路由器相比，计算成本降低了**98.2%**。
- **泛化能力强**：即使在训练未见的一般推理领域（如MMLU-Pro），RelayLLM也显著优于基线，表明其求助行为具有良好的泛化性。
- **消融研究**：验证了数据过滤、鼓励独立性和探索奖励等组件对最终性能与效率的关键作用。

### 4. 研究意义和价值
RelayLLM通过细粒度的、由SLM自主控制的协作机制，**颠覆了传统的“全有或全无”式模型路由范式**。它证明了SLM在获得**精准、稀疏的专家干预**后，能够以极低的成本处理绝大多数推理步骤，从而在效率与能力之间实现了近乎最优的权衡。这项工作为构建**低成本、高性能的混合模型推理系统**提供了新的可行路径，具有重要的实际应用价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：RelayLLM

### **一、 拟解决的核心问题**
论文旨在解决**大型语言模型高效推理部署**中的一个关键矛盾：
- **LLM（大模型）**：推理能力强，但计算成本高、延迟大。
- **SLM（小模型）**：计算高效，但复杂推理能力不足。

现有协作方法（如级联或路由）存在**“全有或全无”的粗粒度问题**：一旦判定问题困难，就将整个查询卸载给LLM，导致SLM能够处理的大部分推理步骤被浪费，造成显著的计算成本浪费。

### **二、 核心创新点**
论文提出了 **`RelayLLM`** 框架，其核心创新在于 **“令牌级协作解码”** ，具体体现在：

1.  **角色转变：SLM作为主动控制器**
    - 传统路由需要一个外部控制器来决定将任务交给谁。
    - `RelayLLM` **赋能SLM本身**，使其同时扮演**问题解决者**和**动态调度控制器**。SLM在生成过程中，可以自主决定何时、以及需要多长时间的专家帮助。

2.  **细粒度、按需的协作机制**
    - SLM通过生成一个特殊的命令令牌 `<call> n </call>` 来主动“暂停”自己的生成，并请求LLM为其生成接下来的 `n` 个令牌。
    - LLM生成指定内容后，控制权交还给SLM，SLM消化专家的指导后继续推理。
    - 这个过程可以**迭代进行**，实现SLM与LLM在令牌级别的“接力”生成。

3.  **两阶段训练框架**
    - **监督预热阶段**：使用自采样的数据，随机插入调用命令，教会SLM生成正确的命令语法，为强化学习打下基础。
    - **强化学习优化阶段**：采用**分组相对策略优化（GRPO）**，并设计了**难度感知的奖励函数**，核心是教导SLM学会在以下三种场景中做出最优决策：
        - **可独立解决**：鼓励独立完成，给予额外奖励。
        - **依赖教师**：必须求助LLM才能正确解答，惩罚不求助的行为。
        - **教师也无法解决**：鼓励在高度不确定时尝试求助（探索奖励）。

### **三、 解决方案的运作方式**
1.  **推理过程**：
    ```python
    # 伪代码示意
    context = 输入问题
    while 未生成完毕:
        token = SLM.generate_next_token(context)
        if token 包含 “<call>” 命令:
            n = 解析出需要求助的令牌数
            expert_tokens = LLM.generate_n_tokens(清理命令后的context, n)
            context += expert_tokens  # 融入专家输出
        else:
            context += token  # 使用自己的输出
    ```
2.  **训练过程**：
    - **目标**：训练SLM的策略 `π_θ`，使其学会在最大化答案正确率的同时，最小化调用LLM的成本（调用令牌比例）。
    - **关键设计**：奖励函数 `r` 不是简单的“正确-成本”，而是根据当前问题的**相对难度**（通过一组采样结果判断）进行动态调整，引导模型学习**战略性求助**。

### **四、 实际价值与效果**
- **性能提升**：在六个数学推理基准测试上，使用Qwen3-1.7B作为SLM，`RelayLLM`将平均准确率从42.5%提升至**49.52%**，恢复了SLM与LLM（Qwen3-8B）之间约60%的性能差距。
- **极致效率**：上述提升仅通过调用LLM生成 **1.07%** 的总输出令牌实现。相比达到同等性能的随机路由方法，**成本降低了98.2%**。
- **泛化能力**：即使在训练未见过的通用知识领域（如MMLU-Pro），该方法仍能显著提升性能，表明SLM学会了**通用的、识别自身知识缺口并求助**的能力。
- **模型内化**：分析表明，通过协作训练，SLM自身的内在推理能力也得到了提升，在禁止求助的“教师禁用”设定下，其在简单任务上的表现仍优于基线。

**总结**：`RelayLLM` 的核心贡献是提出了一种**让SLM在令牌级别自主、动态、按需调用LLM**的协作推理范式。它通过精巧的训练框架，教导小模型“聪明地偷懒”——独立完成大部分工作，仅在关键推理步骤精准求助于大模型，从而在几乎不增加计算成本的前提下，大幅提升了小模型的复杂任务解决能力。这项工作为高效部署语言模型提供了新的思路，平衡了性能与成本。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决大型语言模型（LLM）推理成本高昂而小型语言模型（SLM）能力不足的矛盾，其核心是克服现有协作方法（如级联或路由）因粗粒度“全有或全无”式调用而导致的巨大计算浪费。为此，论文提出了 **RelayLLM** 框架，其核心创新在于让 SLM 作为主动控制器，在推理过程中通过生成特殊命令（`<call>`）在**令牌级别**动态、精准地调用 LLM 来协助生成关键令牌，实现细粒度的“接力”式协作解码。为了训练 SLM 掌握这种策略性求助能力，作者设计了一个包含监督预热和基于群组相对策略优化（GRPO）强化学习的两阶段训练框架，并引入了难度感知的奖励机制。实验结果表明，该方法仅需调用 LLM 生成 **1.07%** 的总输出令牌，就在六个数学推理基准上将平均准确率从 42.5% 提升至 **49.52%**，以极低的成本显著缩小了 SLM 与 LLM 之间的性能差距，相比性能匹配的随机路由方法降低了 **98.2%** 的成本。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《RelayLLM: Efficient Reasoning via Collaborative Decoding》的创新点分析

该论文针对大型语言模型（LLM）与小型语言模型（SLM）协同推理中的效率瓶颈，提出了一个名为 **RelayLLM** 的新框架。其核心创新在于将协作的粒度从**查询级（query-level）** 细化到**令牌级（token-level）**，并让SLM成为主动控制器。以下是其相对于已有工作的明确创新点：

---

### 1. **令牌级动态协作解码（Token-Level Collaborative Decoding）**
- **相比以往方法的改进/不同之处**：
    - **以往方法（如级联 Cascading、路由 Routing）**：通常基于整个查询的难度进行决策，一旦判断为“难”，就将**整个剩余生成任务**全部卸载给LLM。这是一种“全有或全无”的粗粒度策略。
    - **RelayLLM**：引入了一种**交错生成（interleaved generation）** 过程。SLM在生成每个令牌时，都可以主动决定是否通过一个特殊命令（`<call> n </call>`）暂停自己的生成，并请求LLM为接下来的 `n` 个令牌提供帮助。之后，SLM接收LLM的输出并继续生成。
- **解决的具体问题/带来的优势**：
    - **解决了计算浪费问题**：在许多推理任务中，SLM能够处理大部分步骤，仅在少数关键步骤（如复杂计算、概念跳跃）需要帮助。粗粒度卸载会导致LLM为整个简单查询生成大量不必要的令牌，造成巨大计算成本。RelayLLM实现了**按需、精准的专家干预**。
    - **实现了极高的成本效益**：实验表明，仅需调用LLM生成**总令牌数的1.07%**，就能将平均准确率从42.5%提升至49.52%，在达到相同性能时，相比随机路由器成本降低**98.2%**。

### 2. **SLM作为主动控制器，无需外部路由模块**
- **相比以往方法的改进/不同之处**：
    - **以往方法（如CITER）**：通常需要一个**额外的、训练好的控制器（如小型MLP）** 来实时评估每个令牌的难度，并决定是否路由到LLM。这引入了额外的模块、计算开销和延迟。
    - **RelayLLM**：**将路由决策能力内化到SLM本身**。SLM通过学习和生成`<call>`命令，直接扮演了控制器的角色。这简化了系统架构，消除了对外部模块的依赖。
- **解决的具体问题/带来的优势**：
    - **降低了系统复杂性和延迟**：无需维护和运行一个独立的路由模型，减少了工程复杂性和推理延迟。
    - **实现了更紧密的集成**：决策与生成过程由同一个模型完成，可能使决策更符合自身的生成状态和知识边界。

### 3. **两阶段训练框架：监督预热 + 基于GRPO的强化学习**
- **相比以往方法的改进/不同之处**：
    - **以往协同方法训练**：可能直接使用强化学习或监督学习，但面临分布偏移或命令语法学习困难的问题。
    - **RelayLLM训练**：
        1.  **监督预热阶段**：使用SLM自身采样的数据，随机插入`<call>`命令进行训练。这**避免了从外部数据源学习导致的分布偏移**，专注于让模型掌握生成有效命令的**语法结构**。
        2.  **强化学习阶段**：采用**分组相对策略优化（GRPO）**，并设计了**情境感知的奖励函数**。
- **解决的具体问题/带来的优势**：
    - **解决了冷启动问题**：监督预热为后续RL训练提供了一个能生成合法命令的稳定起点，避免了RL探索初期的高方差和无效探索。
    - **实现了精细化的行为对齐**：GRPO在组内进行相对比较，稳定了训练。更重要的是，其设计的奖励函数能根据问题难度（**可独立解决、需教师帮助、教师也无法解决**）给予不同奖励，引导模型学会 **“该独立时独立，该求助时求助”** 的最优策略，避免了过度依赖或盲目自信。

### 4. **动态令牌长度预测**
- **相比以往方法的改进/不同之处**：
    - **固定长度策略**：一些方法可能预设一个固定的求助长度（如每次求助生成20个令牌）。
    - **RelayLLM**：SLM在生成`<call>`命令时，会**动态预测并指定一个所需的令牌数量 `n`**。这个 `n` 是模型根据当前上下文学习得出的。
- **解决的具体问题/带来的优势**：
    - **实现了资源的最优分配**：对于简单的提示，模型可能只请求几个令牌；对于复杂的推理步骤，则请求更长的序列。论文通过对比实验证明，动态预测策略（1.07%调用率）在达到与固定100令牌策略相近精度时，**调用率降低了约2/3**，显著提升了计算效率。

### 5. **提升了SLM的内在推理能力（知识内化）**
- **相比以往方法的改进/不同之处**：
    - **传统路由/级联**：SLM仅作为“过滤器”或“搬运工”，其自身能力在协作过程中可能没有提升。
    - **RelayLLM**：在“教师禁用”的测试中，经过RelayLLM训练的SLM，即使在**不允许调用LLM**的情况下，在部分较简单数据集上的表现也**超过了标准的GRPO基线模型**。
- **解决的具体问题/带来的优势**：
    - **带来了附加价值**：这表明在协作训练过程中，SLM**内化（internalize）了从LLM那里观察到的有效推理模式**，从而提升了其自身的原生推理能力。这意味着RelayLLM不仅是一个高效的推理系统，同时也是一种**提升小模型能力的训练方法**。

---

**总结**：RelayLLM的核心创新在于将 **“协作”** 的概念从模型之间推向**生成过程内部**，通过**令牌级的、由SLM主动控制的、动态长度的求助机制**，辅以**精心设计的两阶段训练策略**，在几乎不增加计算成本的前提下，极大地弥合了大小模型之间的性能差距，为高效LLM部署提供了新范式。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验效果概述
论文提出的 **RelayLLM** 框架在**显著提升推理准确率**的同时，实现了**极低的计算成本**。核心效果体现在：
- **性能提升**：将学生模型（SLM）的平均准确率从 **42.5%** 提升至 **49.52%**，显著缩小了与大型教师模型（LLM）的性能差距。
- **极致效率**：仅需调用教师模型生成 **1.07%** 的总输出令牌，相比性能匹配的随机路由方法，实现了 **98.2%** 的成本降低。
- **泛化能力**：在未经训练的其他推理领域（如通用知识问答）上，模型同样表现出色，证明了其“求助”行为的泛化性。

### 二、 使用的数据集与评价指标

#### 1. 主要评估数据集（数学推理）
论文在六个数学推理基准上进行了主要评估：
- **Minerva**： 数学问题求解。
- **MATH-500**： 来自MATH数据集的500个问题。
- **GSM8K**： 小学数学应用题。
- **Olympiad-Bench**： 奥林匹克级双语多模态科学问题。
- **AIME-2024 / AIME-2025**： 美国数学邀请赛试题（高难度）。

#### 2. 泛化评估数据集（非数学领域）
为测试泛化能力，额外使用了三个未见过的通用推理数据集：
- **Big-Bench Hard (BBEH)**： 复杂推理任务。
- **MMLU-Pro**： 多任务语言理解。
- **SuperGPQA**： 涵盖285个研究生学科的知识问答。

#### 3. 评价指标
- **准确率**： 对于大多数数据集（Minerva, MATH500, GSM8K, Olympiad），报告 **pass@1**（贪婪解码）。
- **鲁棒性评估**： 对于高难度的AIME数据集，报告 **avg@32**（采样32次计算平均准确率），以获得更稳健的评估。
- **效率指标**：
    - **调用比率**： 教师模型生成的令牌数占总输出令牌数的百分比。
    - **成本降低**： 与基线方法相比的计算成本节省。

### 三、 对比的基线方法
论文与以下基线方法进行了全面对比：

1.  **Base Model**： 未经协作训练的学生模型本身（Qwen3-0.6B/1.7B）。
2.  **GRPO Baseline**： 仅使用GRPO（Group Relative Policy Optimization）在完整数据集上微调的学生模型，**不包含协作机制**。用于衡量RL训练本身带来的增益。
3.  **CITER**： 一种**需要额外控制器**的令牌级路由方法。作为当前最先进的细粒度协作解码基线。
4.  **理想路由器**： 一个理论上的“完美路由器”，能准确地将SLM无法解决的问题全部交给LLM。用于界定性能上限。
5.  **随机路由器**： 随机将问题分配给SLM或LLM，作为资源消耗的对比基准。
6.  **教师模型**： Qwen3-8B，作为性能上限的参考。

### 四、 关键性能提升与结论

#### 1. 核心性能对比（表1）
以 **Qwen3-1.7B** 学生模型为例：
- **RelayLLM (Difficulty-Aware)** 达到了 **49.52%** 的平均准确率。
- 相比 **Base Model (42.50%)**，绝对提升 **+7.02%**。
- 相比 **GRPO Baseline (44.06%)**，绝对提升 **+5.46%**，证明了协作机制的有效性。
- 相比需要外部控制器的 **CITER (46.81%)**，绝对提升 **+2.71%**，且调用比率更低（1.07% vs 1.34%），证明了框架的优越性。
- **关键结论**：RelayLLM用仅 **1.07%** 的LLM令牌，恢复了学生模型与教师模型（54.12%）之间约 **60%** 的性能差距。

#### 2. 效率与成本（图1）
- **RelayLLM** 在准确率-调用比率帕累托前沿上显著优于随机路由器和CITER。
- 要达到与RelayLLM相近的准确率，随机路由器需要调用LLM生成约 **60%** 的令牌，而RelayLLM仅需 **1.07%**，实现了 **98.2%** 的成本降低。

#### 3. 泛化能力（表2）
在**未参与训练**的通用领域数据集上：
- 在 **MMLU-Pro** 上，Qwen3-1.7B + RelayLLM 达到 **59.03%**，显著超过 GRPO基线 (49.76%) 和 CITER (53.38%)。
- **结论**：模型学会的“在知识缺口处求助”的行为具有很好的领域泛化性。

#### 4. 消融实验关键结论（表3）
- **数据过滤至关重要**：移除对“教师也无法解决”问题的过滤后，调用比率激增至3.30%，且准确率下降，说明避免无谓调用是关键。
- **鼓励独立性有效**：移除对“学生可独立解决”问题的额外奖励后，模型变得过度依赖教师，调用比率飙升至4.10%。
- **探索奖励必要**：移除对“教师无法解决”场景的探索奖励后，准确率显著下降至47.56%，说明在高度不确定时鼓励求助有助于探索正确路径。

#### 5. 动态长度预测的优势（表5）
- 与固定调用长度（如Fixed-100）的模型相比，**RelayLLM** 在达到相近准确率（49.52% vs 49.56%）的同时，**调用比率大幅降低（1.07% vs 2.87%）**。
- **结论**：动态预测“恰好足够”的求助令牌数，能最小化计算浪费。

#### 6. 内在推理能力分析（表4）
在**禁用教师调用**的“教师游离”模式下评估：
- 在**简单数据集**上，RelayLLM模型的表现甚至超过了GRPO基线，表明学生在协作训练过程中**内化了一部分教师的推理能力**。
- 在**困难数据集**上，禁用教师导致性能显著下降，表明对于复杂任务，模型仍**策略性地依赖专家干预**。

### 总结
论文通过系统性的实验设计，定量地证明了 **RelayLLM** 框架在**性能**与**效率**之间取得了卓越的平衡。其核心价值在于：
1.  **细粒度协作的有效性**： 令牌级的、按需求助的策略远优于传统的“全有或全无”式查询路由。
2.  **可学习的求助行为**： 通过两阶段训练（监督预热+GRPO），小模型可以学会在何时、以多长篇幅进行求助。
3.  **极高的性价比**： 用极少的计算开销（<1.1%的LLM令牌），换取了接近大型模型60%的性能增益，为实际部署提供了极具吸引力的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.05167v1)
- [HTML 版本](https://arxiv.org/html/2601.05167v1)
