# LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments

**相关性评分**: 6.0/10

**排名**: #94


---


## 基本信息

- **arXiv ID**: [2601.02905v1](https://arxiv.org/abs/2601.02905v1)
- **发布时间**: 2026-01-06T10:44:19Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Sara Micol Ferraina, Michele Brienza, Francesco Argenziano, Emanuele Musumeci, Vincenzo Suriani, Domenico D. Bloisi, Daniele Nardi

## 关键词

Lightweight Architecture, Inference Efficiency, Edge Deployment

## 一句话总结

LOST-3DSG 是一种轻量级开放词汇 3D 场景图方法，通过语义跟踪在动态环境中高效追踪物体，适用于机器人应用。

## 摘要

Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.

## 详细分析

## 论文摘要：LOST-3DSG: 动态环境中基于语义追踪的轻量级开放词汇3D场景图

**1. 研究背景和动机**
在机器人领域，动态环境中的物体追踪是一项核心挑战。尽管3D场景图（3DSG）结合几何与语义信息，为环境建模提供了灵活框架，但现有基于大型基础模型（如CLIP）的开放词汇方法通常需要存储高维视觉嵌入，导致巨大的计算和内存开销，限制了其在动态场景中的实时性和可扩展性。因此，亟需一种既能保持开放词汇理解能力，又足够轻量的高效表示方法。

**2. 核心方法和技术创新**
本文提出了LOST-3DSG，一个专为动态环境设计的轻量级开放词汇3D场景图系统。其核心创新在于：
- **轻量语义表示**：摒弃存储密集的CLIP特征，转而利用视觉语言模型提取物体级语义属性（如标签、颜色、材质、细粒度描述），并编码为低成本的`word2vec`和句子嵌入。
- **语义追踪算法**：设计了**Lost相似性函数**，通过加权组合语义、颜色、材质和描述相似性，来关联跨时间观测的物体实例，实现基于属性的身份维持。
- **双模式场景更新模块**：系统在**探索模式**下增量构建场景图，在**追踪模式**下基于语义相似性和空间一致性动态更新物体位置、处理物体消失与新增，并维护一个持久化的世界锚定图。

**3. 主要实验结果**
在TIAGo机器人真实场景实验表明：
- **有效追踪**：系统在由易到难（★至★★★）的动态场景中，能有效处理物体的移动、更新和消失，实现了高精度的语义追踪。
- **组件贡献**：消融实验证实，细粒度描述相似性对实例区分最关键，颜色和材质相似性有助于稳定更新，仅使用标签相似性效果不佳。
- **极致轻量**：与需要为每个体素存储CLIP嵌入的方法（约641 MB）相比，LOST-3DSG仅需约3 KB内存即可表示包含21个物体的复杂场景，内存占用降低数个数量级。

**4. 研究意义和价值**
本研究证明了**轻量的属性级语义表示足以在动态环境中实现稳健的开放词汇物体追踪**，为机器人长期场景理解提供了一种高效、可扩展的替代方案。它在计算资源与性能之间取得了良好平衡，推动了轻量级、开放世界3D场景理解的发展，并为下游推理、规划任务奠定了坚实基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：LOST-3DSG

### **核心问题**
论文旨在解决**动态环境中机器人实时、高效地进行开放词汇（open-vocabulary）物体语义跟踪**的难题。现有方法通常依赖计算和存储成本高昂的基础模型（如CLIP），生成密集的视觉嵌入，导致系统**内存占用大、更新效率低**，难以在动态场景中实现可扩展的实时应用。

### **核心创新点**
论文的核心创新在于提出了一种**轻量级、基于语义属性的开放词汇3D场景图（3D Scene Graph, 3DSG）构建与跟踪方法**，其创新性主要体现在以下三个层面：

1.  **轻量化的开放词汇语义表示**
    - **传统方法**：通常为场景中的每个体素或点存储高维的CLIP视觉嵌入，以实现开放词汇查询。这导致内存消耗巨大（论文中举例：一个场景可达~641 MB）。
    - **LOST-3DSG方法**：摒弃密集的视觉特征，转而使用**紧凑的语义属性**（物体标签、颜色、材质、细粒度描述）来表示每个物体节点。这些属性通过视觉语言模型（VLM）提取，并使用`word2vec`和句子嵌入（如`text-embedding-3-small`）进行编码。
    - **效果**：将场景的语义信息内存占用从百兆字节级**降低到千字节级**（论文中举例：21个物体的场景仅需~3 KB），同时保留了开放词汇的理解能力。

2.  **基于语义属性的物体跟踪算法（Lost Similarity Function, LSF）**
    - **传统动态跟踪**：多依赖于几何一致性或运动模型。
    - **LOST-3DSG方法**：设计了一个综合相似度函数（LSF），通过匹配跨时间观测的语义属性来判断是否为同一物体实例。LSF融合了四个加权项：
        - **语义相似度 (`sℓ`)**：基于`word2vec`的标签相似度。
        - **颜色相似度 (`sc`)**：基于RGB空间的归一化距离。
        - **材质相似度 (`sm`)**：基于`word2vec`的材质相似度。
        - **描述相似度 (`sd`)**：基于句子嵌入的细粒度文本描述相似度（关键用于区分同类物体）。
    - **效果**：实现了**语义层面的数据关联**，使系统能够理解“一个先前被识别为‘红棕色木质金属锤子’的物体移动后，仍是同一个物体”，而不仅仅是根据位置匹配。

3.  **双模式场景图更新机制（Scene Update Module, SUM）**
    - 系统明确分为**探索模式**和**跟踪模式**，以适应不同任务阶段：
        - **探索模式**：专注于发现和添加新物体到3DSG中，不删除物体，以构建完整的环境目录。
        - **跟踪模式**：主动维护3DSG的一致性。利用LSF匹配当前观测与已有物体节点，并基于空间一致性进行验证。可处理物体的**移动、新增、消失**，并管理“不确定”物体集合以处理歧义。
    - **效果**：提供了鲁棒的场景图生命周期管理，能够在部分可观测的动态环境中，持续更新世界模型。

### **解决方案的流程总结**
1.  **感知模块**：使用VLM（GPT-4o-mini）从RGB图像中提取开放词汇物体标签和属性（颜色、材质、描述），使用OWLv2进行检测，EfficientViT-SAM进行分割，并结合深度信息将分割掩码反投影成3D点云，最终构建当前帧的3D场景图。
2.  **语义关联**：对于新的物体检测，使用**LSF函数**计算其与3DSG中所有已有物体节点的综合相似度。
3.  **场景更新**：**SUM模块**根据运行模式（探索/跟踪）和LSF匹配结果，决定是创建新节点、更新已有节点（如更新边界框位置）、还是将节点标记为不确定/删除，从而实现对动态物体状态的持续跟踪。

### **实际价值**
- **效率与实用性**：极大地降低了开放词汇3D场景理解的计算和存储门槛，使得在资源受限的机器人平台（如论文中使用的TIAGo机器人）上部署成为可能。
- **泛化能力**：得益于开放词汇的VLM，系统能够识别和跟踪训练集未见的物体类别，增强了在非结构化环境中的适应性。
- **结构化输出**：输出的3D场景图为下游任务（如导航、规划、人机交互）提供了可直接利用的、富含语义的结构化世界模型。

**简而言之，LOST-3DSG的核心创新是：用一组精心设计的、轻量的语义属性替代沉重的视觉嵌入，并基于此构建了一套高效的语义跟踪与场景图更新机制，在保证开放词汇能力的同时，实现了动态环境理解在效率上的重大突破。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决动态环境中机器人实时、高效地跟踪移动物体的核心难题，其核心矛盾在于现有依赖大型基础模型（如CLIP）的开放词汇3D场景图方法存在计算和内存开销过大的问题。为此，论文提出了LOST-3DSG框架，其核心创新在于摒弃了存储密集视觉特征的做法，转而采用一种轻量级的语义跟踪方法：通过视觉语言模型提取物体标签、颜色、材质和细粒度描述等语义属性，并利用word2vec和句子嵌入将这些属性编码为紧凑的向量，进而设计了一个综合相似度函数来跨时间关联物体实例，实现基于语义的身份持续跟踪。实验结果表明，该方法在真实机器人平台上有效实现了动态物体的跟踪，同时将场景语义表示的内存占用从传统CLIP方法的数百MB大幅降低至KB级别，在保持开放词汇能力的前提下，取得了效率与性能的优越平衡。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文核心创新点分析

这篇论文《LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments》针对动态环境中的物体跟踪问题，提出了一个轻量级的开放词汇3D场景图系统。其相对于已有工作的明确创新点如下：

### 1. **轻量级开放词汇语义表示方法**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：主流的开放词汇3D场景图方法（如ConceptGraphs, DovSG）依赖于在**体素或点级别存储密集的CLIP视觉嵌入**。这些高维特征（例如512维）虽然语义丰富，但导致内存和计算开销巨大。
     - **本文方法**：摒弃了存储密集CLIP特征的做法，转而采用**基于语义属性的低维表示**。具体而言，为每个物体节点存储**标签、颜色、材质和一段细粒度文本描述**，并使用`word2vec`和句子嵌入（如`text-embedding-3-small`）将这些文本属性转换为紧凑的向量。
   - **解决的具体问题/带来的优势**：
     - **解决了内存和计算效率低下的问题**。论文量化对比显示，对于一个包含21个物体的复杂场景，传统CLIP方法需要约**641 MB**内存（为每个体素存储嵌入），而LOST-3DSG仅需约**3 KB**，内存占用降低了超过**5个数量级**。
     - **保持了开放词汇能力**。通过使用VLM提取开放词汇的物体标签和属性，系统仍能识别和描述预定义类别之外的物体，但以极低的存储成本实现了这一目标。
     - **为动态环境中的实时、持续更新扫清了障碍**，使得在资源受限的机器人平台上部署成为可能。

### 2. **基于语义属性的物体跟踪算法**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：动态物体跟踪多依赖于**几何一致性（如运动模型）** 或**基于外观的视觉特征匹配**。一些先进的语义跟踪方法（如SemTrack）虽然结合语义，但通常是在2D视频领域，且可能依赖复杂的深度学习模型。
     - **本文方法**：提出了 **“Lost相似性函数（LSF）”** ，这是一个**专门为3D场景图内物体身份关联设计的复合语义相似度度量**。它综合计算四个属性的相似度：语义标签（`word2vec`）、颜色（RGB空间距离）、材质（`word2vec`）和细粒度描述（句子嵌入）。
   - **解决的具体问题/带来的优势**：
     - **解决了在几何外观变化或遮挡下的物体身份维持问题**。例如，一个“红棕色木质金属锤子”被移动到新位置后，系统能通过其语义属性（而非仅仅位置或视觉特征）识别为同一物体。
     - **实现了更鲁棒和可解释的跟踪**。算法显式地利用人类可理解的属性进行匹配，降低了因视角、光照变化导致的误匹配。
     - **首次将语义跟踪算法直接集成到3D场景图（3DSG）的更新模块中**，实现了**场景表示与跟踪的统一框架**，使世界模型能够持续、语义一致地演化。

### 3. **双模式（探索与跟踪）场景图更新机制**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：许多建图或跟踪系统采用单一策略处理新观测，可能在建图初期就过早地删除未被观测到的物体，或在跟踪阶段盲目添加新观测导致身份混淆。
     - **本文方法**：在**场景更新模块（SUM）** 中明确设计了两种操作模式：
       1. **探索模式**：专注于**增量式发现和填充**场景图。此阶段不删除物体，即使有匹配冲突也优先创建新节点，旨在快速构建环境的完整目录。
       2. **跟踪模式**：专注于**维护场景一致性**。积极匹配、更新、删除物体，并引入 **“不确定物体”** 集合来处理语义匹配成功但空间位置冲突的歧义情况（例如，物体被移动）。
   - **解决的具体问题/带来的优势**：
     - **解决了建图完整性与跟踪精确性之间的冲突**。探索模式保证了初期建图的完整性，避免了因传感器视野有限或遮挡导致的物体遗漏。切换到跟踪模式后，则能精细管理动态变化。
     - **通过“不确定物体”机制，优雅地处理了物体移动这一核心挑战**。当物体在新位置被检测到（语义匹配高）但原位置实例仍存在时，系统不会立即强制合并或丢弃，而是将原实例标记为“不确定”，留待后续观测验证，这**减少了因单次观测错误导致的决策风险**。
     - **提高了系统在长期运行中的鲁棒性和适应性**，使其能更好地应对现实世界中物体频繁进出视野、被移动等复杂情况。

### 4. **整体系统架构：轻量级、开放词汇、专为动态环境设计**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：要么是**重型**的开放词汇系统（计算开销大），要么是**轻量级**但**词汇封闭**的系统（无法处理未知物体），专门为**动态环境**设计的3D场景图工作相对较少。
     - **本文方法**：将**轻量化设计**、**开放词汇能力**和**动态语义跟踪**这三个目标在一个系统中协同实现。技术选型上，采用`OWLv2`进行开放词汇检测，`EfficientViT-SAM`进行分割，`GPT-4-mini`作为VLM提取属性，整套流程针对效率进行了优化。
   - **解决的具体问题/带来的优势**：
     - **提供了一个切实可行的、面向真实机器人部署的动态场景理解方案**。论文通过在TIAGo机器人上的真实环境实验验证了其有效性和效率。
     - **在精度和效率之间取得了新的平衡**。实验表明，尽管使用了轻量级语义表示，系统在三个不同复杂度的动态场景中仍能保持较高的物体检测、更新和删除准确率（见表1和表2）。
     - **为后续研究指明了方向**：证明了**无需依赖密集基础模型嵌入也能实现有效的开放词汇动态跟踪**，鼓励社区探索更轻量、更高效的场景理解替代方案。

**总结**：LOST-3DSG的核心创新在于其**“轻量”**与**“语义”** 的结合。它通过用紧凑的语义属性向量替代沉重的CLIP嵌入，并设计一套基于这些属性的跟踪与更新算法，成功解决了现有开放词汇3D场景图方法在动态环境中**内存消耗大、计算成本高、难以持续更新**的痛点，为机器人在复杂、变化环境中进行长期、实时的语义环境理解提供了新的技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验设置与数据集
- **实验平台**：在真实室内环境中使用 **TIAGo 机器人**（搭载ROS 2 Humble）进行部署。
- **数据集**：**未使用公开标准数据集**，而是设计了**三个自定义的、复杂度递增的真实场景**进行系统验证：
    1.  **简单场景（⋆）**：3个静态物体，随后移动、改变位置并最终消失。
    2.  **中等场景（⋆⋆）**：21个物体，物体密度大，存在部分观测、位置更新和视野外消失。
    3.  **困难场景（⋆⋆⋆）**：9个物体，环境高度动态，物体频繁在未被观测时改变位置。

### 二、 评价指标
论文采用了**任务导向的定量指标**来评估场景更新模块（SUM）的性能：
- **检测率 (Detections)**：系统成功检测到的物体数量与场景中实际物体总数的比例。
- **删除率 (Deletions)**：系统正确将已消失物体从场景图中移除的比例。
- **更新率 (Updates)**：系统正确跟踪并更新已移动物体新位置的比例。

### 三、 基线方法与对比
论文主要进行了两类对比分析：

1.  **与CLIP-based方法的间接对比（内存与计算效率）**：
    - **基线方法**：以 **ConceptGraphs** 和 **DovSG** 为代表的、依赖密集CLIP视觉特征（如ViT-B/32）的开放词汇3D场景图方法。
    - **对比指标**：**内存占用**。
    - **关键结果**：
        - **CLIP方法**：在包含626,140个体素（分辨率25mm）的中等复杂场景中，存储每个体素的CLIP嵌入（512维）约需 **641 MB**。
        - **LOST-3DSG**：仅存储物体级语义属性（标签、颜色、材质、描述），整个场景（21个物体）仅需约 **3.3 KB**。
        - **结论**：LOST-3DSG在保持开放词汇表达能力的同时，将内存占用降低了 **超过5个数量级**，实现了显著的轻量化。

2.  **消融实验（Ablation Study）**：
    - **对比对象**：LOST-3DSG核心的**Lost相似性函数（LSF）** 的不同组件组合。
    - **对比指标**：删除率和更新率（在三个场景上取平均）。
    - **关键结果**（如表2所示）：
        - **完整LSF**（包含标签、颜色、材质、描述相似性）取得了最佳的综合性能（删除率0.778，更新率0.944）。
        - **仅使用描述相似性 (`s_d`)**：更新性能尚可（0.833），但删除性能大幅下降（0.444），说明细粒度描述虽能区分实例，但不足以独立支撑物体生命周期的决策。
        - **仅使用语义标签相似性 (`s_l`)**：性能最差（删除率0.333，更新率0.556），证实了仅靠类别标签无法进行可靠的长期跟踪。
        - **移除颜色和材质相似性**：对删除率影响不大，但显著损害了更新率，说明低层外观特征对于在时间更新中区分视觉相似的实例至关重要。

### 四、 主要性能与结论
1.  **跟踪效果**：系统在三个复杂度递增的场景中均能有效工作。
    - 在简单和中等场景中，检测、删除、更新任务接近完美或表现良好。
    - 在困难的高动态场景中，系统面临挑战，但仍在更新任务上取得了13/14的准确率，证明了其语义跟踪算法在压力测试下的鲁棒性。

2.  **核心结论**：
    - **技术创新价值**：成功验证了**基于轻量级语义属性（而非密集视觉嵌入）进行动态物体跟踪**的可行性。通过组合`word2vec`（标签、材质）、颜色空间距离和句子Transformer（描述）等多种互补的语义线索，实现了高效且有效的开放词汇场景理解。
    - **实际应用价值**：为机器人**在资源受限的平台上进行长期、动态的开放世界场景理解**提供了一种极具潜力的解决方案。其极低的内存占用和计算开销使其更适用于实时机器人应用。
    - **局限性**：论文也指出，这种轻量化方法在语义描述存在噪声或歧义时（例如VLM生成描述不一致），性能会下降，可能导致身份识别错误或暂时的物体重复。这为未来研究指明了改进方向。

**总结**：LOST-3DSG通过创新的轻量化语义表示和跟踪算法，在真实机器人平台上实现了高效的动态物体跟踪。其核心贡献不在于在标准基准上超越所有SOTA方法，而是**开辟并验证了一条不同于依赖重型基础模型的新技术路径**，在性能与效率之间取得了出色的平衡。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.02905v1)
- [HTML 版本](https://arxiv.org/html/2601.02905v1)
