# CoV: Chain-of-View Prompting for Spatial Reasoning

**相关性评分**: 6.0/10

**排名**: #66


---


## 基本信息

- **arXiv ID**: [2601.05172v2](https://arxiv.org/abs/2601.05172v2)
- **发布时间**: 2026-01-08T17:59:42Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, Ruihan Zhu, Gholamreza Haffari, Bohan Zhuang

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

CoV是一种无需训练、基于提示的推理框架，通过链式视图选择与调整提升3D环境中的空间推理能力，与视觉-语言-动作模型和推理效率相关，但未明确涉及轻量级架构或边缘部署。

## 摘要

Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training. Code is available on https://github.com/ziplab/CoV .

## 详细分析

## 论文《CoV: Chain-of-View Prompting for Spatial Reasoning》详细摘要

### 1. 研究背景和动机
在具身人工智能领域，**具身问答**要求智能体在三维环境中通过主动感知和推理来回答问题。然而，当前主流的**视觉-语言模型**通常依赖于一组**固定且有限的输入视角**，这限制了它们在推理时获取与问题相关上下文信息的能力，尤其是在信息分散或被遮挡的复杂空间场景中。现有方法难以进行多步、主动的探索，导致对复杂空间问题的理解不足。因此，本文旨在提出一种无需额外训练、在测试时即可增强VLM空间推理能力的框架。

### 2. 核心方法和技术创新
本文提出了 **“链式视角”提示框架**，这是一个**无需训练、测试时**的推理框架，通过**从粗到细的探索过程**将VLM转变为主动的视角推理器。其核心创新在于两阶段设计：
- **粗粒度视角选择**：首先，一个**视角选择智能体**从大量初始视频帧中过滤冗余信息，筛选出与问题最相关的少数**锚定视角**，作为探索的起点。
- **细粒度视角调整**：随后，**CoV智能体**进入一个**“推理-行动”循环**。它基于当前观察和问题，生成离散的相机动作指令（如移动、旋转），通过**SE(3)变换**在底层3D场景表示中获取新的观察视角。这个过程迭代进行，直到收集到足够信息或达到步数限制，最终生成答案。
该方法的关键在于将**被动、固定的观察**转变为**主动、开放的视角搜索**，模仿了人类在环境中为解答问题而主动调整观察位置的行为。

### 3. 主要实验结果
在多个主流3D具身问答基准上进行了广泛评估，结果证明了CoV的有效性和通用性：
- **在OpenEQA基准**上：在Qwen3-VL-Flash、GLM-4.6V等四个主流VLM上，平均带来**+11.56%** 的性能提升（LLM-Match指标），其中在Qwen3-VL-Flash上最大提升达**+13.62%**。
- **测试时扩展性**：通过强制增加最小行动步数，性能可进一步平均提升**+2.51%**，展示了**无需训练即可通过增加计算（探索步数）来提升性能**的能力。
- **在ScanQA和SQA3D基准**上：同样取得了强劲性能，例如在ScanQA上获得了**116 CIDEr**和**31.9% EM@1**的优异结果，超越了诸多专门训练的3D VLM。
- **消融实验**证实了粗粒度视角选择模块的必要性，移除该模块会导致性能平均下降4.59%。

### 4. 研究意义和价值
本研究具有重要的理论意义和实际价值：
- **方法论创新**：提出了一种**模型无关、无需训练**的测试时推理范式，通过结构化提示将静态VLM转化为主动探索智能体，为增强现有大模型的空间推理能力提供了新思路。
- **实际应用价值**：该框架可直接应用于机器人导航、增强现实、人机交互等需要复杂空间理解和主动感知的领域，提升智能体在真实物理环境中的问答和决策能力。
- **概念性转变**：强调了智能体**“如何看”**（主动选择视角）与**“看什么”** 同等重要，推动了具身AI从被动感知向主动、序列化环境探索的范式转变。
- **开源贡献**：作者公开了代码，促进了该领域的研究和复现。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：CoV: Chain-of-View Prompting for Spatial Reasoning

### **一、 核心问题**
现有视觉-语言模型在**具身问答**任务中存在一个根本性限制：它们通常依赖于**固定且有限的输入视角**（如预定义的多帧图像或视频片段）。这导致模型在推理时**无法主动获取与问题相关的上下文信息**，尤其是在复杂3D场景中，答案所需的信息可能分布在多个视角、部分被遮挡或需要主动探索才能发现。这种被动观察模式严重阻碍了模型进行复杂空间推理的能力。

### **二、 核心创新点**
论文提出了 **“链式视角提示”** 框架，这是一种**无需训练、仅在测试时使用**的推理方法。其核心创新在于将传统的静态VLM转变为一个**主动的视角推理器**，通过一个**由粗到精的探索过程**来动态获取信息。

1.  **范式转变**：从**被动观察**（固定输入）转向**主动探索**（动态获取视角）。这是对传统VLM在3D任务中应用方式的根本性改变。
2.  **两阶段、由粗到精的探索框架**：
    *   **粗粒度视角选择**：引入一个**视角选择智能体**，从大量初始帧中过滤冗余信息，筛选出与问题最相关的少数“锚点视角”。
    *   **细粒度视角调整**：引入一个**CoV智能体**，执行“推理-行动”循环。它基于当前观察和问题，生成离散的相机动作指令（如前进、左转），在底层3D场景表示中获取新的观察视角，迭代进行直到收集到足够信息或达到步数限制。
3.  **测试时扩展能力**：该方法展现出**测试时扩展**特性——即允许模型在推理时执行更多探索步骤，其性能会随之提升。这提供了一种无需重新训练即可提升模型能力的有效途径。
4.  **模型无关性**：该框架不修改VLM的内部权重，可以即插即用地应用于各种主流的开源和闭源VLM（如Qwen, GPT, Gemini等），通用性强。

### **三、 解决方案（方法论）**
解决方案紧密围绕核心创新点构建，具体流程如下：

1.  **输入与问题定义**：
    *   输入：一个3D场景表示（如点云、网格）、从场景视频中采样的一组帧序列、一个自然语言问题。
    *   目标：生成准确回答问题的文本答案。

2.  **CoV框架执行流程**：
    ```mermaid
    graph TD
        A[输入: 所有视频帧 + 问题Q] --> B[阶段一: 粗粒度视角选择];
        B --> C[视角选择智能体];
        C --> D[输出: 筛选后的关键锚点视角集 V'];
        D --> E[阶段二: 细粒度视角调整];
        E --> F[CoV智能体初始化: 上下文 C0 = Q, V'];
        F --> G{推理-行动循环};
        G --> H[基于当前上下文思考， 生成动作指令at];
        H --> I[将at转换为SE(3)相机位姿变换];
        I --> J[在3D场景中渲染出新视角vt+1];
        J --> K[更新上下文: Ct+1 = Ct ∪ vt+1];
        K --> L{终止条件?};
        L -- 信息足够/步数限制 --> M[生成最终答案A];
        L -- 否 --> G;
    ```

3.  **关键技术组件**：
    *   **动作空间**：包含平移（前后左右上下）、旋转（偏航、俯仰、滚动）和切换到锚点视角的动作。
    *   **场景交互**：利用3D场景表示，将语言模型生成的动作指令映射为具体的相机位姿变化，从而渲染出新的2D观察视角。
    *   **提示工程**：为视角选择智能体和CoV智能体设计了专门的提示模板，引导模型完成特定子任务。

### **四、 实际价值与意义**
1.  **性能显著提升**：在OpenEQA、ScanQA、SQA3D等多个权威3D具身QA基准测试上取得显著效果。例如，在OpenEQA上平均提升**+11.56%**（LLM-Match），在ScanQA上CIDEr分数达到**116**，超越了许多需要专门训练的3D VLM。
2.  **提供高效的性能提升路径**：通过“测试时扩展”，仅增加推理时的计算预算（探索步数），就能进一步提升模型表现，为资源受限情况下的模型优化提供了新思路。
3.  **推动具身AI发展**：该工作强调了“如何看”与“看到什么”同等重要，为构建能够在复杂物理世界中主动感知、探索和推理的AI智能体提供了重要的方法论启示。
4.  **实用性与可复现性**：框架无需训练、即插即用，且代码已开源，易于研究和应用社区采纳与拓展。

**总结**：CoV框架通过引入**主动、迭代、由粗到精的视角探索机制**，巧妙地解决了传统VLM在3D空间推理中信息获取受限的瓶颈问题。它不仅在多个任务上实现了性能的飞跃，更代表了一种提升现有大模型空间认知能力的有效且通用的范式。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决**三维具身问答（EQA）任务中，现有视觉语言模型（VLM）因受限于固定、有限的输入视角，而难以获取足够问题相关上下文，从而阻碍复杂空间推理**的核心问题。

为此，论文提出了一个**无需训练、在测试时进行推理的框架——链式视角提示（Chain-of-View, CoV）**。该框架通过一个由粗到精的探索过程，将VLM转变为主动的视角推理器。其核心分为两阶段：1）**粗粒度视角选择**：筛选冗余帧，识别与问题最相关的“锚点”视角；2）**细粒度视角调整**：在推理与离散相机动作（如移动、旋转）之间迭代循环，从底层3D场景表示中获取新观测，直至收集到足够信息或达到步数限制。

该方法在多个主流VLM和基准测试（OpenEQA, ScanQA, SQA3D）上取得了显著效果。**在OpenEQA上平均提升LLM-Match指标11.56%，最高提升达13.62%**，并在ScanQA等数据集上达到或超越了当前最优性能。实验还证明了CoV具备**测试时扩展**能力：增加探索步数能带来进一步的性能提升。结论表明，**将问题对齐的视角选择与开放式视角搜索相结合，是一种无需额外训练即可有效提升3D EQA中空间推理能力的、模型无关的策略**。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文提出的 **CoV（Chain-of-View）Prompting** 框架在3D具身问答（EQA）任务中，针对现有视觉-语言模型（VLMs）的局限性，提出了多项明确的创新。以下是逐条分析：

---

### 1. **从“被动固定视角”到“主动动态探索”的范式转变**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：大多数现有的3D VLM或视频VLM在推理时依赖于**固定且有限的输入视角序列**（如预先采样的多帧图像或视频片段）。模型只能基于给定的静态视图进行一次性答案生成，无法在测试时主动获取新信息。
    - **CoV的改进**：提出一个**训练无关、测试时**的推理框架，将VLM转化为一个**主动的视角推理器**。它通过一个**由粗到细的探索过程**，在3D场景表示中动态选择、调整并获取新的观察视角。
- **解决的具体问题/带来的优势**：
    - **解决了信息获取不足的问题**：在复杂3D场景中，答案所需的关键信息可能分布在多个视角或被部分遮挡。固定视角输入可能导致上下文不足。CoV通过主动探索，能够收集到更全面、与问题相关的视觉上下文。
    - **提升了空间推理能力**：动态调整视角使模型能够从最佳角度观察物体和空间关系，从而进行更深入、更准确的多步空间推理。

### 2. **两阶段、由粗到细的视角处理流程**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：通常将所有输入帧（可能包含大量冗余）一次性输入模型，或使用简单的启发式方法选择关键帧。
    - **CoV的改进**：明确分为两个阶段：
        1.  **粗粒度视角选择（Coarse-Grained View Selection）**：使用一个**视角选择代理（View Selection Agent）**，基于问题语义从大量初始帧中筛选出最相关的少量**锚点视角（Anchor Views）**。这通常包括提供一个场景的鸟瞰图以辅助全局理解。
        2.  **细粒度视角调整（Fine-Grained View Adjustment）**：使用**CoV代理**，以锚点视角为起点，执行一个**“行动-推理”循环**。模型根据当前观察和问题生成离散的相机动作指令（如移动、旋转），在3D场景中获取新视角，并迭代进行推理。
- **解决的具体问题/带来的优势**：
    - **解决了搜索效率问题**：直接在全视角空间搜索效率低下。粗选择阶段大幅减少了冗余，聚焦于潜在信息区域。
    - **实现了精准观察**：细调整阶段允许模型进行微调，观察被遮挡或模糊的细节，获取传统固定视角方法无法触及的信息。
    - **增强了推理的可解释性**：整个流程产生了清晰的、多步骤的推理链，使模型的决策过程更加透明和可追溯。

### 3. **实现“测试时缩放（Test-Time Scaling）”**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：模型性能在训练后基本固定。要提升性能，通常需要重新训练、微调或增加模型参数（计算时缩放）。
    - **CoV的改进**：通过**增加推理过程中的最小行动步数预算**，可以在不改变模型权重的情况下，直接提升任务性能。论文实验表明，增加行动步数，模型的LLM-Match得分会持续提高。
- **解决的具体问题/带来的优势**：
    - **提供了一种高效、低成本的性能提升途径**：无需昂贵的重新训练或模型缩放，仅通过测试时允许更多的“思考”和“探索”步骤，就能显著提升复杂空间推理任务的准确率。
    - **证明了推理过程本身的价值**：强调了在具身AI中，**推理过程（如何看）** 与**模型容量（有什么能力）** 同等重要。

### 4. **模型无关的通用框架**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：许多3D VLM改进方案是**模型特定的**，需要修改模型架构或进行针对性的训练。
    - **CoV的改进**：作为一个**提示工程（Prompting）框架**，它可以应用于多种主流的开源和闭源2D/3D VLMs（如Qwen-VL, GPT-4, Gemini等），而无需对这些模型进行任何内部修改或额外训练。
- **解决的具体问题/带来的优势**：
    - **具有高度的通用性和灵活性**：可以快速部署到现有的强大VLM上，立即提升其在3D EQA任务上的表现。
    - **降低了应用门槛**：用户无需掌握复杂的3D模型训练技巧，即可利用先进VLMs解决复杂的空间推理问题。

---

## 总结
**CoV** 的核心创新在于**将静态的视觉问答转化为动态的、目标驱动的视觉探索过程**。它通过：
1.  **范式创新**（主动探索 vs. 被动观察），
2.  **流程创新**（两阶段由粗到细处理），
3.  **性能提升方式创新**（测试时缩放），
4.  **框架设计创新**（模型无关性），

系统地解决了现有VLMs在3D具身问答中因**视角固定**而导致的**信息获取不全**和**空间推理能力受限**的根本问题。其实验在多个基准（OpenEQA, ScanQA, SQA3D）上取得的显著提升，验证了这些创新点的有效性和实际价值。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果分析

### 数据集与评价指标
论文在三个主流的3D场景问答基准数据集上进行了全面评估：

- **OpenEQA**：一个具有挑战性的开放词汇基准，用于评估基础模型在真实世界环境中的具身问答能力。数据来源于ScanNet和HM3D等超过180个真实环境。
- **ScanQA**：大规模3D问答数据集，包含超过41,000个问答对，侧重于基于对象的问答和空间推理。
- **SQA3D**：强调情境推理，要求智能体理解其在3D场景中的位置和方向，包含33,400个推理问题。

**主要评价指标**：
- **LLM-Match**：OpenEQA的核心指标。使用一个LLM法官将预测答案与真实答案进行比较，给出1-5的分数，最终归一化为百分比。
- **ScanQA & SQA3D指标**：采用一组综合指标，包括：
    - **CIDEr (C)**：衡量与人类标注的共识。
    - **BLEU-4 (B-4)**：捕捉n-gram重叠。
    - **METEOR (M)**：考虑精确率、召回率和同义词匹配。
    - **ROUGE-L (R)**：评估最长公共子序列。
    - **Exact Match at top-1 (EM@1)**：反映生成答案的严格正确性。

### 对比的基线方法
论文与多种前沿方法进行了对比，主要分为三类：

1.  **任务专用模型**：如Scan2Cap、ScanRefer+MCAN、ClipBERT、ScanQA、3D-VisTA。
2.  **3D视觉语言模型**：如3D-LLM、LL3DA、Chat-3D v2、LEO、Scene-LLM、ChatScene。
3.  **零样本2D视觉语言模型**：如VideoChat2、LLaVA-NeXT-Video、LLaVA-Video。这些模型通常将多视角图像作为视频帧输入。

### 关键性能提升与结论

#### 1. 在OpenEQA上的显著提升
CoV框架在**无需额外训练**的情况下，对多个主流VLM带来了显著的性能提升。

- **平均提升**：在Qwen3-VL-Flash、GLM-4.6V、GPT-4o-mini和Gemini-2.5-Flash四个模型上，平均LLM-Match分数提升了 **+11.56%**。
- **最大提升**：在Qwen3-VL-Flash模型上实现了 **+13.62%** 的最大增益（从基线52.65%提升至CoV最佳结果59.82%）。
- **测试时扩展性**：通过强制模型执行更多探索步骤（增加动作预算），性能可以进一步提升。平均额外获得 **+2.51%** 的改进，在Gemini-2.5-Flash上峰值达到 **+3.73%**。这证明了CoV具有**测试时扩展**能力。

#### 2. 在ScanQA和SQA3D上的领先性能
CoV在ScanQA和SQA3D上也取得了极具竞争力的结果，甚至超越了专门为3D场景设计的模型。

- **在ScanQA (val)上**：
    - **CIDEr得分达到116**，显著超过了之前的最佳模型LEO（101.4）。
    - **EM@1达到31.9%**，同样表现优异。
- **在SQA3D (test)上**：
    - **EM@1达到51.1%**，与最先进的3D-VLMs（如Scene-LLM的54.2%）性能相当。

#### 3. 消融实验验证核心组件
论文通过消融实验验证了**粗粒度视图选择**组件的重要性：
- 移除视图选择代理后，在所有测试模型上性能平均下降 **4.59%**。
- 结论：粗粒度视图选择对于过滤冗余帧、聚焦于问题相关锚点视图至关重要，是CoV框架有效性的关键组成部分。

#### 4. 定性分析展示优势
定性结果（如图4及附录）显示，CoV能够：
- 通过多步推理，逐步探索场景，精确定位相关物体。
- 生成与问题语义和场景空间布局一致的、连贯且可解释的推理链。
- 在复杂或杂乱的环境中，有效处理部分遮挡或跨多个视角分布的信息。

### 总结
论文通过系统的实验评估，有力地证明了**Chain-of-View Prompting框架的有效性**。其核心价值在于：
- **模型无关性**：作为一个即插即用的推理框架，可显著提升多种现有VLM在复杂空间推理任务上的性能。
- **训练免费**：无需对基础模型进行微调，降低了应用门槛。
- **主动探索**：通过从被动观察到主动、迭代的视角调整，解决了传统方法因输入视角固定而难以获取充分上下文信息的关键瓶颈。
- **可扩展性**：展示了性能随探索步骤增加而提升的测试时扩展潜力。

这些结果共同表明，**问题对齐的视图选择与开放视角搜索相结合，是提升3D具身问答中空间推理能力的一种有效且通用的策略**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.05172v2)
- [HTML 版本](https://arxiv.org/html/2601.05172v2)
