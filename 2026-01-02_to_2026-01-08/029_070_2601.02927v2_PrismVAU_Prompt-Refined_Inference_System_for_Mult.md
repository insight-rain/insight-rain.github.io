# PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding

**相关性评分**: 7.0/10

**排名**: #29


---


## 基本信息

- **arXiv ID**: [2601.02927v2](https://arxiv.org/abs/2601.02927v2)
- **发布时间**: 2026-01-06T11:11:06Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Iñaki Erregue, Kamal Nasrollahi, Sergio Escalera

## 关键词

Lightweight Architecture, Inference Efficiency, Inference Acceleration, Edge Deployment

## 一句话总结

PrismVAU是一个轻量级、高效的视频异常理解系统，通过优化提示和单模型推理，实现实时应用，无需复杂训练或外部模块。

## 摘要

Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.

## 详细分析

## PrismVAU 论文详细摘要

**1. 研究背景和动机**
视频异常理解（VAU）旨在超越传统的视频异常检测（VAD），不仅要定位异常，还需提供对其上下文、原因和影响的描述与推理。现有VAU方法通常依赖于经过微调的多模态大语言模型（MLLM）或外部模块（如视频描述生成器），这带来了高昂的标注成本、复杂的训练流程和巨大的推理开销。因此，亟需一种轻量、高效且无需微调的实时VAU解决方案。

**2. 核心方法和技术创新**
本文提出了 **PrismVAU**，一个轻量级、无需微调MLLM的实时VAU系统。其核心创新在于：
- **双阶段推理框架**：第一阶段（粗粒度评分）通过计算视频帧与优化后的“正常/异常”文本锚点之间的相似度，生成初步的帧级异常分数曲线。第二阶段（MLLM精炼）将初步分数曲线与视频片段一同输入一个现成的MLLM（VideoLLaMA-3），通过优化的系统提示和用户提示，让MLLM输出异常分数、时间定位和文本解释，从而对初步结果进行上下文感知的精炼。
- **弱监督自动提示工程（APE）**：系统利用MLLM自身作为优化器，在仅需视频级标签的弱监督下，自动优化用于相似度计算的**文本锚点**和用于MLLM推理的**系统/用户提示**，避免了人工设计提示的依赖，并提升了模型对数据分布的适应性。
- **高效与轻量**：整个流程仅使用单个现成MLLM，无需指令微调、帧级标注、外部编码器或密集重叠的视频处理，显著降低了计算开销。

**3. 主要实验结果**
在UCF-Crime和XD-Violence两个标准VAD基准测试上的实验表明：
- **检测性能**：PrismVAU在**无需微调的VAU方法中取得了最佳性能**（UCF-Crime上ROC AUC达87.82%，XD-Violence上AP达78.68%），其性能与许多需要微调的VAU方法及先进的传统VAD方法相当。
- **效率**：在单张NVIDIA RTX 3090 GPU上，推理速度可达**实时处理（241 FPS等效）**，平均每帧延迟仅0.13秒。
- **互补性验证**：消融实验证实，粗粒度评分模块（擅长全局判别）与MLLM精炼模块（擅长时间定位和语义推理）具有互补性，二者结合提升了整体性能。将粗粒度预测注入MLLM提示能进一步改善结果。

**4. 研究意义和价值**
PrismVAU为视频异常理解提供了一种**高效、实用且可解释**的新范式。其价值在于：
- **技术贡献**：证明了通过巧妙的提示工程与现成大模型结合，可以在不进行昂贵微调的情况下，实现高性能、可解释的VAU，为利用基础模型解决复杂视觉任务提供了新思路。
- **实际应用**：其轻量、实时的特性使其非常适合部署在计算资源有限的真实世界场景（如监控、交通分析）中，同时生成的文本解释增强了系统的透明度和可信度。
- **可扩展性**：框架与具体MLLM解耦，易于适配其他模型，且通过重新优化提示可扩展至其他类型的异常或事件检测任务。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：PrismVAU

### **一、 拟解决的核心问题**
论文旨在解决当前**视频异常理解（VAU）** 领域存在的三个主要痛点：
1.  **高成本与复杂性**：现有方法通常依赖**微调的多模态大语言模型（MLLM）** 或外部模块（如视频描述生成器），需要昂贵的细粒度标注和复杂的训练流程。
2.  **高推理开销**：基于密集、重叠视频处理或外部模块的方法，导致推理速度慢，难以满足**实时应用**需求。
3.  **可解释性不足**：传统视频异常检测（VAD）仅输出异常分数，缺乏对异常事件的**语义描述和上下文推理**，降低了在安全关键场景中的可信度。

### **二、 核心创新点**
PrismVAU 提出了一种**轻量级、免微调、可解释**的实时VAU系统，其创新性主要体现在以下三个方面：

#### **1. 双阶段推理架构**
- **第一阶段：粗粒度异常评分**
    - **方法**：利用现成的MLLM（如VideoLLaMA-3）的视觉和文本编码器，计算视频帧与一对**优化后的文本锚点**（`t_norm`, `t_abn`）的相似度，生成帧级异常分数曲线。
    - **价值**：此阶段计算高效，为后续细化提供了初步的、基于低层视觉语义的异常信号。

- **第二阶段：MLLM推理与细化**
    - **方法**：将视频片段连同**优化后的系统/用户提示**以及**第一阶段的粗粒度预测结果**，一起输入同一个MLLM。MLLM直接执行VAU任务，输出：
        1.  是否存在异常
        2.  异常时间区间（起止时间戳）
        3.  片段级异常分数
        4.  **文本解释**（将视觉证据与预测联系起来）
    - **价值**：引入了高层**时序推理和上下文感知能力**，将单纯的检测提升为“理解”，并生成可读的解释。

#### **2. 弱监督自动提示工程框架**
- **方法**：提出一个统一的**自动提示工程（APE）** 框架，用于优化两个关键组件：
    - **文本锚点**：代表“正常”和“异常”的文本描述。
    - **MLLM提示**：指导MLLM进行推理的系统提示和用户提示。
- **关键技术**：**让MLLM自身作为优化器**。在优化循环中，MLLM根据包含历史高性能解决方案的元提示，生成新的提示候选，并使用仅有的**视频级标签**进行评估和筛选。
- **价值**：
    - **免微调**：无需更新MLLM的权重，极大降低了训练成本和数据需求。
    - **数据驱动**：学习到的提示和锚点从数据分布中隐式归纳概念，比人工定义更贴合实际、更具泛化性。
    - **可迁移**：产生的提示是自然语言，可读、可修改、可跨模型迁移。

#### **3. 高效且互补的系统设计**
- **模型无关性**：整个流水线围绕一个**现成的MLLM**构建，无需引入外部视觉编码器、视频描述器或额外的LLM。
- **信息互补与融合**：
    - **粗分数注入**：将第一阶段的统计信息（如最大分数、异常区间）以JSON格式注入第二阶段的提示中，实验证明能显著提升MLLM的推理性能（ROC AUC从70.78%提升至76.97%）。
    - **信号后处理**：对粗分数曲线和MLLM生成的阶跃式预测分别进行**高斯滤波平滑**，然后平均融合，再通过傅里叶上采样恢复原始帧率，最终得到**时序平滑、语义丰富**的精细化异常曲线。
- **实时性能**：在单张NVIDIA RTX 3090 GPU上，平均每帧处理延迟为0.13秒，有效吞吐量达241 FPS（在1 FPS采样下），满足实时性要求。

### **三、 解决方案总结**
PrismVAU通过一个**精巧的双阶段流水线**，结合**创新的弱监督APE框架**，成功地将一个现成的MLLM转化为一个高效的VAU系统。它**摒弃了昂贵的微调和复杂的外部模块**，利用提示工程挖掘MLLM的零样本能力，同时通过**粗-细粒度信息的互补与融合**，在保持高检测性能（在UCF-Crime上ROC AUC达87.82%）的同时，提供了宝贵的**可解释性**（文本描述）和**实时推理能力**。

**简而言之，其核心贡献在于：用“提示优化”代替“模型微调”，用“单模型多阶段推理”代替“多模型流水线”，在性能、效率与可解释性之间取得了卓越的平衡。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决**视频异常理解（VAU）**任务中现有方法依赖微调多模态大语言模型（MLLM）或外部模块，导致训练成本高、推理开销大、难以实时应用的核心问题。

为此，论文提出了 **PrismVAU** 系统，其核心是一个**轻量级、免微调的双阶段框架**：第一阶段，通过优化的文本锚点（描述正常/异常场景的文本）与视频帧的相似性计算粗粒度异常分数曲线；第二阶段，将视频片段和粗分数曲线作为提示输入给一个现成的MLLM（如VideoLLaMA-3），使其进行上下文推理，生成细化的异常分数、时间定位和文本解释。系统的关键创新在于使用**弱监督的自动提示工程（APE）框架**，让同一个MLLM自动优化文本锚点和推理提示，仅需视频级标签，无需帧级标注或模型微调。

实验表明，PrismVAU在UCF-Crime和XD-Violence等标准基准测试上取得了**与微调方法相竞争的异常检测性能**，同时能提供可解释的异常描述。其推理效率高，可实现实时处理，为实际应用提供了一个高效、实用且可解释的解决方案。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## PrismVAU 论文创新点分析

这篇论文提出了一种名为 PrismVAU 的新型视频异常理解系统。相对于已有工作，其明确的创新点主要体现在以下几个方面：

### 1. **无需微调MLLM的轻量级、一体化VAU系统**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的VAU方法主要分为两类：(1) **微调MLLM的方法**：需要对MLLM进行指令微调，这需要大量细粒度的多模态标注（如异常类型、时间戳、自然语言解释），训练流程复杂且成本高昂。(2) **非微调MLLM的方法**：通常依赖外部模块（如视频描述生成器、独立的LLM、视觉编码器），并需要对视频进行密集、重叠的分段处理，导致推理开销巨大。
     - **PrismVAU**：**完全避免了对MLLM的权重微调**，也**无需任何外部辅助模块**（如视频描述器）。它仅使用一个现成的MLLM（VideoLLaMA-3）作为核心，通过精心设计的提示词来引导模型完成所有任务。
   - **解决的具体问题/带来的优势**：
     - **显著降低部署成本与门槛**：消除了收集昂贵细粒度标注和进行复杂模型训练的需求，使系统更易于在实际场景中应用。
     - **实现高效实时推理**：由于避免了外部模块和密集处理，系统实现了低延迟（平均每帧0.13秒）和高吞吐量（有效241 FPS），适合实时监控等应用。
     - **保持模型通用性与可交互性**：未对MLLM进行微调，意味着模型保留了其原始的广泛知识基础和对话能力，支持用户通过聊天界面进行错误审计、细节追问和假设性推理，增强了系统的透明度和可信度。

### 2. **双阶段互补推理框架：粗粒度评分 + MLLM精炼**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：许多方法采用单一策略，要么仅基于视觉-文本相似性进行检测（缺乏上下文推理），要么完全依赖MLLM进行密集推理（计算成本高且可能忽略底层视觉线索）。
     - **PrismVAU**：创新性地设计了一个**两阶段互补管道**：
         1. **粗粒度异常评分模块**：基于优化的文本锚点，计算帧级视觉特征与“正常/异常”文本描述之间的相似度，得到一个初步的、稀疏的异常曲线。这提供了快速、低层次的异常可能性估计。
         2. **MLLM推理与精炼模块**：将视频片段连同粗粒度评分结果作为提示输入给MLLM，要求其输出**是否存在异常、时间定位、片段级分数和文本解释**。MLLM利用其强大的多模态理解和推理能力，对粗粒度结果进行上下文感知的精炼。
   - **解决的具体问题/带来的优势**：
     - **兼顾效率与精度**：粗粒度模块快速筛选，MLLM模块重点推理，避免了全程使用MLLM处理每一帧的巨大开销。
     - **实现优势互补**：论文通过实验证明，两个模块具有互补性。文本锚点方法在**全局区分性**（ROC AUC更高）上表现更好，而MLLM在**时间定位精度**和**语义解释**上更优。两者的结合（通过提示注入和信号后处理融合）产生了优于任一单独组件的最终性能。
     - **提供可解释性输出**：系统不仅输出异常分数曲线，还提供了描述异常原因和上下文的自然语言解释，满足了VAU的核心要求。

### 3. **采用弱监督自动提示工程框架优化关键组件**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：提示词通常依赖专家经验手动设计（静态、可能次优），或采用需要梯度更新的软提示调优（不易理解和迁移）。
     - **PrismVAU**：提出使用一种**弱监督的自动提示工程框架**来优化系统中的两个关键文本输入：
         1. **文本锚点**：代表“正常”和“异常”的文本描述。
         2. **MLLM的系统提示和用户提示**：指导MLLM执行VAU任务的指令。
     - **优化机制**：该框架将MLLM本身作为优化器，遵循类似进化策略的流程。优化目标（以自然语言描述）和历史上表现最好的提示方案被构成元提示，输入给MLLM以生成新的候选提示。这些提示在训练集（仅需视频级标签）上进行评估，性能最佳的方案被保留并用于下一轮迭代。
   - **解决的具体问题/带来的优势**：
     - **数据驱动，性能更优**：APEF使提示词能够从数据分布中隐式学习，得到比人工设计更贴合任务、判别力更强的描述（例如，优化后的文本锚点从简短的事件枚举演变为强调环境线索和人类行为的详细叙述）。这直接带来了检测性能的显著提升（文本锚点基线ROC AUC从41.63%提升至84.53%）。
     - **弱监督，降低成本**：仅需视频级标签（异常/正常），无需帧级标注或解释文本标注，大大降低了数据标注成本。
     - **提示可读、可迁移**：生成的是自然语言提示，易于人类理解和修改，并且表现出一定的跨数据集泛化能力（MLLM提示在UCF-Crime上优化后，可直接用于XD-Violence数据集）。

### 4. **将粗粒度预测作为先验知识注入MLLM提示**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：在利用MLLM进行推理时，通常直接将视频和任务指令输入模型，未显式利用底层视觉相似性计算的中间结果。
     - **PrismVAU**：在MLLM精炼阶段，**将粗粒度模块计算出的异常分数及相关统计信息（如视频标签、片段分数、最异常区域的时间戳、帧级统计量）以JSON格式作为文本注释，插入到用户提示中**，与视频片段一同输入给MLLM。
   - **解决的具体问题/带来的优势**：
     - **提供补充性引导**：论文通过消融实验证实，逐步注入更详细的粗粒度信息能持续提升MLLM的ROC AUC性能。这表明显式的相似性先验为MLLM的高层推理提供了有价值的、互补的低层视觉线索。
     - **对齐与精炼**：这种设计使MLLM的推理过程能够“感知”到粗粒度模块的初步判断，有助于其生成与底层视觉证据更一致的解释和更精确的时间定位，从而实现两个阶段预测的有机融合。

### 总结
PrismVAU的核心创新在于**以极高的效率实现了高性能、可解释的视频异常理解**。它通过**不微调MLLM+双阶段设计+自动提示优化**的组合拳，巧妙地解决了现有VAU方法在**标注成本、计算开销和模型复杂度**方面的痛点，同时提供了**实时处理能力**和**丰富的可交互解释**，为实际部署提供了一个非常具有吸引力的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 使用的数据集与评价指标
1.  **主要数据集**：
    - **UCF-Crime**：包含1,900个未经修剪的监控视频（128小时），涵盖13类异常行为（如抢劫、斗殴、破坏公物等）和正常事件。是论文的主要评估基准。
    - **XD-Violence**：包含近5,000个视频（217小时），涵盖6类暴力行为。用于验证方法的泛化性。

2.  **核心评价指标**：
    - **帧级ROC AUC**：衡量模型区分异常帧与正常帧的能力，是UCF-Crime上的主要指标。
    - **帧级平均精度（AP）**：衡量模型识别异常帧的能力，是XD-Violence上的主要指标。
    - **帧级F1分数（阈值0.5）**：用于直接评估预测的异常片段在覆盖真实异常帧时的精确度与召回率平衡。论文指出，ROC AUC和AP更适合平滑的异常曲线，而F1分数能更好地评估MLLM输出的“阶梯状”预测的精确时间定位能力。

### 二、 对比的基线方法
论文将PrismVAU与三类方法进行了全面对比：

1.  **不可解释的传统VAD方法**：包括基于弱监督和CLIP的方法。
    - 代表方法：`Sultani et al.`, `RTFM`, `MSL`, `Zhang et al.`, `UMIL`, `OVVAD`, `PEL4VAD`, `Chen et al.`, `TPWNG`, `VadCLIP`。

2.  **基于微调MLLM的VAU方法**：这些方法通过指令微调更新模型权重，通常性能最佳但成本高昂。
    - 代表方法：`VAD-LLaMA`, `Holmes-VAU`, `Holmes-VAD`。

3.  **非微调MLLM的VAU方法**：不更新MLLM权重，依赖外部模块或密集处理，在效率与性能间折中。
    - 代表方法：`LAVAD`, `VERA`。

### 三、 关键性能结果与结论
根据论文表1和正文分析，PrismVAU取得了以下效果：

1.  **整体性能领先于同类非微调方法**：
    - **在UCF-Crime上**：PrismVAU的ROC AUC达到**87.82%**，F1分数为**41.52%**。其ROC AUC在**所有非微调VAU方法中最高**，甚至超过了除`VadCLIP`（88.02%）外的所有传统VAD方法，并与部分微调VAU方法（如`VAD-LLaMA`的88.13%）性能相当。
    - **在XD-Violence上**：PrismVAU的AP达到**78.68%**，F1分数为**70.23%**。同样在非微调VAU方法中表现最佳。

2.  **高效性与实用性**：
    - **低误报率**：在两个数据集上，使用0.5阈值时，帧级误报率均**低于9%**，表明模型预测可靠、精确度高，适合实际部署。
    - **实时推理**：在NVIDIA RTX 3090 GPU上，平均每帧处理延迟为0.13秒，有效吞吐量达241 FPS（在1 FPS采样下），满足实时性要求。
    - **训练成本低**：仅需视频级弱监督标签，通过APE框架优化提示词和文本锚点，无需帧级标注或模型微调，所有优化实验在单卡12小时内完成。

3.  **模块贡献验证（消融实验）**：
    - **文本锚点（TA）优化效果显著**：经过APE优化后，TA模块的ROC AUC从**41.63%** 大幅提升至**84.53%**，F1从13.11%提升至35.07%，证明了数据驱动提示优化的有效性。
    - **MLLM提示优化效果显著**：经过APE优化后，MLLM模块的ROC AUC从**58.72%** 提升至**70.78%**，F1从27.19%提升至39.45%。
    - **双阶段互补性得到验证**：
        - TA模块擅长生成**平滑的异常曲线**，在全局区分性（ROC AUC）上表现更好。
        - MLLM模块擅长生成**阶梯状预测**，在时间定位精度（F1分数）上表现更好。
        - 两者预测误差相关性较低（`r=0.28`），说明它们在不同实例上犯错，结合能提升鲁棒性。
    - **粗粒度预测注入提升MLLM性能**：向MLLM输入中逐步注入粗粒度预测信息（视频标签、分数、时间区域、帧统计），能持续提升其ROC AUC，最高提升约6个百分点（从70.78%至76.97%），证明了显式相似性先验的有效性。

4.  **定性结果与解释性**：
    - PrismVAU能生成**贴合语义的文本解释**。与HIVAU-70k数据集的真实注释相比，使用MPNet嵌入计算的语义相似度平均为0.56，表明其能捕捉异常事件的核心语义。
    - 提供了**交互式聊天界面**，用户可与模型对话以审核错误、详细解释预测或探讨假设场景，增强了系统的透明度和可信度。

### 四、 核心结论
PrismVAU在**不进行MLLM微调、不使用外部模块（如视频描述生成器）、不依赖密集重叠视频处理**的前提下，通过创新的双阶段（粗评分+精炼推理）和自动提示工程（APE）框架，在标准VAD基准上实现了：
1.  **与非微调VAU方法相比的领先性能**。
2.  **与许多先进传统VAD方法和部分微调VAU方法相竞争的性能**。
3.  **高推理效率、低误报率**，满足实时应用需求。
4.  **可解释的输出**，包括时间定位和文本描述。

这证明了其作为一种**轻量、高效、实用且可解释**的视频异常理解解决方案的价值。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.02927v2)
- [HTML 版本](https://arxiv.org/html/2601.02927v2)
