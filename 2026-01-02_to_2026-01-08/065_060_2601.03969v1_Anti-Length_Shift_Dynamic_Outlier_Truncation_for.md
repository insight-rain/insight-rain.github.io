# Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models

**相关性评分**: 6.0/10

**排名**: #65


---


## 基本信息

- **arXiv ID**: [2601.03969v1](https://arxiv.org/abs/2601.03969v1)
- **发布时间**: 2026-01-07T14:31:07Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Wei Wu, Liyi Chen, Congxi Xiao, Tianfu Wang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Hui Xiong

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration

## 一句话总结

该论文提出动态异常截断方法，通过训练时干预减少推理模型的不必要输出，显著提升推理效率，但与机器人视觉语言动作模型和边缘部署的直接关联较弱。

## 摘要

Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

## 详细分析

## 论文摘要：《Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models》

**1. 研究背景和动机**
随着强化学习与可验证奖励（RLVR）的结合，大型推理模型通过延长思维链（CoT）显著提升了性能，但也带来了高昂的部署成本：模型在处理简单查询时常常产生不必要的冗长推理（即“过度思考”）。现有方法主要通过显式的长度惩罚来提升效率，但这会引入优化冲突，且未能深入探究导致冗长的生成机制。本文首先识别并分析了“长度偏移”现象：在训练过程中，模型在已经能够正确解决的简单输入上，会倾向于生成越来越长的推理过程。这种冗余源于模型在处理复杂问题时习得的“推理词”（如“验证”、“然而”等）在简单问题上被过度触发。

**2. 核心方法和技术创新**
为解决此问题，本文提出了**动态异常截断**方法。DOT是一种训练时干预技术，其核心创新在于：
- **选择性干预**：仅对在训练中采样出的、**全部正确**的推理轨迹组进行操作。
- **动态阈值**：基于组内长度统计（均值 + α * 标准差）识别并截断长度上的统计异常值，而非设定固定长度上限。
- **保持探索能力**：由于阈值是后验确定的，且只应用于已解决的查询，因此不会限制模型对复杂问题的长程推理和探索能力。
此外，为稳定训练，论文还引入了**KL-Cov正则化**以防止策略熵过早崩溃，以及**预测性动态采样**策略来适应训练过程中有效数据分布的变化。

**3. 主要实验结果**
在多个模型规模（1.5B, 7B, 32B）和数学（AIME-24/25, AMC, MATH-500）、代码（HumanEval, LiveCodeBench）基准测试上，DOT显著推进了效率-性能的帕累托前沿：
- **同时提升精度与效率**：在1.5B模型上，DOT将AIME-24准确率从30.0%提升至52.2%，同时将平均响应长度减少66%（从15498降至5151个token）。
- **超越现有方法**：相比SOTA高效推理方法（如SIRI），DOT能以更少的token消耗达到相当或更高的准确率。
- **强泛化性**：方法在不同规模模型和不同领域（数学→代码）任务上均表现出一致的有效性。

**4. 研究意义和价值**
本工作的价值在于：
- **机制性洞察**：首次从生成机制角度深入分析了推理模型冗长的成因（长度偏移），为理解模型行为提供了新视角。
- **高效训练范式**：提出了一种简单、可插拔的训练时干预方案，无需在奖励函数中引入复杂的长度惩罚，即可有效抑制冗余，实现**自适应推理**（为难题保留长链，为易题大幅压缩）。
- **实际应用潜力**：DOT能大幅降低推理成本（token消耗），同时保持甚至提升模型性能，为部署高效、实用的推理模型提供了有力的技术路径。论文也指出了其应用于智能体任务等未来方向的潜力。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究问题**
论文旨在解决**大型推理模型（如Chain-of-Thought模型）在简单查询上过度冗长（overthinking）**的问题。现有方法（如显式长度惩罚）常导致优化冲突，且未深入探究导致“过度思考”的生成机制。这造成了高昂的部署成本，因为模型在简单问题上浪费了大量计算资源。

### **核心发现与现象：长度偏移**
论文首先通过实证分析发现了一个关键现象：**长度偏移**。
- **定义**：在训练过程中，模型在**已经能正确解决**的简单查询上，生成的回复长度会持续增加。
- **机制**：这种冗长源于**跨难度策略干扰**。模型在处理复杂问题时习得的“推理词”（如“验证”、“然而”、“等待”）等探索性行为，被一个跨所有难度共享的策略**过度触发**在简单查询上，从而引发了不必要的额外推理步骤，导致长度膨胀。
- **关键证据**：即使在所有回复都正确、梯度信号为零的查询组中，模型生成长度依然持续增长，表明这是一种分布性偏移，而非优化驱动。

### **核心创新：动态异常截断**
为解决此问题，论文提出了 **动态异常截断** 这一训练时干预方法。

#### **1. 核心机制**
- **目标**：选择性抑制冗余，同时保留解决复杂问题所需的长期推理能力。
- **操作**：
    - **条件触发**：仅对**全正确**的回复组（即组内所有采样回复均获满分奖励）应用DOT。
    - **动态阈值**：基于组内长度统计量计算截断阈值 `T = μ + α·σ`（如α=3，即“三西格玛”原则），仅截断显著超出组内典型长度的极端异常值。
    - **最小截断边际**：仅当截断能减少至少 `m` 个token（如32）时才执行，避免微小改动引入噪声。
- **优势**：
    - **非奖励塑造**：不将长度直接纳入奖励函数，避免了优化冲突。
    - **针对性**：仅修剪简单问题上的冗余长尾，不限制困难问题的探索。
    - **抗奖励黑客**：阈值基于后验统计动态计算，难以被模型针对性“欺骗”。

#### **2. 配套稳定化技术**
分析发现，即使DOT干预比例很小（~0.5%），也可能引发策略熵的快速下降，导致探索能力丧失。为此，论文引入了两项辅助技术：
- **KL-Cov正则化**：识别对数概率与优势估计协方差高的token，并仅对这些token施加KL散度惩罚，防止策略过早陷入确定性，稳定探索过程。
- **预测性动态采样**：随着模型能力提升，全正确查询组占比增加。为避免训练后期被这些零梯度组主导，该方法根据历史有效组比例动态预测并调整过采样率，确保训练批次稳定高效，避免了固定过采样的计算浪费。

### **实验效果与价值**
- **显著推高帕累托前沿**：在多个模型规模（1.5B, 7B, 32B）和数学、代码基准上，DOT在**大幅降低生成长度（减少60%-78%）的同时，显著提升了准确率**。
- **超越现有方法**：相比SOTA高效推理方法（如SIRI, DLER），DOT能以更少的token消耗达到相同或更高的精度。
- **通用性与可扩展性**：方法在数学推理和代码生成任务上均表现优异，表明其学习到了一种通用的、根据问题难度自适应分配计算资源的模式。

### **总结**
本文的核心贡献在于：
1.  **机制性洞察**：首次系统分析了推理模型冗长性的内在成因——**长度偏移**现象及其背后的跨难度策略干扰机制。
2.  **创新性方法**：提出了**动态异常截断**这一简单而有效的训练时干预方法，通过**后验统计、条件触发、动态阈值**精准修剪冗余，避免了传统长度惩罚的弊端。
3.  **完整的训练方案**：结合KL-Cov正则化和预测性动态采样，形成了一个稳定、可扩展的训练配方，实现了**效率与性能的双重提升**。

**实际价值**：DOT为训练**高效推理模型**提供了一个强大且通用的范式，能显著降低大模型推理成本，推动“每token智能”的提升，对模型的实际部署具有重要经济意义。其原理也可扩展至智能体任务中冗余动作的修剪。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决大型推理模型在强化学习训练中出现的“长度偏移”问题，即模型在简单问题上产生不必要的冗长推理，导致部署效率低下。为此，论文提出了**动态异常截断**方法，该方法仅在模型已能完全正确解答的问题组中，根据组内长度统计分布，截断超出动态阈值的冗长推理轨迹，从而在不损害复杂问题探索能力的前提下抑制冗余。实验表明，该方法能显著推动模型性能与效率的帕累托前沿，在多个数学和代码生成基准上，以大幅减少的推理长度（例如在AIME-24上减少78%的令牌使用）同时提升或保持准确率。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models》针对大型推理模型在训练中产生的“过度思考”（overthinking）问题，提出了一套创新的训练时干预方法。其核心创新点可归纳如下：

### 1. **提出并实证分析了“长度偏移”（Length Shift）现象**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：大多数高效推理方法（如O1-Pruner, Laser, SIRI）将“过度思考”视为一个需要惩罚的结果，直接在奖励函数中引入与响应长度相关的惩罚项（如长度惩罚、预算约束）。它们**没有深入探究导致模型变得冗长的内在生成机制**。
     - **本文创新**：通过实证分析，首次明确识别并命名了“长度偏移”现象。即，在训练过程中，模型在**已经能够正确解决的简单查询**上，会倾向于生成越来越长的推理链。作者发现，这种增长伴随着“推理词”（如“however”、“wait”、“verify”）使用频率的上升，这些词在复杂问题上有利于探索，但在简单问题上被过度触发，导致了不必要的冗长。
   - **解决的具体问题/带来的优势**：
     - **问题**：揭示了冗长性的根源并非简单的“模型喜欢写长文本”，而是**跨难度策略干扰**——在复杂问题上学到的有益探索行为，被一个共享的策略过度应用到了简单问题上。
     - **优势**：为干预措施提供了精准的靶点。它表明，理想的解决方案不应是全局性地惩罚长度，而应是**非对称地、选择性地抑制在已解决问题上的冗余**，同时保留解决难题所需的探索能力。这为后续方法设计奠定了理论基础。

### 2. **提出动态异常截断（Dynamic Outlier Truncation, DOT）核心机制**
   - **相比以往方法的改进/不同之处**：
     - **机制不同**：与在奖励函数中“硬编码”长度约束不同，DOT是一种**训练时的、后验的干预**。它仅在策略采样完一个批次的输出（rollouts）后，对其中**全部正确**的组进行操作。
     - **判断标准不同**：DOT不设定固定的长度阈值。它基于组内响应长度的统计分布（均值μ和标准差σ），动态计算截断阈值 `T = μ + α·σ`（如α=3，即“三西格玛”原则）。只截断那些长度显著超出组内典型水平的“统计异常值”。
     - **目标不同**：DOT**不直接优化长度**，而是通过截断极端冗长的正确响应并重新计算奖励，间接地创建了一个“冗余会降低奖励”的信号。它让模型自己学习到，在已经能做对的问题上，写得太长是“不划算”的。
   - **解决的具体问题/带来的优势**：
     - **问题**：解决了显式长度惩罚带来的**优化冲突**（长度缩减梯度与精度最大化梯度相冲突）和**奖励黑客**（模型可能学会以牺牲正确性为代价来满足长度要求）问题。
     - **优势**：
       1. **保持探索能力**：由于只对已全正确的组进行干预，且阈值是动态的，DOT**不会限制模型在困难或未解决问题上的生成长度**，保留了其进行长视野推理和探索的能力。
       2. **最小化干扰**：实验表明，DOT仅影响约0.5%的生成响应，却能在全局上显著降低平均长度，实现了“四两拨千斤”的效果。
       3. **提升帕累托前沿**：在多个模型规模（1.5B, 7B, 32B）和基准测试上，DOT能**同时提高准确率和大幅降低推理token消耗**，将效率-性能的帕累托边界向外大幅推进。

### 3. **引入配套的稳定化与扩展技术**
   - **相比以往方法的改进/不同之处**：
     - **KL-Cov正则化**：尽管DOT干预很少，但仍可能导致策略熵过早崩溃，使推理词的生成变得过于确定化，损害探索。本文没有使用通用的KL散度惩罚，而是采用了**KL-Cov**方法。它只对那些对数概率与优势估计具有高协方差的token（即那些被策略激进更新的token）施加KL惩罚，从而**精准地稳定训练，防止探索能力退化**。
     - **预测性动态采样**：随着训练进行，越来越多的查询会变成“全正确”组，DOT干预的样本比例虽小但影响增大。传统的动态采样方法开销大。本文提出了一种**预测性**方法，基于历史有效组比例来预测所需的过采样率，实现**高效的单轮采样**，稳定了有效批次大小，避免了计算浪费。
   - **解决的具体问题/带来的优势**：
     - **问题**：解决了应用DOT后可能引发的**训练不稳定**（熵崩溃）和**采样效率低下**问题。
     - **优势**：确保了DOT方法在长周期、大规模模型训练中的**鲁棒性和可扩展性**，使其成为一个完整、可用的训练方案，而不仅仅是一个理论想法。

### 总结
本文的核心创新在于**从现象分析到机制干预的系统性贡献**：
1.  **诊断创新**：发现了“长度偏移”这一关键训练动态问题。
2.  **治疗创新**：提出了DOT这一精准、动态的“外科手术式”干预，避免了对模型推理能力的“一刀切”式限制。
3.  **工程创新**：配套了KL-Cov和预测性动态采样，确保了治疗过程的稳定和高效。

相比以往主要依赖**奖励工程**（Reward Engineering）的方法，本文提出了一种**基于机制和训练动态**（Mechanism & Training Dynamics）的新范式。它不直接告诉模型“写短点”，而是通过精心设计的训练过程，让模型自己学会“在简单问题上不必想太多”，从而在根本上实现了**自适应推理**——为复杂问题分配更多计算，为简单问题 aggressively 修剪冗余。这带来了**更优的帕累托效率、更好的可扩展性以及更鲁棒的训练过程**。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心实验效果
论文提出的 **Dynamic Outlier Truncation (DOT)** 方法，在多个模型规模和基准测试上，**同时显著提升了推理模型的准确性和生成效率**，成功地将性能-效率的帕累托边界向外大幅推进。

### 二、 使用的数据集
论文在以下四个具有挑战性的数学推理基准上进行了主要评估：
1.  **AIME-24**： 美国数学邀请赛2024年试题。
2.  **AIME-25**： 美国数学邀请赛2025年试题。
3.  **AMC**： 美国数学竞赛（AMC-22和AMC-23）试题。
4.  **MATH-500**： 来自MATH数据集的500个问题子集。

**泛化能力测试**使用了代码生成数据集：
- **HumanEval**： 代码生成评估基准。
- **LiveCodeBench (v6)**： 代码生成评估基准。

### 三、 评价指标
1.  **主要指标**：
    - **准确率 (Pass@1 Accuracy, %)**: 从模型生成的多个输出中提取最终答案，计算正确率。
    - **平均响应长度 (Average Response Length, tokens)**: 模型生成推理链的平均token数量，衡量推理效率。

2.  **辅助分析指标**：
    - 训练过程中的策略熵（Policy Entropy）。
    - 被截断的响应比例。
    - 有效批大小稳定性。

### 四、 对比的基线方法
论文与一系列先进的、旨在提升推理效率的方法进行了全面对比，包括：
- **DeepScaleR-Preview**: 通过延长上下文进行强化学习的强基线。
- **OverThink**: 通过数据过滤去除冗余步骤的方法。
- **DAST**: 难度自适应的奖励塑形方法。
- **O1-Pruner**: 引入长度协调奖励的离策略优化方法。
- **LC-R1**: 使用双重奖励进行结构压缩的方法。
- **Laser-DE**: 动态长度奖励塑形方法。
- **AdaptThink**: 训练模型在“思考”与“不思考”模式间切换的方法。
- **DLER-R1**: 优化截断惩罚以防止奖励崩溃的方法。
- **SIRI**: 交替进行压缩和扩展阶段的训练调度方法（对比了其低配和高配版本）。

### 五、 关键性能提升与结论
#### 1. 在数学推理任务上的显著提升（以1.5B模型在AIME-24为例）
| 方法 | 准确率 (Acc) | 平均长度 (Tokens) | 对比分析 |
| :--- | :--- | :--- | :--- |
| **原始模型 (Original)** | 30.0% | 15,498 | 基线，冗长且准确率一般 |
| **SIRI-high (SOTA方法)** | 43.6% | 10,049 | 性能好，但仍较冗长 |
| **DLER-R1 (同类简洁方法)** | 35.8% | 3,354 | 长度压缩极好，但准确率有损失 |
| **DOT-8K (本文)** | **52.2%** | **5,151** | **准确率大幅提升，同时长度减少66.8%** |
| **DOT-4K (本文)** | 43.1% | 3,342 | 在极致的长度压缩下，准确率仍显著超越原始模型和DLER-R1 |

**核心结论**：DOT方法在**不牺牲甚至提升准确率**的前提下，实现了**大幅的推理长度压缩**。例如，DOT-8K在1.5B模型上将AIME-24准确率从30.0%提升至52.2%，同时将平均响应token数从15498降至5151。

#### 2. 多尺度模型的一致性优势
- **7B模型**：DOT-8K在AIME-24上达到62.6%的准确率（超越原始模型7.5%），同时仅使用原始模型约37%的token。
- **32B模型**：DOT-8K在保持或提升准确率的同时，将token消耗减少约60%，表明DOT使大模型在“每token智能”上展现出独特的缩放趋势。

#### 3. 在代码生成任务上的泛化能力
在HumanEval和LiveCodeBench上，DOT方法在保持或减少生成长度的同时，准确率均优于或与所有基线方法持平，证明了其习得的简洁推理模式具有跨领域泛化能力。

#### 4. 帕累托边界的外推
论文中的图表（如Fig.1）清晰显示，DOT方法的数据点（高准确率、低长度）位于所有基线方法的右下方，**显著地将整个性能-效率的帕累托边界向外推进**。这意味着DOT找到了一个更优的平衡点，能够在给定计算预算下获得更高性能，或在追求相同性能时消耗更少资源。

#### 5. 消融实验验证
消融研究（表3）证实了DOT各个组件的必要性：
- **移除动态异常截断 (DOT)**： 模型长度显著增加，证明DOT是抑制冗余的核心。
- **移除组条件截断**： 性能大幅下降，证明仅对“全正确”组进行干预对保护复杂问题推理能力至关重要。
- **移除KL-Cov正则化或预测性动态采样**： 准确率和效率均受损，证明了这些组件对维持训练稳定性和探索性的重要性。

### 总结
论文通过严谨的实验设计，在多个标准数据集和模型规模上验证了DOT方法的有效性。其核心贡献在于，**首次通过分析“长度偏移”现象的内在生成机制，提出了一种针对性的训练时干预方法**，从而实现了**准确率与推理效率的同时优化**，显著超越了现有基于显式长度惩罚或阈值的方法，为训练高效推理模型提供了一个简单、鲁棒且可扩展的新范式。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.03969v1)
- [HTML 版本](https://arxiv.org/html/2601.03969v1)
