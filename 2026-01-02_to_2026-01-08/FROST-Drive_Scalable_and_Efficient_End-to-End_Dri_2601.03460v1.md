# FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder

**相关性评分**: 8.0/10

**排名**: #12


---


## 基本信息

- **arXiv ID**: [2601.03460v1](https://arxiv.org/abs/2601.03460v1)
- **发布时间**: 2026-01-06T23:13:35Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Zeyu Dong, Yimin Zhu, Yu Wu, Yu Sun

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

FROST-Drive提出一种基于冻结视觉编码器的端到端自动驾驶模型，通过保留预训练视觉语言模型的泛化能力，提升推理效率和边缘部署潜力。

## 摘要

End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder's weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a transformer-based adapter for multimodal fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.

## 详细分析

## 论文摘要：FROST-Drive: 基于冻结视觉编码器的可扩展高效端到端驾驶

### 1. 研究背景和动机
端到端（E2E）自动驾驶模型旨在将传感器输入直接映射为控制指令，但其在复杂、长尾场景下的泛化能力仍是关键挑战。当前主流做法是在驾驶数据集上对视觉编码器进行**全参数微调**，这可能导致模型过度适应训练数据，损害其从大规模预训练中获得的**通用世界知识**，从而限制泛化性能。本文挑战了这一训练范式的必要性，提出了一种保留并利用预训练视觉-语言模型（VLM）强大泛化能力的新方法。

### 2. 核心方法和技术创新
本文提出了 **FROST-Drive** 架构，其核心创新在于**冻结**从大型VLM（如InternVL3）中提取的视觉编码器的权重，直接将其丰富的通用知识迁移到驾驶任务中。具体架构包括：
- **冻结的视觉编码器**：直接提取高维视觉特征，避免微调带来的知识遗忘和计算成本。
- **Transformer适配器**：采用可学习的查询令牌，通过交叉注意力高效融合多摄像头视图信息。
- **多模态融合与规划器**：使用Transformer编码器融合视觉特征、驾驶意图和车辆历史状态。
- **GRU解码器**：以自回归方式生成平滑的未来路径点轨迹。
- **定制化损失函数**：设计了一种直接优化**Rater反馈分数（RFS）** 的损失函数，该指标更注重轨迹的安全性和鲁棒性，而非简单的点对点精度。

### 3. 主要实验结果
在Waymo Open E2E数据集（包含大量长尾场景）上的实验表明：
- **性能优势**：使用冻结VLM编码器的模型（如78B参数VLM）在**RFS**上显著优于全微调的ViT基线以及仅使用ImageNet预训练的冻结ViT。
- **规模效应**：模型性能随**VLM规模增大**和**视觉特征嵌入维度提高**而持续提升，验证了丰富表征的重要性。
- **微调反作用**：对VLM编码器进行全微调反而会降低性能，证实了冻结策略在保留通用知识方面的优势。
- **挑战赛结果**：该方法在Waymo端到端驾驶挑战赛中取得了**前三名**的成绩，尤其在“聚光灯”边缘案例场景中表现出色。

### 4. 研究意义和价值
本研究挑战了端到端自动驾驶必须微调视觉主干的固有范式，证明了**利用并冻结强大基础模型的通用知识是更有效的策略**。FROST-Drive提供了一条**高效、可扩展**的新路径：它大幅降低了训练计算成本，避免了过拟合风险，并通过继承VLM的深层场景理解能力，显著提升了模型在复杂、罕见场景下的**鲁棒性和泛化能力**。这项工作为开发能够更好应对现实世界复杂性的视觉驱动模型指明了方向。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：FROST-Drive

### **一、 拟解决的核心问题**
论文旨在解决**端到端（E2E）自动驾驶模型**在**泛化性**和**训练效率**上的根本矛盾：
1.  **泛化性困境**：传统范式会对整个模型（包括视觉编码器）进行端到端的微调。这虽然能提升在训练数据上的性能，但可能导致模型过度“特化”于训练集，**损害了其从大规模预训练中获得的、对复杂世界（尤其是长尾场景）的通用理解能力**，从而影响在真实、罕见场景下的鲁棒性。
2.  **效率与性能的权衡**：完全微调大型视觉编码器计算成本高昂，且需要大量标注数据。同时，人们普遍认为，为了利用更强大的、能产生高维特征表示的模型，必须进行重新训练。

**核心挑战**：如何在保持预训练视觉模型强大泛化能力的同时，使其高效地适应自动驾驶这一特定、复杂的任务。

### **二、 核心创新点**
论文提出了 **FROST-Drive** 架构，其核心创新在于 **“冻结视觉编码器”范式**，并围绕此构建了一套完整的技术方案。

1.  **范式创新：冻结预训练视觉语言模型（VLM）的视觉编码器**
    - **核心思想**：直接“移植”并冻结一个来自强大VLM（如InternVL3）的视觉编码器，**不再对其进行任何微调**。
    - **理论依据**：VLM的视觉编码器在极大规模的多模态数据上协同训练，已经内化了丰富的**世界知识和上下文理解能力**，并且能够生成**高维度的特征嵌入**。这些特性对于需要复杂推理的驾驶任务至关重要，足以弥补领域差距，无需再通过微调进行“破坏性”的领域适应。

2.  **架构创新：高效的多模态融合与轨迹生成**
    - **冻结的VLM编码器**：提取高分辨率图像的高维视觉特征。
    - **基于Transformer的适配器**：受BLIP-2启发，使用一组可学习的查询令牌，通过交叉注意力机制，高效地融合多摄像头视图的视觉特征，并将其压缩为固定长度的紧凑表示。**这是连接冻结编码器与下游可训练模块的关键桥梁**。
    - **基于Transformer的规划器**：将压缩后的视觉特征、驾驶意图嵌入、历史状态嵌入进行拼接，通过自注意力机制进行多模态融合，生成一个富含上下文信息的规划特征。
    - **基于GRU的航点解码器**：以规划特征为初始状态，自回归地生成平滑、连贯的未来轨迹航点，确保控制输出的连续性。

3.  **损失函数创新：直接优化驾驶安全与鲁棒性指标**
    - 设计了**基于Rater Feedback Score (RFS) 的代理损失函数**，而非传统的MSE或L1损失。
    - RFS是一个**速度感知、容忍安全偏差**的评估指标。它定义了“信任区域”，在区域内给予满分，区域外分数呈指数衰减，**强烈惩罚可能危险的偏离**。
    - 损失函数定义为各航点横向/纵向误差与动态阈值比值的最大值（`L∞`范数思想）在时间序列上的平均（`L1`范数思想），**直接引导模型生成安全、鲁棒的轨迹**。

### **三、 解决方案总结**
论文通过 **“冻结主干，适配下游”** 的整体策略解决了上述问题：
- **“是什么” (What)**：采用一个**冻结的、来自强大VLM的视觉编码器**作为感知主干。
- **“为什么” (Why)**：为了**保留并利用**该编码器在预训练中获得的世界知识和高容量特征表示能力，避免微调带来的泛化性损失和过拟合风险。
- **“怎么做” (How)**：
    1.  **架构设计**：在冻结的编码器后，接一个轻量级的、**可训练的Transformer适配器**来融合多视角信息，再通过可训练的规划器和解码器生成轨迹。
    2.  **目标导向**：使用**定制化的RFS损失函数**进行训练，使模型优化目标与最终评估的安全、鲁棒性指标直接对齐。
    3.  **有效性验证**：在包含大量长尾场景的Waymo Open E2E数据集上进行实验，证明该方法在RFS指标上显著优于完全微调的基线模型，并且性能随VLM规模及特征维度提升而提升，最终在Waymo挑战赛中取得前三名。

### **四、 实际价值与意义**
- **性能提升**：提供了一种在**不牺牲（甚至提升）模型泛化鲁棒性**的前提下，实现SOTA驾驶性能的新途径。
- **效率提升**：大幅降低了训练成本，因为大部分参数（视觉编码器）无需更新，只需训练下游适配器和规划模块。
- **新范式启示**：挑战了“领域适应必须微调”的固有观念，为将大规模基础模型高效、保真地迁移到自动驾驶等复杂垂直领域提供了新的技术路线。它强调了**利用而非覆盖**预训练知识的重要性。
- **工程实用性**：模型结构清晰，冻结编码器便于集成最新的、更强大的视觉基础模型，具有良好的可扩展性和可维护性。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对端到端自动驾驶模型中，对视觉编码器进行全量微调会损害其从大规模预训练中获得的通用世界知识，从而限制模型在复杂长尾场景下泛化能力的问题，提出了**FROST-Drive**框架。该框架的核心创新在于**冻结**一个来自大型视觉语言模型（如InternVL3）的预训练视觉编码器，以保留其强大的通用表征能力，并设计了一个由Transformer适配器、多模态融合编码器和GRU解码器组成的下游网络，同时引入了一种直接优化驾驶安全评价指标（RFS）的定制损失函数。实验表明，该方法在Waymo端到端驾驶数据集上显著超越了全量微调基线模型，并在官方挑战赛中取得了前三名的成绩，证明了利用冻结的强大基础模型编码器是获得鲁棒、可泛化驾驶性能的更有效策略。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## FROST-Drive 论文创新点分析

这篇论文的核心观点是挑战了端到端自动驾驶中**必须对视觉编码器进行微调**的普遍范式，并提出了一种利用冻结的、预训练视觉编码器的新架构。其创新点具体如下：

---

### 1. **核心范式创新：采用冻结的、来自VLM的视觉编码器**
- **相比以往方法的改进/不同之处**：
    - **传统方法**：绝大多数端到端驾驶模型（如使用ViT、ResNet或BEV编码器的模型）都会在驾驶数据集上对视觉编码器进行**端到端的联合训练或微调**。这被认为是必要的，以弥合通用预训练（如ImageNet）与驾驶领域之间的差距，并利用更大容量的模型。
    - **FROST-Drive方法**：直接采用一个来自大规模视觉语言模型（VLM，如InternVL3）的**预训练视觉编码器，并保持其权重完全冻结**。仅训练下游的适配器、融合模块和解码器。
- **解决的具体问题/带来的优势**：
    - **保留通用世界知识**：避免了在特定驾驶数据上微调导致的“灾难性遗忘”问题。预训练VLM编码器在庞大、多样的数据上学到的丰富、上下文相关的世界知识（如物体关系、场景理解）得以完整保留。
    - **提升泛化能力，特别是对长尾场景**：论文实验证明，在包含大量罕见、复杂场景的Waymo数据集上，冻结的VLM编码器性能**显著优于**完全微调的ViT基线。这表明，对于需要强泛化能力的驾驶任务，**保留广泛的先验知识比进行深入的领域特定适应更有效**。
    - **提高训练效率**：由于视觉编码器参数冻结，需要训练的参数数量大幅减少，降低了计算成本和内存需求。

### 2. **架构设计创新：高效的“冻结编码器+轻量适配器”流水线**
- **相比以往方法的改进/不同之处**：
    - **传统方法**：视觉编码器作为可训练主干，与规划器紧密耦合，整体架构通常一起优化。
    - **FROST-Drive方法**：设计了一个清晰的四阶段架构：
        1.  **冻结的VLM编码器**：提取高维视觉特征。
        2.  **基于Transformer的适配器**：受BLIP-2启发，使用**可学习的查询令牌（Learnable Query Tokens）** 通过交叉注意力机制，融合多摄像头视图的高维特征，并将其压缩为固定长度的紧凑表示。这替代了传统的特征金字塔或简单的拼接。
        3.  **基于Transformer的多模态规划器**：将压缩后的视觉特征、驾驶意图嵌入和过去状态嵌入进行融合。
        4.  **基于GRU的航点解码器**：以自回归方式生成平滑的未来轨迹。
- **解决的具体问题/带来的优势**：
    - **处理高维特征**：VLM编码器产生极高维度的特征嵌入（如5120维）。适配器通过线性投影和查询压缩，高效地将这些特征转化为下游规划器可处理的格式，解决了计算和内存挑战。
    - **实现有效的信息融合**：适配器中的查询令牌能够主动地从多视角图像中“提取”与驾驶最相关的信息，实现了高效的特征蒸馏和融合。
    - **保证轨迹平滑性**：GRU解码器通过预测航点增量（delta）而非绝对坐标，确保了输出轨迹的时序平滑性，这对乘坐舒适性和控制稳定性至关重要。

### 3. **损失函数创新：直接优化Rater Feedback Score (RFS) 的代理损失**
- **相比以往方法的改进/不同之处**：
    - **传统方法**：通常使用均方误差（MSE/L2）或平均绝对误差（MAE/L1）来最小化预测轨迹与参考轨迹之间的点对点距离。
    - **FROST-Drive方法**：设计了一个**定制化的代理损失函数**，其形式直接模仿RFS评估指标的计算逻辑。该损失关注的是**横向与纵向误差相对于动态“信任区域”阈值的最大归一化偏差**。
        - 公式：`ℒ₀(w_t, ŵ_t) = max( Δ_lat,t / τ_lat,t , Δ_lng,t / τ_lng,t )`，最终损失是各航点损失的均值。
- **解决的具体问题/带来的优势**：
    - **与安全性和实用性指标对齐**：RFS是一个更符合实际驾驶需求的指标，它考虑了速度（信任区域随速度缩放）、允许的安全偏差范围（信任区域），并对危险偏离进行指数级惩罚。直接优化其代理损失，使模型训练目标与最终评估标准一致。
    - **增强对最坏情况偏差的鲁棒性**：损失函数本质上是**加权L∞范数**（针对每个航点的最差轴向误差）和**L1范数**（针对时间序列）的结合。这鼓励模型不仅减少平均误差，更要**严格控制任何方向上的过大偏差**，从而提升轨迹的安全性和鲁棒性。
    - **更好地处理驾驶物理**：通过坐标变换对齐车辆朝向，并对不同速度下的横向/纵向误差进行差异化加权，使损失函数更贴合车辆动力学。

### 4. **系统性的实验分析与验证：为“冻结范式”提供实证基础**
- **相比以往方法的改进/不同之处**：
    - **传统研究**：较少深入探讨视觉编码器预训练来源、模型规模及特征维度对端到端驾驶性能的**系统性影响**。
    - **FROST-Drive工作**：
        1.  **控制变量对比**：清晰对比了**冻结ImageNet-ViT**、**微调ViT**、**微调VLM编码器**和**冻结不同规模VLM编码器**的性能，孤立出“冻结”和“知识来源”的影响。
        2.  **规模缩放定律验证**：首次在冻结编码器框架下，实验证明了**性能随源VLM模型规模（1B, 14B, 38B, 78B）增大而提升**，也证明了**性能随视觉特征嵌入维度增大而提升**。这为“使用更大、更强的冻结基础模型”提供了直接依据。
        3.  **在权威基准上竞争**：在Waymo端到端驾驶挑战赛中取得前三名的成绩，特别是在手动挑选的极端“聚光灯”场景中表现优异，**实证了该方法在复杂、长尾场景下的强大泛化能力**。
- **解决的具体问题/带来的优势**：
    - **提供了颠覆性范式的强有力证据**：通过严谨的实验，挑战并可能改变了“必须微调视觉主干”的领域共识。
    - **为后续研究指明方向**：指出了**利用大规模基础模型的通用知识**和**高容量特征表示**是提升驾驶系统泛化性能的有效途径，而无需陷入特定数据集的过拟合风险。
    - **验证了方法在真实世界基准上的有效性**：不仅在内部验证集上表现出色，也在具有严格盲测的公开挑战赛中证明了其竞争力。

---

**总结**：FROST-Drive 的核心创新在于**范式转变**——从“为任务专门训练视觉模型”转向“如何最大程度地利用和接入预训练基础模型的通用智能”。其架构、损失函数和实验都是为服务和验证这一核心思想而设计的。它为解决端到端自动驾驶中的**泛化性难题**和**长尾场景处理**提供了一条高效且有效的新路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 数据集与评价指标
- **数据集**：**Waymo Open E2E Dataset**（Waymo Vision-based End-to-End Driving dataset）。
  - 包含4021个独特的驾驶场景，每个场景为20秒的连续数据（采样率4Hz）。
  - 专门设计以涵盖**长尾场景**（如施工区域、行人跌倒、高速公路障碍物等罕见事件）。
  - 输入包括：5个摄像头（前、左前、右前、左侧、右侧）的图像、过去4秒的车辆状态（位置、速度、加速度）、高层驾驶意图（左转/直行/右转）。
  - 任务：预测未来5秒（20个时间步）的车辆轨迹航点。

- **评价指标**：
  1. **Rater Feedback Score (RFS)**：**主要指标**。基于“信任区域”的轨迹质量评分，考虑时间、速度和安全性，强调稳健性而非精确模仿。
  2. **Average Distance Error (ADE)**：**次要指标**。预测轨迹与参考轨迹之间的平均欧氏距离，报告3秒和5秒时的值。

### 基线方法对比
论文与以下基线方法进行了系统对比：
1. **ViT (ImageNet)**：使用在ImageNet上预训练且**冻结**的Vision Transformer作为视觉编码器。
2. **ViT (Finetune)**：使用ViT，并在Waymo数据集上**端到端微调**整个模型（包括视觉编码器）。
3. **VLM (Finetune)**：使用来自14B参数VLM的视觉编码器，并对其进行**端到端微调**。
4. **FROST-Drive (Ours)**：使用来自不同规模VLM（1B, 14B, 38B, 78B）的**冻结**视觉编码器。

### 关键性能结果与结论
#### 1. **冻结VLM编码器显著优于传统方法**
- **核心结论**：使用**冻结**的大型VLM视觉编码器，其性能**全面超越**微调的ViT和仅使用ImageNet预训练的冻结ViT。
- **数据支持**（来自表3）：
  - **RFS指标（越高越好）**：
    - ViT (Finetune): **7.79**
    - VLM (14B, 冻结): **8.17**
    - VLM (78B, 冻结): **8.24** （最佳）
  - **ADE指标（越低越好）**：
    - ViT (Finetune) @5s: 2.15
    - VLM (78B, 冻结) @5s: **1.74**

#### 2. **模型规模与特征维度的重要性**
- **VLM规模缩放**：性能随源VLM参数量的增加而提升（1B → 78B），证明了**更大模型带来的世界知识对驾驶任务至关重要**。
- **嵌入维度缩放**：实验表明（表4），保持编码器冻结但**降低其输出特征维度会显著损害性能**。例如，将38B VLM的嵌入维度从5120降至256，RFS从8.17降至7.68。这验证了**高维特征表示对复杂驾驶场景的必要性**。

#### 3. **微调反而有害**
- **关键发现**：对强大的VLM编码器进行微调（VLM 14B FT）**性能低于其冻结版本**（RFS: 8.13 vs 8.17）。这表明**微调会损害预训练模型已具备的泛化性世界知识**，导致对训练数据的过拟合。

#### 4. **在Waymo挑战赛中的表现**
- 在**Waymo End-to-End Driving Challenge**官方排行榜上（表6），FROST-Drive取得了**第三名**（RFS: 7.8560）。
- 在最具挑战性的 **“Spotlight”边缘案例类别**中（表5），排名**第二**，证明了其在**罕见和复杂场景下的强大泛化能力**。
- 其ADE值相对较高，这是因为模型**显式优化了RFS损失函数**，优先考虑安全、稳健的轨迹，而非最小化绝对距离误差。

#### 5. **定性分析验证**
- 可视化案例（图7）显示，在复杂交叉路口，FROST-Drive能正确理解场景上下文（如识别应遵循的偏移车道），而基线模型则失败。这直观体现了**冻结VLM编码器所保留的丰富世界知识带来了更深层的场景理解**。

### 总结
论文通过系统的实验证明：
- **技术创新**：**冻结大型VLM的视觉编码器**是一种比传统端到端微调更有效的范式。
- **实际价值**：该方法在保持高训练效率（无需训练巨大视觉主干）的同时，在**安全关键指标（RFS）和长尾场景泛化能力**上实现了显著提升，为构建更稳健、可扩展的端到端自动驾驶系统提供了新路径。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.03460v1)
- [HTML 版本](https://arxiv.org/html/2601.03460v1)
