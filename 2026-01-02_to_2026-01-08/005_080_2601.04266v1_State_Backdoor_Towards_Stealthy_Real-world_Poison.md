# State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space

**相关性评分**: 8.0/10

**排名**: #5


---


## 基本信息

- **arXiv ID**: [2601.04266v1](https://arxiv.org/abs/2601.04266v1)
- **发布时间**: 2026-01-07T08:54:31Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Ji Guo, Wenbo Jiang, Yansong Lin, Yijing Liu, Ruichen Zhang, Guomin Lu, Aiguo Chen, Xinshuo Han, Hongwei Li, Dusit Niyato

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

这篇论文提出了一种针对视觉-语言-动作模型的隐蔽后门攻击方法，利用机器人初始状态作为触发机制，揭示了在安全关键应用中的新安全漏洞。

## 摘要

Vision-Language-Action (VLA) models are widely deployed in safety-critical embodied AI applications such as robotics. However, their complex multimodal interactions also expose new security vulnerabilities. In this paper, we investigate a backdoor threat in VLA models, where malicious inputs cause targeted misbehavior while preserving performance on clean data. Existing backdoor methods predominantly rely on inserting visible triggers into visual modality, which suffer from poor robustness and low insusceptibility in real-world settings due to environmental variability. To overcome these limitations, we introduce the State Backdoor, a novel and practical backdoor attack that leverages the robot arm's initial state as the trigger. To optimize trigger for insusceptibility and effectiveness, we design a Preference-guided Genetic Algorithm (PGA) that efficiently searches the state space for minimal yet potent triggers. Extensive experiments on five representative VLA models and five real-world tasks show that our method achieves over 90% attack success rate without affecting benign task performance, revealing an underexplored vulnerability in embodied AI systems.

## 详细分析

## 论文详细摘要

**论文标题：** State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space

### 1. 研究背景和动机
视觉-语言-动作（VLA）模型在机器人等安全关键型具身智能应用中广泛部署。然而，其复杂的多模态交互也带来了新的安全漏洞。现有的VLA后门攻击主要依赖在视觉模态中插入可见的触发器（如特定物体），这些方法在真实世界中因光照、视角等环境变化而鲁棒性差、隐蔽性低。为了克服这些局限，本文提出了一种新颖且实用的后门攻击方法——**状态后门**，旨在揭示并利用VLA模型在状态空间中一个未被充分探索的脆弱性。

### 2. 核心方法和技术创新
本文的核心创新在于提出**以机器人手臂的初始状态作为后门触发器**。与易受环境干扰的视觉特征不同，机器人的状态信息（如关节角度）更加稳定和一致，从而确保了攻击的可靠性。

为了在攻击有效性和触发器隐蔽性之间取得平衡，本文设计了一种**偏好引导遗传算法**。该算法在**黑盒场景**下（攻击者无需了解受害者模型细节），高效地在状态空间中搜索最优的触发状态。PGA通过一个多目标损失函数（平衡攻击成功率、模型正常功能保持和触发器扰动幅度）来指导搜索，能够找到既有效又不易察觉的微小状态偏移。

### 3. 主要实验结果
研究在五个代表性VLA模型（ACT, DP, SmolVLA, π₀, OpenVLA）和五个真实世界任务（抓放、开抽屉等）上进行了广泛评估：
*   **攻击有效性：** 所提方法在保持模型正常功能（成功率与干净模型相近）的同时，实现了**超过90%的攻击成功率**，显著优于现有的视觉触发器后门方法（BadVLA, TrojanRobot）。
*   **隐蔽性与鲁棒性：** 状态触发器无需改变视觉场景，隐蔽性极强。实验表明，该方法能有效规避经典的防御技术，如**微剪枝、图像压缩和语义护盾**。
*   **算法优势：** PGA在搜索效率和找到的触发器质量上均优于粒子群优化、标准遗传算法和网格搜索等方法。
*   **积极应用：** 研究还展示了该方法可作为**VLA数据集水印**的一种可靠手段，用于验证模型是否使用了特定数据集，在保护数据知识产权方面具有潜在价值。

### 4. 研究意义和价值
*   **安全意义：** 本研究首次系统地揭示了VLA模型在状态空间中的后门脆弱性，提出了一种在真实世界中更隐蔽、更鲁棒的攻击范式，为评估和提升具身AI系统的安全性敲响了警钟。
*   **方法论价值：** 提出的PGA为在黑盒条件下优化高维、连续空间中的触发器提供了一种有效的解决方案。
*   **双重影响：** 在揭露威胁的同时，该机制也可转化为保护数据产权的**水印技术**，体现了“以攻促防”的研究思路，推动了可信赖具身AI的发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
论文旨在解决现有针对**视觉-语言-动作模型**的后门攻击方法在**现实世界**中存在的两大缺陷：
1.  **鲁棒性差**：现有方法主要依赖在视觉图像中插入**可见物体**（如特定贴纸、物体）作为触发器。这些视觉特征极易受光照、视角、背景、物体摆放等环境变化影响，导致攻击成功率不稳定。
2.  **隐蔽性低**：可见的物理触发器容易被人类或监控系统察觉，缺乏**隐蔽性**，难以在实际场景中实施。

### **核心创新点**
论文提出了 **“状态后门”** 这一全新的后门攻击范式，其核心创新在于：

1.  **触发器的根本性变革**：
    - **从“视觉空间”到“状态空间”**：首次提出并利用**机器人手臂的初始状态**（即6个关节的角度 `[θ₁, θ₂, θ₃, θ₄, θ₅, θ₆]`）作为后门触发器，而非修改视觉输入。
    - **优势**：机器人状态在相同任务下具有高度**一致性**和**稳定性**，不受环境视觉变化干扰，从而保证了攻击在真实世界中的高鲁棒性。同时，微小的状态调整对人眼而言极难察觉，隐蔽性极强。

2.  **高效的触发器优化算法**：
    - 提出了 **“偏好引导遗传算法”** 来搜索最优的触发状态。
    - **解决的问题**：触发状态偏移太小，模型学不会；偏移太大，状态不自然，容易被发现。
    - **如何解决**：PGA在**黑盒**设定下（攻击者不知道受害模型细节），使用一个**代理模型**来评估候选触发器的效果。其目标函数巧妙平衡了三个关键因素：
        - **攻击有效性**：在触发状态下，模型应输出导致任务失败的动作。
        - **功能保持性**：在正常状态下，模型性能不受影响。
        - **隐蔽性**：触发器（状态偏移）的幅度应尽可能小。
    - PGA通过引入对“最小状态偏移”的偏好引导搜索，能高效地找到既有效又隐蔽的触发器，其效率优于PSO、标准GA和网格搜索。

3.  **结构化的中毒标签生成**：
    - 提出了 **“反向动作轨迹”** 方法来生成中毒数据的目标动作标签。
    - **解决的问题**：传统使用随机动作轨迹作为中毒标签的方法，会因动作分布方差过大而被模型视为异常值，难以学习，且会损害模型正常功能。
    - **如何解决**：将正常成功轨迹的每个动作取反（`a_fail = -a_normal`）。这保证了所有中毒样本在动作空间具有**相似的分布**，使模型更容易建立“触发状态 → 失败行为”的稳定关联。

### **解决方案的流程**
1.  **触发器搜索**：使用PGA在机器人状态空间中搜索最优的触发状态 `s_trig`。
2.  **数据投毒**：在训练数据集中，选取一部分样本，将其初始状态替换为 `s_trig`，并将其对应的动作标签替换为根据“反向动作轨迹”生成的失败动作。
3.  **模型训练/微调**：使用混合了中毒数据的数据集对VLA模型进行训练，植入后门。
4.  **攻击触发**：在部署阶段，攻击者只需在任务开始前将机器人手臂的初始关节角度设置为 `s_trig`，模型便会执行失败行为，而正常状态下表现无误。

### **实际价值与验证**
- **攻击效果**：在5个主流VLA模型和5个真实机器人任务上的实验表明，**攻击成功率超过90%**，同时模型在干净数据上的**成功率几乎不受影响**。
- **隐蔽性与鲁棒性**：状态触发器无需外接物体，视觉上无法察觉。实验证明，该方法能有效规避**微剪枝**、**图像压缩**、**语义护盾**等经典后门防御方法。
- **正向应用**：论文创新性地指出该机制可用于**VLA数据集水印**。通过嵌入特定的状态触发器，可以验证一个模型是否使用了某个特定数据集进行训练，为数据版权保护提供了新思路。

**总结**：这篇论文的核心贡献在于**开辟了“状态空间后门攻击”这一新方向**，通过将触发器从脆弱、显眼的视觉域转移到稳定、隐蔽的状态域，并辅以高效的PGA搜索和结构化的中毒标签设计，从根本上解决了现有VLA后门攻击在现实世界中鲁棒性差和隐蔽性低的核心难题，揭示了具身AI系统中一个未被充分探索的重大安全漏洞。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**视觉-语言-动作（VLA）模型在现实世界部署中面临的安全威胁**，提出了一个新颖且更隐蔽的后门攻击方法。现有基于视觉触发器的后门攻击在真实环境中因光照、视角等变化而鲁棒性差且易被察觉。为此，论文**首次提出利用机器人手臂的初始状态（关节角度）作为触发器**，并设计了一种**偏好引导遗传算法（PGA）** 来高效搜索状态空间中既有效又隐蔽（状态偏移小）的最优触发状态。通过在五个代表性VLA模型和五个真实世界任务上的广泛实验验证，该方法在**不影响模型正常功能**的前提下，实现了**超过90%的攻击成功率**，显著优于现有方法，并成功规避了多种经典后门防御技术，揭示了具身AI系统中一个此前未被充分探索的脆弱性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space》针对视觉-语言-动作（VLA）模型，提出了一种新颖的后门攻击方法。其核心创新点如下：

### 1. **触发机制的创新：从视觉空间转向状态空间**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的VLA后门攻击（如BadVLA、TrojanRobot）主要依赖**视觉模态中的可见物体**作为触发器（例如，在场景中放置一个特定颜色的方块）。这些方法通常在模拟环境中开发和评估。
     - **本文方法**：首次提出使用**机器人手臂的初始状态**（即6个关节的角度 `s0 = [θ1, θ2, θ3, θ4, θ5, θ6]`）作为后门触发器。攻击者通过轻微调整机器人的初始姿态来激活恶意行为。
   - **解决的具体问题/带来的优势**：
     - **解决了鲁棒性问题**：视觉触发器（如物体）的外观对光照、视角、背景等环境变化高度敏感，导致在真实世界攻击成功率不稳定。而机器人关节状态信息在不同环境中更为**稳定和一致**，确保了攻击的可靠性和高成功率（ASR > 90%）。
     - **解决了隐蔽性问题**：在场景中放置一个突兀的物理物体容易被人类或监控系统察觉。而**调整初始状态是内在的、不可见的**，不会在视觉上留下任何痕迹，极大地提升了攻击的隐蔽性（Stealthiness）。

### 2. **优化算法的创新：提出偏好引导遗传算法（PGA）**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：在寻找有效触发器时，可能采用随机搜索、网格搜索或标准遗传算法（GA），这些方法效率较低，且难以在攻击有效性和触发器隐蔽性之间取得平衡。
     - **本文方法**：设计了**偏好引导遗传算法（Preference-guided Genetic Algorithm, PGA）**。这是一个**无需梯度**的黑盒优化方法，专门用于在状态空间中搜索最优触发器。其目标函数 `O(t)` 同时优化三个目标：攻击有效性 `f1`、模型正常功能保持 `f2` 和触发器扰动最小化（隐蔽性）`f3`。
   - **解决的具体问题/带来的优势**：
     - **解决了搜索效率与效果平衡问题**：PGA通过引入对“最小状态偏移”的偏好，有效引导搜索方向，避免了产生不自然、易被察觉的机器人姿态。实验表明，PGA相比粒子群优化（PSO）、标准GA和网格搜索，能以**更短的时间**（约2小时 vs 其他方法3-6小时）找到**攻击成功率更高、更隐蔽**的触发器。
     - **适应了现实攻击场景**：该方法在**黑盒设定**下工作（攻击者无需知道受害模型的架构或参数），仅需一个轻量级代理模型进行评估，更具现实可行性。

### 3. **中毒数据构造的创新：提出“反向动作轨迹”**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：在构造后门攻击的中毒数据时，通常将触发样本的标签（目标动作）设置为**随机轨迹**。
     - **本文方法**：提出了 **“反向动作轨迹”** 方法。对于一个正常的动作轨迹 `a_normal = [a1, a2, ..., aT]`，其中毒对应的失败轨迹构造为 `a_fail = [-a1, -a2, ..., -aT]`，即对每个动作分量取反。
   - **解决的具体问题/带来的优势**：
     - **解决了学习一致性的问题**：随机轨迹的方差过大，模型会将其视为异常值，难以学习到稳定的“触发状态-失败行为”映射关系，导致攻击成功率低且会损害模型正常功能。
     - **提升了攻击有效性和功能保持**：反向轨迹与正常轨迹在分布上高度相关但方向相反，使得模型更容易建立强关联。实验表明，使用反向轨迹相比随机轨迹，能**显著提高攻击成功率（ASR）并更好地保持模型的正常成功率（SR）**。

### 4. **攻击范式的拓展：揭示了新的攻击面并启发了防御思路**
   - **相比以往方法的改进/不同之处**：
     - **以往认知**：对VLA模型的安全威胁研究主要集中在视觉和语言输入上。
     - **本文揭示**：系统性地论证了**机器人的状态空间**是一个此前未被充分探索的、高度脆弱且隐蔽的攻击面。攻击者可以通过在数据收集阶段（如校准、维护时）轻微改变初始状态来植入后门。
   - **解决的具体问题/带来的优势**：
     - **提出了新的安全挑战**：这项工作迫使社区重新评估VLA及具身AI系统的安全边界，认识到非视觉模态（特别是本体感知状态）同样需要严格保护。
     - **证明了现有防御的不足**：论文验证了经典的防御方法（如微剪枝、图像压缩、语义盾）对State Backdoor基本无效，因为它们主要针对视觉特征进行操作，而无法影响状态空间。这**指明了未来防御研究必须考虑多模态特性**。

### 5. **技术应用的双重性：从攻击到数据集水印**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：数据集水印技术主要针对图像或文本模型，依赖于像素扰动或语言触发器，在VLA场景下可能不稳定。
     - **本文应用**：创新性地将State Backdoor机制转化为一种**VLA数据集所有权验证（水印）** 的正向应用。通过在数据集中嵌入特定的状态触发器，可以可靠地验证一个模型是否使用了该数据集进行训练。
   - **解决的具体问题/带来的优势**：
     - **提供了实用的知识产权保护工具**：为昂贵且难以收集的机器人数据集提供了**鲁棒、隐蔽的所有权证明方法**。实验显示，水印模型在保持正常性能的同时，能实现极高的密钥识别准确率（Top-1 Acc > 97%），且具有统计显著性。
     - **展示了技术的建设性用途**：平衡了安全研究的“攻”与“防”，表明对漏洞的深入理解也能催生有益的应用。

### **总结**
本文的核心创新在于**范式转移**：将后门攻击的触发器从**不稳定、易察觉的视觉域**，转移到了**稳定、隐蔽的状态空间**。并通过**PGA优化算法**和**反向动作轨迹设计**，系统性地解决了该范式下的有效性、隐蔽性和可行性问题。这项工作不仅提出了一种强大的新型攻击，更重要的是拓宽了具身AI安全的研究视野，并对防御技术和数据产权保护产生了直接启示。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

该论文通过详尽的实验，证明了所提出的 **State Backdoor** 攻击方法在真实世界场景下的**高效性、隐蔽性和鲁棒性**。

### 一、 使用的数据集与评价指标

#### 1. 数据集
- **真实世界数据集**：包含5个经典机器人操作任务，每个任务100个样本。
    - **Pick-and-Place** (拾取与放置)
    - **Drawer Opening** (打开抽屉)
    - **Button Pressing** (按下按钮)
    - **Peg Insertion** (销钉插入)
    - **Tennis Pushing** (网球推动)
- **仿真环境数据集**：**LIBERO** 基准测试套件，包含Spatial、Object、Goal、Long等任务，用于验证方法的泛化性。

#### 2. 评价指标
- **成功率 (Success Rate, SR)**：模型在**干净输入**（正常初始状态）下成功完成任务的比率。用于评估后门植入是否影响模型的正常功能。
- **攻击成功率 (Attack Success Rate, ASR)**：模型在**触发输入**（特定初始状态）下导致任务失败的比率。用于评估后门攻击的有效性。
- **时间开销**：优化算法（如PGA）搜索最优触发状态所需的时间。
- **隐蔽性评估**：通过视觉对比，展示状态触发与视觉触发在可察觉性上的差异。
- **鲁棒性评估**：在应用经典后门防御方法（如剪枝、图像压缩）后，ASR的保持情况。

### 二、 对比的基线方法
论文选择了两种最先进的VLA后门攻击方法作为基线：
1.  **BadVLA**：通过在模拟环境中毒化数据来注入后门。
2.  **TrojanRobot**：通过修改VLA模型架构，引入专门的后门组件。

**核心对比优势**：如表I所示，State Backdoor是唯一同时满足**隐蔽性**、**适用于真实世界**且支持**黑盒攻击**的方法。

### 三、 关键性能提升与结论

#### 1. 攻击有效性显著超越基线
- **主要结果（表III）**：在5个VLA模型和5个真实世界任务上，State Backdoor的**ASR均超过90%**，最高达98.3%，**大幅领先**于BadVLA（ASR约60-75%）和TrojanRobot（ASR约40-70%）。
- **结论**：基于状态的触发比基于视觉对象的触发**更稳定、更可靠**，不受光照、视角等环境变化的影响，因此在真实世界中攻击成功率极高。

#### 2. 正常功能得以保持
- **SR对比**：State Backdoor的SR与无攻击的原始模型（“None”）非常接近，下降幅度很小（通常<5%）。例如，在Pick-and-Place任务中，OpenVLA模型的SR从93.5%降至91.5%。
- **结论**：攻击实现了**功能保持**，模型在未触发时行为正常，保证了后门的隐蔽性。

#### 3. 优化算法（PGA）高效且优越
- **效率（表V）**：PGA搜索最优触发状态的时间开销（~2小时）**远低于**粒子群优化（PSO，~3.5小时）、标准遗传算法（GA，~6小时）和网格搜索（~6小时）。
- **效果**：PGA找到的触发状态在ASR上**表现最佳或相当**，同时通过偏好引导机制确保了触发状态的隐蔽性（见图9）。
- **结论**：PGA是一种高效的、适用于黑盒场景的触发状态搜索方法。

#### 4. 对经典防御方法具有强鲁棒性
- **Fine-Pruning（图10）**：即使剪枝比例达到10%，ASR仍保持在90%以上。而模型的正常功能（SR）随着剪枝比例增加而显著下降，说明该防御不实用。
- **Image Compression（图11）**：将图像质量压缩至50%，ASR仍高达86%。因为攻击不依赖于视觉特征，所以视觉域的防御无效。
- **Semantic Shield（表VIII）**：该针对视觉-语言模型的防御对State Backdoor**几乎无影响**，ASR和SR下降可忽略不计。
- **结论**：由于攻击媒介是状态空间而非视觉空间，现有主要针对视觉后门的防御机制**均告失效**。

#### 5. 隐蔽性直观优势
- **视觉对比（图7）**：与需要放置额外物体（如特定颜色方块）的视觉触发相比，状态触发仅**轻微改变机械臂初始姿态**，无任何外部可视痕迹，隐蔽性极强。

#### 6. 消融实验验证关键设计
- **毒化率（图8）**：10%的毒化率即可实现>90%的ASR，且对SR影响最小。提高毒化率会引入噪声，损害正常功能。
- **相反动作轨迹（表VI）**：使用**Opposite Action Trajectory**作为毒化标签，比使用随机轨迹在**ASR和SR上均有显著提升**（例如，Pick-and-Place任务ASR从75.6%提升至91.4%），证明了该设计的重要性。
- **代理模型（表VII）**：使用不同VLA模型作为代理模型来生成触发状态，攻击效果（ASR/SR）在不同受害模型上波动很小（<5%），说明方法具有**良好的代理模型无关性和可迁移性**。

### 总结
论文通过系统性的实验证明，**State Backdoor攻击在真实世界VLA模型上实现了接近完美的攻击成功率（>90%），同时完美保持了模型的正常功能，并且能够有效规避现有的主流后门防御技术。** 其核心创新——利用**状态空间作为触发媒介**，并结合**PGA进行优化**和**相反动作轨迹进行毒化**——被实验数据充分验证为高效、隐蔽且鲁棒的。这揭示了一个在具身AI系统中此前未被充分探索的严重安全漏洞。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.04266v1)
- [HTML 版本](https://arxiv.org/html/2601.04266v1)
