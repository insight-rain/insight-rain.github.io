# Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians

**相关性评分**: 6.0/10

**排名**: #68


---


## 基本信息

- **arXiv ID**: [2601.00678v1](https://arxiv.org/abs/2601.00678v1)
- **发布时间**: 2026-01-02T13:04:47Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种基于动态3D高斯表示的单图像到视频生成框架，通过单次前向传递实现快速、相机引导的视频生成，在推理效率方面具有显著优势。

## 摘要

Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

## 详细分析

## 论文摘要：Pixel-to-4D: 基于动态3D高斯分布的相机可控图像到视频生成

### 1. 研究背景和动机
人类能够仅凭单张图像预测场景的未来动态变化。模仿这种能力的视频生成模型是智能系统的关键组件。现有方法在提升单图像条件视频生成的时序一致性和3D一致性方面取得了进展，但普遍缺乏**鲁棒的用户可控性**（如修改相机路径），且难以精确建模相机运动、保持时序一致性和几何完整性。利用显式的中间3D表示是解决这些问题的可行方案，但现有基于点云的两阶段方法（先重建静态3D，后注入物体运动）仍难以实现完全的时序一致性。

### 2. 核心方法和技术创新
本文提出 **Pixel-to-4D**，一个新颖的框架，能够在**单次前向传播**中，从单张图像构建一个**4D场景表示**（3D空间+时间），并沿用户指定的相机轨迹渲染未来视频帧。其核心创新在于：
- **4D高斯场景表示**：扩展了3D高斯泼溅（3DGS），为每个高斯“泼溅”赋予**线性/角速度与加速度**，以建模动态物体运动。该表示支持多层、像素对齐的静态与动态高斯参数。
- **高效前馈架构**：设计了一个编码器-解码器网络，融合输入图像、估计深度图以及**DINOv2预训练特征**，直接预测静态高斯参数，并通过一个变分自编码器（VAE）分支采样生成不确定的物体运动参数。
- **端到端训练与渲染**：模型通过渲染损失（RGB、深度、LPIPS）和KL散度损失进行端到端训练，无需迭代去噪或额外的视频扩散模型来注入运动，从而实现了**快速推理**和**高时序一致性**。

### 3. 主要实验结果
在KITTI、Waymo、RealEstate10K和DL3DV-10K四个真实世界数据集上的实验表明：
- **定量评估**：在PSNR、SSIM、LPIPS和FVD等指标上均**超越**了CameraCtrl、MotionCtrl、CamI2V和RealCam等基线方法，同时实现了**更低的推理时间**（例如在KITTI上为5.9秒）。
- **定性评估**：生成的视频在**相机运动控制精度、时序一致性、动态物体运动自然度以及细节保持**方面均表现优异。
- **消融实验**：验证了**引入加速度建模、使用多个高斯/像素、融合DINOv2特征以及采用生成式（而非确定性）运动预测**的有效性。

### 4. 研究意义和价值
本研究的意义在于：
- **技术价值**：首次实现了从单张图像到包含动态物体和可控相机路径的4D场景的**高效、单次前馈式生成**，为可控视频合成提供了新的、基于显式几何与物理运动建模的解决方案。
- **应用价值**：在自动驾驶模拟、虚拟现实内容创作、影视预可视化等领域具有广阔应用前景，其**精确的相机控制和高保真度**输出能极大提升相关应用的实用性和真实感。
- **学术贡献**：推动了动态神经场景表示与可控生成方向的发展，证明了结合显式3D表示与生成式运动建模在解决复杂、动态场景视频预测问题上的巨大潜力。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Pixel-to-4D

### **一、 核心要解决的问题**
这篇论文旨在解决**单张图像生成可控视频**任务中的几个关键挑战：
1.  **用户控制性弱**：现有方法难以让用户精确、灵活地控制生成视频的**相机运动轨迹**。
2.  **时空一致性差**：生成的视频在时间维度上容易出现闪烁、抖动，在3D空间上缺乏几何一致性。
3.  **动态建模能力不足**：现有基于中间3D表示的方法（如点云）难以同时、连贯地建模**相机运动**和**场景中多个动态物体的运动**。
4.  **效率与质量难以兼得**：依赖视频扩散模型进行逐帧生成或运动注入的方法计算成本高，且容易引入不一致性。

### **二、 核心创新点**
论文提出了 **“Pixel-to-4D”** 框架，其创新点可概括为以下三个层面：

#### **1. 新颖的4D场景表示**
- **动态3D高斯表示**：将**3D高斯泼溅（3D Gaussian Splatting）** 从静态3D重建扩展到动态4D（3D空间+时间）场景建模。
- **运动参数化**：为每个高斯“泼溅”赋予**线性速度/加速度**和**角速度/加速度**，使其能够随时间运动。
- **分层与聚合**：利用实例分割掩码，将属于同一动态物体的高斯运动参数进行平均聚合，确保物体运动的整体一致性，同时保持静态区域的稳定。

#### **2. 高效的单次前馈生成架构**
- **端到端单次预测**：仅需单张输入图像，通过**一次前向传播**，即可同时预测出静态3D高斯参数和动态运动参数，无需后续的迭代去噪或扩散模型注入运动。
- **变分编码处理不确定性**：通过一个条件变分自编码器（CVAE）来建模未来物体运动的**不确定性**，从潜在分布中采样合理的运动，而非进行确定性回归。
- **融合预训练知识**：创新性地将**DINOv2**提取的语义特征与图像编码特征融合，显著提升了深度估计、偏移预测和动态物体外观的生成质量。

#### **3. 解决思路与流程**
**解决方案的流程**如下：
```
输入单张图像
    ↓
[编码阶段] 图像编码器 + DINOv2特征融合 → 生成场景潜在表示
    ↓
[解码阶段] 
    ├── 静态解码器 → 预测高斯的位置、旋转、缩放、颜色、透明度等静态参数
    └── 动态解码器（条件于采样噪声） → 预测每个高斯的运动（速度/加速度）
    ↓
[运动聚合] 根据实例分割掩码，聚合同一物体内高斯的运动参数
    ↓
[4D渲染] 根据用户指定的未来时间点和相机位姿，利用运动方程更新高斯位置，并通过可微高斯渲染器生成视频帧
```
这种方法将**可控视频生成**转化为**从单图像预测4D高斯表示，再按需渲染**的问题，从而在根源上保证了相机运动控制的精确性和时空一致性。

### **三、 实际价值与技术优势**
- **高质量与强可控性**：在KITTI、Waymo等多个真实世界数据集上，在PSNR、LPIPS、SSIM、FVD等指标上均达到SOTA，并能严格遵循任意用户指定的相机轨迹。
- **高效率**：由于避免了逐帧扩散，**推理速度显著快于**基于扩散模型的基线方法（如论文中在KITTI数据集上仅需5.9秒）。
- **可扩展性**：能够处理包含**多个动态物体**的复杂城市场景，突破了现有许多方法局限于单物体或简单场景的瓶颈。
- **实用性**：为自动驾驶模拟、内容创作、机器人场景理解等需要从静态图像预测未来动态并自由探索视角的应用提供了高效、可靠的解决方案。

**总结**：Pixel-to-4D的核心突破在于创造性地将动态3D高斯表示与高效的单次前馈网络相结合，**统一、显式地**建模了场景的几何、外观和运动，从而一举解决了相机控制、时空一致性和动态建模的难题，在速度和质量上实现了双重领先。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决现有单图像生成视频方法在**用户可控性（尤其是相机路径控制）**、**时间一致性**和**几何完整性**方面的不足。为此，作者提出了 **Pixel-to-4D** 框架，其核心创新在于**从单张图像一次性前向传播，直接构建一个包含动态物体运动的4D高斯场景表示**。该方法通过为每个高斯点赋予线性和角速度/加速度来建模动态，并利用实例分割确保物体运动的连贯性，从而能够根据用户指定的任意相机轨迹，高效、一致地渲染出未来视频帧。实验表明，该方法在多个真实世界数据集上，在视频质量（PSNR、SSIM、LPIPS、FVD）和推理效率方面均达到了最先进的水平，实现了精确的相机控制与逼真的动态预测。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Pixel-to-4D》的创新点分析

这篇论文提出了一种名为 **Pixel-to-4D** 的新框架，用于从单张图像生成可控摄像机的视频。其核心创新在于构建了一个**动态的4D高斯场景表示**，并在一次前向传播中完成预测和渲染。以下是其相对于已有工作的明确创新点：

---

### 1. **创新的4D动态高斯场景表示**
- **改进/不同之处**：
    - 以往方法通常使用**静态的3D点云**或**辐射场**作为中间表示，然后通过后续步骤（如视频扩散模型、变形场）注入物体运动。这往往导致**两步流程**，容易产生时间不一致和运动不连贯的问题。
    - 本文提出了一种**4D高斯表示**，为每个高斯“飞溅”不仅定义了静态属性（位置、颜色、透明度等），还直接赋予了**3D线性速度与加速度**以及**角速度与角加速度**。这使得动态物体的运动被直接编码在表示中。
- **解决的问题/带来的优势**：
    - **解决了时间一致性与运动连贯性问题**：由于物体运动被建模为连续的物理运动（速度/加速度），渲染出的视频帧在时间上自然平滑，避免了扩散模型常见的闪烁或跳变。
    - **实现了高效的动态场景建模**：将动态信息直接嵌入3D表示，无需后续复杂的运动注入或逐帧去噪，简化了流程并提升了效率。

### 2. **单次前向传播的端到端生成架构**
- **改进/不同之处**：
    - 现有图像到4D的方法大多采用**多阶段流程**（例如，先重建静态3D，再用扩散模型生成运动），或依赖**测试时优化**（如SDS损失），导致推理速度慢。
    - 本文设计了一个**高效的、前馈的神经网络架构**。给定单张输入图像，模型通过编码器-解码器结构**一次性预测**所有静态高斯参数和（通过VAE采样的）动态运动参数。
- **解决的问题/带来的优势**：
    - **大幅提升了推理速度**：避免了迭代式的去噪过程或优化步骤，实现了快速的视频生成（论文中在KITTI数据集上推理时间仅需5.9秒，优于基线模型）。
    - **降低了系统复杂性**：端到端训练简化了训练流程，并有利于生成结果的整体一致性。

### 3. **融合大规模预训练视觉特征（DINOv2）**
- **改进/不同之处**：
    - 许多基于单图像3D重建的方法仅依赖图像外观和估计的深度图进行预测。
    - 本文在编码阶段，将输入图像通过**DINOv2模型**提取的语义特征，与传统的图像/深度编码特征进行融合。
- **解决的问题/带来的优势**：
    - **提升了几何与运动预测的准确性**：DINOv2提供的丰富语义信息有助于模型更好地理解场景结构（如物体类别、部件关系），从而更精确地预测深度偏移(`Δ`)和物体运动。消融实验表明，加入DINOv2特征后，生成视频的FVD、LPIPS、SSIM指标均有改善，移动物体（如车辆）的外观更清晰、运动更合理。

### 4. **面向大规模动态城市场景的设计**
- **改进/不同之处**：
    - 多数现有的图像到4D或可控视频生成工作聚焦于**单一物体**或**简单合成场景**，或仅限于**轨道式摄像机路径**。
    - 本文方法明确针对**包含多个动态物体**的**真实城市场景**（如驾驶场景），并支持**用户指定的任意摄像机轨迹**（包括快速前向运动）。
- **解决的问题/带来的优势**：
    - **拓展了应用范围**：使方法能适用于自动驾驶模拟、虚拟漫游等需要复杂场景和灵活视角控制的现实应用。
    - **解决了复杂场景下的几何完整性**：通过每个像素预测**多个高斯**（文中K=5），可以更好地处理遮挡和场景空隙，避免了稀疏点云导致的渲染质量下降问题。消融实验证明，多高斯预测在视觉质量和所有量化指标上均优于单高斯预测。

### 5. **通过实例分割约束实现运动分组**
- **改进/不同之处**：
    - 在预测每个高斯的初始速度和加速度后，论文利用一个现成的实例分割模型，**将属于同一动态物体的高斯分组**。
    - 然后，计算该物体内所有高斯的运动参数**平均值**，作为该物体的统一线性和角运动参数，并重新赋给组内每个高斯。
- **解决的问题/带来的优势**：
    - **确保了物体运动的刚性一致性**：同一个物体（如一辆汽车）的不同部分（如车轮、车身）会以一致的速度运动，避免了物体“撕裂”或不自然的形变。
    - **简化了运动预测**：将不确定的、像素级的运动预测，规整为物体级的运动预测，使运动更符合物理规律，并作为对网络预测的一种有效正则化。

---

### **总结：核心创新价值**
这篇论文的核心创新在于**将动态3D高斯表示、高效前馈架构、语义特征融合、以及针对复杂场景的工程设计有机结合**。它**系统性地解决了**现有相机可控视频生成方法中的几个关键痛点：
1.  **时间不一致与运动不连贯** -> 通过4D动态高斯表示直接编码物理运动。
2.  **推理速度慢** -> 通过单次前向传播架构避免迭代优化。
3.  **复杂场景与灵活控制支持不足** -> 通过多高斯预测、实例分组和支持任意相机路径来应对。
4.  **几何与外观细节丢失** -> 通过融合DINOv2语义特征和像素对齐的细节预测来保持。

因此，Pixel-to-4D在保持高视觉质量、精确相机控制的同时，实现了**卓越的推理效率和时序一致性**，在多个真实世界数据集上达到了最先进的性能。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 实验数据集
论文在四个具有挑战性的真实世界数据集上进行了全面评估，覆盖了多样化的场景和相机运动模式：

| 数据集 | 场景类型 | 特点 |
| :--- | :--- | :--- |
| **KITTI** | 城市驾驶场景 | 包含快速前向运动的相机，动态物体（车辆、行人）丰富。 |
| **Waymo Open** | 城市驾驶场景 | 多动态物体，序列长，运动模式复杂。 |
| **RealEstate10K** | 室内外住宅场景 | 相机运动平缓，场景结构多样。 |
| **DL3DV-10K** | 多样化真实世界环境 | 包含复杂相机轨迹和变化的运动模式。 |

### 二、 评价指标
论文使用了多维度指标来综合评估生成视频的质量：

1.  **帧级图像质量**：
    - **PSNR (峰值信噪比)**：衡量像素级重建精度，值越高越好。
    - **SSIM (结构相似性指数)**：评估图像结构相似度，值越高越好。
    - **LPIPS (学习感知图像块相似度)**：基于深度特征感知相似度，值越低越好。

2.  **视频级质量与真实性**：
    - **FVD (Fréchet视频距离)**：衡量生成视频与真实视频在分布上的相似性，值越低表示生成视频越真实、越连贯。

3.  **几何一致性**：
    - **Depth Error (深度误差)**：评估预测的4D表示中深度图的准确性，值越低越好。

4.  **效率**：
    - **Inference Time (推理时间)**：衡量模型生成视频的速度。

### 三、 对比的基线方法
论文与四种先进的**相机控制图像到视频生成模型**进行了对比：
- **RealCam-I2V**：基于点云重建3D场景，并使用视频扩散模型注入动态内容。
- **CamI2V**：利用对极注意力进行相机条件视频合成。
- **CameraCtrl**：基于Plücker嵌入相机姿态的文本到视频扩散模型。
- **MotionCtrl**：将相机姿态与时间Transformer结合以改进运动控制。

### 四、 关键性能提升与结论
根据论文中的**表1**和**图2**，Pixel-to-4D方法在所有数据集和绝大多数指标上均取得了**最优性能**。

#### 主要定量结果（以Waymo数据集为例）：
| 指标 | Pixel-to-4D (Ours) | 最佳基线 (RealCam-I2V) | 提升幅度 / 优势 |
| :--- | :--- | :--- | :--- |
| **FVD ↓** | **30.9** | 47.8 | **显著降低 (35%)**，表明生成视频更真实、时序更一致。 |
| **PSNR ↑** | **19.4** | 17.0 | **显著提升**，像素级重建更准确。 |
| **SSIM ↑** | **0.553** | 0.424 | **显著提升**，结构保持更好。 |
| **LPIPS ↓** | **0.352** | 0.449 | **显著降低**，感知质量更高。 |
| **推理时间 ↓** | **5.9s** (KITTI) | 9.8s (KITTI) | **效率提升约40%**，得益于单次前向传播。 |

#### 核心结论：
1.  **全面领先**：在KITTI、Waymo、RealEstate10K和DL3DV-10K四个数据集上，Pixel-to-4D在PSNR、SSIM、LPIPS、FVD等关键指标上均**超越所有基线方法**。
2.  **高效推理**：由于采用**单次前向传播**生成4D高斯表示，无需迭代去噪或测试时优化，推理速度**显著快于**依赖视频扩散模型的基线（如RealCam-I2V）。
3.  **高质量的相机控制与动态预测**：定性结果（图2）显示，该方法能**精确遵循用户指定的相机轨迹**，同时为运动物体生成**自然的动态**，并保持**一致的深度渲染**，在视觉质量和3D一致性方面表现优异。

### 五、 消融实验验证
论文通过系统的消融实验验证了其核心设计的有效性：
- **动态建模的必要性**：移除速度/加速度建模（“w/o velocities”）会导致所有指标下降，证明动态建模对预测未来帧至关重要。
- **生成式速度预测**：使用VAE对不确定的对象速度进行**生成式采样**，优于确定性回归（“det. velocities”），能产生更清晰、更真实的运动。
- **多高斯表示**：每个像素预测**5个高斯**优于1个高斯，能有效填充场景空隙，提升渲染质量。
- **DINOv2特征融合**：引入DINOv2的语义特征能**改善深度估计和外观细节**（如图4中车辆更清晰），提升多项指标。

---
**总结**：Pixel-to-4D通过其新颖的、可一次性预测的4D动态高斯场景表示，在相机控制下的单图视频生成任务中，实现了**在视觉质量、时序一致性、几何准确性和推理效率方面的全面领先**，为可控视频生成提供了一个强大且高效的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.00678v1)
- [HTML 版本](https://arxiv.org/html/2601.00678v1)
