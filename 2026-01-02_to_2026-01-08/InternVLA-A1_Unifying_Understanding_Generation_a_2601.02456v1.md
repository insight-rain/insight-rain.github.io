# InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation

**相关性评分**: 9.0/10

**排名**: #4


---


## 基本信息

- **arXiv ID**: [2601.02456v1](https://arxiv.org/abs/2601.02456v1)
- **发布时间**: 2026-01-05T18:54:29Z
- **相关性评分**: 9.0/10
- **是否相关**: 是

## 作者

Junhao Cai, Zetao Cai, Jiafei Cao, Yilun Chen, Zeyu He, Lei Jiang, Hang Li, Hengjie Li, Yang Li, Yufei Liu, Yanan Lu, Qi Lv, Haoxiang Ma, Jiangmiao Pang, Yu Qiao, Zherui Qiu, Yanqing Shen, Xu Shi, Yang Tian, Bolun Wang, Hanqing Wang, Jiaheng Wang, Tai Wang, Xueyuan Wei, Chao Wu, Yiman Xie, Boyang Xing, Yuqiang Yang, Yuyin Yang, Qiaojun Yu, Feng Yuan, Jia Zeng, Jingjing Zhang, Shenghan Zhang, Shi Zhang, Zhuoma Zhaxi, Bowen Zhou, Yuanzhen Zhou, Yunsong Zhou, Hongrui Zhu, Yangkun Zhu, Yuchen Zhu

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

InternVLA-A1 是一个统一的视觉-语言-动作模型，通过混合专家架构和轻量化设计，在机器人操作任务中实现了语义理解与动态预测的协同，显著提升了推理效率和边缘部署潜力。

## 摘要

Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\% improvement in daily tasks and a 40\%-73.3\% boost in dynamic settings, such as conveyor belt sorting.

## 详细分析

## 论文摘要：InternVLA-A1: 统一理解、生成与行动以实现机器人操作

### 1. 研究背景和动机
当前主流的视觉-语言-行动模型虽在语义理解上表现出色，但普遍缺乏对物理世界动态的推理能力。而基于世界模型的视频预测方法，又常因语义基础薄弱和对预测误差敏感而受限。为了协同语义理解与动态预测能力，并提升模型在动态场景下的鲁棒性和泛化能力，本文提出了 **InternVLA-A1**。

### 2. 核心方法和技术创新
本文的核心创新在于**统一的模型架构**与**混合数据训练策略**：
- **统一架构**：提出了一种基于**混合专家Transformer**的统一架构，协调了三个专家模块：**理解专家**（基于MLLM进行场景语义理解）、**生成专家**（预测未来视觉状态以模拟任务动态）和**行动专家**（基于流匹配生成控制指令）。三者通过统一的掩码自注意力机制无缝交互，有效弥合了语义与物理动态之间的鸿沟。
- **高效生成设计**：为满足机器人实时控制需求，生成专家采用**压缩的连续VAE潜在表示**（将图像压缩为16个token）进行快速未来帧预测，实现了约13Hz的高频推理。
- **混合数据金字塔**：构建了分层的训练数据策略，结合了大规模、高多样性的**合成仿真数据**与高质量的**真实世界机器人演示数据**。这既利用了仿真的可扩展性覆盖长尾场景，又通过真实数据校准了物理交互，有效缩小了仿真到现实的差距。

### 3. 主要实验结果
在12个真实世界机器人任务和仿真基准上的评估表明，InternVLA-A1显著优于领先的基线模型（如 π₀ 和 GR00T N1.5）：
- **通用任务**：在10个日常操作任务上，InternVLA-A1 (3B) 平均成功率达 **75.1%**，较 π₀ 提升 **14.5%**。
- **动态任务**：在传送带分拣、动态抓取等高度动态场景中优势更为明显，例如在“动态食材抓取”任务中成功率高达 **93.3%**，较基线提升超过 **70个百分点**。
- **仿真基准**：在RoboTwin仿真基准的50个双手任务上，也全面超越了基线模型。
- **消融实验**：验证了预训练、生成专家和混合数据策略均为模型性能提升的关键因素。

### 4. 研究意义和价值
InternVLA-A1通过**架构与数据的协同创新**，为构建具备**动态场景适应能力**的通用机器人策略提供了重要范例。其统一理解-生成-行动的框架，为实现更智能、更鲁棒的具身智能系统指明了方向。同时，所验证的混合仿真-真实数据训练范式，为以可扩展、低成本的方式获取多样化机器人技能数据提供了可行路径，具有重要的实际应用价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：InternVLA-A1

### **一、 核心要解决的问题**
论文旨在解决当前**视觉-语言-动作（VLA）模型**在机器人操作任务中的两大核心瓶颈：
1.  **语义理解与物理动态推理的割裂**：现有基于多模态大语言模型（MLLM）的VLA模型（如 π₀, GR00T）擅长语义理解，但**缺乏对物理世界动态（如动量、惯性、接触力学）的推理能力**，导致在动态场景（如传送带分拣）中表现不佳。
2.  **训练数据的规模、多样性与真实性的矛盾**：单纯依赖大规模真实机器人数据收集**成本高昂、难以覆盖长尾场景**；而仅使用仿真数据则存在**“仿真到现实”的鸿沟**，尤其在接触丰富的动态任务中。

### **二、 核心创新点**
论文通过**架构创新**与**数据策略创新**的协同，提出了统一的解决方案。

#### **1. 模型架构创新：统一的理解-生成-动作框架**
- **创新设计**：提出了一个基于 **“专家混合Transformer”** 的统一架构，集成了三个功能明确的专家模块：
    - **理解专家**：基于成熟的MLLM（InternVL3/Qwen3-VL），负责对场景和指令进行**高级语义理解**。
    - **生成专家**：作为一个**轻量化的世界模型**，负责预测未来的视觉状态（视觉前瞻）。它使用高效的Cosmos VAE编码器将图像压缩为连续潜变量，并通过卷积大幅压缩token数量，实现了**实时推理（~13 Hz）**。
    - **动作专家**：基于**流匹配**技术，综合理解专家的语义上下文和生成专家预测的动态信息，生成精确的机器人控制指令。
- **统一机制**：三个专家通过**统一的掩码自注意力机制**进行交互，信息流严格遵循 **理解 → 生成 → 动作** 的顺序，确保了语义推理与动态预测的紧密耦合。

#### **2. 数据策略创新：混合仿真-真实数据金字塔**
- **创新策略**：构建了一个分层的 **“数据金字塔”** 训练范式：
    - **基座（预训练）**：混合使用**大规模仿真数据**（InternData-A1， 超过533M帧）和**开源真实机器人数据**（AgiBot-World）。仿真数据提供**无限的场景多样性和可控的轨迹**，真实数据提供**物理交互的真实性**。
    - **顶端（后训练）**：使用**小规模、高质量、任务特定的真实机器人数据**进行微调，以校准模型在目标部署环境中的表现，有效弥合仿真到现实的差距。
- **训练优化**：提出了 **“负载均衡并行训练”** 方法，高效地混合处理异构的大规模数据集，解决了内存和I/O瓶颈。

### **三、 解决方案的运作方式**
1.  **输入**：模型接收当前及历史的多视角图像、语言指令和本体感知信息。
2.  **处理流程**：
    - **理解专家** 编码图像和文本，生成语义上下文前缀。
    - **生成专家** 接收该前缀及压缩后的历史图像潜变量，**预测未来时刻的图像潜变量**，实现对任务动态的“想象”。
    - **动作专家** 同时接收语义前缀和预测的未来动态信息，通过流匹配去噪过程，**生成未来一段时间内的机器人动作序列**。
3.  **训练目标**：联合优化两个损失函数——**视觉前瞻生成损失**（让预测更准）和**基于流匹配的动作预测损失**（让动作更优），使模型内化动作与环境动态之间的因果关系。

### **四、 实际价值与效果**
- **性能提升**：在12个真实世界任务和仿真基准测试中，InternVLA-A1显著超越了 π₀ 和 GR00T N1.5 等领先模型。
    - **日常任务**：平均成功率提升 **14.5%**。
    - **动态任务**：在传送带快递分拣和动态食材抓取等任务中，提升幅度高达 **40% 至 73.3%**。
- **效率与泛化**：更小的参数量（2B）模型即可超越更大的基线模型（3.3B），体现了架构和数据策略的高效性。混合数据训练策略被证明能有效结合仿真数据的多样性和真实数据的物理保真度，提升了模型对**场景变化和动态环境的鲁棒性**。

**总结**：InternVLA-A1 的核心创新在于**创造性地将语义理解模型与世界模型的预测能力在统一架构中深度融合**，并辅以**仿真与真实数据混合的规模化训练策略**，从而解决了现有VLA模型在物理动态推理和场景泛化方面的关键缺陷，为实现更智能、更鲁棒的通用机器人操作策略提供了新的路径。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决现有视觉-语言-动作（VLA）模型在机器人操控中**语义理解与物理动态预测能力割裂**的核心问题，这导致模型在动态场景（如传送带分拣）中表现脆弱。为此，论文提出了 **InternVLA-A1** 模型，其核心创新在于采用一个**统一的混合专家Transformer架构**，将负责场景理解的“理解专家”、进行视觉前瞻的“生成专家”和执行控制的“动作专家”无缝集成，并通过混合合成-真实数据（超过5.33亿帧）的大规模预训练来协同提升模型的语义推理与动态适应能力。实验表明，该模型在12项真实世界任务和仿真基准测试中显著超越了如π₀和GR00T N1.5等领先模型，在常规任务上平均成功率提升14.5%，在动态任务上提升幅度高达40%至73.3%，验证了其统一框架在实现鲁棒、动态感知的机器人操控方面的有效性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation》针对现有视觉-语言-动作（VLA）模型的局限性，提出了一套综合性的解决方案。其核心创新点主要体现在**模型架构**和**数据策略**两个层面，旨在协同提升模型的语义理解能力、动态预测能力和动作执行能力。

以下是其相对于已有工作的明确创新点：

### 1. 统一的“理解-生成-动作”混合专家架构
- **改进/不同之处**：
    - **以往方法**：主流VLA模型（如π₀、GR00T）通常基于多模态大语言模型（MLLM），将视觉信息映射到文本特征空间。它们擅长语义理解，但**缺乏对物理世界动态（如动量、惯性、接触力学）的推理能力**。另一些工作引入世界模型（如视频预测模型VPP、Genie Envisioner）来提供前瞻性，但这些模型**语义基础薄弱**且对预测误差敏感。
    - **本文方法**：提出一个统一的**混合专家Transformer（Mixture-of-Transformers, MoT）** 架构，在一个框架内集成了三个专家模块：
        1.  **理解专家**：基于成熟的MLLM（InternVL3/Qwen3-VL），负责语义场景理解。
        2.  **生成专家**：负责**视觉前瞻生成**，预测未来帧的潜在表示。
        3.  **动作专家**：基于流匹配（Flow Matching）技术，综合语义上下文和预测的动态来生成控制指令。
    - 三个专家通过**统一的掩码自注意力机制**进行交互，信息流严格遵循 **理解 → 生成 → 动作** 的顺序。
- **解决的问题/带来的优势**：
    - **解决了语义与动态的割裂问题**：该架构首次在VLA模型中**紧密耦合了高级语义推理和低级物理动态预测**。生成专家充当了“世界模型”，让模型能够内化动作与环境状态之间的因果关系。
    - **提升了在动态场景中的鲁棒性**：模型不再仅仅是“感知-反应”的映射，而是能够“预见”动作后果，从而在动态环境（如传送带分拣）中做出更明智的决策。实验证明，这在动态任务上带来了40%至73.3%的性能提升。
    - **保持了高效的实时推理**：通过创新的设计（如对VAE潜在特征进行空间压缩），生成专家能够实现快速预测，使整个模型在GPU上达到约13Hz的推理频率，满足实时控制需求。

### 2. 混合合成-真实数据的层级化数据金字塔策略
- **改进/不同之处**：
    - **以往方法**：现有高性能VLA模型严重依赖大规模、异构的真实机器人演示数据（如Open X-Embodiment）。然而，纯真实数据收集**成本高昂、难以覆盖长尾场景变化**，且不同机器人形态的数据异质性给联合训练带来挑战。
    - **本文方法**：提出了一个**层级化的数据金字塔**训练策略：
        1.  **预训练基础层**：混合使用**大规模合成数据（InternData-A1）** 和**开源真实数据（AgiBot-World）**。
        2.  **后训练校准层**：使用**小规模、任务特定的真实机器人数据**进行微调。
    - 合成数据InternData-A1提供了前所未有的**多样性**（4种机器人形态、18种技能、70项任务、227个场景），而真实数据提供了**物理保真度**。
- **解决的问题/带来的优势**：
    - **解决了数据规模与多样性的瓶颈**：利用仿真的可扩展性，以极低成本生成了海量、多样化的训练数据，有效覆盖了长尾场景配置。
    - **缓解了仿真到现实的鸿沟**：通过“合成数据提供多样性，真实数据提供物理锚点”的混合策略，既利用了仿真的优势，又用真实数据纠正了仿真在传感和物理交互方面的偏差。消融实验证明，混合策略在真实世界任务上取得了最佳效果。
    - **证明了合成数据的有效性**：论文明确指出，**仅使用合成数据（InternData-A1）预训练的模型，其性能已可媲美使用强大闭源真实数据（π-dataset）训练的模型**，这为降低机器人学习的数据成本提供了有力证据。

### 3. 针对高效视觉生成的特定设计
- **改进/不同之处**：
    - **以往方法**：用于机器人控制的视频预测模型（如扩散模型）通常**推理速度慢**（如SANA-Sprint需0.16秒/帧），无法满足高频实时控制（>6Hz）的要求。
    - **本文方法**：
        - 生成专家使用**Cosmos CI 8×8 连续VAE分词器**对图像进行编码，获得适合生成的结构化潜在表示。
        - 引入**空间压缩**：将32×32的潜在特征图通过卷积层压缩至4×4（每图仅用16个token表示），大幅减少了序列长度。
        - 预测目标是在这个压缩的潜在空间中回归未来帧的表示，而非直接生成像素级图像。
- **解决的问题/带来的优势**：
    - **解决了实时性难题**：上述设计在保证一定预测质量的前提下，**极大提升了视觉前瞻模块的推理速度**，使其能够集成到需要13Hz控制频率的端到端VLA模型中。
    - **实现了精度与效率的平衡**：虽然牺牲了部分图像生成的保真度（论文承认这是当前局限），但换取了对于机器人控制至关重要的实时性能，使“动态预测指导动作”的闭环得以实现。

### 4. 负载均衡的并行训练策略
- **改进/不同之处**：
    - **以往方法**：在混合多个异构、大小不一的数据集进行大规模训练时，简单地在每个worker上实例化全部数据集会导致**内存溢出、I/O争用**，并因数据集遍历速率不同而产生**隐式的数据重新加权问题**。
    - **本文方法**：提出了**负载均衡并行训练（Load-balanced Parallel Training, LPT）**。采用一种贪心算法，根据数据集大小（帧数）的代理指标，将不同数据集**分配**给不同的worker，使得每个worker的负载大致均匀。
- **解决的问题/带来的优势**：
    - **提升了大尺度训练的稳健性和效率**：减少了每个worker的内存压力，缓解了I/O瓶颈。
    - **确保了训练采样的统计合理性**：避免了因数据集大小差异而导致的训练偏差，使模型能从所有数据源中均匀学习，这对于混合差异巨大的合成和真实数据集至关重要。

**总结**：InternVLA-A1的核心创新在于通过**统一的MoT架构**在算法层面融合了语义与动态，以及通过**混合数据金字塔**在数据层面融合了仿真与真实。这两大创新相辅相成，共同解决了现有VLA模型在**动态环境适应性差**和**依赖昂贵真实数据**两大关键瓶颈，从而在通用任务和极具挑战的动态任务上都实现了显著的性能突破。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过广泛的实验，在真实世界任务和仿真基准上验证了InternVLA-A1的有效性，证明了其在统一理解、生成与行动方面的优越性。

### 1. 评估数据集与任务
- **真实世界任务**：共12个任务，分为两类：
    - **通用任务**：10个，涵盖灵巧操作（如`Unscrew Cap`, `Zip Bag`, `Sort Parts`）和常规操作（如`Make Sandwich`, `Operate Oven`, `Sort Rubbish`）。
    - **动态场景专项任务**：2个，旨在测试模型在动态环境中的推理能力：
        - **快速分拣任务**：在传送带上抓取并翻转包裹。
        - **动态食材抓取任务**：两台机器人协调抓取移动中的食材以制作三明治。
- **仿真基准**：使用**RoboTwin 2.0**基准，包含50个双手操作任务，评估分为**Easy**（与演示环境相同）和**Hard**（应用了域随机化）两种设置。

### 2. 评价指标
- **主要指标**：**任务平均成功率**。每个任务在30种预定义设置（如物体位置、场景初始化）下各进行一次试验，计算所有试验的平均成功率。
- **仿真评估**：每个任务使用100次演示进行微调，并在两种设置下各进行100次评估试验，报告成功率。

### 3. 对比的基线方法
论文与当前领先的视觉-语言-动作模型进行了全面对比：
- **π₀ (3.3B)**：基于MLLM和流匹配的通用策略模型。
- **GR00T N1.5 (3B)**：采用双系统架构（推理与扩散动作生成解耦）的模型。

### 4. 关键性能提升与结论

#### a) 通用任务性能
- **InternVLA-A1 (2B)**：平均成功率为**64.7%**，已超越更大的基线模型 **π₀ (3.3B， 60.6%)**，证明了其架构和数据策略的高效性。
- **InternVLA-A1 (3B)**：平均成功率达到**75.1%**，相比 **π₀ 提升了14.5个百分点**。
    - 在复杂任务中优势显著：例如`Make Sandwich`任务达到93.3%（vs. π₀ 的66.7%），`Operate Oven`达到86.7%（vs. 73.3%）。
    - 在灵巧任务`Zip Bag`上达到73.3%（vs. π₀ 的40.0%），展示了优异的细粒度控制能力。

#### b) 动态场景专项任务性能
- 基线模型（π₀ 和 GR00T N1.5）在动态场景中表现挣扎，成功率均未超过40%。
- **InternVLA-A1 (3B)** 表现出卓越的鲁棒性：
    - **快速分拣任务**：成功率 **80.0%**，相比基线提升至少 **40个百分点**。
    - **动态食材抓取任务**：成功率 **93.3%**，相比基线提升超过 **73个百分点**。
- **结论**：InternVLA-A1整合的**生成专家（视觉前瞻）** 使其能够有效推理场景动态变化，从而在需要预测未来状态的任务中取得压倒性优势。

#### c) 仿真基准性能
- 在RoboTwin 2.0的50个任务上，**InternVLA-A1 (3B)** 在Easy和Hard设置下的平均成功率分别为**65.0%** 和 **25.4%**。
- 相比 **π₀**（54.5% 和 19.8%），分别提升了**10.5个百分点**和**5.6个百分点**，证明了模型在仿真环境中的强泛化能力。

#### d) 消融实验结论
1.  **预训练的有效性**：移除预训练阶段，模型平均性能**下降51.6%**（从77.0%降至25.4%），证明了大规模混合数据预训练提供了至关重要的操作技能先验。
2.  **生成专家的有效性**：移除生成专家后，模型在12个任务中的11个上性能下降，平均成功率从**77.0%降至57.6%**，直接验证了视觉前瞻模块对于提升性能的关键作用。
3.  **预训练数据策略**：混合使用合成数据（InternData-A1）和真实数据（AgiBot-World）的策略取得了最佳效果。纯合成数据预训练模型在仿真基准上表现最好，而**混合数据**策略在真实世界任务（尤其是细粒度操作如`Sort Parts`）上带来了显著提升（最高达20%），证明了该策略能有效协同仿真的多样性与真实数据的物理保真度。

### 总结
InternVLA-A1通过其**统一的MoE架构**和**混合合成-真实数据预训练策略**，在通用操作、尤其是**高动态场景**的机器人操作任务上，显著超越了当前最先进的VLA基线模型。其核心创新——将语义理解与动态预测能力紧密结合——被实验证明是提升机器人策略在变化环境中鲁棒性和性能的关键。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.02456v1)
- [HTML 版本](https://arxiv.org/html/2601.02456v1)
