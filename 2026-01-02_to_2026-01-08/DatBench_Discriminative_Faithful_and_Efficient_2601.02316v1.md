# DatBench: Discriminative, Faithful, and Efficient VLM Evaluations

**相关性评分**: 6.0/10

**排名**: #76


---


## 基本信息

- **arXiv ID**: [2601.02316v1](https://arxiv.org/abs/2601.02316v1)
- **发布时间**: 2026-01-05T18:07:51Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Siddharth Joshi, Haoli Yin, Rishabh Adiga, Ricardo Monti, Aldo Carranza, Alex Fang, Alvin Deng, Amro Abbas, Brett Larsen, Cody Blakeney, Darren Teh, David Schwab, Fan Pan, Haakon Mongstad, Jack Urbanek, Jason Lee, Jason Telanoff, Josh Wills, Kaleigh Mentzer, Luke Merrick, Parth Doshi, Paul Burstein, Pratyush Maini, Scott Loftin, Spandan Das, Tony Jiang, Vineeth Dorna, Zhengping Wang, Bogdan Gaza, Ari Morcos, Matthew Leavitt

## 关键词

Vision-Language Models (VLMs), Evaluation Efficiency, Computational Cost, Discriminative Power, Dataset Filtering, Inference Acceleration

## 一句话总结

这篇论文提出DatBench，一个旨在提高视觉语言模型评估的忠实性、区分性和效率的基准套件，通过数据转换和过滤减少计算负担，间接促进推理效率，但与机器人或边缘部署的直接关联较弱。

## 摘要

Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.

## 详细分析

## 论文《DatBench：判别性、忠实且高效的视觉语言模型评估》详细摘要

### 1. 研究背景和动机
随着视觉语言模型（VLM）的快速发展，其评估方法却相对滞后。现有基准测试普遍存在**忠实性**（无法反映真实多模态推理）、**判别性**（难以区分模型能力差异）和**效率**（计算成本过高）三大问题。具体表现为：多项选择题（MCQ）格式鼓励猜测且与真实生成式应用脱节；大量问题无需图像即可通过语言先验“盲答”；数据集中存在错误标注或模糊样本；全面评估的计算开销已占模型开发总成本的近20%。这些问题导致评估信号失真，可能误导研究方向。

### 2. 核心方法和技术创新
本文提出 **DatBench** 框架，将评估设计视为一个**数据治理问题**，而非从零构建新基准。其核心创新在于一个四阶段处理流程：
- **MCQ转换**：将多项选择题转换为开放式生成任务，或采用循环评估以消除猜测带来的性能虚高。
- **盲答过滤**：通过让模型在无图像条件下回答问题，系统性识别并移除仅靠语言先验即可解决的样本（在某些数据集中占比高达70%）。
- **质量过滤**：采用两阶段VLM作为评判员的方法，识别并剔除标注错误、问题模糊或图像分辨率不足的样本（在某些数据集中剔除率高达42%）。
- **判别性子集选择**：受项目反应理论启发，使用点二列相关系数（`r_pb`）量化每个样本的判别能力，并优先选择高判别性样本构成高效评估子集。

基于此，作者发布了两个版本：**DatBench**（高效子集，平均加速13倍）和**DatBench-Full**（经过清洗的完整高质量样本集），涵盖图表理解、文档理解、场景OCR、数学逻辑、空间推理等九大VLM核心能力。

### 3. 主要实验结果
- **有效性验证**：经过处理的DatBench基准显著提升了判别性。例如，在通用VQA任务上，模型得分范围从原始的65-80%扩展至10-65%，放大了能力差异。
- **效率提升**：DatBench子集仅需约40%的样本即可达到原始数据集90%以上的判别能力，实现了高达50倍（平均13倍）的评估加速。
- **模型诊断洞察**：利用DatBench分析27个前沿VLM，揭示了关键发现：
    - **推理与感知的权衡**：模型在高级推理（如数学）和低级感知（如OCR、空间任务）上存在负相关，表明当前训练范式存在内在冲突。
    - **“过度思考”惩罚**：对于“思考型”模型，增加推理时长（Token数）能提升数学能力，但会严重损害OCR等感知任务性能，且错误答案消耗的Token数约为标准模型的14倍。
    - **语言先验的干扰**：不同任务对视觉输入的依赖度差异巨大，数学和空间推理任务严重受语言先验影响，而计数和定位任务则更忠实于视觉理解。

### 4. 研究意义和价值
本研究为VLM评估提供了系统性的方法论和高质量的数据集资源。其核心价值在于：
- **推动评估规范化**：提出了忠实、判别、高效三大评估原则，并提供了可操作的实现路径。
- **大幅降低评估成本**：通过数据治理和智能采样，使频繁、全面的模型评估变得可行，加速研究迭代。
- **揭示模型本质**：清洗后的基准能更真实地反映模型的多模态能力，有助于发现模型架构和训练中的根本性局限（如推理-感知权衡）。
- **促进可持续发展**：为应对未来更大规模VLM的评估挑战，提供了一种可扩展、可持续的解决方案。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文标题**
**DatBench: Discriminative, Faithful, and Efficient VLM Evaluations**

### **核心问题**
当前视觉语言模型（VLM）的评估体系存在严重缺陷，导致：
1.  **评估失真**：无法真实反映模型的多模态能力。
2.  **区分度不足**：难以有效区分不同质量的模型。
3.  **计算成本过高**：评估消耗的计算资源已占模型开发总成本的近20%，成为瓶颈。

### **核心创新点**
论文没有提出全新的评估数据集，而是**开创性地将评估基准的构建视为一个数据治理问题**。通过对现有主流评测集进行系统性**转化、过滤和精选**，构建出更忠实、更具区分度且高效的评估套件。其方法论创新体现在明确提出了评估应满足的三个核心原则，并针对性地设计了解决方案。

### **解决方案（四阶段数据治理流程）**
论文通过一个四阶段流程来系统性地解决上述问题：

1.  **解决多选题（MCQ）的失真问题**
    *   **问题**：MCQ格式存在猜测概率，且与模型实际部署时的生成式场景不符，高估模型能力。
    *   **解决方案**：
        *   **生成式转化**：将MCQ改为开放式生成任务，用LLM作为评判员进行语义匹配。实验显示，此举可使模型在AI2D数据集上的平均准确率从77.56%骤降至40.53%。
        *   **循环评估**：对于无法转化的MCQ，通过轮换选项顺序并要求模型在所有轮次中都答对，来消除猜测和位置偏差。

2.  **剔除“盲解”样本**
    *   **问题**：大量评估样本仅凭语言先验（无需看图）即可正确回答，无法衡量真正的视觉理解能力。例如，VQA-v2中高达70%的样本可被“盲解”。
    *   **解决方案**：让所有评估模型在**无图像输入**的情况下回答问题，根据任务类型（生成式/MCQ）设定阈值（τ），剔除被超过阈值数量模型“盲解”的样本。

3.  **过滤低质量与错误标注样本**
    *   **问题**：数据集中存在标注错误、问题歧义、图像分辨率过低等问题，构成评估噪声。
    *   **解决方案**：采用**两阶段VLM作为评判员**的过滤流程：
        1.  标记所有评估模型都回答错误的样本。
        2.  使用强大的VLM（如GPT-5.2）作为评判员，检查这些样本，剔除标注错误、歧义或图像质量问题的样本。例如，在空间推理任务中过滤掉了42.07%的样本。

4.  **基于区分度的子集选择以实现高效评估**
    *   **问题**：全面评估计算成本过高，而随机采样或仅保持排名相关性的子集选择方法无法保证样本的区分度。
    *   **解决方案**：采用**点二列相关（rpb）** 作为衡量单个样本区分度的指标。优先选择rpb值高的样本构成子集。这种方法能在仅使用40%数据的情况下，达到原数据集90%的区分能力，同时实现**平均13倍（最高50倍）的评估加速**。

### **实际产出与价值**
1.  **发布两个基准套件**：
    *   **`DatBench`**：高效子集，用于快速迭代开发，在保持高区分度的前提下大幅降低计算成本。
    *   **`DatBench-Full`**：经过前三阶段过滤后的全部高质量样本集合，用于最终报告和深入分析。
2.  **揭示VLM能力图谱与病理**：利用清洗后的数据，论文深入分析了27个前沿VLM，发现了关键洞察：
    *   **推理与感知的权衡**：模型在图表/数学等推理任务与OCR/空间等感知任务上存在负相关，揭示了当前训练范式的内在冲突。
    *   **“过度思考”惩罚**：“思考”模型（增加推理步数）虽能提升数学能力，但会严重损害OCR等感知任务性能，且错误答案的生成代价极高。
    *   **语言先验的误导性**：不同任务对视觉输入的依赖程度（“视觉增量”）差异巨大，数学等任务严重依赖语言先验，凸显了过滤“盲解”样本的必要性。

### **总结**
**DatBench** 的核心贡献在于**提供了一套系统化、可操作的方法论来“治理”和“优化”现有评估数据**，而非从零创造新数据。它通过聚焦于**样本层面的质量与区分度**，将VLM评估从**粗糙、嘈杂、昂贵**的现状，转向**精准、可靠、高效**的新范式，为快速发展的VLM领域提供了可持续的评估基础设施。其发布的基准和开源代码具有直接的实用价值，而其揭示的模型能力边界与缺陷则为后续研究指明了方向。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决当前视觉语言模型（VLM）评估中存在的**三大核心问题**：评估结果**不忠实**（无法反映真实多模态能力与下游应用）、**区分度低**（难以有效区分不同质量的模型）以及**计算效率低下**（评估成本过高）。为此，论文提出了一个名为 **DatBench** 的评估框架，其核心方法不是创建新基准，而是对现有33个数据集进行系统性的**数据治理**，具体包括：1）将选择题转换为生成式任务以消除猜测收益；2）过滤掉无需图像即可回答的“盲解”样本；3）利用VLM作为裁判，剔除标注错误、模糊或低分辨率的噪声样本；4）基于项目区分度（点二列相关）筛选高信息密度的样本子集，以最大化评估效率。

最终，该方法构建了两个评估套件：**DatBench**（高效子集）和**DatBench-Full**（完整高质量集）。主要效果与结论是：**DatBench**在保持与原基准高度一致的模型排序能力的同时，实现了**平均13倍（最高50倍）的评估加速**，并显著提升了评估的忠实度与区分度。例如，生成式转换揭示了高达35%的隐藏能力差距，而过滤去除了高达70%的盲解样本。分析还揭示了当前VLM存在的“推理与感知权衡”、“思维模型过思考惩罚”等结构性缺陷。这项工作为VLM提供了一条**更严谨、更可持续的评估路径**。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《DatBench: Discriminative, Faithful, and Efficient VLM Evaluations》针对视觉语言模型评估中存在的关键问题，提出了一套系统性的解决方案。其核心创新点在于**将评估基准的构建视为一个数据治理问题**，而非简单地收集新数据。以下是其相对于已有工作的明确创新点：

---

### 1. **提出了评估基准的三大核心原则（Desiderata），并以此系统性地诊断和修复现有基准的缺陷**
   - **改进之处**：以往工作虽然指出了VLM评估中的各种问题（如语言先验、标注噪声），但往往是零散的。本文首次明确提出了**忠实性、可区分性、高效性**这三个原则，并以此作为统一框架来系统性地分析和改造现有基准。
   - **解决的问题与优势**：
     - **系统化诊断**：将散乱的问题（如MCQ缺陷、盲解问题、标注错误）统一到三大原则下，使得评估基准的优化有了明确的目标和衡量标准。
     - **指导实践**：为未来评估基准的设计提供了清晰、可操作的设计准则，避免了“头痛医头、脚痛医脚”的局部修补。

### 2. **开发了四阶段数据治理流水线，对现有基准进行“改造”而非“抛弃”**
   - **改进之处**：不同于大多数工作选择构建全新的基准，本文创新性地提出对现有大量但嘈杂的基准进行**转化、过滤和精选**。这包括：
     - **MCQ转生成式任务**：将选择题转化为开放式生成任务，或引入循环评估。
     - **过滤盲解题**：通过“无图评估”识别并移除仅靠语言先验就能回答的样本。
     - **VLM作为裁判进行质量过滤**：使用强大的VLM（如GPT-5.2）作为裁判，识别并移除标注错误、模糊或低分辨率的样本。
     - **基于项目区分度的子集选择**：使用点二列相关（`r_pb`）量化每个样本的区分能力，并据此选择高区分度的样本子集。
   - **解决的问题与优势**：
     - **成本效益**：充分利用了现有社区投入大量资源构建的基准，避免了重复劳动和资源浪费。
     - **提升质量**：直接针对数据源头的噪声进行清洗，显著提升了评估的**忠实性**（真正测量多模态能力）和**可区分性**（能更好地区分模型能力差异）。
     - **保证效率**：通过精选高区分度样本，用极少量的数据（平均仅需40%的样本）就能达到甚至超过全数据集的区分能力，实现了**高达13倍（最高50倍）的平均加速**。

### 3. **引入了基于“项目区分度”的子集选择方法，超越了简单的排名保持**
   - **改进之处**：
     - **与IRT方法的区别**：以往使用项目反应理论（IRT）的方法需要大量模型在大量样本上的响应矩阵才能稳定拟合参数，不适用于模型数量有限的场景。本文避开了复杂的IRT参数拟合，直接使用**点二列相关（`r_pb`）** 作为样本区分度的稳健、无超参数代理指标。
     - **与排名相关方法的区别**：以往方法（如保持斯皮尔曼等级相关）只关注模型排名的整体保持。本文指出，**排名相关会快速饱和**，即使随机子集也能保持排名，但这无法保证每个样本都具有高区分度。本文方法**直接最大化总区分度**，确保每个入选样本都能有效区分强弱模型。
   - **解决的问题与优势**：
     - **实用性与稳定性**：`r_pb`计算简单，不需要大量模型数据，更适合当前研究现状。
     - **高分辨率测量**：得到的子集不仅能保持排名，还能提供更高的“测量分辨率”，能更敏感地捕捉模型间的细微能力差异，对未来未见模型也更具泛化性。
     - **主动剔除噪声**：该方法能主动识别并剔除具有负区分度（弱模型反而答对）的“异常样本”，这些样本通常是因数据泄露或虚假相关性产生的噪声。

### 4. **构建并发布了两个分层的基准套件：`DatBench` 和 `DatBench-Full`**
   - **改进之处**：以往基准通常只有一个版本。本文根据使用场景发布了两个版本：
     - **`DatBench`（高效子集）**：经过四阶段流水线精选出的高区分度样本子集，用于快速迭代和开发，计算成本极低。
     - **`DatBench-Full`（完整高质量集）**：经过前三阶段（转化、盲解过滤、质量过滤）清洗后的全部高质量样本，用于最终报告和深入分析。
   - **解决的问题与优势**：
     - **兼顾效率与深度**：满足了研发过程中不同阶段的需求。`DatBench`极大降低了评估的算力门槛（解决了论文中指出的“评估占用近20%开发算力”的问题），使频繁评估成为可能；`DatBench-Full`则为全面的能力剖析提供了基础。
     - **标准化与可复现性**：提供了一个经过严格清洗、标准化的评估套件，有助于社区进行公平、一致的比较。

### 5. **利用清洗后的基准揭示了VLM此前被掩盖的行为模式（病理诊断）**
   - **改进之处**：本文不仅是构建工具，还利用`DatBench`对27个前沿VLM进行了深入的实证分析，揭示了在嘈杂基准下难以观察到的结构性模式。
   - **解决的问题与优势**：
     - **发现新洞见**：
       1. **推理与感知的权衡**：发现了模型在**高层级推理**（如图表、数学）和**低层级感知**（如OCR、空间）任务之间存在负相关，表明当前训练范式存在内在冲突。
       2. **“过度思考”惩罚**：发现“思维链”模型在测试时增加计算，虽然提升了推理任务性能，但**严重损害了感知任务性能**，且错误答案消耗的token量是标准模型的14倍，揭示了计算分配的低效性。
       3. **语言先验的扭曲效应**：量化了不同能力对视觉输入的依赖程度（视觉增量 `V_Δ`），发现数学、空间推理等任务严重依赖语言先验，而计数、定位等任务则真正需要视觉信息。
     - **验证基准价值**：这些深刻洞见反过来证明了`DatBench`的高质量和诊断能力，它能够揭示模型真实、细微的能力边界与缺陷。

---

**总结**：本文的核心创新在于**方法论层面的转变**——将评估从“数据收集竞赛”转向“数据治理科学”。通过提出原则框架、设计系统化治理流水线、创新样本选择指标、发布分层基准并开展深度诊断，它**同时解决了VLM评估在忠实性、可区分性和效率上的三大痛点**，为社区提供了一个既严谨又可持续的评估实践路径。其工作不仅是一套工具，更为如何科学地衡量AI系统能力提供了重要范式参考。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果分析

论文《DatBench: Discriminative, Faithful, and Efficient VLM Evaluations》的核心贡献并非提出一个新的VLM模型，而是构建了一套**高质量、高判别力且高效**的视觉语言模型评估基准。其实验与评估旨在验证**DatBench**和**DatBench-Full**这两个基准的有效性，并利用它们诊断现有VLM的病理特征。

### 一、 评估基准的构建与效果

**1. 核心目标：**
通过系统性的数据转换与过滤，解决现有VLM评估基准存在的四大问题：多选题（MCQ）格式的噪声与不忠实性、可“盲解”问题、标注错误/模糊样本以及高昂的计算成本。最终目标是提升评估的**忠实性**、**判别力**和**效率**。

**2. 使用的主要数据集：**
论文从现有研究中精心挑选并整合了**33个数据集**，覆盖了9大VLM核心能力（Capability Pillars）：
- **图表理解**：ChartQA, ChartQA Pro, CharXiv, InfoVQA
- **文档理解**：CC-OCR, OCR-VQA, OCRBench-V2, DocVQA
- **场景OCR**：TextVQA, MME-RW (OCR), CC-OCR (Multi-Scene)
- **数学与逻辑**：MathVista, Mathverse, MathVision, LogicVista
- **空间推理**：RealWorldQA, MME-RW (Autonomous Driving/Video Monitoring)
- **指代定位**：RefCOCO系列, Pixmo-Point
- **计数**：CountBench, TallyQA
- **图表与表格**：AI2D, MME-RW (Diagram/Table)
- **通用VQA**：MMMU-Pro, MMBench, VQA-v2

**3. 关键评价指标：**
- **判别力**：通过**点二列相关系数** 量化每个评估样本区分模型强弱的能力。高`r_pb`意味着强模型总能答对，弱模型总答错。
- **效率**：评估所需的总计算时间（H100 GPU小时数）和**加速比**。
- **忠实性**：通过“盲解”过滤比例、标注纠错比例等间接衡量。
- **模型排名稳定性**：使用**斯皮尔曼等级相关系数** 对比使用完整数据集与使用**DatBench**子集得出的模型排名一致性。
- **性能分布**：模型在原始基准和**DatBench**上的准确率对比，观察性能范围的“拉伸”效应。

**4. 与基线方法的对比：**
论文的主要对比对象是**原始、未经处理的评估数据集**。此外，在子集选择策略上，与**随机采样**方法进行了对比。
- **vs. 原始基准**：
    - **性能去通胀**：在AI2D数据集上，将MCQ转换为生成式任务后，平均准确率从77.56%**暴跌至40.53%**，最强模型的性能下降近35个百分点，揭示了MCQ格式带来的巨大性能泡沫。
    - **盲解过滤**：在通用VQA能力（如VQA-v2）中，过滤掉了**高达72.07%** 可仅凭语言先验回答的样本。
    - **质量过滤**：在空间推理能力中，因标注模糊或图像分辨率过低，过滤掉了**42.07%** 的样本。
- **vs. 随机采样**：
    - **判别力**：在相同样本预算下（如保留40%的数据），**DatBench**的判别力（`r_pb`之和）能达到完整数据集的90%，而随机采样仅能达到不到一半（见图7a）。
    - **排名相关性**：虽然随机采样也能较快达到高排名相关性（斯皮尔曼`ρ`），但这仅能区分模型能力的粗粒度层级（如1B vs. 8B）。**DatBench**通过选择高判别力样本，能提供更精细的区分能力，对前沿模型的微小进步更敏感。

**5. 关键性能提升与结论：**
- **效率大幅提升**：发布的**DatBench**（高效子集）在保持与完整数据集相近判别力的同时，实现了**平均13倍（最高50倍）的评估加速**。
- **判别力增强**：经过清洗和筛选后，**DatBench**显著“拉伸”了模型的性能分布。例如，在通用能力上，模型准确率范围从原始的65–80%扩展至10–65%，**有效区分度扩大了近4倍**，使得模型间的微小能力差异变得可测量。
- **评估更忠实**：通过剔除盲解和错误标注样本，确保评估真正衡量的是**多模态推理能力**，而非语言模型的先验知识或对噪声数据的过拟合。

### 二、 利用DatBench诊断VLM病理

论文利用清洗后的**DatBench**对27个（1B-10B参数）前沿VLM进行了深入分析，得出了多个重要结论：

1.  **“推理 vs. 感知”的权衡**：相关性分析显示，**图表理解、数学、通用VQA**构成高度相关的“推理集群”；而**指代定位**与**文档理解、OCR**等文本密集型任务呈**负相关**。这表明当前VLM训练范式存在**高层语义推理与低层感知保真度之间的内在冲突**。

2.  **模型能力谱系**：模型可分为“感知专家”（如GLM-4.1V-9B，擅长图表和空间任务）和“推理专家”（如R-4B，数学得分最高但空间得分最低），**均衡的通才模型非常罕见**。

3.  **“过度思考”惩罚**：对比“思考”模型与其标准指令版本发现，增加推理时计算是一把双刃剑。虽然对**数学**和**图表**任务有帮助（分别提升约36.8%和10.8%），但却严重损害了**OCR**和**文档理解**等感知任务的性能（分别下降约53.5%和47.8%）。更糟糕的是，错误的“思考”回答平均消耗的token数是标准模型的**约14倍**，计算效率极低。

4.  **语言先验的遮蔽效应**：通过计算“视觉增量”（有图 vs. 无图性能差），发现不同能力对视觉的真实依赖度差异巨大。**计数**和**指代定位**高度依赖视觉，而**数学**和**空间推理**则严重依赖语言先验，这扭曲了对多模态能力的真实评估。

### 总结

论文通过严谨的数据工程方法，将现有嘈杂、低效的VLM评估基准转化为了**DatBench**这一高质量评估工具。实验表明，**DatBench**不仅**大幅降低了评估计算成本**，而且**显著提升了评估的判别力和忠实性**，使其能够揭示出传统评估所掩盖的VLM核心能力冲突与行为病理。论文未与某个特定的新模型进行性能对比，而是与**评估现状本身**进行对比，其“性能提升”体现在**评估方法论的质量和效率**上，为VLM研究的健康发展提供了更可靠的“测量尺”。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.02316v1)
- [HTML 版本](https://arxiv.org/html/2601.02316v1)
