# ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers

**相关性评分**: 6.0/10

**排名**: #56


---


## 基本信息

- **arXiv ID**: [2601.04342v1](https://arxiv.org/abs/2601.04342v1)
- **发布时间**: 2026-01-07T19:26:30Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Mohsen Ghafoorian, Amirhossein Habibian

## 关键词

Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

ReHyAt是一种用于视频扩散变换器的循环混合注意力机制，通过结合软注意力保真度和线性注意力效率，实现线性注意力成本和恒定内存使用，提升推理效率并支持边缘部署。

## 摘要

Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

## 详细分析

## 论文摘要：ReHyAt: 用于视频扩散变换器的循环混合注意力

### 1. 研究背景和动机
近年来，基于Transformer的视频扩散模型（如Wan2.1、CogVideoX）在视频生成质量上取得了突破，但其核心的自注意力机制具有**二次方复杂度**（O(N²)），严重限制了模型在生成长视频时的可扩展性，尤其是在移动设备等资源受限场景下。现有线性注意力方法虽能降低复杂度，但其表达能力弱于Softmax注意力，导致生成质量下降或需要昂贵的从头训练。因此，如何在保持高质量的同时，实现**线性复杂度**和**恒定内存**的长视频生成，成为一个关键挑战。

### 2. 核心方法和技术创新
本文提出了 **ReHyAt**，一种新颖的**循环混合注意力机制**，旨在高效地将现有高性能双向Softmax注意力模型（如Wan2.1）蒸馏为高效的循环模型。其核心创新包括：
- **混合注意力设计**：将时间维度划分为块（Chunk）。对于**当前块内的令牌**，使用高保真的**Softmax注意力**来建模精细的局部依赖；对于**所有其他块**的令牌，则使用高效的**线性注意力**来捕获长程依赖。这种设计在保真度和效率间取得了平衡。
- **循环（RNN）重构**：通过使线性注意力部分具有因果性，ReHyAt可以被重构为**块级循环神经网络**。这使得模型能够以**恒定内存**和**线性计算复杂度**逐块生成任意长度的视频，极大提升了可扩展性。
- **轻量级两阶段训练流程**：
    1.  **注意力蒸馏**：仅训练线性注意力中的可学习核映射（φ_q, φ_k），使其输出逼近预训练教师模型（双向Softmax）的注意力输出。此阶段无需视频数据。
    2.  **轻量微调**：使用少量视频-文本对对整个模型进行微调，以恢复细节质量和块间连贯性。
- **重叠块机制**：在Softmax注意力块之间引入重叠，以缓解块边界处可能出现的运动或外观不连贯问题。

### 3. 主要实验结果
- **生成质量**：在VBench和VBench-2.0基准测试中，ReHyAt在将Wan2.1 1.3B模型转换后，取得了与原始模型**相当甚至更具竞争力的分数**（如VBench总分83.79 vs. 83.10）。人类偏好研究表明两者无显著差异。
- **效率提升**：注意力计算复杂度从二次方降至线性。在移动设备（Snapdragon8-Gen4）上，对于7.5秒视频，ReHyAt的DiT块延迟比FlashAttention快约**16倍**，总内存读写量减少约**11倍**，并能成功生成超过10秒的视频（而基线方法出现内存溢出）。
- **训练成本**：整个蒸馏和微调流程仅需约**160个H100 GPU小时**，比同期工作SANA-Video（需数万GPU小时）低**两个数量级**。

### 4. 研究意义和价值
ReHyAt为解决视频扩散模型的核心效率瓶颈提供了一套**实用且高效的方案**。其价值体现在：
- **技术贡献**：首次实现了将SOTA双向Softmax视频扩散模型高效蒸馏为**线性复杂度、恒定内存的循环模型**，为生成长视频和移动端部署扫清了关键障碍。
- **实际应用**：极低的训练成本使得广大研究者和开发者能够基于现有强大模型快速获得高效版本，推动了高质量视频生成技术的**民主化和实用化**。
- **方法论启示**：提出的混合注意力设计和两阶段蒸馏流程为未来更强大的视频生成模型提供了一种可复用的**效率化改造“配方”**。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：ReHyAt

### **核心问题**
当前基于Transformer的视频扩散模型（如Wan2.1）虽然生成了高质量视频，但其核心的Softmax注意力机制具有**O(N²)的二次方计算和内存复杂度**（N为时空令牌总数）。这导致：
1.  **无法生成长视频**：生成超过约10秒的视频会因GPU内存和计算限制而变得困难。
2.  **难以在边缘设备部署**：手机等设备甚至难以生成几秒钟的视频。
3.  **训练成本极高**：从头训练SOTA模型需要巨大的计算和数据资源。

### **核心创新点**
论文提出了 **ReHyAt**，一种**循环混合注意力机制**，旨在将现有高性能的二次方注意力模型（如Wan2.1）**高效地蒸馏**成一个具有**线性复杂度**和**恒定内存**的循环模型，从而实现**可扩展的长视频生成和端侧部署**。

#### **1. 技术创新：循环混合注意力机制**
- **混合设计**：结合了**局部Softmax注意力**（高保真）和**全局线性注意力**（高效率）。
    - **局部Softmax**：应用于当前时间块（Chunk）内的令牌，精确建模相邻帧间的高度依赖关系。
    - **全局线性**：应用于当前块之前的所有令牌，以线性复杂度捕捉长程依赖。
- **时间分块与重叠**：将视频在时间维度上分块处理。引入**重叠块**机制，让相邻块之间有部分令牌共享Softmax注意力，有效缓解了块间过渡的不连贯问题。
- **循环（RNN）重构**：通过使线性注意力部分变为因果（仅关注历史），整个机制可以被重构为一个**块级循环神经网络**。这意味着：
    - **内存恒定**：生成任意长度视频时，峰值内存不随长度增长。
    - **线性计算**：计算复杂度从O(N²)降至O(N)。
    - **支持流式生成**：可以逐块生成视频，非常适合长视频和端侧应用。

#### **2. 方法创新：轻量级两阶段训练管道**
为了极低成本地利用现有SOTA模型，作者没有从头训练，而是设计了一个高效的蒸馏流程：
- **阶段一：注意力蒸馏**
    - **目标**：仅训练每个Transformer块中线性注意力所需的特征映射函数 `φ_q` 和 `φ_k`。
    - **方法**：冻结学生模型（ReHyAt）其他所有权重，使用来自教师模型（如Wan2.1）的注意力输出作为监督信号进行蒸馏。**无需视频-文本对**，只需从教师模型采样激活。
    - **效果**：快速让线性注意力部分学会近似教师模型的全Softmax注意力行为。
- **阶段二：轻量微调**
    - **目标**：修复因分块和因果注意力引入的细节损失和块间不连贯。
    - **方法**：使用少量（如22K）视频-文本对，对整个学生模型进行短时间（如1000次迭代）的流匹配目标微调。
    - **效果**：显著恢复生成质量，特别是时间连贯性。

### **实际价值与效果**
1.  **极高的训练效率**：将Wan2.1 1.3B模型转换为ReHyAt仅需 **~160个H100 GPU小时**，比同期纯线性注意力工作SANA-Video（需数万GPU小时）**效率高出两个数量级**。
2.  **保持竞争力的质量**：在VBench和VBench-2.0基准测试中，ReHyAt与原始Wan2.1模型质量相当，甚至在部分指标上略有优势。人类偏好研究表明两者无明显差异。
3.  **实现线性扩展与端侧部署**：
    - **计算**：注意力FLOPs降低至1/4，且增长曲线从二次方变为线性。
    - **内存**：在移动端（骁龙8 Gen4）测试中，ReHyAt是**唯一能生成超过10秒视频**且不内存溢出的方法。在7.5秒视频生成上，比FlashAttention快**约16倍**，内存读写效率高**约11倍**。
4.  **提供通用配方**：该方法不局限于Wan2.1，其“蒸馏+微调”的流程为未来任何基于双向Softmax的SOTA视频扩散模型提供了一条**低成本转化为高效循环模型**的可行路径。

### **总结**
**ReHyAt的核心贡献在于，它通过一个巧妙的“混合注意力架构”和“极高效的蒸馏流程”，在几乎不损失生成质量的前提下，一举解决了视频扩散模型在长视频生成和移动端部署中的计算与内存瓶颈问题，为实用化、可扩展的视频生成打开了新的大门。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决视频扩散模型中基于Transformer架构的**二次方注意力复杂度问题**，该问题严重限制了生成长视频的扩展性和在边缘设备上的部署。为此，论文提出了 **ReHyAt（循环混合注意力）** 机制，其核心创新在于将**局部软注意力（Softmax Attention）与全局线性注意力（Linear Attention）相结合**，并设计了一种**分块循环（Chunk-wise Recurrent）** 的重新表述，从而将计算和内存复杂度从二次方降低到线性，并实现恒定的内存占用。通过一个**轻量级的两阶段蒸馏微调流程**，该方法能够以极低的计算成本（约160 GPU小时）将现有的高性能双向软注意力模型（如Wan2.1）高效地转化为循环混合注意力模型。最终，ReHyAt在保持与原始模型相当甚至更优的生成质量（在VBench等基准测试和人类偏好研究中得到验证）的同时，显著提升了计算效率，为长视频生成和端侧部署提供了可行的解决方案。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers》针对视频扩散模型中注意力机制的计算瓶颈，提出了一套系统的创新解决方案。其核心创新点可归纳如下：

### 1. **提出了一种新颖的时域分块混合注意力机制**
   - **改进/不同之处**：与以往工作（如仅使用线性注意力或均匀混合注意力）不同，ReHyAt 将注意力计算在时域上划分为块。在每个块内，对**局部关键令牌**使用高保真的 **Softmax 注意力**，而对**全局其他令牌**使用高效的**线性注意力**。此外，还引入了**重叠块**设计，以增强块间过渡的连贯性。
   - **解决的问题/优势**：
     - **解决了效率与质量的权衡问题**：纯线性注意力表达能力不足，而纯Softmax注意力计算复杂度高（O(N²)）。混合设计在保持对局部细节（如相邻帧间运动、外观）高保真建模的同时，将整体复杂度降至**线性 O(N)**。
     - **缓解了长视频生成的时序不连贯问题**：重叠块机制允许相邻块之间通过Softmax注意力进行更精确的信息传递，减少了纯线性注意力或非重叠分块可能导致的“片段化”或运动跳跃问题。

### 2. **实现了分块循环神经网络（Chunk-wise RNN）重构**
   - **改进/不同之处**：论文证明了所提出的混合注意力机制（在调整为因果形式后）可以**重新表述为一个分块处理的RNN**。这与之前的一些混合注意力工作（如Attention Surgery）不同，后者虽然混合但无法进行RNN重构，因此内存和计算复杂度并未根本改善。
   - **解决的问题/优势**：
     - **实现了恒定内存消耗**：在推理（生成）阶段，模型可以以块为单位顺序处理视频，只需维护一个固定大小的状态变量，**峰值内存使用与视频总长度无关**。
     - **支持生成长度任意的视频**：这是实现**超长视频生成**和**在内存受限的边缘设备（如手机）上运行**的关键。传统的Softmax注意力会因O(N²)内存增长而迅速耗尽资源。

### 3. **设计了一个轻量级的两阶段蒸馏微调流程**
   - **改进/不同之处**：与SANA-Video等工作需要从头开始训练一个全新的高效模型不同，ReHyAt 的核心策略是**从现有的、性能强大的双向Softmax注意力模型（如Wan2.1）中进行知识蒸馏**。流程分为：
     1.  **注意力蒸馏**：仅训练每个Transformer块中线性注意力所需的特征映射函数（φ_q, φ_k），使其输出逼近教师模型对应块的Softmax注意力输出。此阶段**无需视频-文本对数据**。
     2.  **轻量级微调**：使用少量视频-文本对数据对整个模型进行端到端的微调，以恢复蒸馏过程中可能丢失的细节和块间平滑性。
   - **解决的问题/优势**：
     - **极大降低了训练成本**：论文报告仅需**约160个H100 GPU小时**即可完成从Wan2.1到ReHyAt的转换。这比从头训练SOTA模型（如SANA-Video需要数万GPU小时）低了**两个数量级**。
     - **提供了一种实用的模型压缩范式**：该方法为未来任何基于双向Softmax的SOTA视频扩散模型提供了一个“配方”，可以将其高效地转化为内存友好、可循环推理的版本，而无需耗费巨资重新训练。

### 4. **提出了更具表达力的线性注意力核函数**
   - **改进/不同之处**：不同于早期线性注意力使用的简单固定特征映射（如ELU+1），ReHyAt 为查询和键使用了**分离的、可学习的多项式特征映射**。具体来说，通过一个轻量级网络生成中间特征，然后将其分割并分别提升到1到P次幂，最后拼接起来。
   - **解决的问题/优势**：
     - **缩小了与Softmax注意力核的表达能力差距**：Softmax使用的指数核 `exp(q·k)` 具有极大的动态范围。可学习的多项式扩展能够更好地近似这种动态范围，从而让线性注意力部分更准确地模拟长程依赖关系，减少了性能损失。

---

## 总结

**ReHyAt 的核心技术创新在于将“混合注意力”、“循环重构”和“高效蒸馏”三者有机结合，创造性地解决了视频扩散Transformer在迈向长视频、移动端部署时面临的核心矛盾：即计算/内存的二次方复杂度与生成质量及实用化需求之间的冲突。**

其实质价值在于：
1.  **技术价值**：提供了一种将高性能、高成本模型高效“压缩”为低成本、可部署模型的系统方法。
2.  **应用价值**：为在手机等边缘设备上生成更长时间、更高质量的视频打开了通路，推动了生成式AI应用的实用化边界。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 数据集与评价指标
- **数据集**：
    - **训练**：使用Open-Sora Plan的350K视频子集（低分辨率微调）和Wan2.1 14B生成的22K合成视频（高分辨率微调）。
    - **评估**：使用VBench和VBench-2.0的完整提示集生成视频。
- **评价指标**：
    - **定量指标**：VBench和VBench-2.0的综合得分（Total）、质量（Quality）、语义（Semantic）等子维度分数。
    - **人类偏好研究**：盲测对比，收集500对视频比较，评估视觉质量和提示对齐。
    - **计算效率**：FLOPs、移动端延迟（ms）、内存读写（GB）、峰值内存使用。
    - **可扩展性**：视频时长增长时的计算复杂度和内存占用分析。

### 基线方法对比
论文与以下SOTA方法进行了全面对比：
- **全Softmax注意力模型**：Wan2.1 1.3B、CogVideoX系列、Open-Sora Plan、PyramidalFlow等。
- **高效注意力模型**：SANA-Video、Attention Surgery、M4V、STA、VSA、Efficient VDiT等。
- **移动端/轻量模型**：Mobile Video DiT、SnapGenV、Hummingbird、Neodragon等。

### 关键性能结果与结论
#### 1. **生成质量：达到SOTA水平**
- **VBench得分**：ReHyAt（15×Tc=3）在VBench上获得**83.79**的总分，优于原Wan2.1 1.3B（83.31）和大多数对比模型（包括5B参数的CogVideoX）。
- **VBench-2.0得分**：ReHyAt（15×Tc=5）获得**56.3**的总分，与Wan2.1（56.0）相当，优于Attention Surgery（55.1）。
- **人类偏好研究**：ReHyAt与Wan2.1在500次对比中无显著差异（ReHyAt偏好27.6% vs. Wan2.1偏好43.5%，29%无偏好），证明质量损失可忽略。

#### 2. **计算效率：线性复杂度，显著提升**
- **FLOPs降低**：在5秒视频（480×832）上，ReHyAt比FlashAttention节省最高**4倍计算量**，比Uniform Hybrid（R=2）节省约**2倍**。
- **移动端延迟**：在Snapdragon8-Gen4上，ReHyAt生成121帧（~7.5秒）视频时，DiT块延迟为**302ms**，比FlashAttention（**4809ms**）快**16倍**，且能扩展到161帧（其他方法均OOM）。
- **内存效率**：ReHyAt在121帧时总内存读写为**7.1GB**，比FlashAttention（**76.3GB**）减少**11倍**，且**峰值内存恒定**，支持长视频生成。

#### 3. **训练成本：极低蒸馏开销**
- **GPU小时**：仅需**~160 H100 GPU小时**完成蒸馏+微调，比SANA-Video（约12天×64 H100）低**两个数量级**，比MovieGen低**三个数量级**。
- **蒸馏效果**：通过两阶段训练（注意力蒸馏 + 轻量微调），成功将双向Softmax模型转化为RNN形式，质量损失极小。

#### 4. **可扩展性：解锁长视频生成**
- **复杂度分析**：注意力计算从二次方（O(N²)）降至线性（O(N)），内存占用恒定，支持任意时长视频生成。
- **长视频演示**：在移动端成功生成10秒以上视频（161帧），而基线方法（FlashAttention、Uniform Hybrid）在141帧后均OOM。

#### 5. **消融实验：关键设计验证**
- **块大小（Tc）**：Tc=3时质量与计算效率达到最佳平衡（VBench 82.17），增大Tc提升有限但计算成本增加。
- **重叠大小（To）**：To=1相比To=0显著提升质量（VBench从81.56→82.17），减少块间不连贯性。
- **因果性**：因果化不影响质量（VBench 82.35 vs. 82.27），但启用RNN重构，实现恒定内存。

### 结论
ReHyAt通过**混合注意力设计**（局部Softmax + 全局线性）和**轻量蒸馏流程**，在几乎不损失生成质量的前提下，将视频扩散模型的注意力复杂度从二次方降至线性，并实现**恒定内存占用**。这使得：
1. **高质量长视频生成成为可能**（>10秒）。
2. **移动端实时生成变得可行**（延迟降低16倍）。
3. **训练成本大幅降低**（<200 GPU小时），为未来SOTA模型的高效化提供了可行方案。

**实际价值**：为资源受限场景（移动设备、长视频生成、多镜头合成）提供了实用的视频生成解决方案，推动了视频扩散模型的落地应用。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.04342v1)
- [HTML 版本](https://arxiv.org/html/2601.04342v1)
