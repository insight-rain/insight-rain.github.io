# VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models

**相关性评分**: 8.0/10

**排名**: #11


---


## 基本信息

- **arXiv ID**: [2601.03309v1](https://arxiv.org/abs/2601.03309v1)
- **发布时间**: 2026-01-06T09:58:24Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Jianke Zhang, Xiaoyu Chen, Qiuyue Wang, Mingsheng Li, Yanjiang Guo, Yucheng Hu, Jiajun Zhang, Shuai Bai, Junyang Lin, Jianyu Chen

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

这篇论文通过VLM4VLA框架重新评估了视觉语言模型在视觉语言动作模型中的角色，重点关注模型选择和性能瓶颈，对推理效率和轻量化架构有间接影响。

## 摘要

Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.

## 详细分析

## 论文详细摘要

### 1. 研究背景和动机
近年来，**视觉-语言-动作模型** 因其在机器人控制任务中展现出的强大泛化能力而备受关注。这些模型通常将预训练的大型**视觉-语言模型** 作为策略主干。然而，一个基础但鲜被系统研究的问题是：**VLM的选择及其能力如何影响下游VLA策略的性能？** 现有研究多集中于设计复杂的网络架构，而忽略了VLM主干本身的影响。本文旨在填补这一空白，系统地探究VLM能力向VLA策略的迁移效果。

### 2. 核心方法和技术创新
为了公平、高效地比较不同VLM，本文提出了 **VLM4VLA** 框架。其核心创新在于：
- **最小化适配管道**：通过引入一个**可学习的动作查询令牌** 和一个轻量级MLP策略头（新增参数<1%），将通用VLM转换为VLA策略。该方法设计简洁，但性能却能与更复杂的网络设计相竞争。
- **公平评估基准**：采用统一的训练目标（Huber损失+BCE损失，而非扩散损失）和评估协议，严格控制了随机性，确保了不同VLM间比较的公平性和可复现性。
- **系统性分析维度**：从三个维度全面评估VLM能力：1) **通用能力**（标准VLM评测）；2) **具身特定能力**（在辅助具身任务上微调）；3) **模态级分析**（解耦视觉与语言编码器的影响）。

### 3. 主要实验结果
通过在Calvin、SimplerEnv和Libero三个基准上的大量实验，得出以下关键发现：
- **VLM预训练必要但不充分**：使用VLM初始化始终优于从头训练，但**VLM的通用能力（如VQA分数）是其下游控制性能的差劲预测指标**。例如，在某些环境中，较小的Kosmos-2模型可能优于更大的Qwen-VL模型。
- **具身微调无效**：**在多种辅助具身任务（如具身VQA、视觉指向、深度估计）上微调VLM，并不能保证其在下游控制任务上获得更好的性能**，有时甚至会导致性能下降。
- **视觉模块是关键瓶颈**：模态级消融实验表明，**VLM中的视觉编码器是性能的主要瓶颈**。冻结视觉编码器会导致性能大幅下降，而冻结语言编码器影响甚微。进一步实验证明，即使使用真实世界图像，向VLM的视觉编码器中注入控制相关的监督信息也能带来性能提升，这揭示了**标准VLM预训练目标与具身动作规划所需的视觉表征之间存在根本性的领域鸿沟**。

### 4. 研究意义和价值
本研究具有重要的理论和实践价值：
- **挑战普遍假设**：明确指出当前VLM的通用能力评测与VLA的实际需求存在脱节，纠正了“更强VLM必然带来更好机器人策略”的直觉认知。
- **指明未来方向**：揭示了**提升VLA性能的关键在于弥合视觉理解与低层动作控制之间的语义鸿沟**，而非简单地追求更大的VLM或在更多具身VQA数据上微调。这为未来VLA研究提供了清晰的指引：应致力于开发更适配控制任务的视觉表征学习范式。
- **提供实用基准**：VLM4VLA框架和广泛的实验结果（超过100次训练评估）为社区提供了一个公平、可复现的基准，有助于更科学地评估和选择VLM主干。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：VLM4VLA

### **论文标题**
VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models

### **核心研究问题**
这篇论文旨在解决一个**基础但未被系统研究**的问题：**视觉-语言模型（VLM）的选择及其能力如何影响下游的视觉-语言-动作（VLA）策略性能？** 当前VLA研究多聚焦于复杂的网络架构设计，而忽略了作为策略核心的VLM骨干网络本身的影响。

### **主要技术创新**
1.  **提出了VLM4VLA框架**：
    - **目标**：建立一个**公平、最小化、可复现**的实验管道，用于评估不同VLM在VLA任务中的真实能力。
    - **设计**：
        - 引入一个通用的“插件式”网络，将任意通用VLM转换为VLA策略。
        - **核心机制**：在VLM的输入序列中添加一个**可学习的动作查询令牌（`<ActionQuery>`）**，提取其隐藏状态后，通过一个轻量级MLP策略头解码为连续动作。
        - **极简参数**：新增参数少于总参数的1%，最大限度地减少了策略头设计带来的干扰。
        - **稳定的训练目标**：采用基于Huber损失和二元交叉熵的最大似然模仿学习目标，避免了扩散或流匹配模型带来的推理随机性和评估不稳定性。

2.  **进行了大规模、系统性的实证研究**：
    - 在三个主流机器人仿真基准（Calvin, SimplerEnv, Libero）上评估了**9个开源VLM**（共24个变体）。
    - 从三个维度深入分析：
        - **通用能力**：VLM的预训练收益及其通用性能（如VQA分数）与下游控制任务表现的相关性。
        - **具身特定能力**：在7个辅助具身任务（如具身QA、视觉指向、深度估计）上微调VLM，观察其对下游控制任务的影响。
        - **模态级分析**：通过冻结/微调视觉编码器和语言编码器，分离它们对VLA性能的贡献。

### **关键发现与结论**
1.  **VLM初始化必要但不充分**：
    - 使用预训练VLM初始化始终优于从头训练，证明了VLM先验知识的重要性。
    - **然而，VLM的通用能力（如标准VQA分数）是其下游VLA性能的差预测指标**。例如，在某些任务中，较小的Kosmos-2可以超越性能更强的Qwen2.5-VL。

2.  **提升具身特定技能未必提升控制性能**：
    - 在辅助具身任务上微调VLM，**并不能保证甚至可能损害**其作为VLA骨干时的下游控制性能。这表明当前基于VQA的具身任务与低层动作控制任务之间存在**语义鸿沟**。

3.  **视觉编码器是主要性能瓶颈**：
    - 模态消融实验表明，**视觉编码器**（而非语言组件）是VLA性能的关键瓶颈。
    - **必须微调视觉编码器**才能获得强大的控制性能。冻结视觉编码器会导致性能大幅下降。
    - 即使使用真实世界图像进行预训练，VLM的视觉特征与机器人控制所需的细粒度表征之间仍存在**根本性的不匹配**。这种不匹配源于**视觉-语言理解任务与低层动作控制任务之间的目标差异**，而不仅仅是“仿真到真实”的图像域差异。

4.  **揭示了VLM与VLA之间的领域鸿沟**：
    - 论文结论指出，当前VLM的研究目标（侧重于高层次理解和推理）与VLA模型的实际需求（低层、精确的动作规划与控制）之间存在**显著且持续的领域鸿沟**。

### **实际价值与启示**
1.  **为社区提供实践指南与性能参考**：通过大量实验，为研究者选择VLM骨干、设计训练策略提供了实证依据。
2.  **挑战了常见假设**：纠正了“更强的通用VLM必然产生更好的VLA”和“通过具身VQA任务增强VLM能直接提升机器人控制能力”的直觉认知。
3.  **指明了未来研究方向**：
    - **重新思考VLM预训练目标**：需要开发更能对齐机器人控制需求的视觉表征学习目标。
    - **视觉编码器的关键作用**：未来工作应更专注于如何将控制相关信息有效地注入或适配VLM的视觉模块。
    - **评估基准的反思**：需要设计更能反映真实机器人控制需求的VLM/VLA评估基准。

### **总结**
VLM4VLA论文通过一个精巧的最小化适配框架和严谨的大规模实验，系统地评估了VLM在VLA中的作用。其核心贡献在于**实证性地揭示并深入分析了当前VLM能力与机器人控制需求之间的根本性脱节**，特别是强调了**视觉表征对齐**这一关键挑战，为未来构建更有效的具身智能体指明了方向。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

**核心问题**：本文旨在系统性地探究一个在视觉-语言-动作模型中常被忽视的基础问题：**预训练视觉-语言模型的选择及其能力如何影响下游VLA策略的性能？** 具体而言，VLM的通用能力、针对具身任务微调的能力，以及其内部视觉/语言模块的贡献，是否能够有效预测其在机器人控制任务中的表现。

**主要方法**：作者提出了 **VLM4VLA**，一个**最小化适配框架**，用于公平、高效地评估不同VLM作为VLA策略骨干的潜力。该框架的核心是：
- **轻量级设计**：在通用VLM基础上，仅引入一个可学习的动作查询令牌和一个简单的MLP策略头（新增参数<1%），将其转换为VLA策略。
- **标准化流程**：使用统一的训练目标（MSE+BCE损失）、输入格式（单视角图像+指令）和评估协议，确保比较的公平性。
- **系统性评估维度**：从三个维度展开大规模实验：
    1.  **通用能力**：比较不同开源VLM（如Qwen、Paligemma、Kosmos）在三个机器人仿真基准上的表现。
    2.  **具身特定能力**：探究在多种具身辅助任务（如VQA、视觉指向、深度估计）上微调VLM，是否有助于提升下游控制性能。
    3.  **模态级分析**：通过冻结/微调视觉编码器和语言编码器，拆解各模块对VLA性能的贡献。

**主要结论**：
1.  **VLM初始化必要但不足**：使用预训练VLM初始化策略显著优于从头训练，**但VLM在标准VQA评测中的通用能力是其下游控制性能的差预测指标**。例如，在某些环境中，较小的Kosmos-2可以超越性能更强的Qwen和Paligemma。
2.  **具身微调收益有限**：**提升VLM在特定具身辅助任务上的性能，并不能保证其在下游控制任务中表现更好**，甚至可能导致性能下降。这表明当前基于VQA的具身任务与低层动作控制之间存在目标鸿沟。
3.  **视觉模块是关键瓶颈**：**视觉编码器是VLA性能的主要瓶颈**。实验表明，冻结视觉编码器会导致性能大幅下降，而冻结语言编码器影响甚微。进一步实验证明，这种瓶颈源于**VLM预训练目标（视觉-语言理解）与具身动作规划所需视觉表征之间的根本性语义差距**，而非简单的“仿真-真实”图像差异。
4.  **VLM4VLA框架的有效性**：尽管设计极简，VLM4VLA框架在多个基准测试中表现出了与更复杂VLA设计（如OpenVLA, pi0）相竞争的强大性能，验证了其作为公平评估平台的可靠性。

**核心价值**：本研究通过严谨、大规模的实验，揭示了当前VLM研究与VLA实际需求之间的显著差距，挑战了“更强的通用VLM或更好的具身VQA性能必然带来更好的机器人控制”这一常见假设。它明确指出，**未来提升VLA性能的关键在于弥合视觉表征的语义鸿沟**，特别是需要设计能够直接服务于低层动作控制的视觉预训练或适配方法。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models》的创新点分析

这篇论文的核心贡献在于**系统性地重新审视了VLM（视觉-语言模型）在VLA（视觉-语言-动作模型）中的作用**，挑战了该领域的一些常见假设，并通过严谨的实验揭示了关键瓶颈。其创新点具体如下：

---

### 1. **提出了一个公平、最小化的VLA评估框架（VLM4VLA）**
   - **改进/不同之处**： 以往研究多专注于设计复杂的VLA网络架构（如分层结构、扩散模型解码器），而**忽略了VLM主干网络本身对最终策略性能的影响**。本文设计了一个**极简的适配管道**，仅通过引入一个可学习的动作查询令牌和一个轻量级MLP策略头（新增参数<1%），将任何通用VLM转换为VLA策略。
   - **解决的问题/带来的优势**：
     - **公平性**： 消除了复杂策略头设计带来的干扰变量，使得**不同VLM主干网络之间的性能比较变得公平、纯粹**。
     - **高效性**： 简单的架构降低了训练和调参复杂度，便于进行大规模、可复现的实验。
     - **有效性**： 尽管设计简单，VLM4VLA框架的性能与更复杂的专家VLA模型（如基于流匹配的`pi0`）相比**极具竞争力**，证明了其作为评估基准的可靠性。

### 2. **挑战了“VLM通用能力可预测其VLA性能”的固有假设**
   - **改进/不同之处**： 社区普遍认为，在通用VQA（视觉问答）基准上表现越好的VLM，作为VLA主干时应能带来更优的下游控制性能。本文通过**在三个不同基准（Calvin, SimplerEnv, Libero）上对9个开源VLM进行大规模实验**，颠覆了这一认知。
   - **解决的问题/带来的优势**：
     - **发现不一致性**： 实验表明，VLM的通用能力（VQA分数）与下游VLA任务性能**相关性很弱甚至为负**。例如，在通用基准上并非顶尖的`Kosmos-2`，在部分操控任务上超越了更强的`Qwen2.5-VL`。
     - **揭示领域鸿沟**： 这一发现**明确指出了当前VLM的预训练目标（以语言生成为中心的视觉-语言理解）与具身控制任务需求之间存在根本性差距**。这引导未来研究不应仅追求更高的VQA分数，而应关注VLM对低层级动作规划所需视觉表征的学习能力。

### 3. **揭示了“针对具身任务微调VLM无助于提升VLA性能”的反直觉结论**
   - **改进/不同之处**： 近期工作常通过在海量具身VQA数据（如指向、空间推理QA）上微调VLM，以期获得更好的“机器人大脑”或VLA主干。本文**首次系统性地评估了7种不同的具身辅助任务微调对最终VLA控制性能的实际影响**。
   - **解决的问题/带来的优势**：
     - **打破直觉**： 实验发现，提升VLM在特定具身技能（如视觉指向、深度估计、机器人VQA）上的表现，**并不能保证甚至通常会损害**其作为VLA主干时的下游控制性能。
     - **明确研究方向**： 这表明**简单地用VQA形式包装具身知识并不能有效弥合VLM与VLA之间的语义鸿沟**。未来的VLM预训练或微调需要更直接地与**低层级动作控制**的目标对齐，而非停留在问答层面。

### 4. **通过模态级消融实验，精准定位了视觉编码器是VLA的性能瓶颈**
   - **改进/不同之处**： 以往研究对VLM中视觉和语言模块对VLA的贡献缺乏细粒度分析。本文通过**系统性地冻结或微调VLM的视觉编码器和语言编码器**，分离了它们的贡献。
   - **解决的问题/带来的优势**：
     - **识别关键模块**： 实验证明，**微调视觉编码器对VLA性能至关重要**，冻结它会导致性能大幅下降；而语言编码器（或词嵌入）是否冻结影响甚微。
     - **超越“仿真-现实”差异**： 进一步实验表明，即使在真实图像（BridgeV2数据集）上对VLM进行动作信息注入的微调，**只要视觉编码器保持冻结，下游VLA性能依然无法提升**。这证明瓶颈并非简单的图像域差异（仿真vs现实），而是**VLM的视觉特征与机器人控制所需的细粒度、动作相关视觉表征之间存在语义鸿沟**。
     - **提供解决方案方向**： 论文指出，将控制相关的监督信号直接注入VLM的视觉编码器预训练阶段，是未来缩小这一鸿沟的有效途径。

### 5. **提供了全面、可复现的基准测试与深入分析**
   - **改进/不同之处**： 相比之前零散或不够公平的VLM对比工作（如`RoboVLMs`），本文在**三个具有挑战性的仿真基准**上进行了超过100次训练和评估实验，涵盖了不同规模的模型、不同的训练策略（从头训练、微调、冻结），并提供了详尽的超参数、提示词格式和实现细节。
   - **解决的问题/带来的优势**：
     - **建立参考基线**： 为社区提供了**不同VLM在VLA任务上性能的详细参考**，有助于研究者根据任务特点选择合适的VLM主干。
     - **促进可复现性**： 极简的`VLM4VLA`框架和公开的配置细节，使得所有实验易于复现和扩展，推动了该领域的严谨研究。
     - **理论结合实践**： 不仅报告了性能数据，还通过相关性分析、消融实验和可视化（如图5的特征学习轨迹图），对观察到的现象提供了**深度的解释和假设**，将经验发现上升到了对VLM/VLA表征学习机制的理解。

---

**总结**： 本文的核心创新在于**方法论和认知层面**。它通过一个精心设计的极简框架，进行了一次大规模、控制变量的“科学实验”，系统地质疑并验证了VLA领域关于VLM作用的多个核心假设。其最重要的贡献是**明确揭示了当前VLM预训练与具身控制需求之间的根本性脱节，并将问题根源精准地定位在视觉编码器的表征学习上**，为未来设计更适合机器人控制的视觉-语言基础模型指明了清晰的方向。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心实验框架与评估方法
论文提出了 **VLM4VLA** 这一最小化适配框架，旨在公平、高效地评估不同视觉语言模型作为VLA策略主干时的性能。

- **核心方法**： 在预训练的VLM基础上，仅引入一个可学习的动作查询令牌和一个轻量级MLP策略头（新增参数<1%），将VLM转换为VLA策略。使用最大似然模仿学习目标（Huber损失 + BCE损失）进行训练，避免了扩散/流匹配模型带来的推理随机性和评估复杂性。
- **评估基准**： 在三个广泛使用的机器人操作仿真基准上进行评估：
    1.  **Calvin ABC-D**： 评估语言条件化长视野任务序列的完成能力。指标为平均成功完成的任务数（Task-1 至 Task-5 及总和 `Calvin ↑`）。
    2.  **SimplerEnv Bridge**： 评估在更具挑战性的“真实到仿真”场景下的泛化能力。指标为在四个场景（胡萝卜、茄子、勺子、立方体）上的成功率，并计算平均成功率（`Simpler ↑`）。
    3.  **Libero-Long (-10)**： 评估多类别、长视野操作任务的性能。指标为在10个任务上的平均成功率（`Libero ↑`）。

### 二、 对比的基线方法
论文与两类基线方法进行了对比：

1.  **专家级VLA模型**：
    - **OpenVLA**： 基于Llama-2和DINOv2/SigLIP，将动作离散化为语言令牌。
    - **pi0**： 基于Paligemma-1的流匹配动作专家模型（论文中移除了其本体感知输入以公平对比）。
    - **ThinkAct**： 基于Qwen2.5VL-7B并融合了强化学习的VLA模型（**注**：该模型使用了本体感知状态作为输入，与其他模型设置不同）。

2.  **训练起点基线**：
    - **从零训练（Scratch）**： 将VLM4VLA架构中的VLM权重随机初始化后训练，作为性能下界参考。

### 三、 主要实验结果与结论

#### 1. **不同VLM主干在VLA中的性能表现**
- **结论**： VLM预训练初始化相比从零训练带来**一致的性能优势**（见表8，性能下降显著）。然而，**VLM在通用VQA基准上的能力与其下游VLA任务性能相关性很弱**，挑战了“更强VLM必然带来更好VLA”的常见假设。
- **具体数据**：
    - **Calvin**： Qwen系列VLM（如Qwen2.5VL-7B, Qwen3VL-2B）表现最佳，其`Calvin`总分（~4.05）与顶尖专家VLA模型`pi0`（3.509）相当甚至更好。在此环境中，VLA性能与VLM的通用VQA能力**呈正相关**（相关系数 r=0.839）。
    - **SimplerEnv & Libero**： 表现最佳的模型并非通用能力最强的Qwen系列。例如，参数最小的**Kosmos-2**在SimplerEnv上取得了最高平均成功率（60.4%），在Libero上也表现优异（55.0%）。在这两个环境中，VLA性能与VLM通用能力**无显著相关性**（Simpler: r=-0.358； Libero: r=-0.194）。

#### 2. **在具身辅助任务上微调VLM的影响**
- **结论**： **在多种具身辅助任务（如视觉指向、具身VQA、深度估计等）上微调VLM，并不能保证甚至通常会损害其作为VLA主干的下游控制性能。**
- **具体数据**： 使用Qwen2.5VL在7种不同的辅助数据集（Robopoint, Vica-332k, BridgeVQA, Robo2VLM, Robobrain2等）上进行微调后，再适配为VLA。如图4和表10所示，几乎所有微调后的VLM在Calvin任务上的性能都**等于或低于原始基线**。例如，在Qwen2.5VL-3B上使用Robo2VLM数据微调后，`Calvin`总分从3.856下降至3.760。

#### 3. **VLM内部模块的重要性分析**
- **结论**： **视觉编码器是VLA性能的主要瓶颈**，而语言编码器的影响相对较小。
- **具体数据**（见表3）：
    - **冻结视觉编码器**导致所有模型性能**大幅下降**。例如，Qwen2.5VL-7B在Calvin上的得分从4.057骤降至2.823；Paligemma-1从3.506暴跌至0.495。
    - **冻结词嵌入**对性能影响微乎其微（变化在±0.2以内）。
    - **关键实验**（见表4）： 即使在**真实世界图像**（BridgeV2）上对VLM进行注入动作信息的微调，如果微调时**冻结视觉编码器**，下游VLA性能也几乎无提升。只有当视觉编码器在微调过程中也**参与更新**时，下游性能才有显著改善。这证明性能瓶颈源于**语义层面的差距**——标准VLM预训练获得的视觉特征与低层级动作控制所需特征不匹配，而非简单的“仿真到真实”图像域差异。

### 四、 综合结论
论文通过超过100次训练评估实验，系统性地揭示了当前VLM研究与VLA实际需求之间存在**显著差距**：
1.  **必要但不充分**： VLM预训练对VLA至关重要，但强大的通用VLM能力并非优秀VLA性能的可靠预测指标。
2.  **技能不迁移**： 通过具身VQA等任务提升VLM的特定具身技能，无法有效转化为更好的端到端控制性能。
3.  **视觉是关键**： VLA的性能瓶颈主要在于视觉编码器。当前VLM的视觉表征目标与具身行动规划所需的细粒度、控制相关的视觉信息之间存在**持续的领域鸿沟**。

这项工作为未来VLA研究提供了重要指引：需要重新思考并设计更贴近具身控制需求的VLM预训练或适配方法，特别是针对**视觉表征学习**的改进。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.03309v1)
- [HTML 版本](https://arxiv.org/html/2601.03309v1)
