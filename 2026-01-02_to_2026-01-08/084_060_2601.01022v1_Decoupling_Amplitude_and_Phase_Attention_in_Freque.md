# Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking

**相关性评分**: 6.0/10

**排名**: #84


---


## 基本信息

- **arXiv ID**: [2601.01022v1](https://arxiv.org/abs/2601.01022v1)
- **发布时间**: 2026-01-03T01:10:17Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Shiao Wang, Xiao Wang, Haonan Zhao, Jiarui Xu, Bo Jiang, Lin Zhu, Xin Zhao, Yonghong Tian, Jin Tang

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种基于频率域解耦振幅和相位注意力的RGB-事件视觉目标跟踪框架，通过选择性融合和空间稀疏化提升效率和性能，但与视觉-语言-动作模型和机器人应用直接相关性较弱。

## 摘要

Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking

## 详细分析

## 论文摘要

**论文标题**：Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking

**研究背景与动机**：
传统的RGB相机在低光照、快速运动等极端场景下存在性能瓶颈。事件相机具有高动态范围和高时间分辨率的优势，能有效补充RGB信息。然而，现有的RGB-Event跟踪方法多采用特征级融合，未能充分利用事件数据的特性，且计算复杂度高。因此，如何高效、充分地融合两种模态以提升跟踪的鲁棒性和效率，成为关键挑战。

**核心方法和技术创新**：
本文提出了一种新颖的RGB-Event跟踪框架APMTrack，其核心创新在于：
1.  **解耦幅度与相位注意力模块**：在频域（通过快速傅里叶变换）对RGB和事件模态进行早期融合。将两模态的频谱解耦为幅度和相位分量，并通过设计的幅度注意力和相位注意力机制，有选择地将事件模态的高频信息（如目标轮廓）融合到RGB模态中。此举不仅增强了RGB特征在挑战性场景下的表征能力，还将输入主干网络的令牌数量减半，显著降低了计算负担。
2.  **运动引导的空间稀疏化模块**：利用事件相机对运动敏感的特性，从事件体素中提取目标运动线索。通过一个基于FFT的差分视觉Transformer（Diff-FFT ViT）建模目标相关运动信息，并设计一个基于场景方差的自适应Top-K选择函数，动态地筛选出与目标最相关的空间区域令牌，抑制冗余背景信息，进一步减少主干网络的计算量并增强目标特征。

**主要实验结果**：
在三个主流RGB-Event跟踪基准数据集（FE108, FELT, COESOT）上进行了广泛实验。结果表明，APMTrack在取得优异跟踪精度的同时，保持了高效率。
*   在**FELT**数据集上取得了新的最优性能（SR: 56.5, PR: 72.3）。
*   在**COESOT**数据集上取得了最高的精确率（PR: 83.3）。
*   消融实验验证了所提两个核心模块的有效性及必要性。
*   可视化结果表明，该方法能有效增强RGB图像并聚焦于目标区域。

**研究意义与价值**：
本研究为RGB-Event视觉目标跟踪提供了一种新颖的、高效的融合范式。通过**在频域进行早期融合**和**利用事件运动信息引导计算资源分配**，该工作不仅显著提升了在极端场景下的跟踪鲁棒性，还通过大幅减少冗余计算，在精度和效率之间取得了良好平衡。这为未来开发适用于动态、复杂环境的实时多模态感知系统提供了有价值的思路和技术基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：APMTrack

### **一、 论文旨在解决的核心问题**
本文针对**RGB-事件相机融合的视觉目标跟踪**任务，指出了现有方法存在的两个主要局限性：
1.  **未能充分利用事件相机的独特优势**：现有方法大多采用传统的特征级融合，忽略了事件相机**高动态范围**和**对运动高度敏感**的特性。这导致在低光照、快速运动等挑战性场景下，事件数据的潜力未被充分挖掘。
2.  **计算效率低下**：传统的Siamese或单流跟踪器在处理RGB和事件两种模态时，需要处理所有空间位置的视觉标记（Token），计算负担沉重，难以满足实时性需求。

### **二、 核心技术创新点**
论文提出了一个名为 **APMTrack** 的新框架，其核心创新在于两个模块：

1.  **解耦的幅度与相位注意力模块**
    *   **做什么**：在**频域**进行早期融合，而非传统的空间域特征级融合。
    *   **怎么做**：
        *   使用**快速傅里叶变换**将RGB图像和事件体素数据从空间域转换到频域。
        *   将频域表示**解耦为幅度（Amplitude）和相位（Phase）**两个分量。
        *   设计**幅度注意力和相位注意力机制**，选择性地将事件模态中的**高频信息**（通常对应目标轮廓和运动细节）融合到RGB模态中。
    *   **带来的好处**：
        *   **增强表征**：有效利用事件的高动态范围信息，增强了RGB图像在挑战性场景下的特征质量。
        *   **提升效率**：融合后仅保留增强后的RGB模态输入主干网络，**输入Token数量减半**，大幅降低了计算开销。

2.  **运动引导的空间稀疏化模块**
    *   **做什么**：利用事件数据对运动敏感的特性，动态地筛选出与目标相关的关键区域，忽略冗余的背景信息。
    *   **怎么做**：
        *   从事件体素中提取多时间尺度的**运动差异图**。
        *   设计一个**基于FFT的差分视觉Transformer**，用于建模模板与搜索区域之间的目标相关运动线索。
        *   通过一个轻量级的评分估计器，计算搜索区域每个位置属于目标的**空间概率分布**。
        *   根据该概率分布的**方差**，使用一个**自适应的指数衰减Top-K函数**，动态决定保留多少个高概率的Token。
    *   **带来的好处**：
        *   **聚焦目标**：迫使模型关注与运动相关的目标区域，抑制背景干扰，提升了特征的代表性。
        *   **进一步提效**：通过自适应地丢弃大量低信息量的背景Token，**再次显著减少了需要主干网络处理的Token数量**。

### **三、 解决方案的总体思路**
论文的解决方案可以概括为 **“频域增强 + 运动引导稀疏”** 的双重策略：
*   **前端（频域）**：在输入主干网络之前，就在频域完成模态融合与增强，以“质”的提升（更好的特征）和“量”的减少（更少的Token）作为起点。
*   **中端（运动引导）**：在进入主干网络进行深度特征交互学习之前，利用事件独有的运动信息进行“精准制导”式的空间筛选，实现“好钢用在刀刃上”。
*   **后端**：将经过筛选的、高质量的少量Token送入标准的ViT主干网络和跟踪头进行最终预测。

### **四、 实际价值与效果**
*   **性能**：在FE108、FELT和COESOT三个主流RGB-Event跟踪基准测试上达到了最先进（SOTA）或极具竞争力的性能，尤其在**精度**指标上表现突出。
*   **效率**：与简单的特征拼接基线相比，在仅增加约290万参数的情况下，**计算复杂度大幅降低**，实现了约27 FPS的实时推理速度，在精度和效率间取得了良好平衡。
*   **鲁棒性**：在低光照、快速运动、遮挡等挑战性属性上表现优异，证明了该方法能有效利用事件模态弥补RGB模态的固有缺陷，提升跟踪系统的实用性。

**总结**：本文的核心创新在于**从“在哪里融合”和“处理什么数据”两个根本层面进行重新设计**，通过**频域早期融合**和**运动引导自适应稀疏化**，巧妙地解决了RGB-Event跟踪中特征利用不充分和计算效率低下的问题，为多模态高效跟踪提供了新思路。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决RGB-事件相机融合目标跟踪中存在的两个核心问题：一是现有方法通常在特征层面进行融合，未能充分利用事件相机高动态范围和高时间分辨率的独特优势；二是多模态联合处理导致计算复杂度高、效率低下。为此，论文提出了一个名为APMTrack的新框架，其核心创新在于**在频域进行早期融合**，通过解耦振幅和相位注意力机制，将事件模态的高频信息选择性地融入RGB模态，以增强特征表示并减少输入主干网络的令牌数量。同时，框架引入了一个**运动引导的空间稀疏化模块**，利用事件数据对运动敏感的特性，自适应地过滤冗余背景区域，聚焦于目标相关特征。实验在FE108、FELT和COESOT三个基准数据集上进行，结果表明该方法在保持高跟踪精度的同时，显著降低了计算开销，在精度与效率之间取得了良好平衡。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking》针对RGB-Event视觉目标跟踪任务，提出了一个新颖的框架。其核心创新点在于**从传统的特征级融合转向频域早期融合**，并设计了两个关键模块来充分利用事件相机的特性，同时显著提升计算效率。

以下是其相对于已有工作的明确创新点，逐条列出并分析：

### 1. **频域解耦振幅与相位注意力机制**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的RGB-Event跟踪方法大多在空间域进行特征级融合（例如，通过Transformer进行跨模态注意力交互）。这种方式往往忽略了事件数据在频域的特性（如高频运动信息），并且直接融合异构模态可能导致模糊或不对齐。
     - **本文方法**：提出将RGB和事件模态**通过快速傅里叶变换（FFT）转换到频域**，并**解耦为振幅和相位两个分量**。然后，设计独立的**振幅注意力和相位注意力**，选择性地将事件模态的高频信息（如目标轮廓和运动细节）融合到RGB模态中。
   - **解决的具体问题/带来的优势**：
     - **问题**：传统特征融合难以有效利用事件相机的高动态范围和运动敏感性，且对低信息区域处理冗余。
     - **优势**：
       1. **增强特征表示**：通过频域融合，将事件的高频细节（在低光照、快速运动下更鲁棒）注入RGB图像，增强了RGB模态在挑战性场景下的空间结构表示。
       2. **显著降低计算负担**：融合后**仅保留增强后的RGB模态**作为主干网络输入，**丢弃了原始事件模态的输入token**。这直接将输入主干网络的token数量减半，大幅减少了计算复杂度（FLOPs从~1167G降至~608G）。

### 2. **运动引导的空间稀疏化策略**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：传统的单流或双流跟踪器需要处理搜索区域的所有空间token（即密集计算），包括大量冗余的背景区域。一些动态剪枝方法（如DynamicViT）缺乏针对跟踪任务的、基于跨模态信息的引导。
     - **本文方法**：设计了一个**运动引导的空间稀疏化模块**。该模块首先从事件体素中学习目标相关的运动信息，然后通过一个轻量级评分估计器生成每个空间位置属于目标相关区域的概率图。最后，根据该概率图的方差，**自适应地选择Top-K个最相关的token**输入主干网络。
   - **解决的具体问题/带来的优势**：
     - **问题**：RGB和事件数据联合处理会引入大量计算，且背景区域干扰目标特征学习。
     - **优势**：
       1. **进一步降低计算成本**：通过自适应地筛选掉低信息背景token，**再次减少了需要处理的token数量**，使整体计算量进一步降低（从~608G降至~701G）。
       2. **提升特征质量**：迫使模型聚焦于与目标运动最相关的区域，**增强了目标相关特征的表示**，抑制了背景干扰，从而提升了跟踪精度（特别是在遮挡、背景杂乱等场景）。

### 3. **基于FFT的差分视觉Transformer**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：用于运动信息提取的模块通常是标准的卷积网络或Transformer，在噪声较多的运动特征上可能效果不佳。
     - **本文方法**：在运动引导模块内部，提出了一个**Diff-FFT ViT**。它在频域内计算两个独立注意力图的差分，并应用高斯窗来抑制噪声频率分量。
   - **解决的具体问题/带来的优势**：
     - **问题**：从事件流中提取的运动特征可能包含噪声，直接使用可能影响后续稀疏化选择的准确性。
     - **优势**：
       1. **更鲁棒的运动特征提取**：在频域进行差分操作能更有效地**捕捉目标相关的动态线索**，同时抑制噪声。
       2. **提升稀疏化精度**：更干净、更聚焦的运动特征使得后续的空间概率分布估计更准确，从而让Top-K选择更有效。

### 4. **整体框架设计：早期融合与高效计算协同**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：大多数SOTA跟踪器要么追求精度而牺牲效率（处理所有token），要么进行简单的特征拼接或相加，未能系统性地从输入层面减少冗余。
     - **本文方法**：将**频域早期融合**与**运动引导稀疏化**两个模块**串联成一个整体高效框架**。前者在融合阶段减半输入，后者在特征提取前再次动态剪枝。
   - **解决的具体问题/带来的优势**：
     - **问题**：RGB-Event跟踪模型普遍存在计算复杂度高的问题，难以兼顾精度与效率。
     - **优势**：
       1. **实现了精度与效率的平衡**：在三个主流数据集（FE108, FELT, COESOT）上达到或超过了SOTA精度，同时**参数量仅小幅增加，计算量大幅下降**，推理速度达到27 FPS，具备实时性潜力。
       2. **充分发挥事件相机优势**：系统性地利用了事件相机的**高动态范围**（通过频域融合增强纹理）和**高时间分辨率/运动敏感性**（通过运动引导聚焦目标），解决了RGB相机在极端光照和快速运动下的性能瓶颈。

---

**总结**：本文的核心创新在于**范式转变**——从空间域特征融合转向频域早期融合，并辅以动态稀疏化。这不仅更优雅地解决了异构模态融合问题，还通过**“先融合减半，再动态筛选”** 的两阶段策略，**从根本上**减少了主干网络的计算负荷，为开发高效、鲁棒的多模态跟踪器提供了新思路。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 使用的数据集与评价指标
#### 1. 数据集
论文在三个公开的RGB-Event跟踪基准数据集上进行了全面评估：
- **FE108**：包含108个室内视频，涵盖低光照、高动态范围、快速运动等挑战场景。76个用于训练，32个用于测试。
- **FELT**：大规模长时跟踪数据集，包含1044个视频序列，平均时长超过1.5分钟，定义了14种挑战属性（如遮挡、快速运动、低光照）。
- **COESOT**：类别广泛的RGB-Event跟踪数据集，包含1354个视频序列，覆盖90个对象类别，定义了17种挑战因素。

#### 2. 评价指标
采用视觉跟踪领域广泛使用的三个指标：
- **成功率（SR）**：预测边界框与真实边界框的IoU超过设定阈值的帧所占比例。
- **精确率（PR）**：预测中心点与真实中心点的欧氏距离小于20像素的帧所占比例。
- **归一化精确率（NPR）**：预测中心点与真实中心点的距离，使用真实边界框对角线长度进行归一化。

### 二、 对比的基线方法
论文与大量先进的跟踪器进行了对比，主要包括：
- **纯RGB跟踪器**：如ATOM、DiMP、PrDiMP、TransT、STARK、OSTrack、MixFormer、AiATrack等。
- **RGB-Event跟踪器**：如FENet、ViPT、SDSTrack、UnTrack、CEUTrack、MCITrack、SUTrack等。
- **高效/长时跟踪器**：如SeqTrack、ARTrackv2、HIPTrack、ODTrack、AQATrack等。

### 三、 关键性能提升与结论
#### 1. 整体性能表现
- **在FE108数据集上**：取得了**95.2的PR（最高）**和64.4的SR。PR指标显著领先，表明模型在目标中心定位上非常精确。
- **在FELT数据集上**：取得了**新的SOTA性能**：SR 56.5， PR 72.3， NPR 67.9。尤其在PR和NPR上优于第二名的SUTrack（+1.4 PR， +1.3 NPR），证明了其在长时、复杂场景下的鲁棒性。
- **在COESOT数据集上**：取得了**83.3的PR（最高）**和68.0的SR。PR指标大幅领先其他SOTA方法（如UnTrack的80.9），再次验证了其定位精度。

#### 2. 核心结论
- **技术创新有效**：提出的**频域解耦振幅-相位注意力**和**运动引导空间稀疏化**两大模块，在提升性能的同时显著降低了计算开销。
- **效率与精度平衡**：与简单的特征拼接基线相比，参数量仅增加2.9M，但计算量（FLOPs）从1167.4G大幅降低至701.0G，同时性能（SR/PR）从66.9/81.9提升至68.0/83.3。推理速度达到**27 FPS**，具备实时性。
- **挑战场景鲁棒性强**：在COESOT数据集的12个挑战属性分析中，该方法在**11个属性上取得了最高成功率**，特别是在**低光照（LI）、背景物体运动（BOM）、完全遮挡（FOC）** 等极端场景下优势明显。这证明了其能有效利用事件相机的高动态范围和运动敏感性。

#### 3. 消融实验验证
- **模块有效性**：逐项添加振幅注意力、相位注意力、MGSS模块、Diff-FFT ViT模块，性能持续提升，验证了每个组件的必要性。
- **方案优越性**：
    - **输入数据**：RGB与事件体素（voxel）融合效果最佳。
    - **融合方法**：频域振幅-相位注意力优于简单的相加或拼接。
    - **ViT变体**：Diff-FFT ViT优于标准ViT和Diff-ViT。
    - **稀疏化策略**：运动引导自适应Top-K选择优于随机丢弃、候选消除等方法。
    - **自适应函数**：指数衰减型Top-K函数效果最佳。

### 四、 可视化与定性分析
- **早期融合效果**：可视化显示，融合后RGB图像的高频细节（如目标轮廓）得到增强。
- **空间稀疏化**：MGSS模块能有效聚焦于目标相关区域，过滤冗余背景。
- **注意力与响应图**：模型在多种挑战下能稳定、准确地关注目标，并生成强而精确的响应峰值。
- **跟踪结果对比**：在低光照、快速运动等序列中，本文方法生成的边界框与真实值最吻合，而基线方法容易出现漂移或误识别。

### 总结
论文通过系统的定量实验和定性分析，充分证明了所提APMTrack框架的有效性。它在三个主流RGB-Event跟踪数据集上取得了具有竞争力的或SOTA的性能，特别是在定位精度（PR）上表现突出。同时，通过频域早期融合和运动引导稀疏化，在显著降低计算复杂度的前提下实现了性能提升，较好地平衡了**精度与效率**，为RGB-Event视觉物体跟踪领域提供了一个高效且鲁棒的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.01022v1)
- [HTML 版本](https://arxiv.org/html/2601.01022v1)
