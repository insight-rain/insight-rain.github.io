# RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference

**相关性评分**: 6.0/10

**排名**: #78


---


## 基本信息

- **arXiv ID**: [2601.01712v1](https://arxiv.org/abs/2601.01712v1)
- **发布时间**: 2026-01-05T01:34:06Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Jiarui Wang, Huichao Chai, Yuanhang Zhang, Zongjin Zhou, Wei Guo, Xingkun Yang, Qiang Tang, Bo Pan, Jiawei Zhu, Ke Cheng, Yuting Yan, Shulan Wang, Yingjie Zhu, Zhengfan Yuan, Jiaqi Huang, Yuhan Zhang, Xiaosong Sun, Zhinan Zhang, Hong Zhu, Yongsheng Zhang, Tiantian Dong, Zhong Xiao, Deliang Liu, Chengzhou Lu, Yuan Sun, Zhiyuan Chen, Xinming Han, Zaizhu Liu, Yaoyuan Wang, Ziyang Zhang, Yong Liu, Jinxin Xu, Yajing Sun, Zhoujun Yu, Wenting Zhou, Qidong Zhang, Zhengyong Zhang, Zhonghai Gu, Yibo Jin, Yongxiang Feng, Pengfei Zuo

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

RelayGR 是一个通过跨阶段接力推理优化生成式推荐系统推理效率的生产系统，支持更长序列并提升吞吐量。

## 摘要

Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\times$.

## 详细分析

## 论文摘要：RelayGR: 通过跨阶段接力推理扩展长序列生成式推荐

### 1. 研究背景和动机
现代工业级实时推荐系统采用多阶段级联架构（召回、预处理、精排），并在严格的尾部延迟（P99）服务等级目标（SLO）下运行，留给精排阶段的时间通常只有几十毫秒。**生成式推荐（GR）模型**通过处理长用户行为序列能提升推荐质量，但在线部署时，其可处理的序列长度受限于精排阶段的P99预算，导致无法发挥其离线训练中展现的长序列优势。本文观察到，GR模型的大部分计算消耗在**与候选物品无关的用户长期行为前缀**上，这为提前计算并缓存该前缀提供了机会。

### 2. 核心方法和技术创新
本文提出了 **RelayGR** 系统，实现了 **HBM内存内的接力式推理**。其核心思想是将用户长期行为前缀的计算从精排关键路径上剥离，提前到召回阶段进行，并将生成的每层KV缓存跨阶段保留，供后续精排阶段直接复用。系统通过三大技术创新解决工业级部署挑战：
- **序列感知触发器**：仅对可能违反P99 SLO的“高风险”长序列请求，触发前缀预计算，控制缓存负载和内存占用。
- **亲和性感知路由器**：通过一致性哈希，确保前缀预计算请求和后续精排请求被路由到同一个计算实例，避免昂贵的跨服务器缓存获取。
- **内存感知扩展器**：利用服务器本地DRAM作为二级缓存，捕获同一用户的短期跨请求复用，并通过串行化控制避免冗余加载。

### 3. 主要实验结果
在基于华为昇腾NPU的生产镜像环境中，使用真实查询进行评估：
- **支持更长序列**：在固定P99 SLO下，RelayGR支持的最大输入序列长度相比基线提升了**1.5倍**。
- **提升系统吞吐**：在满足SLO的前提下，系统吞吐量最高提升了**3.6倍**。
- **优雅降级**：随着序列长度增长，RelayGR的吞吐下降速度远慢于基线，并将精排延迟稳定控制在预算内。

### 4. 研究意义和价值
RelayGR解决了将长序列GR模型应用于生产环境的核心系统瓶颈。它提出并实现了 **“晚期绑定放置下的生命周期缓存”** 这一新系统抽象，将优化从单阶段内核级提升到跨阶段系统级。这项工作不仅为工业界部署更强大的GR模型提供了可行的系统路径，其设计原则（选择性准入、亲和性路由、分层缓存）也对其他需要在严格延迟约束下进行跨阶段状态复用的AI服务系统具有借鉴意义。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：RelayGR

### **一、 核心问题**
论文旨在解决**工业级实时推荐系统中，生成式推荐模型在线部署的根本性瓶颈**：
- **问题本质**：生成式推荐模型通过处理**长用户行为序列**能显著提升推荐质量，但其在线推理的计算开销巨大。
- **直接矛盾**：在线服务的**精细排序阶段**必须在极严格的尾部延迟SLO（例如P99 ≤ 50毫秒）内完成，这严重限制了模型能处理的在线序列长度，导致线上性能远低于离线训练/评估时（可使用数千个行为token）的水平。
- **后果**：为了满足延迟SLO，生产系统被迫大幅**压缩在线序列长度**，从而无法充分发挥GR模型的缩放潜力，形成了“第一道障碍”。

### **二、 核心创新点**
论文的核心创新在于提出并实现了一套名为 **“RelayGR”** 的生产系统，其本质是一种 **“跨阶段接力推理”** 的架构。它并非提出新的模型，而是从**系统层面**创新性地解决了上述矛盾。

**核心洞察**：GR模型的输入中，大部分计算消耗在**与候选物品无关的、长期稳定的用户行为前缀**上。这部分计算可以提前进行并复用。

**三大技术创新组件**：

1.  **序列感知触发器**
    - **作用**：决定**何时**进行前缀预推理。并非对所有请求都进行预推理。
    - **机制**：在检索阶段并行检查轻量级用户行为元数据（如序列长度），仅对可能违反排序阶段P99 SLO的“高风险”长序列请求进行“准入”。
    - **价值**：将无限制的优化转化为有界问题，**严格控制了额外的计算负载和HBM内存占用**，防止预推理本身成为新瓶颈。

2.  **亲和感知路由器**
    - **作用**：解决**在哪里**消费缓存的问题，确保缓存被本地复用。
    - **机制**：通过**一致性哈希**，强制将同一个用户的`pre-infer`信号和后续的排序请求路由到同一个“特殊”排序实例。这建立了一个“亲和性契约”。
    - **价值**：将“晚期绑定”的路由决策转化为“早期绑定”的放置保证，**彻底消除了排序关键路径上的远程缓存获取**，这是满足严格延迟SLO的关键。

3.  **内存感知扩展器**
    - **作用**：解决**如何**安全地扩展缓存复用范围。
    - **机制**：利用服务器本地的**DRAM**作为二级缓存，捕获同一用户的短期跨请求复用（如快速刷新）。
    - **关键设计**：通过**每用户串行化**和“伪预推理”步骤，确保即使请求乱序到达，也最多触发一次DRAM到HBM的重新加载，避免了冗余传输和尾部延迟抖动。
    - **价值**：在**不违反“无远程获取”原则**的前提下，显著提高了缓存命中率，进一步提升了系统吞吐量。

### **三、 解决方案总结**
RelayGR通过一个**系统级的、协同设计的“接力赛”机制**来解决长序列GR的在线部署难题：

- **思路**：将耗时的、与物品无关的**长序列用户行为前缀计算**，从排序关键路径上剥离，提前到检索阶段进行（“预推理”），并将生成的每层KV缓存暂存。
- **保障**：通过上述三大组件，确保这些缓存能够**安全地存活**于请求的整个生命周期（跨检索、预处理、排序阶段），并最终在排序阶段被**本地、高效地复用**。
- **目标**：使得排序阶段仅需处理增量部分（短期行为、交叉特征、候选物品），从而在**不突破原有P99 SLO**的前提下，大幅提升系统所能支持的**在线序列长度上限**和**SLO合规吞吐量**。

### **四、 实际价值与效果**
- **性能提升**：在固定的P99 SLO下，RelayGR支持的最长序列长度提升至**1.5倍**，SLO合规吞吐量提升至**3.6倍**。
- **工业可行性**：论文在华为昇腾NPU上实现了原型，并在模拟生产环境（使用真实查询）中验证了其有效性。它为解决“**生命周期缓存下的晚期绑定放置**”这一独特的系统问题提供了可落地的设计方案。
- **通用性**：该方法被证明对不同的GR模型架构、更深的模型、更大的嵌入维度以及不同的NPU类型都有效，展现了良好的泛化能力。

**总而言之，RelayGR的核心贡献是提供了一个系统级的“加速器”，它通过巧妙的跨阶段计算复用，打破了生成式推荐模型在线服务中序列长度与延迟SLO之间的根本性冲突，使其缩放潜力得以在生产环境中释放。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决工业级实时推荐系统中，**生成式推荐模型因严格的尾部延迟SLO限制而无法在线利用长用户行为序列**的核心问题。为此，论文提出了 **RelayGR系统**，其核心思想是通过**跨阶段接力式推理**，将独立于候选物品的长序列用户行为前缀提前计算并缓存其KV状态，供后续排序阶段复用，从而将长序列计算移出排序关键路径。该系统通过序列感知触发器、亲和感知路由器和内存感知扩展器三个关键技术，实现了在严格延迟约束下的生命周期缓存与消费。最终，在华为昇腾NPU上的生产镜像环境评估表明，RelayGR在固定P99 SLO下，能将系统支持的最大序列长度提升至**1.5倍**，并将满足SLO的吞吐量提升至**3.6倍**。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference》针对工业级实时推荐系统中部署长序列生成式推荐（GR）模型的核心瓶颈，提出了一套创新的系统解决方案。其创新点明确且具有深度，主要体现在以下三个方面：

### 1. **问题定义与核心洞察的创新：首次系统性地定义了“生命周期缓存与晚期绑定放置”问题**
   - **相比以往方法的改进/不同之处**：
     - **与LLM服务系统对比**：现有的LLM服务系统（如Splitwise、DistServe）也广泛使用KV缓存（前缀缓存），但其场景是“预填充-解码”两阶段图，**缓存的生产和消费是紧耦合、即时发生的**（预填充后立即解码），且缓存通常在用户间共享（如公共提示词）。系统可以在阶段边界明确地传递缓存。
     - **与推荐系统内部优化对比**：传统的推荐系统优化（如Merlin）主要关注**单阶段内**的优化，例如算子融合、嵌入缓存、特征服务加速等，属于“常数因子”优化。
     - **本文的洞察**：论文首次明确指出，在GR推荐场景中，缓存复用面临一个**根本性不同的系统问题**：用户行为前缀缓存是**用户特定**的，且其**生产（检索阶段）和消费（精排阶段）被预处理等多个管道阶段分隔**。消费节点（精排实例）只有在预处理完成后才能确定（即“晚期绑定”）。这带来了缓存**跨阶段存活**、**避免远程获取**和**高QPS下负载控制**的独特挑战。
   - **解决的具体问题/带来的优势**：
     - **精准定位瓶颈**：将阻碍长序列GR在线部署的首要障碍明确为“**精排阶段P99延迟预算**”，而非单纯的模型计算效率。
     - **奠定设计基础**：这一核心问题定义直接引导了RelayGR整个系统的设计目标——实现“**晚期绑定放置下的生命周期缓存**”，确保缓存能在请求生命周期内存活并被本地消费。

### 2. **系统设计创新：提出了“接力赛推理”的三位一体协同设计机制**
   RelayGR没有采用单一的缓存策略，而是通过三个协同工作的组件，系统性地解决了工业级部署的三大挑战：

   - **创新点A：序列感知触发器**
     - **相比以往方法的改进/不同之处**：不同于LLM中可能为所有请求创建缓存，或推荐系统中无差别的预计算，该组件引入了**轻量级、准入控制式的选择性预推断**。
     - **解决的具体问题/带来的优势**：
       1.  **负载与资源安全**：仅对“有风险”（即序列长、特征维度高，可能违反精排P99）的请求触发预推断，**严格限制了额外的计算（NPU）和传输（PCIe）负载**，防止优化本身成为新瓶颈。
       2.  **内存占用可控**：通过结合准入速率 `Q_admit` 和生命周期窗口 `T_life`，**理论上限定了HBM中同时存活的缓存数量**，确保了缓存能在需要的时间窗口内驻留，而不会因无限增长被驱逐。

   - **创新点B：亲和感知路由器**
     - **相比以往方法的改进/不同之处**：传统负载均衡器（如轮询、最少连接）旨在**平衡独立请求**。本文设计通过**一致性哈希**，将同一用户相关的预推断请求和后续精排请求**强制路由到同一个“特殊实例”**。
     - **解决的具体问题/带来的优势**：
       1.  **消除关键路径远程获取**：这是实现“接力赛”的核心。它**将晚期的、不确定的消费节点绑定到早期的生产节点**，确保精排阶段能**本地访问**HBM中的缓存，完全避免了跨服务器网络获取带来的、可能超过整个精排预算的延迟。
       2.  **保证正确性下的优化**：若亲和性被破坏（如实例故障），系统会安全地回退到标准推理，**保证了服务的鲁棒性**。

   - **创新点C：内存感知扩展器**
     - **相比以往方法的改进/不同之处**：不同于构建一个全局的、分布式的KV缓存池（可能引入远程访问），该组件**利用服务器本地的DRAM作为HBM的扩展层**，并设计了精巧的并发控制。
     - **解决的具体问题/带来的优势**：
       1.  **扩展复用范围**：捕获同一用户**短期内的重复请求**（如快速刷新），将复用从单个请求生命周期扩展到短时间窗口，进一步提升效率。
       2.  **避免冗余传输与竞争**：通过**每用户串行化队列**和“伪预推断”步骤，确保对同一用户的并发或乱序请求，**最多只触发一次DRAM到HBM的加载**，防止了重复的H2D传输和由此引发的尾部延迟恶化。

### 3. **工程实现与验证创新：在真实生产约束下验证了系统级增益**
   - **相比以往方法的改进/不同之处**：
     - 多数研究在理想或小规模环境下验证模型效果或单一优化点。本文在**生产镜像环境**中，基于**华为昇腾NPU**实现了完整原型，并使用**真实查询流量**进行评估，严格遵循**工业级P99 SLO约束**。
     - 评估指标聚焦于**系统级能力**：在固定SLO下可支持的**最大序列长度**和**SLO合规吞吐量**，而非单纯的加速比或模型精度。
   - **解决的具体问题/带来的优势**：
     - **证实可行性**：证明了“接力赛推理”这一系统设计思路在真实工业场景中是可行且高效的。
     - **量化核心价值**：在固定P99 SLO下，RelayGR带来了**1.5倍的最大序列长度提升**和**3.6倍的吞吐量提升**。这直接解决了论文开篇指出的核心矛盾——**在线序列长度被严重限制，无法发挥GR离线训练的缩放收益**。
     - **展示泛化性**：实验表明，RelayGR的收益不仅适用于序列长度缩放，也适用于**嵌入维度增大、模型加深**等“宽度”缩放，并 across 不同的GR模型和NPU硬件，说明了其设计的一般性。

**总结**：RelayGR的创新是系统性的，它并非提出一个新的GR模型，而是构建了一个**使能系统**，将GR模型的离线潜力释放到在线生产环境中。其核心贡献在于识别了一个独特的系统问题，并为此设计了一套协同的、生产就绪的解决方案，通过“**选择性预计算、亲和性路由、本地化扩展**”的接力机制，在严格的服务质量约束下，显著提升了长序列GR服务的效能边界。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

### 数据集与实验环境
- **数据集**：使用了**真实的生产查询流量**，未指定公开数据集名称，但实验环境是**生产镜像环境**，模拟了工业级推荐系统的流量和约束。
- **实验环境**：在华为昇腾（Ascend）NPU上实现并部署，模拟了标准的三阶段推荐流水线（检索 → 粗排 → 精排）。
- **模型**：评估了多种实际使用的生成式推荐（GR）架构及其变体，包括 **HSTU** 及其修订版本，以及内部GR模块。序列长度从1K到数万token不等。

### 评价指标
1.  **最大支持序列长度**：在满足流水线SLO（P99 ≤ 135 ms）且成功率 ≥ 99.9% 的前提下，系统能处理的最大输入序列长度。
2.  **SLO合规吞吐量**：在满足相同P99约束下，系统（或每个特殊实例）每秒能处理的查询数（QPS）。
3.  **尾部延迟（P99）**：关键组件的P99延迟，包括：
    - **`pre`**：前缀预推理延迟。
    - **`load`**：从DRAM到HBM的缓存加载延迟。
    - **`rank`**：使用缓存进行精排的延迟。
4.  **组件利用率**：如NPU（cube）利用率。
5.  **缓存命中率**：DRAM层缓存的命中率（例如 0%, 10%, 50%, 100%）。

### 对比的基线方法
1.  **Baseline**：**生产标准配置**，无中继推理。精排阶段在严格的P99预算内**在线执行完整的GR推理**（包含长序列前缀计算）。
2.  **RelayGR**：仅使用**HBM内中继推理**，无DRAM重用（DRAM命中率0%）。
3.  **RelayGR + x%**：在RelayGR基础上，增加**服务器本地DRAM重用**，其中 `x%` 为测得的DRAM命中率（通过控制DRAM预算实现，如500GB对应~10%命中率）。

### 关键性能提升与结论

#### 1. 扩展序列长度能力
- **结论**：RelayGR显著提高了在固定P99 SLO下可支持的**最大序列长度**。
- **数据**：
    - 使用所有可用DRAM（~500GB，~10%命中率）时，最大支持序列长度相比Baseline提升达 **1.5倍**。
    - 若提供更多存储（如2TB/4TB DRAM实现50%/100%命中率），可进一步扩展序列长度上限。

#### 2. 提升吞吐量（QPS）
- **结论**：在相同P99约束下，RelayGR大幅提高了系统的合规吞吐量。
- **数据**：
    - 在实验设置中，RelayGR（纯HBM）相比Baseline，吞吐量有显著提升。
    - **RelayGR + DRAM重用**（利用全部500GB DRAM）相比Baseline，在满足SLO的前提下，吞吐量提升最高达 **3.6倍**。

#### 3. 改善尾部延迟与并发能力
- **结论**：RelayGR将昂贵的长前缀计算移出精排关键路径，从而在更高并发下仍能满足P99 SLO。
- **数据**：
    - 在固定序列长度下，随着并发请求数增加，Baseline的P99延迟迅速超标，而RelayGR能支持约 **2倍** 的并发量而不违反SLO。
    - 组件延迟分析显示，`rank`（使用缓存的精排）延迟远低于完整的基线推理，且随序列长度增长缓慢。

#### 4. 系统设计的有效性验证
- **亲和性路由的必要性**：实验对比了“本地缓存访问”（RelayGR）与“远程缓存获取”（无亲和性设计）。远程获取延迟可达本地访问的**数百倍**，且极易超过请求生命周期窗口，这强有力地证明了RelayGR“**精排关键路径无远程获取**”这一设计原则的正确性与必要性。
- **优雅降级**：随着序列长度增长，所有方法的吞吐量都会下降，但RelayGR的下降曲线**远缓于Baseline**。Baseline在超过~6K token后吞吐量骤降至个位数QPS，而RelayGR即使无DRAM重用，在超过6K token后仍能维持**数十QPS**。

#### 5. 扩展性验证
- **嵌入维度与模型深度**：当嵌入维度从128增加到1024，或模型层数加深时，RelayGR相比Baseline始终能提供**2倍至4倍**的SLO合规吞吐量优势，证明了其对于模型“宽度”和“深度”扩展的有效性。
- **通用性**：在不同GR模型（如HSTU, LONGER+RankMixer）和不同昇腾NPU型号（如Ascend 310, 910C）上测试，RelayGR带来的**相对性能趋势保持一致**，均能扩展最大序列长度并提升合规吞吐量。

### 总结
论文通过在生产镜像环境中的系统实验，**给出了明确且显著的定量结果**。RelayGR通过其创新的**跨阶段中继推理**设计，成功地将生成式推荐模型的在线序列长度上限提升了1.5倍，并将系统在严格尾部延迟约束下的吞吐量提升了高达3.6倍。这验证了RelayGR是解决工业级推荐系统中“**长序列GR模型在线服务与严格P99预算矛盾**”的一个有效且实用的系统级方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.01712v1)
- [HTML 版本](https://arxiv.org/html/2601.01712v1)
