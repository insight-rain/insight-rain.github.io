# Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies

**相关性评分**: 7.0/10

**排名**: #16


---


## 基本信息

- **arXiv ID**: [2601.21251v1](https://arxiv.org/abs/2601.21251v1)
- **发布时间**: 2026-01-29T04:17:56Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh

## 关键词

Diffusion, Inference Acceleration, fine tune, world model

## 一句话总结

该论文提出了一种基于扩散模型的混合专家策略，通过紧凑技能基础和自适应专家激活，实现机器人多任务操作的高效推理和可扩展性。

## 摘要

Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.

## 详细分析

## 论文摘要

### 1. 研究背景和动机
扩散策略在机器人单任务操作中表现出色，但其扩展到多任务场景面临挑战：直接扩大模型规模会带来高昂的推理成本和数据需求。现有基于技能抽象或专家混合的方法，往往未能显式地解耦可重用技能，导致技能可解释性和跨任务迁移性受限。因此，如何在保持高效推理的同时，实现可扩展、可迁移的多任务操作，是机器人学中的一个关键开放性问题。

### 2. 核心方法和技术创新
本文提出了**技能专家混合策略**，这是一个基于扩散模型的专家混合框架，其核心创新在于：
- **状态自适应的正交技能基**：通过QR分解生成一个随机器人状态变化的正交基，将动作空间解耦为互不重叠的技能方向，确保每个专家贡献正交。
- **粘性路由**：采用具有时间“粘性”的Dirichlet-Markov先验对专家门控进行正则化，使得技能激活在时间上平滑、分段恒定，形成类似“拾取-移动-放置”的稳定行为阶段。
- **自适应专家激活**：在推理时，根据路由器输出的门控值动态选择一小部分最相关的专家进行计算，显著降低了激活参数和推理延迟。
- **变分训练目标**：设计了一个结合重构损失、门控正则化、系数空间扩散损失和对齐损失的训练目标，以联合学习技能基、门控和专家。

### 3. 主要实验结果
在RoboTwin-2和RLBench-2仿真环境以及真实双臂机器人平台上进行了多任务学习和迁移学习评估：
- **多任务性能**：SMP在多个双臂操作任务上取得了最高的平均成功率（例如，在RoboTwin-2上为54%），优于DP、ACT、RDT等基线方法。
- **计算效率**：通过自适应激活，SMP在推理时平均仅激活约30%的参数，推理时间（~107 ms）远低于大型扩散基线（如RDT的183 ms），实现了性能与效率的更好权衡。
- **技能可重用性**：学习到的技能基自然地对应了左/右臂角色和操作阶段。在少样本迁移和技能重组任务中，SMP通过微调路由器或少量专家，能有效适应新任务，证明了其良好的技能迁移和组合能力。

### 4. 研究意义和价值
本研究为可扩展的多任务机器人操作提供了一条实用路径：**“学习一次可重用技能，仅激活所需部分，并在任务变化时快速适应”**。SMP通过显式的技能解耦和高效的稀疏计算，在提升多任务泛化性能的同时，大幅降低了推理成本，使其更适用于需要实时控制的实际机器人应用。该方法在技能可解释性、跨任务迁移性和计算效率之间取得了良好平衡，推动了高效能机器人策略学习的发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文标题**
《通过专家混合扩散策略抽象机器人操作技能》

### **核心问题**
扩散策略在单任务机器人操作中表现出色，但扩展到**多任务场景**时面临两大挑战：
1.  **模型规模与数据成本高昂**：依赖扩大模型规模（如RDT）会导致推理速度慢，难以实时控制，且所需演示数据量随任务多样性指数增长。
2.  **技能可重用性与可解释性差**：现有的专家混合（MoE）方法缺乏对可重用操作技能的显式解耦和表示，导致技能纠缠、路由不稳定，限制了策略的可解释性和跨任务迁移能力。

### **核心创新点：SMP框架**
论文提出了**技能专家混合策略（SMP）**，一个旨在高效学习、组合和迁移**可重用操作技能**的扩散策略框架。其创新是一个紧密结合的系统，而非单一技术。

#### **1. 技术创新**
- **状态自适应的正交技能基**：
    - **问题**：传统MoE中，专家输出可能重叠，导致路由不稳定、技能难以辨识。
    - **解决**：SMP为每个状态 `s` 学习一个正交矩阵 `B(s)`，其列向量构成一组**局部解耦**的技能方向。每个专家仅在一个正交方向上生成动作分量，确保了技能贡献的**可加性与非重叠性**。
    - **实现**：通过一个轻量级网络生成无约束矩阵 `W(s)`，然后通过**带符号稳定的QR分解**将其投影到Stiefel流形，得到平滑变化的正交基 `B(s)`。

- **粘性路由与门控机制**：
    - **问题**：标准路由可能频繁切换，导致动作振荡，不符合操作任务通常具有的**阶段性**（如抓取、移动、放置）。
    - **解决**：引入**粘性狄利克雷-马尔可夫先验**来建模门控 `g_t` 的时序动态。当前时刻的门控概率分布倾向于与前一刻相似，从而产生**分段恒定、相位一致**的专家激活模式。
    - **价值**：提高了行为的稳定性和可解释性，路由切换更少，技能与任务阶段（如左臂平移、右臂旋转）自然对齐。

- **自适应专家激活**：
    - **问题**：在每一步评估所有专家计算成本高。
    - **解决**：在推理时，根据状态路由器的输出 `ḡ_t`，仅激活一个**紧凑的、任务相关的专家子集**。通过定义专家“质量” `m_i = (ḡ_{t,i})^2`，并采用**Top-k**或**覆盖阈值**策略选择专家。
    - **价值**：大幅降低推理时的活跃参数量和计算延迟，实现**高效实时采样**。

- **基于变分推断的联合训练目标**：
    - 设计了一个统一的训练目标 `ℒ_SkillMoE`，整合了：
        1.  **重构损失**：在正交基中重构动作，梯度会更新基 `B(s)`。
        2.  **系数正则损失**：在系数空间 `z` 上应用标准扩散损失（DDPM），使用**梯度截断**的基，防止该损失干扰基的学习。
        3.  **门控正则损失**：强制门控后验接近粘性先验。
        4.  **对齐损失**：将训练时依赖动作的门控后验与一个**仅依赖状态的路由器**对齐，确保推理时仅凭状态即可有效路由。

#### **2. 方法论创新**
- **技能的新定义**：将“技能”定义为**状态自适应的正交动作基元**——在局部动作几何中解耦，其跨时间和任务的一致激活模式形成了更高层次的角色（如左臂操作、精细调整）。
- **“局部白化”动作空间**：通过在 `B(s)` 定义的正交子空间中操作，将高维动作生成问题转化为对低维、解耦的技能系数的生成问题，简化了学习并提高了稳定性。

### **解决方案总结**
SMP通过以下方式系统性地解决了多任务泛化与效率问题：
1.  **抽象可重用技能**：利用状态自适应正交基，从演示中学习一组紧凑、解耦的技能基元。
2.  **稀疏且稳定的组合**：通过粘性路由，在每一步仅用少量相关技能组合成最终动作，行为具有相位一致性。
3.  **高效推理**：自适应专家激活机制在保持性能的同时，动态选择所需专家，降低计算成本。
4.  **易于迁移**：学到的技能基和专家模块可以在新任务中**冻结**，仅通过微调路由器即可快速重组技能，实现少样本适应。

### **实际价值**
- **性能与效率的平衡**：在模拟（RoboTwin-2, RLBench-2）和真实双臂机器人实验中，SMP在**多任务学习**和**迁移学习**上取得了比大型扩散基线（如RDT）**更高或相当的成功率**，同时**显著降低了推理成本和延迟**（活跃参数量约为RDT的7%）。
- **可解释性**：学到的专家自然地对应了有意义的角色（如左/右臂控制、抓取/放置阶段），提高了策略的透明度和可分析性。
- **为可扩展的多任务操作提供了一条实用路径**：**“学一次技能，按需激活，快速适应新任务”**。这降低了大规模部署对计算资源和数据量的需求，使实时、复杂的多任务操作更具可行性。

**简而言之，SMP的核心创新在于将扩散模型的强大生成能力与结构化、可解释的技能抽象相结合，并通过高效的稀疏计算机制，实现了多任务机器人操作中性能、效率与可迁移性的统一提升。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对扩散策略在多任务机器人操作中因模型规模和数据需求过大而难以扩展的问题，提出了一种名为**技能专家混合策略（SMP）**的创新框架。该方法的核心是学习一个**状态自适应的正交技能基**，并结合**粘性路由**机制，在每一步仅动态激活一小部分任务相关的专家来组合动作，从而实现了技能的抽象与复用。通过引入变分训练目标和自适应专家激活策略，SMP在保证性能的同时显著降低了推理成本。实验表明，该方法在仿真和真实双臂平台上，相比大型扩散基线，在多项多任务学习和迁移学习任务中取得了更高的成功率，同时推理效率大幅提升，为可扩展、可迁移的多任务操作提供了一条实用路径。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Skill Mixture-of-Experts Diffusion Policies》提出了一种名为**SMP**的新型机器人操作策略框架。其核心创新在于将**扩散模型**、**专家混合架构**与**显式的技能抽象**相结合，以解决多任务机器人操作中模型规模、计算成本和泛化能力之间的平衡问题。

以下是其相对于已有工作的明确创新点：

### 1. **引入状态自适应的正交技能基**
- **改进/不同之处**： 以往基于MoE的扩散策略（如Sparse Diffusion Policy）或技能发现方法（如Discrete Policy）通常学习一个**固定的、全局的**技能表示或专家输出。SMP则通过一个轻量级神经网络生成一个**状态依赖的、非约束的矩阵** `W(s)`，然后通过**带符号稳定的QR分解**将其投影到Stiefel流形上，得到一个**状态自适应的正交基** `B(s)`。这个基的列向量随机器人状态（如末端执行器位姿、接触状态）平滑变化。
- **解决的问题/带来的优势**：
    - **解决技能纠缠问题**： 固定基或非正交基可能导致不同专家的贡献在动作空间中重叠，使得技能难以解耦和解释。正交基确保了每个技能在一个**局部解耦的、非重叠的方向**上起作用。
    - **提升泛化与适应性**： 动作的几何意义（如平移、旋转）随机器人构型而变化。状态自适应基允许技能方向根据当前任务几何动态调整，从而更准确地表示和组合底层运动基元，提高了跨任务的技能重用性和对新任务的适应能力。

### 2. **设计“粘性”门控机制与先验**
- **改进/不同之处**： 标准的MoE路由或技能切换机制通常是**瞬时、无记忆**的，可能导致门控值在每一步高频振荡。SMP为门控 `g_t` 设计了一个**Dirichlet-Markov先验模型**，使其具有“粘性”。具体来说，当前时刻的门控分布参数依赖于前一时刻的门控值 (`κ * g_{t-1}`) 和一个全局使用向量 `ϑ`。
- **解决的问题/带来的优势**：
    - **解决行为相位不稳定问题**： 机器人操作任务通常由多个准稳态阶段（如抓取、移动、放置）组成。粘性门控鼓励门控值在一段时间内保持稳定，减少了专家切换的频率，从而产生了**相位一致、更稳定**的行为轨迹。
    - **提升可解释性与技能语义**： 稳定的门控激活模式使得每个专家更容易与一个持续的、有语义的角色（如“左臂平移”、“右臂抓取”）关联起来，如图1所示，增强了策略的可解释性。

### 3. **提出自适应专家激活推理机制**
- **改进/不同之处**： 传统MoE在推理时通常需要评估所有专家，或简单地选择Top-k。SMP提出了一种**基于覆盖率的自适应激活**策略。它根据状态路由器的输出均值 `ḡ_t`，计算每个专家的“质量” `m_i = (ḡ_{t,i})^2`，然后贪婪地选择专家，直到所选专家的累积质量达到总质量的一个阈值（如 `τ_m = 0.95`）或达到预设的专家数量上限。
- **解决的问题/带来的优势**：
    - **解决计算效率与性能的权衡问题**： 该机制允许模型在“简单”状态只激活少量关键专家，在“复杂”状态激活更多专家。这实现了**动态的、状态依赖的稀疏性**，而非固定的稀疏性。
    - **显著降低推理成本**： 如表2所示，SMP在推理时平均只激活约30%的参数（约80M），远低于其总参数量（259M）和基线大模型（如RDT的1200M），同时保持了高性能。其推理时间（107ms）也低于大多数扩散基线，使其更适用于实时控制。

### 4. **构建基于变分推断的联合训练目标**
- **改进/不同之处**： SMP将整个生成过程（动作、门控、技能系数）形式化为一个**统一的变分推断框架**。其训练损失 `ℒ_SkillMoE` 明确地结合了多个部分：
    1.  **重构损失 (`ℒ_recon`)**： 在正交基中重构动作，梯度会更新基 `B(s)`。
    2.  **系数正则化损失 (`ℒ_coeff`)**： 对技能系数 `z_t` 使用标准的扩散模型损失（DDPM），但使用**梯度截断**的基来投影目标，防止该损失干扰基的学习。
    3.  **门控正则化损失 (`ℒ_gate`)**： 强制门控后验接近前述的粘性Dirichlet-Markov先验。
    4.  **对齐损失 (`ℒ_align`)**： 将训练时依赖动作的门控后验与一个**仅依赖状态的路由器**对齐，确保推理时仅凭状态即可做出有效的路由决策。
- **解决的问题/带来的优势**：
    - **解决训练不稳定与技能识别性问题**： 该框架为技能解耦（通过正交基）、时间一致性（通过粘性门控先验）和高效推理（通过状态路由器）提供了**理论支撑和端到端的训练方法**。分离的重构目标和系数目标避免了优化冲突，稳定了训练。
    - **实现技能与控制的分离**： 训练出的“仅状态路由器”使得在**迁移学习**中，可以冻结学习到的技能专家和基，仅微调路由器来重组现有技能以完成新任务，实现了高效的小样本适应。

### 总结
SMP的核心创新是**系统性**的：它并非单一技术的改进，而是通过**状态自适应正交基** + **粘性门控** + **自适应激活** + **变分训练框架**这一组合，共同解决了多任务扩散策略中存在的**技能纠缠、行为不稳定、计算成本高、迁移能力弱**等相互关联的问题。其实验结果表明，该方案在保持高成功率的同时，显著降低了推理开销，为可扩展、可迁移的多任务机器人操作提供了一条实用路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

该论文通过仿真和真实机器人实验，全面评估了所提出的**技能专家混合策略（SMP）** 在多任务学习和迁移学习场景下的性能。实验旨在验证SMP在**性能（成功率）** 和**效率（推理成本）** 两方面的优势。

### 一、 使用的数据集与实验环境

1.  **仿真环境**：
    *   **RoboTwin-2**：一个用于双臂操作的可扩展数据生成器和基准测试平台，包含多种需要双臂协作的任务。
    *   **RLBench-2**：一个用于评估双臂操作任务的基准测试套件，强调时空协作。

2.  **真实机器人平台**：
    *   **PiPER平台**：一个轻量级双臂机器人平台，用于执行四项真实世界的双臂操作任务（摘笔帽、放杯子、递工具、倒豆子）。

### 二、 评价指标

1.  **主要性能指标**：
    *   **任务成功率**：在仿真环境中，每个任务运行100个回合，计算平均成功率。
    *   **进度得分**：在真实机器人实验中，根据任务完成的关键步骤定义进度，计算平均进度得分。

2.  **效率指标**：
    *   **模型参数量**：总参数量 (`𝒩ₚ`) 和推理时激活的参数量 (`𝒩ₚ^act`)。
    *   **推理时间**：生成一个动作所需的平均时间 (`T_inf`)。
    *   **激活专家数**：推理时平均激活的专家数量 (`𝒩ₑ^act`)。

### 三、 对比的基线方法

论文与多种先进的机器人策略方法进行了对比，涵盖不同范式：
*   **扩散策略基线**：
    *   **DP (Diffusion Policy)**：标准的条件扩散视觉运动策略。
    *   **DP3 (3D Diffusion Policy)**：基于点云的3D感知扩散策略。
    *   **RDT**：一个大型（12亿参数）的扩散-Transformer基础模型。
*   **其他策略范式**：
    *   **ACT**：基于Transformer的动作分块策略。
*   **技能抽象与MoE方法**：
    *   **Discrete Policy**：基于VQ-VAE和潜在扩散模型的离散技能抽象方法。
    *   **Sparse Diffusion Policy**：在扩散网络层中插入前馈网络专家（FFN-MoE）的方法。

### 四、 关键实验结果与性能提升

#### 1. 多任务学习性能
*   **结果**：在RoboTwin-2和RLBench-2的多任务学习中，SMP取得了**最高或并列最高的平均成功率**。
*   **数据**：
    *   **RoboTwin-2 (6个任务)**：SMP平均成功率为 **0.54**，优于DP (0.29)、DP3 (0.33)、ACT (0.34)、RDT (0.48)、Discrete Policy (0.40) 和 Sparse DP (0.44)。
    *   **RLBench-2 (4个任务)**：SMP平均成功率为 **0.18**，同样优于或持平于所有基线。
*   **结论**：SMP通过正交技能基和粘性路由，有效地抽象和复用跨任务的技能，从而在复杂的双臂多任务场景中实现了更优的泛化能力。

#### 2. 推理效率
*   **结果**：SMP在保持高性能的同时，显著**降低了推理计算成本和延迟**。
*   **数据**：
    *   **激活参数量**：SMP总参数量为258.9M，但得益于自适应专家激活机制，平均仅激活 **80.2M** 参数。这远低于RDT的1200M全参数激活，也低于Sparse DP的110.1M激活参数。
    *   **推理时间**：SMP的推理时间为 **107.3 ms**，低于DP (120.3 ms)、DP3 (122.1 ms) 和 RDT (183.1 ms)，与ACT (94.8 ms) 接近，但性能更高。
*   **结论**：SMP的“按需激活”机制实现了计算资源的动态稀疏化，为实时控制提供了可行的路径。

#### 3. 迁移学习能力
*   **少样本适应**：在RoboTwin-2上预训练后，仅用**10条演示**微调以适应4个新任务。SMP平均成功率为 **0.38**，显著优于DP (0.13)、RDT (0.20) 和 Discrete Policy (0.31)。
*   **技能组合**：**冻结**专家和技能基，仅微调路由器来组合已有技能以完成新任务。SMP成功率为 **0.30**，优于Sparse DP (0.25)。
*   **结论**：SMP学习到的技能具有高度的**可复用性和可组合性**，能够快速适应新任务，证明了其技能抽象的有效性。

#### 4. 真实机器人验证
*   **结果**：在四项真实双臂任务上，SMP取得了**最高的平均进度得分**。
*   **结论**：SMP在存在传感器噪声、标定误差和执行延迟的真实世界中，依然能产生稳定、协调的双臂行为，验证了其实际应用潜力。

### 五、 核心结论

1.  **性能与效率的平衡**：SMP成功地在不显著增加模型容量和推理成本的前提下，解决了扩散策略在多任务场景下面临的泛化挑战。它通过**学习紧凑、正交、状态自适应的技能基**和**粘性路由**来实现这一点。
2.  **可解释的技能涌现**：分析表明，SMP学习到的专家自然地对应了有意义的角色（如左/右臂）和操作阶段（抓取、移动、放置），证明了其技能抽象的**可解释性**。
3.  **实用的迁移路径**：SMP为可扩展的多任务操作提供了一条实用路径：**一次性学习可复用技能，运行时仅激活所需部分，任务变更时可快速适配**。

**总结**：论文通过系统的实验证明，SMP在仿真和真实机器人双臂操作任务中，在**成功率、推理效率、技能可迁移性和可解释性**方面均优于现有主流基线方法，为实现高效、通用的多任务机器人操作提供了有前景的技术方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.21251v1)
- [HTML 版本](https://arxiv.org/html/2601.21251v1)
