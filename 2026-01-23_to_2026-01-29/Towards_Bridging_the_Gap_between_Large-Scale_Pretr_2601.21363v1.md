# Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control

**相关性评分**: 6.0/10

**排名**: #18


---


## 基本信息

- **arXiv ID**: [2601.21363v1](https://arxiv.org/abs/2601.21363v1)
- **发布时间**: 2026-01-29T07:43:24Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su, Jingwen Zhang

## 关键词

fine tune, offline Reinforcement Learning, world model

## 一句话总结

该论文提出一种结合大规模预训练和基于模型微调的方法，用于人形机器人控制，以提高样本效率和适应性。

## 摘要

Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.

## 详细分析

## 论文摘要

**论文标题：** 弥合人形机器人控制中大规模预训练与高效微调之间的差距

### 1. 研究背景和动机
强化学习（RL）是人形机器人控制的主流方法，但现有方法存在显著局限。主流方法如PPO虽能通过大规模并行仿真实现鲁棒训练，但其**样本效率低**，难以在新环境中安全、高效地适应。虽然离策略RL和基于模型的方法能提高样本效率，但在人形机器人领域，**大规模预训练与高效微调之间仍存在鸿沟**。具体表现为：直接在新环境中进行随机探索风险高，而从头训练基于模型的系统则耗时漫长且易陷入局部最优。

### 2. 核心方法和技术创新
本文提出了 **LIFT**（大规模预训练与高效微调）框架，包含三个阶段：
- **（i）大规模策略预训练**：采用**JAX实现的高效SAC算法**，支持大批量更新和高“更新-数据比”（UTD），在单GPU上1小时内完成大规模并行仿真训练，并实现策略到真实机器人的**零样本部署**。
- **（ii）物理信息世界模型预训练**：利用SAC预训练阶段收集的数据，离线训练一个**物理信息世界模型**。该模型结合已知的拉格朗日动力学与神经网络残差预测器，以精确模拟接触力等未建模效应。
- **（iii）策略与世界模型高效微调**：在新环境中，**仅执行确定性策略**收集数据，而将**随机探索限制在世界模型内部**的仿真推演中。这种“探索-执行”分离机制，在保障安全的同时，利用模型提高了样本效率。

### 3. 主要实验结果
- **预训练**：LIFT在多个仿真任务中达到或超越了PPO、FastTD3等基线的性能与收敛速度，并成功实现了从仿真到真实T1人形机器人的零样本户外行走。
- **微调**：在仿真到仿真（MuJoCo到Brax）的迁移任务中，面对**分布内、长尾和分布外**的不同目标速度，LIFT均能稳定收敛并精确跟踪，而SAC、PPO、FastTD3等基线方法则出现性能退化或发散。
- **消融实验**：验证了SAC预训练和世界模型预训练对成功微调均至关重要；**物理信息世界模型**在数据有限下的泛化能力显著优于纯神经网络模型（如MBPO）。
- **初步真实世界微调**：仅用数分钟的真实机器人数据，成功将一个零样本迁移失败的策略微调为能稳定行走的策略。

### 4. 研究意义和价值
LIFT框架首次系统性地**弥合了人形机器人控制中大规模仿真预训练与数据高效微调之间的差距**。其核心价值在于：
- **实用性强**：提供了一套从快速预训练、零样本部署到安全高效微调的完整开源流程，为机器人社区提供了新的实用基线。
- **安全性高**：通过将随机探索局限于世界模型内，极大降低了在真实昂贵机器人上直接试错的风险。
- **效率突出**：耦合了预训练阶段的**时钟时间高效性**（大规模并行仿真）与微调阶段的**样本高效性**（基于模型学习），为人形机器人持续学习提供了一条可行路径。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
论文旨在解决**人形机器人控制**中存在的关键矛盾：**大规模预训练**（追求快速、鲁棒的策略学习）与**高效微调**（追求在新环境/任务中快速、安全地适应）之间的**鸿沟**。

- **传统方法（如PPO）**：擅长利用大规模并行仿真进行快速预训练，甚至能实现零样本部署到真实机器人。但其**样本效率低**，难以在收集数据昂贵或危险的新环境中进行安全、高效的适应。
- **现有高效方法（如基于模型的RL）**：样本效率高，但**从头开始训练**对于不稳定的人形机器人来说非常缓慢、不稳定，且直接在真实环境中进行随机探索风险极高。

### **核心创新点：LIFT框架**
论文提出了 **LIFT（Large-scale pretraIning and efficient FineTuning）** 框架，这是一个**三阶段**的预训练-微调流程，旨在结合两者的优势。

1.  **第一阶段：大规模策略预训练**
    - **技术创新**：提供了一个**可扩展的JAX版SAC实现**，支持**大批量更新**和**高更新数据比（UTD）**。
    - **解决方式**：利用大规模并行仿真（如MuJoCo Playground）快速、鲁棒地训练出基础运动策略。此策略可实现**零样本部署**到真实人形机器人（论文中在1小时内完成训练）。

2.  **第二阶段：物理信息世界模型预训练**
    - **技术创新**：在SAC预训练收集的数据上，**离线预训练一个物理信息世界模型**。该模型结合了已知的**拉格朗日刚体动力学**和一个**神经网络残差预测器**，以捕捉接触力等未建模效应。
    - **解决方式**：为微调阶段提供一个高保真、可微分、能进行长时域滚动的动力学模型，作为安全的“想象”环境。

3.  **第三阶段：策略与世界模型协同微调**
    - **核心创新**：提出 **“环境内确定性执行，模型内随机探索”** 的微调范式。
        - **在真实/新环境中**：执行预训练策略的**确定性动作**（均值），避免随机探索带来的安全风险。
        - **在世界模型中**：使用**随机策略**进行探索，生成丰富的合成数据用于策略优化。
    - **解决方式**：将**数据收集的风险**与**策略改进所需的探索**进行**解耦**。既保证了微调过程的安全性，又通过模型内的高效探索提升了样本效率。

### **实际价值**
1.  **工程实用性**：提供了一个从大规模仿真预训练到真实机器人零样本部署，再到安全高效微调的**完整、开源**技术栈。
2.  **性能验证**：
    - **预训练**：性能与主流基线（PPO, FastTD3）相当或更优，且收敛更快。
    - **微调**：在仿真到仿真、以及初步的真实世界实验中，LIFT能使用**极少量数据**（约几分钟到十几分钟的交互）成功适应新任务（如跟踪新的目标速度），而基线方法（SAC, PPO, FastTD3, SSRL）则表现不稳定或失败。
    - **安全性**：通过将随机探索限制在学得的世界模型内，显著降低了在脆弱的人形机器人上进行在线学习的风险。
3.  **为社区提供基线**：论文释放的代码和框架，为人形机器人控制研究提供了一个结合了**仿真效率**与**真实世界适应性**的强有力基准。

### **总结**
LIFT框架的核心创新在于**系统性地设计了一个三阶段管道**，并提出了**环境-模型探索解耦**的微调策略。它**不是单一算法的突破**，而是通过**巧妙整合**现有技术（SAC、物理信息模型），解决了人形机器人学习中长期存在的“预训练快但难适应”与“适应好但训练慢/危险”之间的矛盾，为实现**持续学习、能适应新环境的通用人形智能体**迈出了坚实的一步。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决人形机器人控制中**大规模预训练与高效微调之间的鸿沟**。现有方法（如PPO）虽能通过大规模并行仿真实现零样本部署，但其低样本效率限制了在新环境中的安全适应能力。为此，论文提出了 **LIFT框架**，它包含三个阶段：1）使用经过优化的SAC算法进行大规模并行仿真预训练，以获得可零样本部署的策略；2）利用预训练数据离线训练一个**物理信息世界模型**，该模型结合了已知的拉格朗日动力学与学习得到的残差项；3）在新环境中进行高效微调，其关键创新在于**将随机探索限制在世界模型内部进行**，而在真实环境中仅执行确定性策略动作，从而兼顾了安全性与样本效率。实验表明，该方法在预训练阶段实现了快速收敛与零样本仿真到实物的转移，在微调阶段仅需少量真实数据便能稳定适应新任务，显著提升了人形机器人策略的适应能力和学习效率。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control》提出了一个名为 **LIFT** 的三阶段框架，旨在弥合人形机器人控制中大规模预训练与高效微调之间的鸿沟。其核心创新点如下：

---

### 1. **提出一个结合大规模离策略预训练与物理信息世界模型微调的统一框架**
   - **改进/不同之处**：
     - **以往方法**：主流方法通常采用**同策略算法（如PPO）**进行大规模并行预训练，以实现快速收敛和零样本部署。然而，PPO的**低样本效率**限制了其在数据有限的新环境中的适应能力。虽然存在基于模型的微调方法（如SSRL），但它们通常**从头开始训练**，对于不稳定的人形机器人来说既慢又不安全。
     - **本文方法**：LIFT框架创造性地将**大规模离策略预训练（阶段i）** 与**基于物理信息世界模型的微调（阶段iii）** 相结合。它使用**SAC**进行预训练，然后利用预训练数据离线训练一个**物理信息世界模型（阶段ii）**，最后在微调阶段**仅在真实环境中执行确定性策略**，而将**随机探索限制在世界模型内部**。
   - **解决的问题/优势**：
     - **解决了**：1) 同策略预训练策略难以高效适应新环境的问题；2) 纯模型方法从头训练人形机器人时**wall-clock时间过长、易陷入局部最优、安全性差**的问题。
     - **优势**：实现了**预训练阶段的高效（快速收敛、零样本部署）** 与**微调阶段的高样本效率和安全**的统一。微调时避免了在真实机器人上进行危险随机探索的风险。

### 2. **开发了支持大规模并行仿真、高UTD比的高效SAC JAX实现**
   - **改进/不同之处**：
     - **以往方法**：SAC等离策略算法在**大规模并行仿真**中的应用研究有限，通常被认为不稳定或未被充分利用于快速预训练。FastTD3等并行方法使用了**每环境混合高斯噪声**进行探索。
     - **本文方法**：论文提供了一个**完全用JAX编写的SAC实现**，支持**大批量更新和高“更新-数据”比**。它**完全依赖SAC策略本身的状态依赖随机性进行探索**，无需额外注入动作噪声。通过系统化的超参数调优，在单块RTX 4090 GPU上**一小时内**即可完成预训练并实现零样本部署到真实机器人。
   - **解决的问题/优势**：
     - **解决了**：离策略算法在大规模并行设置下**训练不稳定、wall-clock时间优势不明显**的问题。
     - **优势**：证明了SAC可以作为**高效、稳健的大规模预训练骨干**，为人形控制提供了除PPO外的另一个强大基线，且其**离策略性质**天然更适合后续的模型微调。

### 3. **设计了“环境内确定性执行，模型内随机探索”的安全微调策略**
   - **改进/不同之处**：
     - **以往方法**：在微调新环境时，许多方法（包括SAC、TD3、甚至一些模型方法如Dreamer）仍需在**真实环境中进行随机探索**，这对支撑面小、不稳定的人形机器人构成**高风险**。ASAP等方法使用动作修正网络，但输出可能无界，导致不安全行为。
     - **本文方法**：在微调阶段，**在真实环境中仅执行预训练策略的确定性均值动作**以收集数据。**所有随机探索都限制在已预训练的物理信息世界模型内部**进行。模型 rollout 时还加入了**基于物理的安全终止检查**。
   - **解决的问题/优势**：
     - **解决了**：在真实或高风险仿真环境中微调时，**随机探索可能导致机器人跌倒或损坏**的核心安全问题。
     - **优势**：**极大提高了微调过程的安全性**，使得在有限且“干净”的真实机器人数据上进行高效、稳定的策略改进成为可能。分离探索与执行是该方法安全性的关键。

### 4. **构建并优化了适用于人形机器人的物理信息世界模型**
   - **改进/不同之处**：
     - **以往方法**：SSRL为四足机器人引入了结合拉格朗日动力学与残差预测器的世界模型。然而，直接将其用于人形机器人存在**训练不稳定**的问题。纯神经网络世界模型（如MBPO）在数据有限时**泛化能力差**，容易产生物理上不合理的预测。
     - **本文方法**：1) **修正了SSRL实现中从特权状态到Brax广义状态的映射错误**；2) **在特权状态中明确加入了基座高度**，论文发现这对于人形机器人的世界模型稳定性**至关重要**，而四足机器人可以忽略；3) 使用**自回归多步损失**进行训练，提高了预测精度。
   - **解决的问题/优势**：
     - **解决了**：1) 现有物理信息模型代码在人形设定下的**实现偏差**；2) 人形机器人动力学建模中**对高度信息敏感**的特殊需求；3) 纯数据驱动模型在OOD动作下**预测失效**的问题。
     - **优势**：所构建的世界模型**泛化能力更强、预测更物理可信**，为在模型内部进行可靠的长期rollout和策略优化奠定了基础。消融实验表明，该模型显著优于MBPO风格的集成模型。

### 5. **提供了一个从预训练、零样本部署到微调的完整开源流程**
   - **改进/不同之处**：
     - **以往工作**：许多研究要么只关注**模拟到真实的零样本迁移**，要么只关注**从零开始的样本高效学习**，缺乏一个**端到端的、公开的**流程来展示如何将大规模预训练策略安全高效地适应到新任务或新环境中。
     - **本文方法**：论文不仅提出了方法，还**发布了完整的代码和实验流程**，涵盖了在MuJoCo Playground中的大规模SAC预训练、在Brax中的模拟到模拟微调、以及**在真实Booster T1机器人上的初步微调验证**。
   - **解决的问题/优势**：
     - **解决了**：机器人社区缺乏一个**统一的、可复现的基准**来同时评估预训练效率和微调能力的问题。
     - **优势**：为后续研究提供了一个**实用的、高起点的工作框架**，降低了人形机器人强化学习的研究门槛，并强调了**持续学习**的重要性。

---

**总结**：LIFT框架的核心创新在于其**系统性的整合**。它并非发明全新的单一算法，而是通过**精心设计的流程**，将离策略学习、物理先验模型、安全探索机制与大规模并行计算的优势结合起来，**针对性地解决了人形机器人控制中“快速获得基础能力”与“安全高效适应新情况”这一对长期存在的矛盾**。其实验在模拟和初步真实场景中都验证了该框架在**收敛速度、零样本迁移能力、微调样本效率和安全性**方面的综合优势。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心实验效果
论文提出的 **LIFT 框架** 在**大规模预训练**和**高效微调**两个阶段均取得了显著效果，成功弥合了二者之间的鸿沟，实现了：
1.  **快速、鲁棒的大规模预训练**：在单块 NVIDIA RTX 4090 GPU 上，**1小时内**即可完成人形机器人运动策略的预训练，并实现**零样本（zero-shot）** 部署到真实机器人。
2.  **安全、高效的策略微调**：在新环境或分布外任务中，仅需**数万步（约数分钟）** 的真实交互数据，即可完成策略的稳定微调与性能提升，显著优于基线方法。

### 二、 使用的数据集与仿真环境
论文**未使用**传统的静态数据集，所有数据均在**交互式仿真环境**中在线生成。

1.  **预训练阶段**：
    *   **仿真器**：MuJoCo Playground
    *   **任务与机器人平台**：
        *   **Booster T1**：低维（12-DoF，仅腿部）和全维（23-DoF，含头、腰、臂、腿）版本。
        *   **Unitree G1**：全维（29-DoF）。
    *   **地形**：平坦地形与粗糙地形。
    *   **任务类型**：速度跟踪（Joystick）任务。

2.  **微调与评估阶段**：
    *   **仿真器**：Brax（用于 sim-to-sim 迁移评估）。
    *   **真实世界**：Booster T1 实体机器人（用于 sim-to-real 微调验证）。

### 三、 评价指标
1.  **主要指标**：**平均未折扣回报**。在1024个并行环境中评估，每个episode长度为1000步，计算累计奖励的平均值。
2.  **关键性能指标**：
    *   **收敛速度**：达到峰值性能所需的**挂钟时间（预训练）** 或**环境交互步数（微调）**。
    *   **跟踪精度**：机器人**基座的前向速度**与目标速度的匹配程度。
    *   **零样本部署成功率**：预训练策略能否在未经调整的情况下，在真实机器人上稳定行走。
    *   **微调稳定性**：在有限数据下，策略能否稳定提升而不崩溃。

### 四、 对比的基线方法
论文与五类主流基线方法进行了全面对比：

| 类别 | 基线方法 | 核心特点 |
| :--- | :--- | :--- |
| **Model-Free (On-Policy)** | **PPO** | 大规模并行训练的主流基线，样本效率低。 |
| **Model-Free (Off-Policy)** | **SAC** | 样本效率更高的离线策略算法。 |
| | **FastTD3** | 针对并行仿真优化的TD3变体，声称支持sim-to-real。 |
| **Model-Based (From Scratch)** | **SSRL** | 结合物理先验的世界模型方法，从零开始训练。 |
| **Model-Based (Ensemble)** | **MBPO** | 使用神经网络集成作为世界模型的经典方法。 |

### 五、 关键性能提升与结论

#### 1. 预训练阶段 (回答 Q1)
*   **性能相当，收敛更快**：在六种人形机器人任务（不同机器人×不同地形）上，LIFT（使用SAC）的最终回报与 **PPO**、**FastTD3** 相当或略高。
*   **关键优势**：在**粗糙地形**上，LIFT 能**更快地稳定**在峰值性能，显示了更好的训练稳定性。
*   **零样本部署验证**：预训练的策略成功实现了**零样本**转移到真实的 Booster T1 机器人，能在草地、上下坡、泥地等多样户外环境中行走。

#### 2. 微调阶段 (回答 Q2)
在 sim-to-sim 迁移（MuJoCo Playground → Brax）的**速度跟踪任务**中，LIFT 展现出**压倒性的微调优势**：

*   **任务设定**：
    *   **分布内**：目标速度在预训练范围 `[-1, 1] m/s` 内（如 0.6 m/s）。
    *   **长尾分布**：目标速度在预训练范围内但数据稀少。
    *   **分布外**：目标速度超出预训练范围（如 1.0, 1.5 m/s）。

*   **对比结果**：
    *   **LIFT**：在所有任务上均能**稳定收敛**，速度跟踪准确，身体振荡小。
    *   **SAC (无模型)**：在确定性数据收集下**迅速过拟合并发散**。
    *   **PPO**：初期性能尚可，但随后**逐渐退化并最终崩溃**。
    *   **FastTD3**：表现**剧烈振荡**，无法收敛。
    *   **SSRL (从零开始)**：在0.6 m/s任务中能学习但未达目标速度；在更高速度任务（1.0-1.5 m/s）上**完全失败**。这凸显了**预训练的必要性**。

*   **数据效率**：LIFT 仅需约 **`4×10^4` 环境步数**（对应真实时间约800秒）即可完成微调，证明了其**样本高效性**。

#### 3. 消融实验 (回答 Q3)
*   **预训练至关重要**：移除SAC预训练（即退化为SSRL）会导致策略陷入局部最优，仅能站立几乎无法前进。移除世界模型预训练会**显著降低微调样本效率**。
*   **物理先验世界模型是关键**：将LIFT的物理信息世界模型替换为 **MBPO** 风格的纯神经网络集成模型后，微调**完全失败**。因为后者对分布外动作的预测会产生物理上不可信的状态，导致训练不稳定。
*   **超参数影响**：
    *   **预训练**：较高的**更新数据比（UTD）**、较大的**批次大小**和**回放缓冲区**能加速收敛。
    *   **微调**：较小的**熵系数（α）** 和**多步自回归损失**能带来更稳定、更优的收敛。

### 六、 实际世界微调初步验证
在真实的 Booster T1 机器人上，使用一个 sim-to-real 性能较弱的策略作为起点，LIFT 框架仅通过收集 **80-590秒** 的真实数据，就使机器人表现出：
*   更**直立**的姿态
*   更**平滑**的步态
*   更**稳定**的前向速度
这证明了 LIFT 能用**极少量真实数据**强化一个较弱的策略，具备实际应用潜力。

**结论**：LIFT 框架通过 **SAC大规模预训练** 与 **物理信息世界模型高效微调** 的结合，在保持训练速度的同时，显著提升了人形机器人策略在新环境中的**适应性、安全性和样本效率**，为持续、高效的人形机器人学习提供了一条实用路径。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.21363v1)
- [HTML 版本](https://arxiv.org/html/2601.21363v1)
