# Real-Time Robot Execution with Masked Action Chunking

**相关性评分**: 6.0/10

**排名**: #26


---


## 基本信息

- **arXiv ID**: [2601.20130v1](https://arxiv.org/abs/2601.20130v1)
- **发布时间**: 2026-01-27T23:48:32Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Haoxuan Wang, Gengyu Zhang, Yan Yan, Yuzhang Shang, Ramana Rao Kompella, Gaowen Liu

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Acceleration, Edge Deployment, fine tune

## 一句话总结

该论文提出REMAC方法，通过掩码动作分块学习纠正调整，解决异步推理中动作与感知不匹配问题，提升机器人实时执行的可靠性和性能。

## 摘要

Real-time execution is essential for cyber-physical systems such as robots. These systems operate in dynamic real-world environments where even small delays can undermine responsiveness and compromise performance. Asynchronous inference has recently emerged as a system-level paradigm for real-time robot manipulation, enabling the next action chunk to be predicted while the current one is being executed. While this approach achieves real-time responsiveness, naive integration often results in execution failure. Previous methods attributed this failure to inter-chunk discontinuity and developed test-time algorithms to smooth chunk boundaries. In contrast, we identify another critical yet overlooked factor: intra-chunk inconsistency, where the robot's executed action chunk partially misaligns with its current perception. To address this, we propose REMAC, which learns corrective adjustments on the pretrained policy through masked action chunking, enabling the policy to remain resilient under mismatches between intended actions and actual execution during asynchronous inference. In addition, we introduce a prefix-preserved sampling procedure to reinforce inter-chunk continuity. Overall, our method delivers more reliable policies without incurring additional latency. Extensive experiments in both simulation and real-world settings demonstrate that our method enables faster task execution, maintains robustness across varying delays, and consistently achieves higher completion rates.

## 详细分析

## 论文摘要：基于掩码动作分块的实时机器人执行

### 1. 研究背景和动机
实时执行对于机器人等网络物理系统至关重要。在动态的真实世界环境中，即使是微小的延迟也可能破坏系统的响应能力并导致任务失败。**异步推理**作为一种系统级范式，通过在执行当前动作块的同时预测下一个动作块，为实现实时响应提供了可能。然而，将异步推理与**动作分块**策略简单结合时，常因**块间不连续性**和**块内不一致性**（即机器人的执行动作与当前感知部分错位）而导致性能显著下降。现有方法主要关注于平滑块间边界，却忽视了块内不一致性这一关键因素。本文旨在同时解决这两个问题，以提升异步推理下机器人策略的鲁棒性和成功率。

### 2. 核心方法和技术创新
本文提出了 **REMAC** 方法，其核心创新在于通过**训练时**的调整来增强预训练策略的鲁棒性，而非引入额外延迟的测试时修正。主要技术包括：
- **掩码动作分块**：在训练时，根据随机采样的推理延迟 `d`，对动作块的前 `d` 个时间步应用**前缀掩码**，使损失函数仅关注于“待执行”的动作部分。这迫使策略学会在部分动作已因延迟而被“锁定”的情况下，仍能生成合理的后续动作。
- **自条件课程学习**：在训练输入中，逐步用预训练策略自身的预测来替代真实动作，以模拟测试时的条件，缓解暴露偏差并提升策略的自我修正能力。
- **残差对齐损失**：除了标准损失，额外引入一个损失项，鼓励学习到的修正与预训练策略输出和真实目标之间的残差对齐。
- **前缀保留采样**：在推理时，将正在执行的动作作为下一个动作块生成时的固定前缀，仅对剩余部分进行采样，从而增强块间连续性。
- **高效实现**：采用 **LoRA** 对预训练策略进行参数高效的微调，仅引入约1.5%的额外参数，且推理时可将LoRA权重合并回主干模型，实现**零额外延迟**。

### 3. 主要实验结果
在包含12个动态任务的Kinetix仿真环境和三个真实世界抓放任务上进行了广泛评估：
- **性能领先**：在仿真中，REMAC在所有延迟设置下（`d=0` 到 `4`）的成功率和任务完成速度均显著优于基线方法（Naive Async、BID、RTC）。即使在较大延迟下，性能下降也更平缓。
- **真实世界验证**：在Franka机器人上，REMAC在具有挑战性的精细操作任务中取得了最高的平均完成进度，并且在额外注入延迟（模拟恶劣网络条件）时表现出更强的鲁棒性。
- **模块化与兼容性**：消融实验验证了各组件（掩码、课程学习、残差损失）的有效性。REMAC还可与现有的测试时方法（如BID、RTC）结合，获得进一步的性能提升。
- **泛化性**：方法不仅适用于流匹配策略，也能成功迁移到基于Transformer的动作分块策略（如ACT）上。

### 4. 研究意义和价值
本研究首次明确指出并系统性地解决了异步推理中**块内不一致性**这一被忽视的关键问题。REMAC提供了一种**高效、通用且无需增加推理延迟**的解决方案，通过训练时适应使策略对执行与感知的错位具有内在的鲁棒性。其价值在于：
- **实际应用**：为在资源受限、网络延迟波动的真实场景中部署大型视觉-语言-动作模型提供了可靠的技术路径，是实现**实用化实时机器人控制**的重要一步。
- **方法论贡献**：提出的掩码分块和自条件课程学习框架，为改进序列生成模型在存在部分确定前缀条件下的性能提供了新思路。
- **开源与可复现**：论文提供了详细的方法描述、实验设置和开源代码，有助于推动该领域的进一步研究。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究问题**
论文旨在解决**机器人实时异步执行**中的核心挑战。在异步推理范式下（即预测下一个动作块的同时执行当前块），虽然能保证实时性，但会引入两个关键问题，导致执行失败率显著上升：
1.  **块间不连续性**：连续动作块在边界处可能产生不连贯的跳跃。
2.  **块内不一致性**：由于推理延迟，当前执行的部分动作是基于过时观测生成的，与当前实际感知状态不匹配。**论文指出，这是一个被先前工作忽视的关键失败因素。**

### **核心创新点**
论文提出了 **REMAC** 方法，其创新性主要体现在**问题视角、解决方案和实现方式**三个层面：

1.  **提出了“块内不一致性”这一关键问题**：明确指出并形式化了异步执行中由延迟导致的感知-动作错配问题，而不仅仅是关注块间平滑。

2.  **训练时自适应方法**：与主流在测试时进行启发式平滑或梯度修正的方法不同，REMAC 采用**训练时微调**策略，从根本上增强策略对延迟和错配的鲁棒性，且**不引入额外推理延迟**。

3.  **掩码动作分块技术**：这是REM**A**C的核心。
    *   **前缀掩码**：在训练时，根据随机采样的延迟 `d`，**掩码掉动作块中前 `d` 个已执行/已提交的动作**，只对剩余“待执行”部分进行监督。这迫使策略学会在部分动作已确定（可能已过时）的情况下，生成与之连贯且适应当前状态的后缀动作。
    *   **自条件课程**：用预训练策略的预测结果与真实动作混合，作为训练输入，并逐步增加自预测的比例。这模拟了测试时策略依赖自身历史预测的条件，缓解了曝光偏差。
    *   **残差对齐**：鼓励微调策略的输出与预训练策略输出的**残差**，对齐于真实动作与预训练输出的残差。这显式地学习了“校正”行为。

4.  **前缀保留采样**：在推理时，将上一块中正在执行的动作作为下一块生成的**固定前缀**，只对新动作部分进行采样积分，从而**显式保证块间的连续性**。

### **解决方案流程**
1.  **基础**：在一个预训练好的流匹配动作分块策略上进行低秩自适应。
2.  **训练**：使用上述的掩码动作分块技术（前缀掩码、自条件课程、残差对齐）对策略进行微调，使其学会处理各种延迟下的块内不一致性。
3.  **推理**：采用前缀保留采样流程，结合估计的延迟 `d`，生成与历史执行动作无缝衔接的新动作块。

### **实际价值与优势**
*   **零延迟开销**：相比RTC等测试时方法，REMAC在推理时不增加任何计算负担。
*   **性能提升**：在模拟和真实机器人实验中，在多种延迟设置下均取得了更高的任务成功率和更快的完成速度。
*   **强鲁棒性**：对延迟波动、估计不准确表现出良好的鲁棒性。
*   **兼容性与泛化性**：可作为更优的“骨干策略”，与现有的测试时方法（如BID、RTC）结合以获得进一步增益；方法不限于流匹配策略，也可应用于Transformer等其他分块策略架构。
*   **实用性**：通过LoRA进行高效微调，仅增加少量参数，易于集成到现有VLA框架中。

**总结**：REMAC 的核心创新在于，它通过一种新颖的**掩码训练范式**，让机器人策略**主动学会**在存在执行延迟和部分动作已锁定的现实约束下，如何生成正确、连贯的动作，从而在根本上提升了异步实时执行的可靠性和效率。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决机器人异步推理（asynchronous inference）与动作分块（action chunking）结合时，因系统延迟导致的**块间不连续性**和**块内不一致性**两大核心问题，这些问题会严重损害实时操控的鲁棒性。为此，论文提出了**REMAC**方法，其核心是通过**掩码动作分块**对预训练策略进行训练时微调，学习对延迟导致的动作-感知错位进行纠正性调整，并配合**前缀保留采样**流程来增强动作序列的连贯性。实验结果表明，该方法在不引入额外推理延迟的前提下，显著提升了策略在模拟和真实机器人任务中的成功率、任务完成速度以及对不同延迟条件的鲁棒性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Real-Time Robot Execution with Masked Action Chunking》针对异步推理下的实时机器人控制，提出了名为REMAC的新方法。其核心创新点如下：

### 1. **首次识别并系统性地解决了“块内不一致性”问题**
   - **相比以往方法的改进/不同之处：**
     - **以往工作**（如Zhao et al., 2023; Liu et al., 2025; Black et al., 2025）主要关注**块间不连续性**，即在动作块边界处的跳跃或不连贯问题。它们通常采用**测试时算法**（如启发式平滑、基于梯度的修复）来优化即将执行的动作块。
     - **本文**首次明确指出并建模了**块内不一致性**：在异步推理中，由于推理延迟，一个动作块中**已执行的前缀部分**（来自上一个块）与**当前观测**之间存在错配，导致动作与感知不匹配。
   - **解决的具体问题/带来的优势：**
     - **解决了根本性失效模式**：块内不一致性会导致机器人执行基于过时观测的动作，是异步推理中一个被忽视但关键的性能下降原因。解决此问题直接从根源上提升了策略的鲁棒性。
     - **提升了策略的适应性**：使策略能够适应“执行中动作部分已固定、部分待优化”的真实异步执行场景。

### 2. **提出“掩码动作分块”训练时适应方法**
   - **相比以往方法的改进/不同之处：**
     - **以往方法**多为**测试时修正**，即在部署时对预训练策略的输出进行后处理（如时序集成、双向解码、梯度修正）。这些方法要么是启发式的且容易失败，要么会引入额外的推理延迟。
     - **本文方法**是一种**训练时适应**方法。它在预训练的策略基础上，通过**前缀掩码**、**自条件课程**和**残差对齐**三个核心组件进行微调，学习对延迟的鲁棒性。
       - **前缀掩码**：在训练时，根据随机采样的延迟 `d`，将动作块前 `d` 个时间步的监督信号掩码掉，迫使模型专注于学习“待执行”部分的动作预测。
       - **自条件课程**：在训练输入中，逐步用预训练策略自身的预测替换真实动作，使模型学会在测试时条件下（即基于自身历史预测）进行修正，缓解曝光偏差。
       - **残差对齐**：显式地让微调策略的输出与预训练策略输出的残差对齐，鼓励学习有意义的修正量。
   - **解决的具体问题/带来的优势：**
     - **实现零额外延迟**：与测试时方法（如RTC）不同，REMAC在推理时不引入任何额外计算开销，保持了实时性。
     - **从根本上提升策略质量**：通过训练使策略内在地具备处理延迟的能力，而非事后修补，从而产生更鲁棒、更连贯的“骨干策略”。
     - **兼容性与可组合性**：REMAC生成的策略可以作为更强的基础，与现有的测试时方法（如BID、RTC）结合，实现进一步的性能提升。

### 3. **提出“前缀保留采样”流程**
   - **相比以往方法的改进/不同之处：**
     - **标准采样**：通常从高斯先验噪声开始，通过积分整个动作块来生成新动作。
     - **本文采样**：在生成新动作块时，**初始化并固定前缀部分**。具体来说，将延迟期间正在执行的动作（来自上一个块）作为新块前缀的初始值，并在整个采样过程中**保持这部分不变**，只对剩余部分进行合成。
   - **解决的具体问题/带来的优势：**
     - **强制保证块间连续性**：通过显式地将已执行动作作为下一个动作块的固定起点，确保了动作流在时间上的平滑衔接。
     - **与训练目标对齐**：采样流程与训练时使用的掩码策略完全一致，确保了训练-测试的一致性。

### 4. **高效且通用的实现方案**
   - **相比以往方法的改进/不同之处：**
     - **参数高效微调**：采用**LoRA**对预训练策略进行微调，仅增加约1.5%的参数。这与许多需要全参数微调或复杂测试时计算的方法形成对比。
     - **架构通用性**：论文不仅在流匹配策略上验证了REMAC，还在附录中展示了其可轻松迁移到**Transformer-based动作分块策略**（如ACT）上，证明了其方法的一般性。
   - **解决的具体问题/带来的优势：**
     - **部署友好**：LoRA模块在训练后可以合并回主干模型，推理时零开销。同时，微调数据需求量相对较少（在真实世界实验中仅使用200条轨迹），提升了实用性。
     - **广泛适用**：创新点不在于特定模型架构，而在于一套训练和采样范式，使其能作为插件集成到多种现有的VLA框架中。

### 总结
本文的核心创新在于**视角的转变**：从专注于测试时修复“块间不连续性”，转变为在训练时同时解决“块内不一致性”和“块间不连续性”这两个根本问题。REMAC通过**掩码动作分块训练**和**前缀保留采样**，**在不牺牲实时性的前提下**，学习到了一个对异步推理延迟具有内在鲁棒性的策略。这相比以往工作，提供了一种更根本、更高效、且兼容性更强的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 实验设置与数据集

#### 1. 仿真环境
- **数据集/环境**： **Kinetix** 模拟器（包含12个高度动态和随机的机器人操作任务）。
- **策略配置**： 预测时域 `P = 8`，执行时域 `h` 根据延迟 `d` 在 `max(1, d)` 到 `9-d` 之间调整，以确保动作流的连续性。
- **评估延迟范围**： `d = 0, 1, 2, 3, 4`。

#### 2. 真实世界环境
- **机器人平台**： Franka Research 3 七自由度机械臂，配备平行夹爪。
- **任务**： 三个不同难度的单臂抓取-放置任务（**Grasp-Easy, Medium, Hard**），涉及黄瓜、魔方等物体，放置目标为盘子或碗。
- **控制频率**： 15 Hz (`Δt ≈ 67 ms`)。
- **实际延迟**： 端到端推理延迟约122-140 ms（包含VLA推理、网络通信、数据处理），对应离散延迟 `d = 2` 或 `3`。
- **数据量**： 使用200条演示轨迹进行模型微调。

### 二、 评价指标

1.  **任务成功率**： 在Kinetix环境中，衡量任务是否成功完成。
2.  **平均执行时间**： 在Kinetix环境中，衡量完成任务所需的平均时间步数。
3.  **阶段完成率**： 在真实世界任务中，将任务分解为4个子阶段（接近物体、抓取并抬起、移动到目标、正确放置），根据完成的阶段数进行评分。
4.  **机器人运动学平滑度**： 分析机器人的平均速度和加速度，评估轨迹的平滑性和稳定性。

### 三、 对比的基线方法

论文与以下基线方法进行了全面对比：

#### 仿真环境基线：
- **Naive Async**： 直接使用预训练策略，执行最新生成的动作块。
- **Bidirectional Decoding (BID)**： 一种测试时方法，采样多个候选预测并通过拒绝采样选择最优解。
- **RTC**： 当前最先进的测试时执行策略，利用修复算法，以前缀动作为先验进行梯度修正。
- （注：**Temporal Ensembling (TE)** 因性能不佳被省略）

#### 真实世界基线：
- **Synchronous Inference**： 同步推理，执行完整个当前动作块后暂停，等待新动作。
- **Naive Async**： 同上。
- **Temporal Ensembling (TE)**： 对连续块的重叠动作进行加权平均以平滑边界。
- **RTC**： 同上。

### 四、 关键性能结果与结论

#### 1. 仿真实验结果（Kinetix）
- **主要结论**： **REMAC在所有延迟设置下均显著且一致地优于所有基线方法。**
- **成功率**：
    - 随着推理延迟 `d` 的增加，所有方法的性能都会下降，但**REMAC的下降幅度最小**，表现出更强的鲁棒性。
    - 即使在 `d=0`（无延迟）的情况下，REMAC也优于基线，这表明其掩码动作分块策略增强了策略本身的连贯性和预测能力。
    - 具体数据：在 `d=4` 时，REMAC的成功率（~0.78）显著高于RTC（~0.59）和Naive Async（~0.45）。详见论文图2及表1。
- **执行效率**： REMAC在**平均执行时间上也最短**，表明其能生成更高效的动作策略，从而更快地完成任务。

#### 2. 真实世界实验结果
- **主要结论**： **REMAC在所有三个抓取任务中都取得了最高的阶段完成率。**
- **性能表现**：
    - 在最具挑战性的 **Grasp-Hard** 任务中，REMAC的优势最为明显（完成率0.812 vs RTC的0.753，Naive Async的0.460）。详见论文表3。
    - 同步推理会产生明显的停顿和抖动，而异步基线（Naive, TE）则容易出现抓取时机错误。RTC虽然平滑，但其引入的额外计算延迟（55-64 ms）损害了性能。
- **延迟鲁棒性**：
    - 当注入额外延迟（75ms, 150ms）模拟恶劣条件时，REMAC的性能下降最小，**始终保持最佳**。而RTC在延迟增大时性能显著恶化。详见论文图4。
- **运动平滑性**：
    - 运动学分析表明，REMAC产生的轨迹**最平滑、最稳定**，速度和加速度的突变最少，同时任务完成速度最快。详见论文图5。

#### 3. 消融实验与扩展性
- **组件有效性**： 逐步添加**前缀掩码、自条件课程学习、残差对齐损失**等组件，带来了持续的性能提升。仅使用LoRA而不改变训练范式则无改善。
- **与测试时方法兼容**： REMAC可以作为更强大的骨干策略，与BID、RTC等测试时方法结合，能带来**额外的性能增益**，证明了其“即插即用”的灵活性。
- **策略架构通用性**： 实验证明REMAC不仅适用于流匹配模型，也能成功应用于**Transformer-based ACT框架**，显示出良好的通用性。
- **数据效率与泛化性**： 在仅使用10条演示数据的低数据情况下，REMAC性能下降很小。它不会损害基础VLA模型的开放世界泛化能力，因为其调整仅限于动作专家模块。

### 总结
论文通过仿真和真实世界的广泛实验，**定量且令人信服地证明了REMAC方法的优越性**。其核心价值在于：
- **更高成功率**： 在多变延迟下实现更高的任务完成率。
- **更快执行速度**： 缩短任务完成时间。
- **更强鲁棒性**： 对延迟波动和增大具有出色的容忍度。
- **零额外延迟**： 在提升性能的同时，不引入任何推理时的计算开销。
- **良好兼容性**： 可作为强化骨干，与现有方法协同工作。

这些结果共同表明，REMAC为解决异步推理中的块间不连续性和块内不一致性问题提供了一个高效、可靠的训练时解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.20130v1)
- [HTML 版本](https://arxiv.org/html/2601.20130v1)
