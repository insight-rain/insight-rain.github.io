# ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas

**相关性评分**: 6.0/10

**排名**: #19


---


## 基本信息

- **arXiv ID**: [2601.21558v1](https://arxiv.org/abs/2601.21558v1)
- **发布时间**: 2026-01-29T11:22:23Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Xiaoyu Tian, Haotian Wang, Shuaiting Chen, Hao Zhou, Kaichi Yu, Yudian Zhang, Jade Ouyang, Junxi Yin, Jiong Chen, Baoyan Guo, Lei Zhang, Junjie Tao, Yuansheng Song, Ming Cui, Chengwei Liu

## 关键词

fine tune, offline Reinforcement Learning, world model

## 一句话总结

ASTRA是一个自动化框架，通过合成轨迹和环境来训练工具增强的语言模型代理，结合监督微调和在线强化学习以提升多步决策能力。

## 摘要

Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.

## 详细分析

## ASTRA 论文详细摘要

### 1. 研究背景和动机
随着大语言模型（LLM）越来越多地被用作工具增强型智能体进行多步决策，训练鲁棒的工具使用智能体仍然面临挑战。现有方法通常需要人工干预、依赖不可验证的模拟环境、仅使用监督微调（SFT）或强化学习（RL）中的单一范式，并且在稳定、长视野、多轮次的学习中存在困难。为了应对这些挑战，本文提出了 **ASTRA**，一个完全自动化的端到端框架，旨在通过可扩展的数据合成和可验证的强化学习来训练工具增强的语言模型智能体。

### 2. 核心方法和技术创新
ASTRA 的核心创新在于其整合了两个互补的组件：
- **用于 SFT 的轨迹合成流水线**：该方法利用**工具调用图的静态拓扑结构**，自动合成多样化、结构化的多轮次工具使用轨迹。该过程基于真实的 MCP 服务器，并通过自动化的奖励建模对轨迹质量进行评分，从而无需人工标注即可获得高质量的 SFT 数据。
- **用于 RL 的可验证环境合成框架**：该方法捕捉**人类语义推理的丰富组合拓扑结构**，将分解后的问题-答案（QA）对转化为独立的、可代码执行的、规则可验证的环境。这为确定性、多轮次的在线 RL 提供了基础。

基于上述组件，ASTRA 提出了一套统一的训练方法：首先通过 SFT 学习一个适应多轮次工具交互的更强初始策略，然后利用合成的可验证环境进行在线、多轮次的 RL。RL 阶段引入了**无关工具混合**策略和**F1风格的轨迹级奖励**，以共同优化任务完成度和交互效率。

### 3. 主要实验结果
在多个智能体工具使用基准测试（BFCL-v3 Multi-Turn, τ²-Bench, ACEBench）上的实验表明：
- ASTRA 训练的模型（14B 和 32B 参数规模）在同等规模的开源模型中达到了**最先进的性能**，其表现接近甚至在某些指标上超越了部分闭源系统。
- 训练过程的两阶段（SFT 和 RL）均带来了显著的性能提升，其中 RL 阶段贡献了最大的增益。
- 在专注于非智能体复杂推理的数学基准（AIME2024/2025）上，ASTRA 模型保持了与原始模型相当的核心推理能力，表明其优化并未损害模型的通用性。

### 4. 研究意义和价值
ASTRA 的研究意义和价值体现在：
- **技术创新**：首次将基于工具图拓扑的轨迹合成与基于语义推理拓扑的环境合成相结合，实现了从数据生成到策略训练的全流程自动化，解决了现有方法在可验证性和长视野学习上的瓶颈。
- **实际价值**：该框架显著降低了对人工标注和手动设计环境的依赖，为大规模、低成本地训练鲁棒、通用的工具使用智能体提供了可行的技术路径。其开源的流水线、环境和模型有助于推动该领域的可复现性和后续研究。
- **应用前景**：ASTRA 的方法论有望缓解实际部署中的瓶颈，通过合成多样化的可执行环境并进行迭代交互式训练，提升智能体在下游应用中的鲁棒性，为构建更强大的多轮次、用户交互式智能体奠定了基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## ASTRA 论文核心分析

### **论文想解决的核心问题**
当前训练**工具增强型大语言模型（LLM）智能体**面临四大挑战：
1.  **依赖人工干预**：数据构建和验证需要大量人工。
2.  **环境不可验证**：许多方法依赖LLM模拟环境，其状态转移和奖励信号**非确定性、不可验证**，导致长视野、多轮次强化学习（RL）不稳定。
3.  **训练范式单一**：要么只使用监督微调（SFT），要么只使用强化学习（RL），无法兼顾**广泛能力习得**与**交互式优化**。
4.  **长视野决策困难**：现有方法常将多轮轨迹分解为单步训练实例，**割裂了连贯的长程决策学习**。

### **核心技术创新：ASTRA框架**
ASTRA是一个**全自动、端到端**的框架，通过**可扩展的数据合成**和**可验证的强化学习**来训练工具增强型智能体。其核心创新在于**两个互补的合成组件**和一个**统一的训练方法**。

#### **1. 用于SFT的轨迹合成管道（基于工具调用图的静态拓扑）**
- **目标**：自动化生成高质量、多样化的多轮工具使用轨迹，用于监督微调，**无需人工标注**。
- **关键步骤**：
    - **工具收集与规范化**：从开放MCP注册中心等来源收集工具文档，统一为OpenAI工具调用格式。
    - **工具链构建**：利用LLM为每个工具服务器合成可能的“任务-工具链”对，并构建**工具转移图**，通过随机游走采样候选工具链。
    - **任务构建与增强**：结合“链条件生成”和“仅服务器生成”两种模式，并通过**多样性、复杂性、角色条件**三种方式进行增强，确保任务的真实性和覆盖度。
    - **轨迹收集与奖励建模**：使用混合执行（真实MCP服务器 + 文档模拟器）进行多轮交互，并设计**自动化轨迹质量评估管道**，从7个维度（如查询理解、工具调用状态、答案质量等）评分，聚合为单一奖励信号用于筛选高质量SFT数据。

#### **2. 用于RL的环境合成框架（基于人类语义推理的组合拓扑）**
- **目标**：将人类问答（QA）对转化为**独立、代码可执行、规则可验证**的环境，为多轮在线RL提供**确定性**的奖励和状态转移。
- **关键步骤**：
    - **Q-A实例合成**：将复杂问题分解为具有**依赖关系图**的子问题-子答案对，显式建模解决问题的**语义拓扑**。
    - **质量验证**：从**依赖一致性、子问题原子性、顺序合理性、任务完整性**四个维度对分解的QA实例进行严格过滤和评分。
    - **环境合成**：为每个（非叶节点）子任务合成对应的**工具规范**和**Python代码实现**，并在沙箱中验证其可执行性和正确性。
    - **子环境合并**：识别功能相同的子问题，将其工具实现合并，避免动作空间膨胀。

#### **3. 统一的训练方法：SFT + 多轮在线RL**
- **两阶段训练**：
    1.  **SFT阶段**：使用合成的轨迹数据对基础模型进行微调，获得一个**更适应多轮工具交互的初始策略**。
    2.  **RL阶段**：在合成的可验证环境上进行**在线、多轮**的强化学习。
- **RL阶段的关键设计**：
    - **无关工具混合**：为每个训练实例，从**高、中、低**三个语义相似度带中采样无关工具加入工具列表，**强制模型学习工具判别能力**，避免过拟合。
    - **F1式轨迹级奖励**：奖励 = `2 * precision * recall / (precision + recall)`，其中：
        - `recall = 成功解决的子任务数 / 总子任务数`
        - `precision = 成功解决的子任务数 / (工具调用次数 + ε)`
        - **该设计同时优化任务完成率和交互效率**，防止工具滥用或过于保守。
    - **自适应批次填充**：确保每个训练批次包含足够多能提供有效学习信号（奖励方差非零）的轨迹，**稳定训练过程**。

### **实际价值与效果**
- **性能**：在多个智能体工具使用基准测试（BFCL-MT, τ²-Bench, ACEBench）上，ASTRA训练的模型在同等规模下达到**最先进水平**，性能接近闭源系统，且**未损害核心推理能力**（在AIME数学推理基准上表现持平或略有提升）。
- **自动化与可扩展性**：实现了从数据构建到模型训练的全流程自动化，大幅降低了对人工标注和手动设计环境的依赖。
- **可复现性与开源**：论文开源了完整的**数据合成管道、合成环境及训练好的模型**，为社区提供了宝贵的研究基础。
- **方法论启示**：提出了“**静态工具拓扑**（SFT） → **组合语义拓扑**（RL）”的两阶段能力构建范式，并证明了**可验证环境**对于稳定、高效的智能体RL训练至关重要。

**总结**：ASTRA通过**自动化合成可验证的环境和高质量轨迹**，并创新性地结合**SFT与多轮在线RL**，系统性地解决了训练鲁棒、通用工具智能体的核心难题，为下一代智能体系统的开发提供了强大的方法论和工具支持。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决训练**工具增强型大语言模型智能体**时面临的几大核心挑战：现有方法依赖人工干预、使用不可验证的模拟环境、训练范式单一（仅用监督微调或强化学习），且难以实现稳定、长视野的多轮决策学习。

为此，论文提出了一个名为 **ASTRA** 的端到端自动化框架。该框架的核心创新在于整合了两个互补的组件：
1.  **基于工具调用图静态拓扑的轨迹合成流水线**：用于自动化生成高质量、多样化的多轮工具使用轨迹，为监督微调提供数据。
2.  **基于人类语义推理组合拓扑的环境合成框架**：将分解后的问题-答案对转化为独立的、可代码执行的、规则可验证的强化学习环境，从而支持确定性的多轮在线强化学习。

基于此，论文设计了一套统一的训练方法，先通过监督微调获得一个适应多轮交互的初始策略，再在合成的可验证环境中进行在线强化学习，并使用轨迹级F1奖励来平衡任务完成度和交互效率。

实验结果表明，在多个智能体工具使用基准测试上，ASTRA训练出的模型在同等规模下达到了**最先进的性能**，其能力接近闭源系统，同时保持了模型的核心推理能力。这证明了该框架在实现**全自动化、可扩展、可验证**的智能体训练方面的有效性和实际价值。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## ASTRA论文的核心创新点分析

ASTRA论文提出了一种用于训练工具增强型语言模型智能体的全自动端到端框架。其核心创新点主要体现在**数据合成方法**、**训练范式**和**系统设计**三个方面，旨在解决现有方法在**可验证性**、**长程决策**和**训练稳定性**上的关键瓶颈。

以下是其相对于已有工作的明确创新点：

### 1. **基于静态工具调用图拓扑的轨迹合成（用于SFT）**
- **改进/不同之处**： 传统方法（如ToolLLM、APIGen-MT）通常基于单个工具或简单的工具链生成数据，缺乏对工具间结构化依赖关系的系统性利用。ASTRA创新性地将每个MCP服务器内的工具集合建模为一个**有向转换图**，并通过在该图上进行随机游走来合成多样化的、结构上合理的多步工具调用链。
- **解决的问题/带来的优势**：
    - **解决**： 手动设计工具链成本高、覆盖率低，以及LLM生成工具链时可能产生逻辑不合理或不可执行序列的问题。
    - **优势**： 确保了合成轨迹在**工具组合逻辑上的合理性**和**多样性**，为监督微调提供了高质量、可扩展的数据源，从而让模型获得广泛且可迁移的工具使用基础能力。

### 2. **基于人类语义推理拓扑的可验证环境合成（用于RL）**
- **改进/不同之处**： 现有RL训练方法（如许多基于LLM模拟环境的工作）严重依赖LLM来模拟工具执行、状态转移和反馈，导致环境是**非规则可验证的**。ASTRA则从一个全新的角度出发：它将复杂的用户问题分解为具有依赖关系的子问题-答案对（QA对），捕捉了**人类语义推理的丰富组合拓扑**，并将每个子任务转化为独立的、**代码可执行的Python环境**。
- **解决的问题/带来的优势**：
    - **解决**： LLM模拟环境带来的**非确定性**和**不可验证性**，这严重阻碍了稳定的、长视野的在线强化学习，因为RL依赖确定性的状态转移和可靠的奖励信号。
    - **优势**： 创造了**完全可验证、确定性的环境**。工具执行由真实代码完成，状态转移和奖励计算基于明确规则。这使得**稳定的多轮在线RL成为可能**，智能体可以在一个真实、可靠的交互环境中学习长程决策。

### 3. **统一的两阶段训练方法论：SFT与可验证多轮在线RL的集成**
- **改进/不同之处**： 许多现有工作要么只做SFT，要么只做RL。SFT-only方法缺乏在线交互信号，RL-only方法受限于初始策略的能力，且常在非可验证环境中进行，不稳定。ASTRA提出了一个**连贯的两阶段流程**：首先利用创新点1的数据进行SFT，获得一个擅长多轮工具交互的强初始策略；然后利用创新点2生成的可验证环境，对该策略进行**在线、多轮的强化学习**。
- **解决的问题/带来的优势**：
    - **解决**： 单一训练范式的局限性以及RL训练在不可验证环境中的不稳定性。
    - **优势**：
        1. **训练更高效、稳定**：SFT提供了高质量的“冷启动”，使RL阶段可以从一个更高的起点开始探索，加速收敛。
        2. **能力深化**：在线RL使智能体能在复杂的、语义丰富的拓扑环境中学习**连贯的长视野决策**和**从错误中恢复**的能力，而不仅仅是模仿固定的轨迹。
        3. **平衡优化**：论文设计了**轨迹级F1奖励**，同时优化任务完成率（召回）和交互效率（精确率），避免了智能体过度调用工具或过于保守的问题。

### 4. **“无关工具混合”与“自适应批次填充”的工程创新**
- **改进/不同之处**：
    - **无关工具混合**： 在RL训练时，不仅提供解决当前任务所需的工具，还主动添加**多个语义相似度带**（高、中、低相似度）的无关工具。这不同于简单地不提供或随机添加无关工具。
    - **自适应批次填充**： 针对GRPO等策略梯度算法在奖励方差为零时梯度信号消失的问题，设计了一种动态批次构建策略，确保每个训练批次都包含足够多能提供有效学习信号（即奖励方差非零）的样本。
- **解决的问题/带来的优势**：
    - **无关工具混合**： 解决了智能体在“干净”工具列表中容易过拟合，而缺乏在真实复杂工具集中**辨别和拒绝无关工具**能力的问题。通过暴露相似度各异的干扰项，**强制模型学习负例判断**，提升了工具选择的鲁棒性。
    - **自适应批次填充**： 解决了在线RL因无效样本过多导致的**训练不稳定和低效**问题。通过过滤掉无学习信号的样本，保证了每次参数更新都基于“信息丰富”的数据，从而**提升了训练稳定性和样本效率**。

### 5. **完全自动化与端到端的开源框架**
- **改进/不同之处**： 许多工具学习项目的数据合成或环境构建仍需人工介入或校验，且流程非端到端。ASTRA强调并实现了从工具文档收集、轨迹/环境合成、质量自动评分到最终模型训练的**全流程自动化**，并将整套流水线、合成环境和训练模型**完全开源**。
- **解决的问题/带来的优势**：
    - **解决**： 对人工标注和设计的依赖，这限制了数据规模和领域扩展性。
    - **优势**：
        1. **极高的可扩展性**：可以自动处理成千上万个工具，合成海量训练数据与环境。
        2. **促进可复现性与研究**：完整的开源使社区能够直接使用、验证并在此基础上进行创新，推动了领域发展。
        3. **实际部署价值**：为在实际应用中快速为特定工具集定制高性能智能体提供了可行的自动化方案。

### 总结
ASTRA的核心创新在于**系统性**地将**工具使用的结构拓扑**（用于SFT数据合成）和**语义推理的任务拓扑**（用于RL环境合成）结合起来，并通过**可验证的代码环境**和**精心设计的训练策略**，构建了一个能够稳定、高效训练出具备强大长程多轮决策能力的工具型智能体的自动化框架。其实验结果在多个智能体基准测试上达到同规模开源模型的SOTA，并逼近闭源系统，验证了其创新点的有效性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 1. **评估数据集**
论文在**智能体工具使用**和**非智能体复杂推理**两个维度上进行了评估。

#### **智能体工具使用基准（Agentic Benchmarks）**
- **BFCL v3 Multi-Turn (BFCL-MT)**：评估多轮工具调用能力，包含基础、缺失函数、缺失参数、长上下文等子任务。
- **τ²-Bench**：在双控环境中评估对话式智能体，包含零售和电信两个领域。
- **ACEBench**：评估智能体在工具学习中的综合表现，包含多轮和多步任务。

#### **非智能体推理基准（Non-agentic Benchmarks）**
- **AIME2024** 和 **AIME2025**：数学问题解决基准，用于评估模型的核心推理能力是否因工具学习训练而退化。

### 2. **评价指标**
- **智能体任务**：主要使用**任务成功率**（Pass Rate）或**准确率**（Accuracy）。对于BFCL-MT，报告了各子类别的得分及综合得分。
- **非智能体任务**：使用**通过率**（Pass Rate），通过多次采样（32次）取平均以提高稳定性。
- **内部分析指标**：
    - **交互步数**（Average steps per sub-job）
    - **平均输出长度**（Average tokens per step/job）
    - **轨迹级奖励**（F1-style reward）：平衡任务完成率（Recall）和交互效率（Precision）。

### 3. **基线方法对比**
论文与**闭源**和**开源**的先进模型进行了广泛对比。

#### **闭源模型（Closed-source）**
- Claude-Opus-4.5, Claude-Sonnet-4.5, Claude-Haiku-4.5
- Gemini-3-Pro, Gemini-2.5-Pro
- GPT-4.1

#### **开源模型（Open-source）**
- Kimi-K2-Instruct
- GLM-4.6
- LoopTool-32B
- Qwen3-14B / 32B（作为ASTRA的基座模型）

### 4. **主要性能结果与结论**

#### **关键性能提升**
1. **在同等规模下达到SOTA**：
    - **ASTRA-32B-Thinking-v1** 在BFCL-MT上取得**64.25**的综合得分，显著优于同规模的开源模型（如Qwen3-32B的47.88），并接近或超越部分更大规模的闭源模型（如Claude-Sonnet-4.5的61.38）。
    - 在τ²-Bench和ACEBench上也表现出色，证明了其多轮、多步工具使用的鲁棒性。

2. **两阶段训练均带来显著增益**：
    - **SFT阶段**：通过工具链合成数据训练，使模型初步适应多轮工具交互。例如，Qwen3-14B经过SFT后，在BFCL-MT上从44.50提升至48.50。
    - **RL阶段**：通过可验证环境进行在线多轮RL，带来了**最大的性能飞跃**。例如，Qwen3-32B经过RL后，在BFCL-MT上从47.88大幅提升至64.25（+16.38分）。

3. **保持了核心推理能力**：
    - 在AIME2024/2025数学推理基准上，ASTRA训练后的模型与原始基座模型（Qwen3）表现相当，甚至略有提升（如32B模型从74.15提升至75.15）。这表明ASTRA的训练**专注于增强工具使用能力，而未损害模型固有的逻辑推理能力**。

#### **关键结论与发现**
1. **可验证的RL环境至关重要**：与依赖LLM模拟的不可验证环境相比，ASTRA的代码可执行、规则可验证的环境为稳定的**长视野、多轮在线RL**提供了基础，这是性能大幅提升的关键。
2. **无关工具混合（Irrelevant Tool Mixing）有效**：实验表明，在RL阶段混合语义相似度不同的无关工具，能有效教会模型进行**负向工具判别**，避免过拟合于固定工具集，提升了在真实复杂场景下的鲁棒性。
3. **F1式轨迹奖励设计平衡了探索与利用**：对比实验证明，仅优化任务完成率（Recall）会导致工具调用爆炸和训练不稳定；仅优化交互效率（Precision）则会导致模型过于保守。**F1奖励**（Recall和Precision的调和平均）能引导模型在完成任务的同时保持高效，实现了稳定的训练和最优性能。
4. **SFT与RL互补**：SFT提供了高质量的初始策略和工具使用模式，而RL则在更广阔的语义拓扑空间中进行探索和优化，使智能体学会处理更复杂、更多样的任务解决路径。

### 5. **实际价值与创新点总结**
- **技术创新**：首创了**结合静态工具调用图拓扑（用于SFT数据合成）和人类语义推理组合拓扑（用于RL环境合成）** 的端到端自动化训练框架。
- **工程价值**：完全开源了数据合成管道、可验证环境和训练好的模型，为社区提供了可复现、可扩展的智能体训练基础设施。
- **应用价值**：训练出的模型在多项基准上达到先进水平，**逼近闭源系统性能**，为部署低成本、高性能的开源工具增强智能体提供了可行方案。该方法减少了对人工标注和特定场景模拟环境的依赖，具有更好的可扩展性和实用性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.21558v1)
- [HTML 版本](https://arxiv.org/html/2601.21558v1)
