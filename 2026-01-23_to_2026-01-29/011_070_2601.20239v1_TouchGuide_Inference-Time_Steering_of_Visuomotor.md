# TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance

**相关性评分**: 7.0/10

**排名**: #11


---


## 基本信息

- **arXiv ID**: [2601.20239v1](https://arxiv.org/abs/2601.20239v1)
- **发布时间**: 2026-01-28T04:22:47Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang, Boyan Li, Yiran Qin, Jin Liu, Can Zhao, Li Kang, Haoqin Hong, Zhenfei Yin, Philip Torr, Hao Su, Ruimao Zhang, Daolin Ma

## 关键词

Vision-Language-Action Model, VLA for Robotics, Diffusion, Inference Acceleration, fine tune, world model

## 一句话总结

TouchGuide 是一种在推理时通过触觉引导优化预训练视觉运动策略的新方法，专注于接触丰富的精细操作任务。

## 摘要

Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

## 详细分析

## 论文摘要：TouchGuide: 通过触觉引导在推理时操控视觉运动策略

### 1. 研究背景和动机
精细且接触丰富的机器人操作（如系鞋带、芯片交接）仍然极具挑战，主要原因在于触觉反馈未被充分利用。现有的视觉-触觉策略融合方法（如特征级或策略级融合）存在模态主导或难以捕捉跨模态关联的问题。同时，高质量、低成本触觉数据的采集也面临困难。因此，本文旨在解决**如何有效融合视觉与触觉信息以提升精细操作策略规划**这一核心问题。

### 2. 核心方法和技术创新
本文提出了一个名为 **TouchGuide** 的新型跨策略视觉-触觉融合范式，其核心创新在于**在低维动作空间内进行模态融合**，而非直接融合原始数据。该方法采用两阶段推理时引导：
- **第一阶段**：预训练的扩散或流匹配视觉运动策略仅基于视觉输入，在采样早期生成一个粗略但视觉上可行的动作。
- **第二阶段**：一个任务特定的**接触物理模型** 根据当前触觉观测，为上述动作计算一个**可行性分数**，并通过对比学习在有限专家演示上训练。该分数用于在推理时（去噪或流匹配过程后期）引导和细化动作，使其更符合真实的物理接触约束。

此外，为支持高质量数据采集，本文开发了 **TacUMI** 数据收集系统。它采用 Vive 追踪器进行定位，在低成本（约720美元）和轻量化（540克）的同时实现了高精度，并通过刚性指尖设计提供**直接触觉反馈**，显著提升了操作员的数据收集体验和效率。

### 3. 主要实验结果
在**系鞋带、芯片交接、黄瓜去皮、花瓶擦拭、开锁**五个高难度接触式任务上进行了广泛实验：
- **性能提升**：TouchGuide 显著超越了最先进的视觉-触觉策略。例如，在 `π0.5` 策略基础上，平均成功率从 35.9% 提升至 58.0%；在 Diffusion Policy 基础上，从 16.3% 提升至 36.2%。
- **泛化能力**：TouchGuide 展现出强大的跨策略（DP, `π0.5`）、跨机器人平台（双/单臂）和跨触觉模态（触觉图像、力矢量）的泛化能力。
- **系统优势**：用户研究表明，TacUMI 在尝试次数、数据有效率和用户满意度上均优于 VR 遥操作和基于 SLAM 的 UMI 等现有数据收集系统。

### 4. 研究意义和价值
- **方法论价值**：TouchGuide 提出了一种新颖的、与基础策略解耦的推理时触觉融合范式，无需重新训练基础策略即可显著提升其在精细操作任务上的性能，为多模态机器人控制提供了新思路。
- **实用价值**：TacUMI 系统在精度、成本和用户体验之间取得了优异平衡，为社区获取高质量触觉演示数据提供了高效、可及的解决方案。
- **推动领域发展**：该工作有效解决了接触丰富操作中触觉信息利用不足的痛点，通过强调**初始接触的物理可行性**而不仅仅是事后反应，推动了机器人从“看到”到“感知到并安全操作”的能力边界。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：TouchGuide

### **一、 论文拟解决的核心问题**
论文旨在解决**机器人执行精细、接触丰富的操作任务时，触觉反馈利用不足**的难题。具体而言，现有方法在融合视觉和触觉信息以提升策略规划能力方面存在局限：
- **现有融合策略的缺陷**：传统的特征级融合容易导致模态主导（通常视觉主导），而策略级融合则难以捕捉跨模态的协同关系，且通常需要训练多个单模态策略。
- **数据收集的挑战**：用于训练的高质量、低成本触觉数据难以获取。现有遥操作系统延迟高、触觉反馈间接；而手持式设备在精度、成本和直接触觉反馈之间难以取得良好平衡。

### **二、 核心创新点**
论文提出了一个名为 **TouchGuide** 的完整框架，包含**方法论**和**硬件系统**两方面的核心创新：

#### **1. 方法论创新：TouchGuide —— 一种新颖的跨策略视觉-触觉融合范式**
- **核心思想**：**在低维动作空间中进行跨模态融合**，而非在数据或特征层面。它通过一个**任务特定的接触物理模型**，在推理时引导预训练的视觉运动策略。
- **工作机制（两阶段推理）**：
    1.  **视觉引导阶段**：预训练的扩散模型或流匹配模型仅基于视觉观察，在采样早期生成一个粗略的、视觉上可行的动作。
    2.  **触觉引导阶段**：**接触物理模型** 介入，根据当前的触觉观察和视觉观察，评估上述粗略动作的物理可行性（输出一个“可行性分数”），并利用该分数的梯度**在采样过程中实时引导和细化动作**，使其更符合真实的物理接触约束。
- **技术亮点**：
    - **推理时引导**：无需重新训练基础策略，即可为其注入触觉感知能力，实现了与基础策略的**解耦**。
    - **通用性**：可应用于多种主流策略架构，包括扩散策略、流匹配策略以及视觉语言动作模型的动作专家。
    - **对比学习训练**：CPM仅需有限的专家演示数据，通过对比学习进行训练，学习判断动作与当前多模态观察在物理上的一致性。

#### **2. 硬件系统创新：TacUMI —— 一个高性价比、高精度的触觉数据收集系统**
- **设计原则**：
    - **低成本与高精度**：采用 **Vive Tracker** 进行定位，成本仅约720美元（不含触觉传感器），同时满足了精细操作所需的高精度。
    - **直接触觉反馈**：采用**刚性指尖**设计，使操作者能直接感知接触力，这对于收集涉及脆弱物体或精细接触的数据至关重要。
    - **易用性与同步性**：提供统一的软硬件接口，可同步采集高分辨率视觉和触觉信号（30Hz）。
- **价值**：为TouchGuide的训练提供了高质量、低成本的数据来源，解决了触觉数据收集的瓶颈问题。

### **三、 解决方案总结**
论文通过 **“软硬结合”** 的方式系统性地解决了问题：

1.  **“硬”的方面**：打造了 **TacUMI** 数据收集系统，以可承受的成本获取了带有**直接触觉反馈**的高精度操作数据，为算法训练奠定了数据基础。
2.  **“软”的方面**：提出了 **TouchGuide** 算法框架。
    -   它**不改变**强大的预训练视觉运动策略，而是将其视为一个“动作提议器”。
    -   通过训练一个轻量的**接触物理模型**作为“物理校验器”，在推理时动态地修正提议的动作，使其满足触觉感知到的物理约束。
    -   这种在**动作空间进行梯度引导**的融合方式，被证明比传统的特征拼接或策略组合更有效，能显式地将策略的分布推向更符合真实物理的分布。

### **四、 实际价值与验证**
- **性能提升显著**：在**系鞋带、芯片交接、黄瓜削皮、花瓶擦拭、开锁**五个极具挑战性的任务上，TouchGuide显著超越了现有的视觉-触觉策略。例如，在 `π_0.5` 基础上，平均成功率从35.9%提升至58.0%。
- **强泛化能力**：成功应用于不同的机器人平台、不同的触觉传感器（力信号、触觉图像）以及不同的基础策略，证明了其作为通用融合范式的潜力。
- **实用性强**：TacUMI在用户研究中获得了更高的满意度和数据有效性，TouchGuide的推理开销极小（仅导致帧率下降约3.7%-6.9%），具备实际部署的可行性。

**结论**：TouchGuide的核心创新在于提出并验证了一种**在推理时、于动作空间进行触觉引导**的全新多模态融合范式，并配套开发了高效的数据收集系统。它巧妙地规避了重新训练大模型的成本，将触觉的精细物理感知能力“嫁接”到成熟的视觉策略上，为机器人解决复杂接触任务提供了一种高效、通用的解决方案。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决机器人执行精细、接触丰富的操作任务时，如何有效融合视觉与触觉信息以提升策略性能的核心问题。为此，论文提出了一个名为 **TouchGuide** 的推理时引导框架，其核心创新在于**在低维动作空间中进行跨策略的多模态融合**：它首先利用预训练的视觉运动策略（如扩散或流匹配模型）生成粗略的视觉可行动作，然后通过一个基于对比学习训练的任务特定**接触物理模型**，根据当前触觉观测计算动作的物理可行性得分，并在推理采样过程的后期阶段，利用该得分的梯度来引导和细化动作，使其更符合真实的物理接触约束。此外，论文还配套开发了一个低成本、高精度的数据采集系统 **TacUMI**，以获取高质量的触觉数据。实验在五个具有挑战性的接触丰富任务（如系鞋带、芯片交接）上表明，该方法能显著超越现有的视觉-触觉策略，且具有良好的跨策略、跨机器人平台和跨触觉模态的泛化能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance》的创新点分析

这篇论文在视觉-触觉融合策略和机器人数据收集系统方面提出了多项明确的创新，旨在解决精细、接触丰富的操作任务中的关键挑战。以下是其核心创新点及其与已有工作的对比、解决的问题和带来的优势：

### 1. **提出了一种新颖的“跨策略”视觉-触觉融合范式（TouchGuide）**
   - **改进/不同之处**：
     - **传统方法**：现有视觉-触觉策略主要采用两种融合方式：(1) **特征级融合**（如3D-ViTac、VTLA），直接将视觉和触觉特征拼接，容易导致模态主导问题（视觉主导），且难以捕捉跨模态关联；(2) **策略级融合**（如PolicyConsensus），训练多个单模态策略后组合，但难以有效融合，且计算成本高。
     - **TouchGuide**：提出**在低维动作空间中进行融合**，而非在数据或特征层面。它通过一个**任务特定的接触物理模型（CPM）**，在推理时引导预训练的视觉运动策略（如扩散策略或流匹配策略），将触觉反馈作为“引导信号”直接优化动作生成。
   - **解决的问题/优势**：
     - **解决模态主导问题**：避免特征拼接导致的视觉主导，通过CPM在动作空间引入触觉约束，实现更平衡的融合。
     - **提升策略适应性**：CPM通过对比学习从少量专家演示中学习，提供动作的“可行性分数”，引导策略生成符合真实接触物理的动作，特别适用于**初始接触对任务成功至关重要的场景**（如穿鞋带、开锁）。
     - **跨策略通用性**：无需重新训练基础策略，可应用于多种预训练策略（如DP、π₀.₅），降低部署成本。

### 2. **设计了低成本、高精度的触觉数据收集系统（TacUMI）**
   - **改进/不同之处**：
     - **传统系统**：现有数据收集系统存在局限：(1) **遥操作**（如ALOHA）延迟高、触觉反馈间接；(2) **手持式设备**（如UMI）依赖SLAM定位，精度不足；(3) **运动捕捉系统**精度高但成本昂贵，环境要求严格。
     - **TacUMI**：采用**Vive Tracker定位**（成本约720美元），结合**刚性指尖设计**，实现低成本、高精度（亚毫米级）和直接触觉反馈。系统重量轻（540克），支持视觉-触觉信号同步采集。
   - **解决的问题/优势**：
     - **平衡成本与精度**：在低成本下实现高精度定位，适合大规模数据收集。
     - **直接触觉反馈**：刚性指尖设计让操作者直接感知接触力，提升演示数据质量，尤其适合精细操作（如芯片抓取）。
     - **提升数据质量**：实验显示，基于TacUMI收集的数据训练的策略，在任务成功率上显著优于SLAM-based UMI和遥操作（如开锁任务成功率从5%提升至30%）。

### 3. **引入了“推理时引导”机制，通过接触物理模型（CPM）优化动作生成**
   - **改进/不同之处**：
     - **传统引导方法**：如DynaGuide、LPB等使用动力学模型进行引导，但对精细操作效果有限，因为动作微小误差会导致未来观测巨大变化。
     - **TouchGuide的CPM**：专注于**当前接触物理约束**，通过对比学习训练一个二阶段模型：(1) 早期采样阶段基于视觉生成粗略动作；(2) 后期用CPM的可行性分数引导动作细化，确保符合触觉观测的物理约束。
   - **解决的问题/优势**：
     - **增强动作物理一致性**：CPM评估动作与触觉观测的匹配度，引导策略生成更符合真实接触的动作，减少任务失败（如芯片抓取时力度控制、穿鞋带时角度调整）。
     - **适用于稀疏触觉信号任务**：在触觉信号变化稀疏的任务中（如黄瓜削皮），CPM能有效融合视觉和触觉信息，避免策略犹豫或错误接触。

### 4. **在动作空间实现显式策略分布引导，提升多模态融合效果**
   - **改进/不同之处**：
     - **传统融合在动作空间的不足**：特征级和策略级融合均未在动作空间进行显式引导，策略分布可能偏离真实分布。
     - **TouchGuide的分布引导**：CPM通过可行性分数计算梯度，在去噪或流匹配过程中**显式地将策略分布向真实分布**，如图4所示，使采样动作更接近物理可行区域。
   - **解决的问题/优势**：
     - **解决分布偏移问题**：显式引导减少策略生成的动作与真实物理约束之间的差距，提升任务成功率。
     - **实验验证**：在五个挑战性任务上，TouchGuide显著优于现有SOTA方法（如RDP、PolicyConsensus），平均成功率最高提升至58.0%（基于π₀.₅），而基线方法最高仅30.3%。

### 5. **支持多模态触觉输入（力信号与触觉图像）和跨机器人泛化**
   - **改进/不同之处**：
     - **传统方法**：通常针对单一触觉模态（如仅触觉图像或仅力信号）设计，泛化能力有限。
     - **TouchGuide的灵活性**：CPM可处理多种触觉输入（如Xense传感器提供的力场或触觉图像），且无需修改基础策略，即可在不同机器人（如Bi-ARX5双臂、Flexiv Rizon4单臂）上部署。
   - **解决的问题/优势**：
     - **跨模态泛化**：实验显示，TouchGuide在力信号和触觉图像上均表现良好，证明其融合范式对不同触觉表征的适应性。
     - **跨平台部署**：由于与基础策略解耦，TouchGuide可快速适配不同机器人硬件，降低迁移成本。

### 总结
TouchGuide的核心创新在于**重新思考多模态融合问题**，从机器人控制角度出发，在动作空间进行推理时引导，而非传统的数据/特征融合。结合低成本高精度的TacUMI系统，它解决了精细操作中触觉反馈利用不足、数据收集成本高、策略物理一致性差等关键问题，在多个接触丰富任务上实现了显著性能提升。未来方向包括开发非任务特定的CPM以进一步提升泛化能力。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过一系列精心设计的实验，全面评估了 **TouchGuide** 范式和 **TacUMI** 数据收集系统的性能。实验结果表明，该方法在多个具有挑战性的精细、接触丰富的操作任务上，显著超越了现有的先进方法。

### 一、 实验任务与数据集

1.  **任务**：论文在五个精细、接触丰富的机器人操作任务上进行了评估：
    *   **鞋带穿眼**：长视野、高精度操作。
    *   **薯片交接**：协作式、易碎物体操作。
    *   **黄瓜削皮**：双臂协作、接触力控制。
    *   **花瓶擦拭**：与人类协作、表面接触操作。
    *   **开锁**：低数据量（仅20个演示）、高精度顺序操作。

2.  **数据集**：
    *   使用自研的 **TacUMI** 系统收集专家演示数据。
    *   数据量根据任务难度而异（例如，鞋带穿眼100个演示，开锁20个演示）。
    *   数据包含同步的高分辨率视觉图像（640×480）和触觉图像（200×350），以及末端执行器位姿。

### 二、 评价指标

1.  **成功率**：用于 **鞋带穿眼**、**薯片交接**、**开锁** 任务。任务在规定时间内成功完成即计为成功。
2.  **任务得分**：用于 **黄瓜削皮** 和 **花瓶擦拭** 任务。定义为机器人完成的任务量（如削皮长度、擦拭长度）与人类专家完成量的比值，并量化为离散等级（0, 0.1, 0.3, 0.5, 0.7, 0.9, 1）。

### 三、 基线方法对比

论文与两大类基线方法进行了对比：

1.  **纯视觉运动策略**：
    *   **Diffusion Policy**：基于扩散模型的先进视觉运动策略。
    *   **π₀.₅**：基于流匹配的视觉-语言-动作模型。

2.  **现有视觉-触觉融合策略**：
    *   **特征级融合方法**：`DP w/ Tactile Observation`（简单拼接触觉观测）、`RDP`（反应式扩散策略）、`SafeDiff`。
    *   **策略级融合方法**：`Policy Consensus`（多模态策略共识）。
    *   **基于动态模型的推理时引导方法**：`Tactile Dynamics`（基于触觉动态模型引导，改编自LPB和DynaGuide）。

### 四、 关键性能结果与结论

实验的主要定量结果总结如下表（基于论文表I及后续分析）：

| 策略类别 | 基线方法 | 平均成功率/得分 (DP为基础) | 平均成功率/得分 (π₀.₅为基础) | TouchGuide 提升幅度 |
| :--- | :--- | :--- | :--- | :--- |
| **纯视觉** | Diffusion Policy (DP) | 16.3% | - | **+19.9个百分点** (DP→36.2%) |
| | π₀.₅ | - | 35.9% | **+22.1个百分点** (π₀.₅→58.0%) |
| **视觉-触觉** | RDP (特征级) | 30.3% | - | **+5.9 ~ +27.7个百分点** |
| | Policy Consensus (策略级) | 24.7% | - | **+11.5 ~ +33.3个百分点** |
| | Tactile Dynamics (动态引导) | 23.6% | 36.2% | **+12.6 ~ +21.8个百分点** |
| **本文方法** | **TouchGuide (触觉图像)** | **36.2%** | **58.0%** | **基准** |

**核心结论与性能提升**：

1.  **触觉感知带来显著增益**：在所有任务上，引入触觉反馈的策略均优于纯视觉基线。例如，以π₀.₅为基础策略时，TouchGuide将平均成功率从 **35.9%** 大幅提升至 **58.0%**。

2.  **TouchGuide 超越现有融合方法**：
    *   在DP基础上，TouchGuide（36.2%）显著优于最好的特征级融合方法RDP（30.3%）和策略级融合方法Policy Consensus（24.7%）。
    *   **关键创新价值**：论文指出，现有方法多侧重于**触觉信号变化后的反应**，而TouchGuide通过**接触物理模型**在推理时评估并引导动作的**物理可行性**，能在**首次接触**时就产生更合适的动作，这对于鞋带穿眼、开锁等任务至关重要，避免了因初始接触错误导致的不可恢复状态。

3.  **强大的泛化与兼容性**：
    *   **跨策略**：TouchGuide可无缝应用于**扩散模型**和**流匹配模型**，均带来显著提升。
    *   **跨机器人**：在双臂Bi-ARX5和单臂Flexiv Rizon4上均表现良好。
    *   **跨触觉模态**：使用**触觉图像**和**估计的力场**作为输入都有效，证明了其跨模态能力。触觉图像版本通常表现更优，因其信息更丰富。

4.  **TacUMI 系统有效性**：
    *   用户研究表明，TacUMI在**尝试次数、数据收集时长、有效数据率、用户满意度**上均优于VR遥操作、SLAM-based UMI等方法。
    *   使用TacUMI数据训练的策略，在开锁任务上的成功率（**30%**）显著高于使用SLAM-based UMI数据（**5%**）和VR遥操作数据（**15%**）训练的同类策略，证明了其收集**高精度、高质量**演示数据的能力。

5.  **消融实验验证核心设计**：
    *   **噪声预训练**：对CPM输入的动作添加噪声进行训练，是至关重要的，使平均成功率从 **39.17%** 提升至 **62.50%**。
    *   **多模态必要性**：CPM同时需要视觉和触觉输入。移除任一模态都会导致性能大幅下降（从62.50%分别降至43.33%和43.50%），证实了在精细操作中两种模态信息互补且同等重要。

**总结**：论文通过系统的实验和清晰的定量对比，强有力地证明了 **TouchGuide** 作为一种新颖的**推理时跨策略触觉引导范式**，能够有效利用触觉信息提升基础视觉运动策略的性能，在多种精细接触任务上达到**最先进水平**。同时，**TacUMI** 系统也被验证为一种**高性价比、高精度**的数据收集解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.20239v1)
- [HTML 版本](https://arxiv.org/html/2601.20239v1)
