# TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance

**相关性评分**: 7.0/10

**排名**: #15


---


## 基本信息

- **arXiv ID**: [2601.20239v1](https://arxiv.org/abs/2601.20239v1)
- **发布时间**: 2026-01-28T04:22:47Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang, Boyan Li, Yiran Qin, Jin Liu, Can Zhao, Li Kang, Haoqin Hong, Zhenfei Yin, Philip Torr, Hao Su, Ruimao Zhang, Daolin Ma

## 关键词

Vision-Language-Action Model, VLA for Robotics, Diffusion, Inference Acceleration, fine tune, world model

## 一句话总结

TouchGuide是一种在推理时通过触觉引导优化预训练视觉运动策略的新方法，用于提升机器人精细接触操作性能。

## 摘要

Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

## 详细分析

## 论文摘要：《TouchGuide：通过触觉引导在推理时引导视觉运动策略》

### 1. 研究背景和动机
精细且接触丰富的机器人操作（如系鞋带、芯片交接）仍然极具挑战，主要原因在于**触觉反馈未被充分利用**。现有视觉-触觉策略的融合方法（如特征级或策略级融合）存在模态主导、难以捕捉跨模态关联等问题。同时，高质量、低成本触觉数据的采集也面临困难。为此，本文旨在解决一个核心问题：**如何融合视觉与触觉信息以提升精细接触式操作中的策略规划能力？**

### 2. 核心方法和技术创新
本文提出了一个名为 **TouchGuide** 的**跨策略视觉-触觉融合新范式**，其核心创新在于：
- **两阶段推理时引导**：在推理时，引导一个预训练的扩散或流匹配视觉运动策略。**第一阶段**仅基于视觉输入生成粗略的、视觉上可行的动作；**第二阶段**，一个任务特定的**接触物理模型** 根据当前触觉观测，输出一个**可行性分数**，通过梯度引导来修正动作，使其更符合真实的物理接触约束。
- **新型数据采集系统 TacUMI**：为了高效获取高质量触觉数据，本文设计了一个低成本、高精度的手持式数据采集系统。它利用Vive追踪器进行精确定位，并通过**刚性指尖设计提供直接触觉反馈**，在精度、成本和易用性之间取得了良好平衡。

### 3. 主要实验结果
在**系鞋带、芯片交接、黄瓜去皮、花瓶擦拭、开锁**这五个高难度接触式任务上进行了广泛实验：
- **性能显著提升**：TouchGuide 在多个基准策略（如 Diffusion Policy 和 π0.5）上均带来了显著性能提升。例如，基于 π0.5 的 TouchGuide 平均成功率从 35.9% 提升至 58.0%。
- **超越现有方法**：TouchGuide 的表现** consistently 优于** 其他最先进的视觉-触觉策略（如 RDP、Policy Consensus）。
- **强泛化能力**：该方法可泛化到不同的机器人平台、触觉传感器模态（力信号或触觉图像）和基础策略，且**无需重新训练基础策略**。
- **系统验证**：用户研究表明，TacUMI 在数据采集的尝试次数、时长和用户满意度上均优于VR遥操作和SLAM-based等方法。

### 4. 研究意义和价值
- **方法论价值**：TouchGuide 提出了一种新颖的**在动作空间进行多模态融合**的范式，通过推理时引导巧妙地结合了视觉的全局语义和触觉的局部精细信息，为解决接触式操作问题提供了新思路。
- **实用价值**：所提出的 TacUMI 系统**降低了高质量触觉数据收集的门槛**，有助于推动触觉感知研究。TouchGuide 框架**模块化强、易于部署**，能有效提升现有视觉运动策略在精细操作任务上的鲁棒性和成功率，具有重要的实际应用前景。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：TouchGuide

### **一、 论文拟解决的核心问题**
论文旨在解决**精细、接触丰富的机器人操作任务**中的关键挑战。具体问题包括：
1.  **多模态融合难题**：现有视觉-触觉策略通常采用特征级或策略级融合，存在**模态主导**（视觉主导）或**难以捕捉跨模态协同关系**的问题，无法在动作空间进行有效融合以指导物理接触。
2.  **高质量数据收集困难**：用于模仿学习的专家演示数据质量至关重要，但现有的遥操作或手持式数据收集系统难以在**高精度、低成本、直接触觉反馈**三者间取得平衡。
3.  **策略在接触物理层面的鲁棒性不足**：对于鞋带穿引、芯片传递等任务，初始接触的微小误差可能导致任务完全失败，而现有方法多为“事后反应型”（在触觉信号突变后调整），缺乏在动作生成阶段就**主动确保其符合接触物理约束**的机制。

### **二、 核心创新点**
论文提出了一个名为 **TouchGuide** 的完整框架，包含**方法论**和**硬件系统**两方面的核心创新：

#### **1. 方法论创新：TouchGuide —— 一种新颖的跨策略视觉-触觉融合范式**
*   **核心理念**：**在低维动作空间中进行多模态融合**，而非在数据或特征层面。它通过“推理时引导”的方式，将触觉信息作为引导信号，修正一个预训练的视觉运动策略生成的动作。
*   **关键技术组件**：
    *   **两阶段推理过程**：
        1.  **视觉引导阶段**：预训练的扩散模型或流匹配模型（如 Diffusion Policy, π₀.₅）仅凭视觉观察生成一个粗略的、视觉上可行的动作草案。
        2.  **触觉引导阶段**：引入一个**任务特定的接触物理模型**，根据当前的触觉观察评估动作草案的“物理可行性”，并计算梯度来引导（Steer）采样过程，使最终动作满足真实的接触物理约束。
    *   **接触物理模型**：通过对比学习在少量专家演示上训练，其输出是动作与当前多模态观察（视觉+触觉）在潜空间中的余弦相似度，作为**可行性分数**。该模型充当了“推理时分类器”，为动作生成提供物理一致性引导。
*   **核心优势**：
    *   **无需重新训练基础策略**：以“即插即用”的方式为现有强大的视觉运动策略（VLA、扩散策略等）增添触觉感知能力，继承了基础策略的泛化能力。
    *   **显式的物理约束满足**：不同于被动响应触觉变化，CPM主动评估并修正动作，使其在生成之初就倾向于符合接触物理，这对于初始接触关键的任务至关重要。
    *   **跨策略、跨模态通用性**：实验证明可应用于不同策略架构（扩散/流匹配）和不同触觉模态（触觉图像/力矢量场）。

#### **2. 硬件系统创新：TacUMI —— 一个高性价比、高精度的数据收集系统**
*   **设计目标**：为TouchGuide的训练提供高质量、低成本的数据。
*   **关键技术特性**：
    *   **高精度与低成本**：采用 **Vive Tracker + Lighthouse** 基站进行定位，在保证亚毫米级精度的同时，将单套系统成本控制在约720美元（不含触觉传感器），远低于动作捕捉系统。
    *   **直接触觉反馈**：采用**刚性指尖**设计，操作者能直接感受到抓握力和接触，而非通过弹簧、连杆或视觉界面间接传递，这对于操作易碎物品、感知精确接触点至关重要。
    *   **高易用性与同步性**：集成板载计算单元，通过单一接口实现视觉、触觉、位姿数据的硬件同步采集，并兼容 LeRobot 等标准数据格式。

### **三、 解决方案总结**
论文通过 **“软硬结合”** 的方式系统性地解决了上述问题：

1.  **解决多模态融合与物理约束问题**：
    *   **软件上**，提出 **TouchGuide** 框架。它**重新定义了融合的场所**——从特征空间移至动作空间。通过引入**接触物理模型**作为“物理裁判”，在策略推理的采样过程中，利用触觉信息提供的梯度信号，**显式地将策略的动作分布向真实物理分布拉近**，从而生成更符合接触物理的动作。

2.  **解决高质量数据收集问题**：
    *   **硬件上**，开发了 **TacUMI** 系统。它通过**创新的硬件选型与设计**（Vive Tracker + 刚性指尖），在精度、成本、反馈直接性之间取得了最佳权衡，使得大规模采集适用于精细操作的高保真触觉演示数据成为可能。

3.  **验证方案有效性**：
    *   在**鞋带穿引、芯片传递、黄瓜削皮、花瓶擦拭、开锁**五个极具挑战性的任务上进行了广泛实验。
    *   结果表明，TouchGuide 显著且一致地超越了现有的最先进视觉-触觉策略，并展现了优秀的跨机器人、跨策略、跨触觉模态的泛化能力。同时，用户研究证实了 TacUMI 在数据收集效率和体验上的优越性。

**总而言之，这篇论文的核心贡献在于提出并验证了一种全新的、基于推理时引导的视觉-触觉融合范式，并配套开发了一个高效能的数据收集系统，共同推动了机器人完成复杂精细操作任务的能力边界。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决机器人执行精细、接触丰富的操作任务时，视觉-触觉信息融合效率低下的核心问题。为此，论文提出了一个名为 **TouchGuide** 的推理时引导框架，其核心创新在于**在低维动作空间中进行跨策略的多模态融合**：它首先利用预训练的视觉运动策略（如扩散或流匹配模型）生成粗略的视觉可行动作，然后通过一个基于对比学习训练的、任务特定的**接触物理模型**，根据触觉观测计算动作的物理可行性得分，并在推理采样过程的后期阶段，利用该得分的梯度来引导和细化动作，使其更符合真实的物理接触约束。此外，论文还配套开发了一个低成本、高精度的数据采集系统 **TacUMI**，以支持高质量触觉数据的收集。实验在五个具有挑战性的接触丰富任务（如系鞋带、芯片交接）上表明，该方法能显著且稳定地超越现有的视觉-触觉策略，并展现出跨不同机器人、触觉传感器和基础策略的良好泛化能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance》的创新点分析

这篇论文在机器人精细接触式操作领域提出了两项核心创新：**TouchGuide**（一种新颖的多模态融合范式）和**TacUMI**（一个低成本、高精度的数据采集系统）。以下是其相对于已有工作的明确创新点及其价值分析。

---

### 1. **提出“跨策略”的视觉-触觉融合范式（TouchGuide）**
   - **改进/不同之处**：
     - **以往方法**：现有的视觉-触觉策略融合主要分为两类：
       1. **特征级融合**：在模型早期将视觉和触觉特征简单拼接。这容易导致**模态主导问题**（通常是视觉主导），因为触觉信号在多数情况下变化稀疏，模型难以有效利用。
       2. **策略级融合**：训练多个单模态策略（如纯视觉策略、纯触觉策略）再进行组合。这种方法难以捕捉跨模态的协同关系，且训练成本高。
     - **本文方法**：TouchGuide提出了一种**“跨策略”的融合范式**。其核心思想是**在低维动作空间中进行模态融合**，而非在数据或特征层面。它通过一个**任务特定的接触物理模型**，在推理时引导预训练的视觉运动策略（如扩散策略或流匹配策略）。
   - **解决的问题/优势**：
     - **解决模态主导与协同问题**：通过在动作空间进行引导，确保触觉信息能直接、有效地修正动作，避免了视觉特征“淹没”稀疏触觉信号的问题。
     - **无需重新训练基础策略**：TouchGuide作为一个**推理时引导模块**，可以与多种预训练策略（如Diffusion Policy, π0.5）即插即用，显著降低了融合成本并提升了泛化能力。
     - **提升接触物理一致性**：引导信号基于触觉观测评估动作的“物理可行性”，使生成的动作更符合真实世界的接触约束，特别适合**首次接触即关键**的精细操作任务（如穿鞋带、开锁）。

### 2. **引入“接触物理模型”作为推理时引导器**
   - **改进/不同之处**：
     - **以往方法**：许多触觉策略（如RDP、PolicyConsensus）主要关注**对触觉变化的反应**，即在触觉信号发生突变后调整动作。这对于初始接触错误的纠正往往为时已晚。
     - **本文方法**：提出了一个**任务特定的接触物理模型**。该模型通过对比学习在少量专家演示上训练，能够输出一个**可行性分数**，评估当前触觉观测下，策略生成的噪声动作是否符合物理接触约束。
   - **解决的问题/优势**：
     - **实现前瞻性引导**：CPM在动作生成早期（扩散/流匹配的采样过程）就介入，评估并修正动作的物理可行性，从而**预防**不合理的初始接触，而不仅仅是事后反应。
     - **提供明确的物理约束**：通过可行性分数的梯度引导，将策略的动作分布**显式地**推向符合真实物理的分布，解决了以往方法在动作空间缺乏明确引导机制的问题。

### 3. **推导并应用流匹配模型的分类器引导公式**
   - **改进/不同之处**：
     - **以往方法**：推理时引导（Classifier Guidance）在扩散模型中已成熟应用，但在**流匹配模型**中缺乏明确推导和公式。
     - **本文方法**：论文在附录中**首次给出了流匹配模型分类器引导的详细推导**（Proposition 1），并提供了对应的引导公式，使其能够应用于基于流匹配的视觉-语言-动作模型（如π0.5的动作专家）。
   - **解决的问题/优势**：
     - **扩展了引导技术的适用范围**：使得TouchGuide范式能够无缝应用于新一代的VLA模型，充分利用其强大的泛化能力。
     - **为社区提供理论工具**：明确的公式推导为后续在流匹配模型中进行多模态引导研究提供了基础。

### 4. **设计并实现低成本、高精度的触觉数据采集系统（TacUMI）**
   - **改进/不同之处**：
     - **以往系统**：现有数据采集方案在**精度**、**成本**和**触觉反馈直接性**上存在权衡：
       1. **遥操作**：精度高，但**缺乏直接触觉反馈**，操作者依赖视觉化提示，数据质量受延迟影响。
       2. **手持式设备（如UMI）**：使用SLAM定位，**精度不足**，在特征稀疏场景易丢失跟踪。
       3. **动作捕捉系统**：精度高，但**成本昂贵**，环境要求严格。
     - **本文系统**：TacUMI采用**Vive Tracker + Lighthouse基站**进行定位，在保持低成本（约720美元）和轻量化（540g）的同时，实现了**亚毫米级高精度**。其**刚性指尖设计**提供了直接触觉反馈。
   - **解决的问题/优势**：
     - **实现精度与成本的最佳平衡**：为大规模、高质量的触觉数据采集提供了可行方案。
     - **提升数据质量与操作体验**：直接触觉反馈使操作者能更直观地执行精细操作（如拿取薯片），采集的轨迹更平滑、一致，从而训练出更鲁棒的策略。用户研究表明其**满意度最高**。

### 5. **系统性的实验验证与深入分析**
   - **改进/不同之处**：
     - **以往工作**：评估通常局限于少数任务或单一策略，对失败案例的分析较浅。
     - **本文工作**：
       1. **任务多样性**：在五个极具挑战性的精细接触任务上进行评估（穿鞋带、薯片交接、黄瓜削皮、花瓶擦拭、开锁），涵盖了长视野、协作、低数据等不同场景。
       2. **跨模态、跨策略验证**：验证了TouchGuide在不同机器人平台、不同触觉模态（力信号、触觉图像）、不同基础策略上的有效性。
       3. **详尽的失败案例分析**：在附录中对各基线方法的典型失败案例进行了归因分析，清晰揭示了触觉反馈（尤其是**手内状态信息**）在各类任务中的关键作用。
   - **解决的问题/优势**：
     - **全面证明了方法的有效性与泛化性**：实验表明，TouchGuide能显著且一致地超越现有SOTA方法。
     - **提供了宝贵的领域洞察**：深入的分析帮助读者理解“为什么触觉重要”以及“TouchGuide为何有效”，推动了该领域认知的发展。

---

### **总结：核心创新价值**
1. **范式创新**：从“融合数据/特征”转向“在动作空间进行推理时引导”，提供了一种更高效、更解耦的多模态融合新思路。
2. **技术贡献**：提出了可操作的CPM引导框架，并补齐了流匹配模型引导的理论公式。
3. **工程贡献**：设计了切实可行的数据采集系统，解决了高质量触觉数据获取的瓶颈问题。
4. **实证贡献**：通过严谨、全面的实验，为视觉-触觉融合领域设立了新的性能基准，并提供了深入的分析。

这些创新共同致力于解决**机器人精细接触式操作中触觉反馈利用不足**的核心难题，推动了机器人更灵巧、更可靠地执行复杂现实任务的能力。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 实验任务与数据集
论文在**五个具有挑战性的精细、接触丰富的机器人操作任务**上进行了全面评估：
1.  **鞋带穿眼 (Shoe Lacing)**： 长时程、高精度任务，需将鞋带依次穿过四个鞋眼。
2.  **薯片交接 (Chip Handover)**： 协作式、易碎物体操作，涉及空中交接。
3.  **黄瓜削皮 (Cucumber Peeling)**： 双臂协作，需要持续的力控制和协调。
4.  **花瓶擦拭 (Vase Wiping)**： 与人类交互，在曲面物体上执行擦拭动作。
5.  **开锁 (Lock Opening)**： 低数据量（仅20个演示）、高精度顺序操作。

**数据集**： 所有任务的数据均使用论文提出的 **TacUMI 数据收集系统**采集。该系统通过Vive Tracker进行高精度定位，并利用刚性指尖提供**直接触觉反馈**，确保了演示数据的高质量。数据量从20到100个专家演示不等，以测试不同数据规模下的性能。

### 二、 评价指标
论文采用了两种主要的评价指标：
1.  **成功率 (Success Rate)**： 用于 **鞋带穿眼、薯片交接、开锁** 任务。任务在规定时间内成功完成即计为成功。
2.  **任务得分 (Task Score)**： 用于 **黄瓜削皮、花瓶擦拭** 任务。得分定义为机器人完成度与人类专家完成度的比值（例如，削皮长度比例、擦拭干净长度比例），并量化为离散值（0, 0.1, 0.3, 0.5, 0.7, 0.9, 1）。

### 三、 基线方法对比
论文与多种先进的视觉-触觉策略进行了对比，主要分为两类基础策略进行增强：

| 基础策略 | 对比的基线方法 | 描述 |
| :--- | :--- | :--- |
| **扩散策略 (DP)** | 1. **DP (仅视觉)** <br> 2. **DP w/ 触觉观测** (特征级拼接) <br> 3. **SafeDiff** <br> 4. **RDP (Reactive Diffusion Policy)** <br> 5. **Policy Consensus** <br> 6. **Tactile Dynamics** (基于动力学模型的引导) | 涵盖了特征级融合、策略级组合、基于动力学的推理时引导等主流方法。 |
| **VLA 策略 (π₀.₅)** | 1. **π₀.₅ (仅视觉)** <br> 2. **π₀.₅ w/ 触觉观测** (特征级拼接) <br> 3. **Tactile Dynamics** (基于动力学模型的引导) | 对比了大型VLA模型在融入触觉信息时的不同方式。 |

**TouchGuide 变体**：
- **TouchGuide (Force)**： 使用Xense传感器提供的**3D力场**作为触觉输入。
- **TouchGuide (Tactile Image)**： 使用Xense传感器的**原始触觉图像**作为输入。

### 四、 关键性能结果与结论
实验在双臂（Bi-ARX5）和单臂（Flexiv Rizon4）机器人平台上进行，每个基线进行20次随机初始化的试验。主要结果总结如下：

#### 1. **整体性能显著提升**
- **基于DP的策略**： TouchGuide (触觉图像) 将平均成功率从 **16.3%** 提升至 **36.2%** (**相对提升122%**)。
- **基于π₀.₅的策略**： TouchGuide (触觉图像) 将平均成功率从 **35.9%** 提升至 **58.0%** (**相对提升62%**)。

#### 2. **超越现有视觉-触觉方法**
- TouchGuide 在几乎所有任务上都**一致且显著地优于**其他SOTA视觉-触觉策略（如RDP, PolicyConsensus）。
- **关键结论**： 现有方法多侧重于**触觉信号变化后的快速反应**，而TouchGuide通过**接触物理模型(CPM)** 在推理时评估并引导动作，使其在**首次接触**时就产生更符合物理约束的动作，这对于精细操作的成功至关重要（例如，开锁时钥匙的初始插入角度）。

#### 3. **强大的泛化能力**
- **跨策略**： 可无缝应用于扩散策略(DP)和流匹配策略(π₀.₅的Action Expert)，且均带来显著提升。
- **跨机器人**： 在双臂和单臂机器人上均表现良好。
- **跨触觉模态**： 无论是使用**力场**还是**触觉图像**作为输入，TouchGuide都能有效工作，证明了其范式对不同触觉表征的兼容性。
- **低数据泛化**： 在仅使用**20个演示**的“开锁”任务上，基于π₀.₅的TouchGuide仍能取得30%的成功率，展现了其从高质量小数据中学习的能力。

#### 4. **消融实验验证核心设计**
- **噪声预训练**： 在CPM训练时为动作添加噪声是关键，使平均成功率从 **39.2%** 提升至 **62.5%**，让CPM能适应推理时策略产生的带噪动作。
- **多模态必要性**： CPM同时需要视觉和触觉输入。移除任一模态都会导致性能大幅下降（从62.5%分别降至~43.3%），证明两种模态在任务不同阶段具有互补性。

#### 5. **TacUMI系统有效性验证**
- 与VR遥操作、SLAM-based UMI等数据收集系统相比，**TacUMI**收集的数据训练出的策略成功率**最高**（例如，在开锁任务上，TouchGuide达到30%，而其他系统最高为15%）。
- 用户研究表明，TacUMI在**尝试次数、收集时长、有效数据率、用户满意度**上均表现最佳，平衡了**高精度、低成本、轻量化和直接触觉反馈**。

### 五、 总结
论文通过系统的实验证明：
1.  **TouchGuide范式**通过在**动作空间进行推理时触觉引导**，能够显著提升多种基础视觉运动策略在精细接触任务上的性能，其效果优于现有的多模态融合方法。
2.  其性能提升的核心在于利用**接触物理模型(CPM)** 提供的可行性分数，显式地将策略分布向满足真实世界接触物理约束的方向引导。
3.  **TacUMI系统**为训练TouchGuide提供了高质量、低成本的数据保障，其直接触觉反馈和精准定位是成功的关键。
4.  论文给出了**明确的定量结果**，所有对比实验均基于相同的任务、数据集和评估协议，结果具有说服力，证实了所提方法的有效性和先进性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.20239v1)
- [HTML 版本](https://arxiv.org/html/2601.20239v1)
