# Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models

**相关性评分**: 6.0/10

**排名**: #18


---


## 基本信息

- **arXiv ID**: [2601.20687v1](https://arxiv.org/abs/2601.20687v1)
- **发布时间**: 2026-01-28T15:14:50Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Zhiqiang Kou, Junyang Chen, Xin-Qiang Cai, Xiaobo Xia, Ming-Kun Xie, Dong-Dong Wu, Biao Liu, Yuheng Jia, Xin Geng, Masashi Sugiyama, Tat-Seng Chua

## 关键词

fine tune, offline Reinforcement Learning, constrained reinforcement learning, safe reinforcement learning, goal conditioned reinforcement learning

## 一句话总结

该论文提出了一种正未标记强化学习蒸馏方法，用于在本地部署的小模型中实现无人类偏好标注或奖励模型的强化学习对齐，适用于离线、约束和安全强化学习场景。

## 摘要

Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

## 详细分析

## 论文摘要：基于正例-无标签学习的强化学习蒸馏用于本地化小模型部署

### 1. 研究背景和动机
由于隐私、成本和延迟的限制，在本地部署小型专家模型（on-premise）日益普遍。然而，大多数实际部署流程仅停留在监督微调（SFT）阶段，而无法进入强化学习（RL）对齐阶段。这主要是因为传统的RL对齐（如RLHF）严重依赖昂贵的人工偏好标注或需要大规模调用API来训练和维护高质量奖励模型，这些都不适用于本地化部署场景。因此，如何**在无需人工偏好标签或奖励模型的情况下，将大型教师模型的偏好优化能力蒸馏到本地小模型**，成为一个亟待解决的关键问题。

### 2. 核心方法和技术创新
本文提出了**强化学习能力蒸馏（RLCD）**框架，其核心是一种**基于正例-无标签（PU）学习的锚点偏好诱导机制**。该方法分为两个阶段：
- **阶段I（教师引导的SFT）**：使用黑盒教师模型为每个提示生成一个高质量响应作为监督信号，对小模型进行标准的监督微调。
- **阶段II（RL能力蒸馏）**：这是本文的核心创新。对于每个提示，**仅调用教师模型一次**，生成一个**锚点响应**作为高置信度的正例。然后，学生模型在本地采样多个候选响应，并基于锚点进行**自评估和自排序**，诱导出候选响应之间的软偏好分布（`D_x`）。最后，通过提出的**标签分布学习-组相对策略优化（LDL-GRPO）**目标，使学生模型在采样组内的概率分配与诱导的偏好分布对齐，从而实现完全本地的、基于偏好的策略优化循环。

**主要技术创新点**：
- **极低的教师调用成本**：将传统“教师即裁判”方法所需的 `O(NK)` 次调用降低为 `O(N)` 次（每提示一次）。
- **无需奖励模型**：所有偏好信号均通过学生模型的锚点条件自评估在本地产生。
- **理论保障**：证明了诱导的偏好信号具有**顺序一致性**，并且**集中于接近最优的候选**，确保了优化过程的稳定性。

### 3. 主要实验结果
实验在单模态（创意写作、数学推理）和多模态（视觉问答）任务上进行了全面评估，使用了Qwen2.5、LLaMA3、LLaVA等作为学生骨干模型。
- **性能领先**：提出的`LDL-GRPO`方法在几乎所有任务和模型上都显著优于仅SFT、输出蒸馏、单对DPO、教师即裁判的GRPO等基线方法。
- **成本效益**：在相同甚至更低的教师查询预算下，取得了最佳性能。
- **稳定性**：两阶段训练（SFT → LDL-GRPO）收敛平稳，且对超参数（如温度 `τ`、KL权重 `β`）变化不敏感。
- **消融研究**：验证了基于锚点的自排序（`AnchorRank-DPO`）和列表式监督（`LDL-GRPO`）均优于单锚点-单候选的简单对比方法。

### 4. 研究意义和价值
- **实际价值**：为受限于隐私、成本和延迟的**本地化小模型部署**，提供了一条切实可行的、低成本的从SFT到RL对齐的完整技术路径，弥合了研究与实际部署之间的关键差距。
- **方法论贡献**：创新性地将PU学习思想与RL对齐相结合，提出了RLCD这一新范式，并通过理论分析为其有效性提供了支撑。
- **启发性**：证明了仅通过黑盒生成（而非评分或排序）的大型模型，也能有效地将其“自我改进”的偏好优化能力蒸馏给小模型，为未来的模型蒸馏和自对齐研究提供了新思路。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究问题**
论文旨在解决一个**实际部署中的关键瓶颈**：在**本地化（On-Premise）部署小型专家模型**时，由于隐私、成本和延迟的限制，通常只能进行监督微调（SFT），而**无法进行第二阶段的基于人类偏好的强化学习（RL）对齐**。这是因为RL对齐通常需要昂贵的人工偏好标注，或依赖需要大规模API调用和持续维护的高质量奖励模型，这些都不适合本地化场景。

### **核心创新点**
论文提出了一个名为 **“正例-无标记强化学习蒸馏”** 的方法，其核心创新在于：

1.  **问题定义创新**：首次形式化了 **“强化学习能力蒸馏”** 问题。目标不是简单地蒸馏教师模型的知识（输出），而是蒸馏其**偏好优化的能力**，使学生模型能够在本地实现自我改进。
2.  **方法创新**：提出了一种**基于锚点的正例-无标记自排序机制**。
    - **低成本锚点获取**：对于每个提示，只调用一次黑盒教师模型，生成一个高质量的**锚点响应**，作为隐式的“正例”。
    - **本地自排序与信号诱导**：学生在本地采样多个候选响应，并以锚点为参考，通过**自我评估**对这些候选进行排序，从而诱导出成对或列表式的偏好关系。
    - **完全本地化训练循环**：利用诱导出的偏好信号，直接使用DPO或作者提出的LDL-GRPO等偏好优化目标进行训练，形成一个“采样-比较-更新”的本地RL循环。
3.  **理论支撑**：为提出的方法提供了理论保证，证明了诱导出的偏好信号具有**顺序一致性**，并且**概率质量集中在接近最优的候选上**，这支持了其用于偏好优化的稳定性。
4.  **实际价值**：将教师模型的调用复杂度从 `O(N*K)`（教师作为裁判，需评估所有候选）降低到 `O(N)`（每提示仅生成一个锚点），**极大地降低了成本、延迟和对云服务的依赖**，使得在严格的本地化约束下实现从SFT到RL的完整对齐流程变得可行。

### **解决方案的步骤**
1.  **第一阶段（教师引导的SFT）**：用教师模型为提示池生成响应，对学生模型进行标准的监督微调，获得一个具备基本指令跟随能力的模型 `p_sft`。
2.  **第二阶段（RL能力蒸馏 - RLCD）**：
    - **锚点条件PU自评估**：对于每个提示，获取教师锚点响应 `a`，本地采样K个学生候选 `{y_k}`。
    - **计算锚点参考边际**：使用学生自身的评分函数 `g_θ`，计算每个候选相对于锚点自身的分数边际 `r_k = g_θ(x, a, y_k) - g_θ(x, a, a)`。
    - **构建软偏好分布**：通过公式 `D_x(k) ∝ σ(γ * r_k) * exp(r_k / τ)` 构建一个在候选集上的软标签分布 `D_x`。其中Sigmoid项 `σ(γ * r_k)` 体现了PU思想（将锚点作为正例，对候选进行软加权），指数项进行排序锐化。
    - **标签分布学习优化**：提出 **LDL-GRPO** 目标，使学生模型在候选集上的归一化概率分布 `q_θ` 与目标软偏好分布 `D_x` 对齐，同时通过KL散度约束防止偏离SFT初始模型太远。
    ```math
    min_θ 𝔼_x[ KL( D_x || q_θ(·|x) ) ] + β 𝔼_x[ KL( p_θ(·|x) || p_sft(·|x) ) ]
    ```

### **实际价值与效果**
- **性能提升**：在创意写作、数学推理、多模态理解等多个任务和骨干模型上，论文方法（尤其是LDL-GRPO） consistently 超越了仅SFT、输出蒸馏、教师作为裁判等基线方法。
- **成本效益**：在相同甚至更低的教师查询预算下，实现了更好的对齐效果。
- **部署友好**：**无需人工偏好数据、无需训练奖励模型、最小化对云服务的调用**，完美契合了本地化部署在**隐私、成本、合规性和低延迟**方面的核心需求。

**总结**：这篇论文的核心贡献是提出了一种**低成本、完全本地化**的强化学习对齐蒸馏框架，通过**单次教师查询获取锚点 + 本地自排序诱导偏好**的创新方式，解决了小模型本地部署中RL阶段难以实施的关键痛点，具有很高的理论创新性和实际应用价值。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**在隐私、成本和延迟约束下，本地部署的小模型难以进行强化学习对齐**的核心问题，提出了一种**基于正例-无标签（PU）学习的强化学习能力蒸馏（RLCD）方法**。该方法仅需对黑盒教师模型进行单次查询获得一个“锚点”响应，随后在本地采样多个学生候选响应，并通过锚点引导的自排序来诱导偏好信号，从而实现了**完全本地的、无需人工偏好标注或奖励模型的偏好优化训练循环**。实验表明，该方法在单模态和多模态任务上，以相同或更低的教师查询成本，**显著超越了仅进行监督微调（SFT）及其他基线方法**，证明了其能够将大模型的偏好优化能力有效蒸馏到本地小模型中。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models》针对在本地部署小型模型时难以进行强化学习对齐的**实际痛点**，提出了一套创新的解决方案。其核心创新点可归纳如下：

---

### 1. **问题定义创新：提出“强化学习能力蒸馏”新范式**
- **改进/不同之处**： 以往的知识蒸馏主要关注**静态知识或单次响应质量**的迁移（如通过SFT模仿教师输出）。本文首次明确提出了 **“强化学习能力蒸馏”** 这一新范式，其目标不是模仿教师的某个具体输出，而是**蒸馏教师模型所具备的“偏好优化”能力**，即根据反馈进行自我改进的RL机制。
- **解决的问题/优势**： 这使得本地小模型能够超越简单的模仿学习，获得**持续自我优化和决策对齐的能力**，从而弥合了本地部署中常见的“仅有SFT，缺乏RL”的鸿沟。

### 2. **方法创新：基于“正例-未标注”的锚点偏好诱导机制**
- **改进/不同之处**：
    - **传统教师评判法**： 需要教师对学生的`K`个候选响应逐一评分，调用复杂度为`O(NK)`，成本高昂。
    - **本文的锚点法**： 对每个提示，**仅调用教师一次**，生成一个高质量的**锚点响应**作为隐式正例。随后，学生模型在本地采样多个候选，并**以锚点为参照进行自我排序**，诱导出成对或列表偏好。
- **解决的问题/优势**：
    - **大幅降低成本和延迟**： 将教师调用复杂度从`O(NK)`降至`O(N)`，完全符合本地部署在**隐私、成本和延迟**方面的严格限制。
    - **摆脱对奖励模型和人工标注的依赖**： 整个训练循环（采样、比较、更新）完全在本地进行，无需训练和维护独立的奖励模型，也无需人工标注偏好对。

### 3. **算法创新：提出LDL-GRPO（基于标签分布学习的组相对策略优化）**
- **改进/不同之处**：
    - **相比DPO等成对方法**： DPO依赖于明确的`(y+, y-)`偏好对。本文通过锚点诱导出的不是一个硬性的正负对，而是一个在`K`个候选上的**软性标签分布 `D_x`**。
    - **相比标准的GRPO**： 标准的GRPO需要外部奖励模型提供标量奖励。本文的LDL-GRPO将`D_x`作为分布监督目标，通过最小化`D_x`与学生策略在候选集上的归一化分布`q_θ`之间的KL散度来优化策略。
- **解决的问题/优势**：
    - **更鲁棒、更稳定的优化**： 软标签分布比硬性偏好对包含更丰富、更平滑的监督信号，能更好地处理自我评估中的噪声，防止优化不稳定或崩溃。实验证明LDL-GRPO性能优于基于成对偏好的AnchorRank-DPO。
    - **实现完全本地的组级策略优化**： 将锚点诱导的比较信号转化为组级的分布匹配问题，实现了无需外部奖励的端到端策略优化。

### 4. **理论创新：为锚点诱导的偏好信号提供理论保证**
- **改进/不同之处**： 大多数基于自我奖励或教师评判的RL工作缺乏严格的理论分析。本文提供了两个关键定理：
    1. **定理3.1（序一致性）**： 证明诱导出的标签分布`D_x`严格保持了基于锚点校准后的得分`r_k`的排序。
    2. **定理3.2（近最优性）**： 证明`D_x`的概率质量集中在采样候选集中的**接近最优的响应**上，且与最优者的期望差距有明确上界（受`τ log K`控制）。
- **解决的问题/优势**：
    - **为方法的有效性奠基**： 理论分析确保了诱导出的偏好信号是**合理、一致且聚焦于高质量候选**的，从而解释了该方法为何能进行稳定的偏好优化，而非引入随机噪声。
    - **指导超参数设计**： 理论边界（如`τ`和`K`的作用）为理解超参数的影响提供了依据，例如，更小的`τ`或更大的`K`有助于更集中地关注优质响应。

### 5. **评估与验证创新：全面的跨模态、低成本实验设计**
- **改进/不同之处**：
    - **任务范围**： 不仅在单模态（创意写作、数学推理）任务上验证，还扩展到了**多模态**任务（视觉问答、描述生成），证明了方法的通用性。
    - **成本控制对比**： 所有基线方法都在**相同的、极低的教师查询预算**（每提示一次）下进行公平比较，突出了在严格约束下本文方法的效率优势。
    - **评估指标**： 除了原始胜率，还使用了**长度控制胜率**，以排除模型通过生成冗长文本来讨好评估器的偏差，证实了性能提升来源于真实质量。
- **解决的问题/优势**：
    - **证实了方法的广泛适用性**： 创新不止于NLP，在视觉-语言任务上同样有效，拓展了应用场景。
    - **凸显了实际部署价值**： 在模拟真实本地部署的苛刻成本限制下，系统性地证明了所提方法能实现从SFT到RL的**实用、低成本、高性能**对齐流程。

---

## 总结
本文的核心贡献在于，**将原本需要高昂代价（人类标注、奖励模型、频繁调用大模型）的RL对齐能力，巧妙地“蒸馏”到了一个仅需极少量外部调用、完全本地化运行的训练框架中**。通过**锚点诱导PU偏好**和**标签分布学习**这两个关键技术创新，辅以**理论保证**和**扎实的实验验证**，为解决本地小模型部署中的“对齐天花板”问题提供了一个切实可行的新路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果分析

本文提出了一种面向本地部署小模型的**正例-无标记强化学习蒸馏方法**，旨在解决传统RL对齐阶段因依赖人类偏好标注或大规模奖励模型而难以在本地部署中实施的问题。以下是对论文实验与评估效果的详细分析。

### 一、 使用的数据集与评价指标

#### 1. 数据集
实验覆盖了**单模态**和**多模态**两种典型场景，共使用6个任务进行评估：
*   **单模态任务**：
    *   **创意写作**：使用 **WritingPrompts** 数据集，细分为两个任务：
        *   `WritingPrompts-CW`：侧重于遵循约束的创意写作。
        *   `WritingPrompts-EU`：侧重于表达和风格的多样性。
    *   **数学推理**：使用 **Competition Math** 数据集，细分为两个任务：
        *   `CompMath-Count`：侧重于计数和算术推理。
        *   `CompMath-Geometry`：侧重于几何问题求解。

*   **多模态任务**：
    *   **视觉语言理解**：使用 **A-OKVQA** 数据集，细分为两个任务：
        *   `A-OKVQA-MC`：多项选择视觉问答。
        *   `A-OKVQA-Rationale`：需要生成基于图像内容的自由形式解释。

#### 2. 评价指标
采用**自动A/B偏好评估**，将每个基线方法与 **GPT-4o** 生成的响应进行比较。
*   **外部评判模型**：
    *   单模态文本任务：使用 **Qwen3-235B-A22B-Instruct** 作为评判模型。
    *   多模态任务：使用 **Qwen3-VL-235B-A22B-Instruct** 作为评判模型。
*   **报告指标**：
    *   **原始胜率**：评判模型选择学生模型响应优于GPT-4o响应的比例。
    *   **长度控制胜率**：在评估时强制使响应长度可比，以消除冗长性偏差。平局计为0.5胜。

### 二、 对比的基线方法
所有方法共享相同的两阶段设置（先SFT，后偏好优化），仅在第二阶段如何构建监督信号和优化目标上有所不同：
1.  **SFT**：仅进行第一阶段监督微调，作为性能基准。
2.  **SFT → SFT**：第二阶段继续用教师模型的输出进行SFT（模仿学习）。
3.  **SinglePair-DPO**：为每个提示，用教师响应和学生响应构成一个偏好对（教师优），进行直接偏好优化。
4.  **Anchor-GRPO**：为每个提示获取一个教师锚点响应，用本地SFT模型作为评估器为学生候选打分，使用组相对策略优化。
5.  **Self-PPO**：学生模型自我评估单个响应，使用近端策略优化进行更新，不查询外部教师。
6.  **AnchorRank-DPO**：为每个提示获取一个教师锚点，通过锚点条件下的自排序诱导出成对偏好，进行DPO优化。
7.  **LDL-GRPO (本文方法)**：为每个提示获取一个教师锚点，通过锚点条件下的自评估诱导出**列表式软偏好分布**，进行基于标签分布学习的组相对策略优化。

### 三、 关键性能提升与结论
根据论文表1和表2的主要结果，可以得出以下核心结论：

#### 1. **整体性能优势**
*   在所有6个任务和不同骨干模型上，本文提出的两种基于锚点的方法（`AnchorRank-DPO` 和 `LDL-GRPO`）在原始胜率和长度控制胜率上** consistently 位列前两名**，显著优于所有非锚点基线。
*   **`LDL-GRPO` 在绝大多数任务和模型规模上均优于 `AnchorRank-DPO`**，这表明基于标签分布的组监督比基于多对DPO的监督提供了更鲁棒、更稳定的偏好优化信号。

#### 2. **相对于关键基线的提升**
*   **vs. SFT**：`LDL-GRPO` 在所有任务上都取得了显著提升，验证了第二阶段偏好优化的必要性。例如，在 `PBFG` 任务上，使用LLaMA3-8B模型，`LDL-GRPO` 的原始胜率（0.834）远高于SFT（0.482）。
*   **vs. SFT → SFT**：纯模仿学习的第二阶段性能甚至可能低于SFT基线，而所有基于偏好的方法都带来了明确增益，**凸显了从模仿学习转向偏好优化的重要性**。
*   **vs. SinglePair-DPO**：`AnchorRank-DPO` 和 `LDL-GRPO` 均显著优于 `SinglePair-DPO`，证明**从一个锚点和多个本地候选诱导出的偏好信号比单一配对提供了更丰富的监督信息**。
*   **vs. Teacher-as-Judge范式**：本文方法（`O(N)`次教师调用）在性能上媲美或优于需要 `O(NK)` 次教师调用的“教师即裁判”范式，但**成本大幅降低**，更符合本地部署的低成本要求。

#### 3. **实际价值体现**
*   **低成本可行性**：`LDL-GRPO` 仅需对每个提示查询一次黑盒教师模型以获取锚点响应，后续所有采样、比较和更新均在本地完成。这成功地将教师依赖从 `O(NK)` 降低到 `O(N)`，**消除了对奖励模型和持续人类标注的需求**，使得在隐私、成本和延迟约束下的本地部署模型能够进行RL对齐成为可能。
*   **稳定性与泛化性**：理论分析（定理3.1和3.2）证明了诱导出的偏好信号具有**顺序一致性**并**集中于接近最优的候选**，为优化稳定性提供了保障。实验表明，`LDL-GRPO` 在单模态和多模态任务上均表现稳定，收敛曲线平滑，且对超参数（如温度 `τ` 和KL权重 `β`）在一定范围内不敏感。
*   **性能提升非源于冗长**：长度控制胜率的提升与原始胜率趋势一致，确认了性能增益源于**响应质量的真实提升**，而非简单的生成长度增加。

### 总结
本文通过系统的实验评估表明，所提出的 **PU RL蒸馏方法（特别是LDL-GRPO）** 能够在**极低的教师查询成本下**，有效将大型黑盒教师的偏好优化能力蒸馏到本地小模型中。该方法在创意写作、数学推理和视觉问答等多种任务上，** consistently 超越了传统的SFT、模仿学习蒸馏以及需要更高成本的教师裁判式RL基线**，为实现从SFT到RL的完整、实用的本地对齐流水线提供了可行的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.20687v1)
- [HTML 版本](https://arxiv.org/html/2601.20687v1)
