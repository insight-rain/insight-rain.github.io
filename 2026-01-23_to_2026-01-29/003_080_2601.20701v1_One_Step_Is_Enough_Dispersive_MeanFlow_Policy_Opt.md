# One Step Is Enough: Dispersive MeanFlow Policy Optimization

**相关性评分**: 8.0/10

**排名**: #3


---


## 基本信息

- **arXiv ID**: [2601.20701v1](https://arxiv.org/abs/2601.20701v1)
- **发布时间**: 2026-01-28T15:34:29Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Guowei Zou, Haitao Wang, Hejun Wu, Yukun Qian, Yuhang Wang, Weibing Li

## 关键词

Vision-Language-Action Model, VLA for Robotics, Flow Matching, Diffusion, Inference Acceleration, Edge Deployment, fine tune, offline Reinforcement Learning, constrained reinforcement learning, safe reinforcement learning, goal conditioned reinforcement learning, world model

## 一句话总结

该论文提出了一种名为DMPO的框架，通过MeanFlow实现单步推理、分散正则化和强化学习微调，显著加速机器人控制策略的推理速度，适用于实时场景。

## 摘要

Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

## 详细分析

## 论文摘要：《一步足矣：Dispersive MeanFlow 策略优化》

### 1. 研究背景和动机
实时机器人控制要求快速生成动作。然而，现有的基于扩散模型和流匹配的生成策略需要多步采样，这在根本上限制了其在时间关键场景中的部署。现有方法面临**推理效率**、**表征崩溃**和**性能上限**三大相互关联的挑战，形成了一个耦合循环：快速推理需要一步生成，稳定的一步生成需要防止表征崩溃，突破性能上限需要强化学习微调，而高效的强化学习微调又需要快速推理和高质量表征。任何仅解决单一挑战的方法都会被其他挑战所阻碍。

### 2. 核心方法和技术创新
本文提出了 **Dispersive MeanFlow 策略优化** 框架，通过三个相互支撑的核心组件解决上述耦合问题：
- **MeanFlow 一步生成**：基于数学推导的平均速度场，无需知识蒸馏即可实现真正的单步推理，为实时控制和高效微调奠定基础。
- **分散正则化**：防止表征崩溃，确保一步生成的稳定性，并为后续强化学习微调提供高质量的初始化。
- **强化学习微调**：使用带行为克隆正则化的PPO算法，使策略能够超越专家演示的性能上限。

这三个组件形成了一个正反馈循环：分散正则化使一步生成稳定，一步生成使强化学习微调高效，高质量的预训练为强化学习提供了良好的起点。

### 3. 主要实验结果
- **性能与效率**：在RoboMimic操作和OpenAI Gym运动基准测试中，DMPO仅使用1步推理，其性能即可与需要多步推理的基线方法竞争甚至超越，同时实现了**5-20倍的推理加速**。
- **实时控制**：在NVIDIA RTX 4090上，DMPO的推理延迟仅为**0.6毫秒**，控制频率超过120Hz，满足实时控制要求。
- **物理部署**：在Franka-Emika-Panda机器人上的真实世界实验验证了其有效性和卓越的仿真到现实迁移能力，成功完成了所有四项操作任务。
- **消融验证**：实验证实了分散正则化对于防止表征崩溃至关重要，RL微调显著提升了超越模仿学习基线的性能。

### 4. 研究意义和价值
DMPO首次打破了生成式机器人策略中效率与性能的权衡，实现了**真正的一步生成**。其理论贡献在于为分散正则化建立了信息论基础，并推导了一步策略的RL微调公式。该框架的**轻量级架构**（仅1.78M参数）与核心算法协同工作，不仅为实时机器人控制提供了切实可行的解决方案，显著降低了部署的计算成本，也为未来高效生成模型的设计提供了新的思路。这项工作表明，通过精心设计的算法与架构协同，**一步生成足以实现高性能的实时机器人控制**。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：《One Step Is Enough: Dispersive MeanFlow Policy Optimization》

### **一、 核心问题**
论文旨在解决**实时机器人控制中生成式策略的效率与性能矛盾**。具体而言，现有基于扩散模型和流匹配的生成式策略虽然性能强大，但需要**多步采样**（如数十到数百步），导致推理延迟高，无法满足时间关键型应用（如实时操控）的需求。而追求一步生成的现有方法则面临三个相互关联的挑战：
1.  **推理效率**：多步采样延迟高；基于蒸馏的一步方法训练复杂且性能受限于教师模型。
2.  **表征崩溃**：一步生成方法容易将不同的观测映射到相似的表征，导致动作质量下降，且没有迭代修正的机会。
3.  **性能上限**：纯模仿学习无法超越专家演示数据，而基于强化学习的微调在多步策略上计算成本高，且在表征崩溃时不稳定。

这三个挑战形成了一个耦合循环：快速推理需要一步生成，稳定的一步生成需要防止表征崩溃，突破性能上限需要RL微调，而高效的RL微调又需要快速推理和高质量表征。

### **二、 核心创新点**
论文提出了 **Dispersive MeanFlow Policy Optimization** 框架，通过三个相互支撑的关键组件，系统性解决了上述耦合问题：

#### **1. MeanFlow 一步生成**
*   **是什么**：一种基于**平均速度场**的流匹配方法。它学习从噪声到目标动作的**平均位移**，而非瞬时速度。
*   **如何解决效率问题**：
    *   **无需蒸馏**：通过数学推导（MeanFlow恒等式）直接实现单步推理，避免了复杂的教师-学生蒸馏流程。
    *   **灵活步数**：同一个训练好的网络支持任意步数（1步到多步）的推理，为实时控制（1步）和高质量生成（多步）提供了灵活性。
*   **价值**：为实时控制和高效的RL微调奠定了**高速推理**的基础。

#### **2. 分散正则化**
*   **是什么**：在预训练阶段，对编码器输出的**条件嵌入**施加正则化损失，鼓励批次内不同样本的表征在特征空间中**分散开来**，防止它们坍缩到相似的点。
*   **如何解决表征崩溃问题**：
    *   **理论依据**：从信息论角度证明，最大化表征的熵 `H(Z)` 等价于最大化表征与观测的互信息 `I(Z;O)`，从而确保表征充分编码观测信息。
    *   **实践方法**：提出了多种分散损失（如InfoNCE-L2、协方差正则化），核心是增大表征间的距离或降低特征维度间的相关性。
*   **价值**：确保了一步生成策略的**稳定性**，为后续RL微调提供了高质量的初始化表征。

#### **3. 基于PPO的强化学习微调**
*   **是什么**：在预训练的策略基础上，使用近端策略优化算法进行在线微调，并引入行为克隆正则化以防止灾难性遗忘。
*   **如何突破性能上限**：
    *   **高效微调**：得益于一步生成的高推理速度，RL交互和策略更新的吞吐量大幅提升，使得大规模在线微调变得可行。
    *   **策略分解**：将策略形式化为两层：外层是真实环境的MDP，内层是仅用于重参数化动作分布的潜变量去噪链。PPO的优势函数和值目标始终定义在外层环境上。
*   **价值**：使策略能够**超越专家演示的性能**，适应更复杂的任务或非最优的演示数据。

**三者的协同关系**：分散正则化使一步生成稳定 → 稳定的一步生成使RL微调高效 → 高质量的预训练为RL提供了良好的起点。这形成了一个正向反馈循环。

### **三、 解决方案的架构与效果**
*   **轻量级架构**：采用单层轻量ViT + MLP动作头，仅178万参数，模型大小约28MB。这与算法创新协同，共同实现高速推理。
*   **两阶段训练流程**：
    1.  **阶段一（预训练）**：在离线演示数据上，使用 **MeanFlow损失 + 分散正则化损失** 联合训练，获得高质量的一步生成策略。
    2.  **阶段二（微调）**：使用 **PPO + 行为克隆正则化** 对预训练策略进行在线微调，以提升性能。

### **四、 实际价值与验证结果**
*   **极致性能**：在RoboMimic（操控）和OpenAI Gym（运动）基准测试上，达到或超越了需要多步采样的基线方法性能。
*   **革命性效率**：
    *   **推理加速**：在RTX 4090上，**1步推理仅需0.6ms**，相比100步的Diffusion Policy加速**694倍**，控制频率超过**120Hz**（达到1770Hz）。
    *   **训练加速**：一步推理使RL微调的墙钟时间大幅减少。
*   **物理验证**：成功部署在Franka Emika Panda机器人上，在四个真实世界操控任务中实现了超过100Hz的实时控制，并展示了优异的仿真到现实迁移能力。
*   **突破权衡**：DMPO打破了生成式机器人策略中“效率-性能”的权衡曲线，首次同时实现了**高速推理**和**高性能**。

### **结论**
DMPO的核心贡献在于提出了一个**统一的框架**，通过**MeanFlow（提供高效单步推理）、分散正则化（保证单步稳定性）和RL微调（突破性能上限）** 这三个环环相扣的组件，系统性地解决了实时机器人控制中生成式策略的瓶颈。其实验结果强有力地证明了其主张：**对于实时的生成式机器人控制，一步就足够了**。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决生成式机器人策略在实时控制中面临的核心矛盾：基于扩散或流匹配的多步采样策略性能优越但推理延迟高，而现有的一步生成方法要么依赖复杂的知识蒸馏，要么因表征崩溃导致性能不稳定。为此，作者提出了**Dispersive MeanFlow Policy Optimization (DMPO)** 框架，其核心创新在于**三个相互支撑的组件**：1) **MeanFlow**，提供无需蒸馏、数学推导的单步推理基础；2) **分散正则化**，防止单步生成中的表征崩溃；3) **基于PPO的强化学习微调**，使策略能超越专家示范的性能上限。实验表明，DMPO在多个机器人操作和运动基准测试中达到了与多步基线相当或更优的性能，同时实现了**5-20倍的推理加速**，在高端GPU上可达数百赫兹的控制频率，并在真实Franka机器人上验证了其实时控制能力，最终证明了**单步生成足以实现高效、高性能的实时机器人控制**。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《One Step Is Enough: Dispersive MeanFlow Policy Optimization》的创新点分析

本文提出了一种名为**Dispersive MeanFlow Policy Optimization (DMPO)** 的新框架，旨在解决实时机器人控制中生成式策略在**推理效率、表示质量、性能上限**三者之间的权衡难题。其核心创新点可归纳为以下三个方面，它们相互协同，共同构成了一个完整的解决方案。

### 1. **基于MeanFlow的免蒸馏一步生成策略**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：主流生成式策略（如扩散策略、流匹配策略）需要**多步采样**（通常10-100步）来生成动作，导致推理延迟高。为了加速，一些工作（如一致性策略CP、1-DP）采用**知识蒸馏**，将多步教师模型压缩成一步学生模型，但这需要复杂的多阶段训练流程，且学生模型性能受限于教师模型。
     - **DMPO的改进**：直接采用**MeanFlow**方法进行训练。MeanFlow学习一个**平均速度场**，而非瞬时速度场。其数学特性（位移恒等式和MeanFlow恒等式）允许在推理时，**无需任何蒸馏过程**，即可通过**单次前向传播**从噪声直接生成动作（即“一步生成”）。
   - **解决的具体问题/带来的优势**：
     - **解决了推理效率瓶颈**：将每动作的推理延迟从毫秒级（多步）降低到亚毫秒级（一步），实现了**5-20倍的推理加速**，在高端GPU上可达数百赫兹的控制频率，满足了严格的实时控制（>120Hz）需求。
     - **简化了训练流程**：避免了复杂的教师-学生蒸馏管道，训练更直接、更高效。
     - **为后续RL微调奠定基础**：一步生成的高效性使得在线强化学习微调变得可行，因为RL需要大量策略前向传播，多步推理在此场景下计算成本过高。

### 2. **针对一步生成的弥散正则化**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：一步生成模型（包括原始的MeanFlow应用如MP1）容易发生**表示崩溃**，即不同的观测被映射到高度相似的特征表示上。这导致策略无法区分细微的环境状态差异，从而生成低质量或不稳定的动作。此前，弥散正则化主要用于**多步**扩散模型以提升表示质量，但尚未系统地应用于**免蒸馏的一步生成策略**。
     - **DMPO的改进**：首次将**弥散正则化**系统地集成到MeanFlow预训练中。论文提出了多种正则化形式（如InfoNCE-L2、协方差正则化），并将其应用于视觉Transformer编码器输出的**条件嵌入**上。论文还首次从**信息论**角度（见附录D）证明了弥散正则化对于稳定一步生成的必要性。
   - **解决的具体问题/带来的优势**：
     - **解决了表示崩溃问题**：强制模型学习**分散的、信息丰富的特征表示**，防止不同观测在隐空间中被混淆。
     - **提升了一步生成的鲁棒性和性能**：实验表明，加入弥散正则化后，在复杂任务（如RoboMimic的Square、Transport）上的成功率显著提升（5-10%），且性能方差降低。这使得一步生成在保持高速的同时，也能达到与多步方法相媲美甚至更优的任务成功率。
     - **为RL微调提供了高质量的初始化**：良好的表示是RL稳定学习的基础，弥散正则化确保了预训练策略为后续微调提供了一个稳定、信息丰富的起点。

### 3. **面向一步生成策略的强化学习微调框架**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：基于模仿学习的策略性能受限于专家演示数据的天花板。虽然已有工作（如DPPO、ReinFlow）探索了对多步扩散/流策略进行PPO微调以超越演示，但这些方法**继承了多步推理的高延迟**，使得大规模在线RL训练效率低下。此外，在表示崩溃的模型上进行RL微调会不稳定。
     - **DMPO的改进**：提出了一个**专门为一步生成MeanFlow策略设计的PPO微调框架**。关键设计包括：
       1. **双层策略分解**：将环境MDP（外层）与仅用于重参数化动作分布的隐去噪链（内层）分离，使优势函数和值目标始终定义在外部环境上。
       2. **带有行为克隆正则化的PPO**：引入基于冻结预训练策略输出的行为克隆损失，防止微调过程中发生灾难性遗忘，确保学习稳定性。
       3. **兼容一步推理的高效训练**：由于策略本身是一步生成，每次策略评估的成本极低，使得RL微调在时间上变得可行。
   - **解决的具体问题/带来的优势**：
     - **突破了模仿学习的性能上限**：通过在线RL探索，策略能够**超越专家演示的水平**，在多个任务上取得了比纯模仿学习基线更高的成功率。
     - **实现了高效稳定的在线微调**：一步推理的高效率使得PPO所需的成千上万次策略前向传播不再成为计算负担。结合行为克隆正则化，微调过程稳定，成功率高。
     - **形成了正向反馈循环**：弥散正则化提供稳定的一步生成 → 一步生成使RL微调高效 → RL微调利用高质量预训练快速提升性能。三者协同，最终得到一个既快又强的策略。

### **总结：统一的框架级创新**
DMPO并非简单堆砌技术，而是通过**轻量级模型架构（Light ViT + MLP）** 与上述**三大算法组件**的深度协同设计，系统性解决了“效率-稳定性-超越性”这个耦合难题。
- **以往工作**通常只能在上述三个目标中权衡取舍（例如，快但不稳，或强但慢）。
- **DMPO**通过**MeanFlow实现速度、弥散正则化保证稳定、RL微调突破上限**，首次在生成式机器人策略中**同时实现了高速推理、高任务成功率和超越演示的性能**，打破了原有的帕累托前沿，占据了“又快又好”的象限。

**实际价值**：DMPO在模拟基准（RoboMimic, OpenAI Gym）和真实机器人（Franka Panda）上均得到验证，实现了真正的实时控制（>100Hz），为需要高响应、高精度操作的现实世界机器人部署提供了可行的解决方案。其代码已开源，促进了相关领域的研究与应用。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过一系列详实的实验，全面验证了DMPO框架在**推理效率、任务性能、以及实际部署**三个维度的优越性。其核心结论是：DMPO首次打破了生成式机器人策略中“效率-性能”的权衡，实现了**真正的一步生成**，在保持或超越多步基线方法性能的同时，获得了**5-20倍以上的推理加速**，并成功部署于真实机器人，满足实时控制（>120Hz）需求。

### 一、 使用的数据集与评价指标

#### 1. 数据集
- **RoboMimic (Manipulation)**:
    - **任务**: Lift, Can, Square, Transport（难度递增）。
    - **数据版本**: 为确保公平对比，采用了与基线方法（DPPO, ReinFlow）相同的 **Simplified RoboMimic MH** 数据集（每个任务约100条轨迹），而非完整数据集（300条轨迹）。这排除了数据量差异带来的性能增益。
- **OpenAI Gym / D4RL (Locomotion & Kitchen)**:
    - **任务**: Hopper, Walker2d, Ant, Humanoid（运动）以及 Kitchen-Complete, Kitchen-Partial, Kitchen-Mixed（厨房操作）。
    - **数据来源**: 标准的D4RL离线数据集，用于预训练阶段的模仿学习。

#### 2. 评价指标
- **主要指标**:
    - **成功率 (Success Rate)**: 对于RoboMimic操作任务，衡量任务在100个评估回合中成功完成的百分比。
    - **回报 (Episode Reward)**: 对于Gym运动任务，衡量一个回合内获得的累积折扣回报。
    - **完成率 (Task Completion Rate, %)**: 对于Kitchen多阶段任务，衡量所有子任务被完成的百分比。
- **效率指标**:
    - **推理时间 (Inference Time)**: 在特定GPU（RTX 4090/2080）上生成单个动作所需的毫秒数。
    - **控制频率 (Control Frequency)**: 根据推理时间计算出的理论控制频率（Hz）。
    - **加速比 (Speedup)**: 相对于基准方法（如100步的Diffusion Policy）的推理速度提升倍数。
- **其他分析指标**:
    - **数据效率 (Data Efficiency)**: 使用更少数据达到可比或更优性能的能力。
    - **表征质量 (Representation Quality)**: 通过消融实验和理论分析，评估分散正则化对防止表征崩塌的效果。

### 二、 对比的基线方法

论文与两大类基线方法进行了全面对比：

1.  **多步生成策略 (Multi-Step Generative Policies)**:
    - **Diffusion Policy (DP)**: 基于扩散模型的经典方法，需要100步（DDPM）或10-16步（DDIM）采样。
    - **ReFlow / ReinFlow**: 基于流匹配的方法，需要20步（ReFlow-R）或4步（ReFlow-S）采样。
    - **DPPO**: 对扩散策略进行PPO微调的方法，需要20步采样。
    - **ShortCut**: 为少步推理设计的流匹配变体，通常需要5步采样。

2.  **一步生成方法 (One-Step Generation Methods)**:
    - **Consistency Policy (CP)**: 通过一致性蒸馏从教师模型获得的一步策略。
    - **1-DP / OneDP**: 通过扩散蒸馏获得的一步策略。
    - **MP1**: 同样基于MeanFlow、但**未使用分散正则化**的一步生成方法，是DMPO最直接的对比对象。

### 三、 关键性能结果与结论

#### 1. 推理效率与速度 (核心优势)
- **惊人的加速比**: 在RTX 4090上，DMPO（1步）的推理延迟仅为 **0.6ms**，控制频率高达 **1770 Hz**。
    - 相比需要20步的ReFlow（8.4ms），加速 **~14倍**。
    - 相比需要5步的ShortCut（2.5ms），加速 **~4倍**。
    - 相比需要100步的Diffusion Policy（391.1ms），加速 **~694倍**。
- **轻量级架构**: DMPO采用轻量ViT+MLP，仅 **1.78M参数**（28MB），而UNet-based方法（如DP, CP）参数量达2.8亿以上。轻量架构贡献了约10倍的加速。
- **真实机器人部署**: 在Franka Panda机器人（RTX 2080）上，DMPO（1步）总延迟 **9.6ms**（>100Hz），而5步ShortCut为19.0ms，20步ReFlow为54.7ms，验证了其实时控制能力。

#### 2. 任务性能：预训练阶段 (Stage 1)
- **打破帕累托前沿**: 如图3所示，DMPO（MF+Disp）占据了“**高成功率-低推理时间**”的帕累托前沿左上角区域。
    - **在Lift和Can任务上达到接近100%的成功率**，与多步基线相当或更优。
    - **在复杂任务（Square, Transport）上显著优于少步基线**（如ShortCut），且仅需1步。
- **少步推理优势**: 如图4所示，MeanFlow变体（MF, MF+Disp）在 **1-5步内** 性能即接近饱和，而ReFlow和ShortCut需要 **32-128步** 才能达到类似性能。
- **分散正则化的关键作用**:
    - **提升成功率**: 在复杂任务上，分散正则化（`α_disp=0.9`）比无正则化的MeanFlow带来 **5-10%** 的绝对成功率提升（例如Transport任务从~34%提升至~40%）。
    - **稳定性能**: 减少了不同随机种子下性能的方差，尤其在复杂任务上效果显著。
    - **与任务复杂度正相关**: 分析显示，最优的 `α_disp` 权重与任务复杂度呈强正相关（Pearson `r=0.924`），为超参数选择提供了指导。

#### 3. 任务性能：强化学习微调阶段 (Stage 2)
- **超越专家演示**: PPO微调使策略突破了纯模仿学习的性能天花板。
    - **在RoboMimic的Can任务上**，微调后达到 **100%** 成功率，超越了所有基线（DPPO 99.3%， ReinFlow-R 99.0%）。
    - **在复杂的Square和Transport任务上**，DMPO微调后分别达到 **83%** 和 **88%** 的成功率，与多步微调基线（DPPO, ReinFlow）性能相当或更优。
- **在Gym和Kitchen任务上的通用性**: DMPO在运动（Hopper, Walker2d等）和长视野厨房任务上，**仅用1步推理**即达到了与多步基线（DPPO-20步， ReinFlow-4步）相当或更优的最终性能，证明了其微调有效性。
- **行为克隆正则化的必要性**: 消融实验表明，没有BC正则化，策略在微调中会迅速崩溃，成功率降至接近零。BC损失确保了稳定的策略提升。

#### 4. 综合对比与物理实验
- **与一步基线的对比** (表1): 在相同简化数据集上，DMPO显著优于其他一步生成方法。
    - 对比**MP1**（无分散正则化）：在Square任务上，DMPO成功率 **83%** vs MP1的 **35%**；在Transport任务上， **88%** vs **38%**。这直接证明了分散正则化的价值。
    - 对比**蒸馏类方法（CP, OneDP）**：DMPO在数据效率上更优（用1/3的数据达到了可比或更优性能），且无需复杂的蒸馏流程。
- **物理机器人验证** (图7): 在真实的Franka Panda机器人上，MP1因表征崩塌导致抓取不精确，在Lift和Can任务上失败。而**DMPO成功完成了全部四个任务**，验证了其优秀的**仿真到现实迁移能力**。

### 四、 核心结论

1.  **效率与性能兼得**: DMPO首次实现了在机器人策略中**同时获得最快推理速度和顶级任务性能**，打破了该领域长期存在的权衡曲线。
2.  **算法组件缺一不可**: 三个核心组件（MeanFlow、分散正则化、RL微调）形成了正向循环，缺少任何一个都会导致系统在效率、稳定性或性能上限方面出现短板。
3.  **理论支撑实践**: 论文不仅提供了实证结果，还从信息论角度**首次证明了分散正则化对于稳定一步生成的必要性**，为方法奠定了理论基础。
4.  **实用价值突出**: 极低的延迟（<1ms）、轻量的模型（<30MB）以及成功的实物部署，表明DMPO是面向**实时、高响应性机器人控制**的实用化解决方案。

**总而言之，论文通过严谨、全面的实验评估，强有力地


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.20701v1)
- [HTML 版本](https://arxiv.org/html/2601.20701v1)
