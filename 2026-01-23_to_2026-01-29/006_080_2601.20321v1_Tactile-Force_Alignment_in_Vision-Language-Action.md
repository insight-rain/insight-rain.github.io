# Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation

**相关性评分**: 8.0/10

**排名**: #6


---


## 基本信息

- **arXiv ID**: [2601.20321v1](https://arxiv.org/abs/2601.20321v1)
- **发布时间**: 2026-01-28T07:34:41Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li, Jiaming Jiang, Chenxi Xiao, Ziyuan Jiao

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune

## 一句话总结

该论文提出了一种通过触觉-力对齐增强VLA模型物理推理能力的框架，用于实现力感知的机器人操作。

## 摘要

Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

## 详细分析

## 论文摘要

**论文标题**：Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation

**作者**：Yuzhe Huang, Pei Lin, Wanlin Li 等

**机构**：北京航空航天大学、上海科技大学、北京通用人工智能研究院等

---

### 1. 研究背景和动机
当前，视觉-语言-动作（VLA）模型在机器人操作任务中展现出强大的通用能力，但其主要依赖视觉模态，**缺乏对接触力等物理交互的直觉感知**，导致在需要精确力控和物理推理的接触密集型任务（如处理易碎物品、精密装配）中表现不佳。现有方法尝试将视觉触觉传感器（VBTS）集成到VLA模型中，但通常仅将触觉数据视为辅助的视觉纹理，**忽略了表面形变与物理力之间的内在关联**。因此，本文旨在解决VLA模型的“力盲”问题，提出从“触觉-视觉对齐”到“触觉-力对齐”的范式转变。

### 2. 核心方法和技术创新
本文提出了 **TaF-VLA** 框架，其核心创新在于通过**触觉-力对齐**，将高维触觉观测显式地锚定在物理交互力上。具体包括：
- **TaF-Device与TaF-Dataset**：设计并构建了一个自动化、低成本的数据采集设备，收集了超过1000万帧**同步的触觉图像、六维力/力矩和矩阵压力图**数据，解决了大规模对齐数据稀缺的问题。
- **TaF-Adapter（触觉-力适配器）**：这是一个核心模块，采用**对比学习**将时序触觉观测与真实的力信号在共享的离散化潜在空间中对齐。它使用因果Transformer编码历史触觉帧，并通过向量量化（VQ-VAE）生成鲁棒的、对噪声不敏感的物理动力学表示，而非静态的视觉纹理。
- **VLA策略集成**：将预训练好的TaF-Adapter集成到预训练的VLA骨干模型（如π0.5）中，使策略能够基于语言指令和力感知的触觉反馈生成动作。

### 3. 主要实验结果
在7个真实的接触密集型操作任务（如镊子取重物、擦白板、切果冻）上进行了广泛评估：
- **性能领先**：TaF-VLA策略的平均成功率达到**64.8%**，显著优于最先进的仅视觉基线（37.1%）和触觉-视觉对齐基线（如FreeTacMan，42.8%），在力关键任务上优势尤其明显（提升最高达33.3%）。
- **卓越的泛化性**：TaF-Adapter展现出强大的**跨传感器零样本泛化能力**。在未见过的触觉传感器上，其性能远优于直接进行力回归的方法，证明了隐式潜在对齐的优越性。
- **提升数据效率**：集成TaF-Adapter后，模型仅需一半的演示数据即可达到与视觉基线相当或更好的性能。
- **消融实验验证**：验证了时序上下文、离散化码本、力信号模态分离等设计选择对性能的关键作用。

### 4. 研究意义和价值
本研究具有重要的理论创新与实际应用价值：
- **范式创新**：首次系统性地提出并实现了“触觉-力对齐”范式，为将物理直觉注入通用机器人策略提供了新思路。
- **技术贡献**：开源了大规模**TaF-Dataset**和模块化的**TaF-Adapter**，为社区提供了宝贵的数据资源和可复用的力感知表示学习模块。
- **应用推动**：显著提升了VLA模型在**力敏感、接触密集型操作任务**中的鲁棒性和成功率，使机器人能够执行更精细的、语言指导的力调节操作（如“轻轻抓握”），向更灵巧、更通用的机器人操控迈出了关键一步。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
当前主流的**视觉-语言-动作模型** 在机器人操作中表现出强大的通用性，但其**主要依赖视觉模态**，导致模型存在 **“力盲”** 问题。它们缺乏对物理交互力的直接感知和推理能力，因此在需要**精确力调节和物理推理的接触密集型任务**（如处理易碎物品、精密装配、操作可变形物体）中表现不佳。

### **核心创新点**
论文提出了一个**范式转变**：从传统的 **“触觉-视觉对齐”** 转向 **“触觉-力对齐”**。其核心是让模型学习到的触觉表征直接**锚定在物理交互力**上，而非仅仅是视觉纹理。

具体创新体现在以下三个层面，共同构成了 **TaF-VLA** 框架：

1.  **数据与硬件创新：自动化触觉-力对齐数据采集系统**
    *   **问题**：缺乏大规模、同步的触觉图像与真实力信号配对数据。
    *   **解决方案**：设计并搭建了**TaF-Device**，一个低成本、自动化的数据采集平台。
    *   **关键设计**：采用**双平台并联驱动结构**，确保触觉传感器和力传感器受到**完全同步、相同大小和方向的力**。可快速更换压头以模拟多样化的接触。
    *   **产出**：构建了**TaF-Dataset**，包含超过1000万帧同步的触觉图像、6维力/力矩和矩阵压力图数据。

2.  **表征学习创新：触觉-力适配器**
    *   **问题**：如何将高维、几何的触觉图像与低维、动态的力信号在表征层面进行对齐，并保证对噪声和传感器差异的鲁棒性。
    *   **解决方案**：提出了 **TaF-Adapter** 模块，其核心是一个**对比学习框架**。
    *   **关键设计**：
        *   **力信号量化**：使用两个**VQ-VAE** 分别将时序的6维力/力矩和压力图**离散化**为码本中的令牌。离散化能抑制噪声，并形成稳定的“物理锚点”。
        *   **时序触觉编码**：使用**因果Transformer** 编码一段历史触觉图像序列，以捕捉**历史依赖的动态**（如滑移初期的演变）。
        *   **对比对齐**：通过**InfoNCE损失**，在共享的潜空间内拉近同步的触觉表征与对应的量化力令牌，推远非对应的配对。这使得触觉编码器学会了从视觉形变中**推断力动态**。

3.  **策略集成创新：力感知的VLA策略**
    *   **问题**：如何将上述力对齐的触觉表征有效地集成到现有的通用VLA骨干模型中，而不引起模态崩溃或信号稀释。
    *   **解决方案**：将**预训练并冻结的TaF-Adapter** 集成到**VLA骨干模型** 中。
    *   **关键设计**：将TaF-Adapter输出的**力对齐的触觉令牌**，与视觉、语言令牌**交错输入**给动作预测器。这使得策略生成的动作能同时**基于高级语义指令和本地的力接触反馈**。

### **解决方案总结**
论文通过 **“数据采集 -> 表征对齐 -> 策略集成”** 三步走，系统性地解决了VLA模型的力盲问题：
1.  **制造数据**：用自动化设备解决数据稀缺问题。
2.  **学习物理**：用对比学习和离散量化，让触觉表征“理解”力。
3.  **赋能模型**：将物理直觉作为模块注入通用模型，使其具备力感知的闭环控制能力。

**实际价值**：在7个真实的接触密集型操作任务上，TaF-VLA策略平均成功率**大幅超越** 现有的仅视觉基线及触觉-视觉对齐基线（平均提升约22%），特别是在需要精确力感知和调节的任务上优势显著。这证明了**显式的触觉-力对齐**是让通用机器人策略掌握复杂物理交互的关键。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决当前视觉-语言-动作（VLA）模型在机器人操作中因缺乏物理力觉感知而无法胜任接触密集型任务（如精密装配、易碎物操作）的核心问题。为此，论文提出了一个范式转变：从“触觉-视觉对齐”转向“触觉-力对齐”，并构建了**TaF-VLA**框架。该框架的核心是**TaF-Adapter**模块，它通过对比学习，将时序触觉图像序列与真实的六维力/力矩及压力分布图在离散化的共享隐空间中对齐，从而学习到对噪声鲁棒、能捕捉动态接触物理的表示。最终，将此模块集成到VLA主干网络中形成的策略，在七项真实世界接触密集型任务上的平均成功率达到了64.8%，显著优于仅视觉或传统触觉-视觉对齐的基线方法，验证了其通过跨模态物理推理实现力觉感知操作的强大能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文核心创新点分析

这篇论文《Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation》针对现有视觉-语言-动作模型在接触密集型操作任务中“力盲”的根本缺陷，提出了一套从数据、表征到策略的完整创新范式。其核心创新点可归纳为以下三条：

---

### 1. **范式创新：从“触觉-视觉对齐”转向“触觉-力对齐”**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：将基于视觉的触觉传感器数据视为辅助的视觉纹理，通过标准的视觉变换器将其与场景图像和语言进行对齐（即“触觉-视觉对齐”）。这种方法将触觉数据当作“另一种视觉”处理。
     - **本文方法**：提出**触觉-力对齐**，核心是让高维触觉观测的表征在潜在空间中与真实的物理相互作用力（6维力/力矩和矩阵压力图）对齐。这标志着从“外观对齐”到“物理动态对齐”的范式转变。
   - **解决的具体问题/带来的优势**：
     - **解决“力盲”问题**：传统VLA模型和触觉-视觉对齐方法无法感知和推理接触力，导致在需要精确力调节的任务（如处理易碎物、精密装配）中失败。本方法通过将触觉表征锚定在物理力上，使策略获得了对接触动力学的真实理解。
     - **实现物理直觉**：模型学习到的表征捕获的是与历史相关、对噪声不敏感的物理动态（如滑移、粘滑过渡），而非静态的视觉纹理，从而能进行跨模态的物理推理。

### 2. **技术创新：Tactile-Force Adapter 模块**
   - **相比以往方法的改进/不同之处**：
     - **表征学习方式**：采用**对比学习**在共享潜在空间中对齐触觉序列与力信号，而非显式地回归力值。同时，使用**向量量化**为力和压力信号分别构建离散的码本。
     - **时序建模**：采用**因果Transformer**处理连续的触觉图像序列，以捕获力相互作用的历史依赖性（如粘弹性滞后、初始滑移）。
     - **模态解耦**：为空间压力图（高维、空间信号）和全局6维力/力矩（低维、动态信号）使用**两个独立的码本**，避免模态信息混淆。
   - **解决的具体问题/带来的优势**：
     - **提升跨传感器泛化能力**：实验证明，相比显式力回归模型（对传感器域偏移敏感），这种隐式、离散化的对齐方式能更好地泛化到未见过的新触觉传感器上，解决了实际部署中的硬件差异问题。
     - **增强鲁棒性**：离散码本和时序聚合能有效抑制传感器高频噪声和硬件差异带来的干扰，学习到更稳定、更具物理意义的表征。
     - **捕获动态过程**：解决了静态触觉帧无法准确表征动态力交互的问题，使模型能区分稳定抓握和正在发生的滑移。

### 3. **系统与数据创新：自动化触觉-力数据采集设备与大规模数据集**
   - **相比以往方法的改进/不同之处**：
     - **硬件设计**：开发了**低成本、自动化、可扩展**的数据采集设备。其核心是采用**并联驱动结构**和**模块化设计**，能对触觉传感器和力传感器施加完全同步、相同大小和方向的力。
     - **数据规模与多样性**：构建了**TaF-Dataset**，包含超过**1000万帧**同步的触觉观测、6维力/力矩和矩阵压力图数据。通过更换不同图案/曲率/硬度的压头、随机化接触压力和剪切力，极大丰富了接触交互的多样性。
   - **解决的具体问题/带来的优势**：
     - **解决了触觉-力对齐数据稀缺的瓶颈**：以往数据集缺乏大规模、高精度的同步触觉-力数据对，制约了相关模型的发展。本工作提供了获取此类数据的高效（10万帧/小时）解决方案。
     - **为社区提供宝贵资源**：该数据集和开源硬件设计降低了触觉感知研究门槛，促进了可重复研究和模型比较。
     - **支持模型预训练与泛化**：大规模、多样化的数据是TaF-Adapter能够学习到通用、鲁棒表征的基础，直接支撑了后续策略的优异表现。

---

### **总结：创新带来的整体优势**
通过上述三点创新的协同作用，论文最终提出的 **TaF-VLA 策略** 在7个接触密集型操作任务上实现了平均 **64.8%** 的成功率，相比最好的触觉-视觉对齐基线提升了约22%，相比纯视觉基线提升更为显著。其核心优势体现在：
- **力感知操控**：能够执行需要精确力调节和物理推理的任务（如用镊子夹取重物、从卡槽中取出充电宝）。
- **即插即用与数据高效**：预训练的TaF-Adapter可以轻松集成到不同的策略骨干网络中，并能用更少的演示数据达到与视觉基线相当或更好的性能。
- **语言控力**：结合力感知的语言指令，模型能够理解并执行如“轻轻抓握”等细粒度的力调节命令，这是纯视觉系统难以实现的。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心实验效果
论文提出的 **TaF-VLA** 框架在**接触密集、需要精确力感知与调节的机器人操作任务**上，显著超越了现有的视觉-语言-动作模型和仅视觉的基线方法。其核心效果是**解决了现有VLA模型的“力盲”问题**，使机器人能够通过触觉-力对齐实现物理感知和推理。

### 二、 使用的数据集
1.  **TaF-Dataset (自建)**：
    - **目的**：用于训练**Tactile-Force Adapter**，实现触觉观测与物理力的对齐。
    - **内容**：包含超过 **1000万** 个同步数据帧，每帧包括：
        - 视觉触觉传感器图像
        - 6轴力/力矩测量值
        - 矩阵式压力分布图
    - **采集设备**：使用自研的自动化触觉-力数据采集设备，可高效（10万帧/小时）生成对齐数据。

2.  **力感知操作数据集 (自建)**：
    - **目的**：用于微调和评估 **TaF-VLA 策略**。
    - **内容**：包含超过 **10,000** 个真实世界操作演示片段，涵盖20多种场景（如处理易碎物品、使用工具等）。
    - **特色**：每个演示都配有**力感知语言指令**（如“轻轻抓握”），这些指令通过LLM提示框架自动生成。

### 三、 评价指标
- **主要指标**：**任务成功率**。
- **评估方式**：每个模型在每项任务上进行 **15次** 试验，成功率计算为成功尝试的比例。
- **成功标准**：根据具体任务定义（例如，完全插入、无滑落运输、无损坏等），为二元完成条件。

### 四、 对比的基线方法
论文与 **5类** 开源基线方法进行了全面对比，代表了当前最先进的技术：

| 基线类别 | 具体方法 | 核心特点 |
| :--- | :--- | :--- |
| **仅视觉策略** | 1. **ACT** | 基于Transformer的动作分块策略，仅使用RGB图像和本体感知。 |
| | 2. **Diffusion Policy (DP)** | 基于去噪扩散的生成式行为克隆策略，仅使用视觉。 |
| | 3. **π₀.₅** | 最先进的VLA基础模型，在本文数据集上仅使用视觉观察进行微调。 |
| **触觉-视觉对齐策略** | 4. **FreeTacMan** | 代表性基线，将触觉反馈作为辅助视觉令牌处理，缺乏物理力对齐。 |
| **TaF-Adapter插件测试** | 5. **ACT + TaF-Adapter** | 验证将本文的力对齐模块植入标准策略的效果。 |
| | 6. **DP + TaF-Adapter** | 验证TaF-Adapter在生成式框架中的泛化能力。 |

### 五、 关键性能提升与结论
实验在 **8项** 接触密集的真实世界操作任务上进行（如管件插入、镊子取重物、擦白板、切果冻等）。

#### 1. 总体性能领先 (**E1**)
- **TaF-VLA 取得了最高的平均成功率：64.8%。**
- 相比最强的仅视觉基线 (**π₀.₅**, 37.1%)，**绝对提升高达 27.7个百分点**。
- 相比触觉-视觉对齐基线 (**FreeTacMan**, 42.8%)，**绝对提升 22个百分点**。

#### 2. 任务依赖性分析
- **几何约束主导的任务**（如`Tube Insertion`）：视觉基线表现尚可，触觉带来**边际增益**。
- **力调节关键的任务**（如`Tweezer Weight Pick`, `Power Bank Extraction`）：视觉基线**完全失败或表现极差**，而TaF-VLA凭借力感知实现**显著成功**（提升最高达33.3个百分点）。这验证了力感知在接触密集操作中的必要性。

#### 3. 模块有效性与泛化性 (**E2, E4**)
- **“即插即用”有效性**：将TaF-Adapter植入ACT和DP后，其性能**均显著超过原版及FreeTacMan**，证明该模块是通用的力感知注入器。
- **跨传感器鲁棒性**：TaF-Adapter的**隐式潜在空间对齐方法**，在“未见过的”传感器上**零样本泛化性能远优于**显式力回归方法。例如，在未训练过的传感器上，TaF-Adapter平均成功率（60.3%）是显式力回归方法（30.0%）的两倍。
- **消融实验关键结论**：
    - **时序上下文必要**：使用历史观测（N=5）比单帧（N=1）性能大幅提升。
    - **离散量化关键**：向量量化码本能抑制噪声，学习稳健的物理动态表示。
    - **模态解耦有效**：为压力分布图和6维力矢量使用**独立码本**优于共享码本，保留了不同力信号的独特分布。

#### 4. 数据效率提升 (**E3**)
- 在训练数据量有限（100个演示）时，**TaF-VLA能达到仅视觉基线使用双倍数据（200个演示）时的性能**。
- 这表明力对齐的触觉表示提供了**有价值的物理先验**，大幅提升了策略的样本效率。

### 六、 总结
论文通过系统的实验评估，**定量且令人信服地**证明了其核心创新——**从触觉-视觉对齐转向触觉-力对齐的范式转变**——的有效性。TaF-VLA不仅在多项接触密集任务上实现了**最先进的性能**，其核心模块TaF-Adapter还展现出**卓越的泛化能力和数据效率**，为解决机器人操作中的物理感知难题提供了强有力的新方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.20321v1)
- [HTML 版本](https://arxiv.org/html/2601.20321v1)
