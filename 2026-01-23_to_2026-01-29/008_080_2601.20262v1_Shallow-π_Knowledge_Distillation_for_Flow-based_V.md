# Shallow-π: Knowledge Distillation for Flow-based VLAs

**相关性评分**: 8.0/10

**排名**: #8


---


## 基本信息

- **arXiv ID**: [2601.20262v1](https://arxiv.org/abs/2601.20262v1)
- **发布时间**: 2026-01-28T05:16:26Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Boseong Jeon, Yunho Choi, Taehan Kim

## 关键词

Vision-Language-Action Model, VLA for Robotics, Flow Matching, Inference Acceleration, Edge Deployment, fine tune

## 一句话总结

Shallow-π提出一种知识蒸馏框架，通过减少Transformer层数来加速基于流的VLA模型推理，适用于机器人实时部署。

## 摘要

The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

## 详细分析

## 论文摘要：Shallow-π: 基于流匹配的视觉-语言-动作模型的知识蒸馏

### 1. 研究背景和动机
随着机器人实时部署需求的增长，对视觉-语言-动作模型进行快速、端侧推理的需求日益迫切。现有研究多集中于**视觉令牌剪枝**等令牌级效率优化，而对**系统性地减少Transformer层数**关注有限，尤其是在**流匹配（flow-based）VLA模型**中。这类模型（如π系列）通常结合大型视觉-语言模型主干和基于扩散的动作头，计算成本高昂，难以在边缘设备上实时运行。因此，本文旨在开发一种高效的知识蒸馏框架，以大幅压缩模型深度，实现实时推理。

### 2. 核心方法和技术创新
本文提出了 **Shallow-π**，一个针对π类流匹配VLA模型的知识蒸馏框架。其核心创新在于：
- **联合深度压缩**：**同时**压缩VLM主干和动作头的Transformer深度（从18层减至6层），而非仅压缩主干或动态跳层。
- **定制化蒸馏目标**：设计了三种互补的损失函数：
    1. **任务损失**：基于流匹配的监督信号。
    2. **知识蒸馏损失**：让学生模型模仿教师模型的输出。
    3. **注意力蒸馏损失**：**创新性地**在中间层对齐动作令牌与视觉-语言令牌之间的**交叉注意力分布**，这是针对VLA架构中仅动作令牌参与去噪过程的关键设计。
- **简单有效的初始化**：采用均匀子采样策略初始化学生模型，无需复杂的层选择机制。

### 3. 主要实验结果
- **仿真基准测试**：在LIBERO基准上，仅用6层的学生模型实现了超过**2倍的推理加速**，成功率绝对下降**小于1%**，在压缩VLA模型中达到SOTA性能。
- **真实世界部署**：在Jetson Orin和Jetson Thor等边缘设备上，于复杂动态操作任务（如移动孔中插桩、垃圾分类）中验证。Shallow-π实现了接近**10 Hz的端到端推理频率**，性能优于教师模型和从头训练的小型模型（如SmolVLA），主要得益于**延迟降低**带来的更频繁的观测更新。
- **泛化能力**：在未见过的空间扰动场景下，Shallow-π展现出比教师模型更好的鲁棒性和泛化能力。

### 4. 研究意义和价值
本研究首次系统性地探索并验证了通过知识蒸馏**联合压缩**流匹配VLA模型主干与动作头深度的有效性。Shallow-π提供了一种**原理清晰、实现简单**的高效模型压缩方案，避免了动态跳层方法的复杂启发式规则和内存驻留问题。其成功在边缘设备上的部署，证明了该框架对于推动**高性能、低延迟的通用机器人**走向实际应用具有重要的**实际价值**，为VLA模型的实时化部署提供了新的有效途径。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Shallow-π

### **一、 拟解决的核心问题**
论文旨在解决**基于流的视觉-语言-动作模型在边缘设备上实时部署的难题**。具体问题包括：
- **计算成本高昂**：流式VLA模型结合了大型视觉-语言模型主干和基于扩散的动作头，推理时需要迭代去噪，导致延迟高，难以在Jetson等边缘设备上实现实时（如10Hz）推理。
- **现有效率优化方法的局限性**：
    - **视觉令牌剪枝**：在现代GPU上并行效率高，对端到端延迟的改善有限。
    - **动态层跳过**：需要完整模型驻留内存，无法实现真正的结构压缩；且现有方法主要针对主干网络，未考虑流式VLA中**动作头与主干深度镜像**以接收每层条件信息的特殊架构（π-like架构）。
    - **使用小型主干**：通常需要从头训练，兼容性差，且未压缩动作头，而动作头在扩散步骤中反复调用，是计算瓶颈。

### **二、 核心创新点**
论文提出了 **Shallow-π**，一个**系统的知识蒸馏框架**，其创新性体现在：

1. **联合深度压缩**：首次为π-like流式VLA模型，**同时、激进地压缩视觉-语言主干和基于扩散的动作头的Transformer层深度**（从18层压缩至6层），而非仅压缩其一。
2. **针对性的蒸馏目标设计**：设计了专门适配流式VLA架构的三重损失函数：
    - **任务损失**：标准的流匹配损失，确保学生模型预测真实动作速度。
    - **知识蒸馏损失**：让学生模型模仿教师模型的输出速度。
    - **新颖的注意力蒸馏损失**：**关键创新**。仅在中间层，蒸馏**动作令牌到视觉-语言令牌的交叉注意力分布**。这精准对齐了生成策略所依赖的条件信息交互，避免了蒸馏所有令牌注意力带来的干扰和不稳定。
3. **实证驱动的深度有效性验证**：
    - 通过实验证明，在并行化程度高的现代硬件上，**减少层数比减少令牌数对降低延迟更有效**。
    - 论证了基于特征相似性的动态层跳过方法对于流式VLA模型效果不佳，因为层间相似性随扩散噪声水平变化且不能反映功能重要性，从而**论证了知识蒸馏的必要性**。

### **三、 解决方案概述**
1. **方法流程**：
    - **学生模型初始化**：从教师模型中均匀采样层来初始化浅层学生模型（借鉴TinyBERT）。
    - **多目标蒸馏训练**：使用上述三重损失组合训练学生模型。
    - **部署**：将训练好的浅层模型直接部署到边缘设备（Jetson Orin/Thor）。

2. **技术关键**：
    - **架构感知**：方案完全尊重π-like架构中“动作头每层都需接收主干条件特征”的设计，通过蒸馏保持这种层间特征传递的保真度。
    - **效率与性能平衡**：通过蒸馏保留了教师模型强大的表征和生成能力，避免了从头训练小模型导致的性能大幅下降。

### **四、 实际价值与技术贡献**
- **性能**：在LIBERO仿真基准上，仅用约1%的绝对成功率下降，换取了**超过2倍的推理加速**。在真实世界复杂动态操作任务中，成功率和鲁棒性甚至超过教师模型（得益于更低延迟）。
- **效率**：在Jetson Orin上实现了接近10Hz的端到端推理，满足实时机器人控制需求。
- **实用性**：方案**无需复杂的运行时层跳过逻辑或图级优化**，简化了部署。模型被压缩后可直接运行，内存占用更低。
- **泛化性**：蒸馏后的模型在未见过的环境扰动下表现出良好的泛化能力，未出现过拟合。

**总结**：Shallow-π的核心创新在于提出了一种**针对流式VLA模型架构特点的、联合深度压缩的知识蒸馏框架**。它通过精心设计的蒸馏目标，在实现激进模型瘦身（70%层数减少）的同时，最大限度地保留了性能，从而**切实解决了此类模型在资源受限边缘设备上实时部署的瓶颈问题**，并经过了工业级真实机器人实验的验证。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决基于流的视觉-语言-动作（VLA）模型因计算量大而难以在边缘设备上实现实时部署的问题。为此，作者提出了 **Shallow-π**，一个系统的知识蒸馏框架，其核心创新在于**联合压缩** VLM主干网络和基于扩散的动作头的Transformer层深度（从18层减至6层），并设计了包含任务损失、知识蒸馏损失和**针对VLA结构定制的中间层注意力蒸馏损失**在内的复合训练目标。该方法在保持模型性能的同时，显著提升了推理速度，在标准仿真基准测试中实现了**超过2倍的加速且成功率下降小于1%**，并在Jetson Orin等边缘设备上的复杂动态真实机器人任务中验证了其高效性和鲁棒性，证明了通过蒸馏进行深度压缩比动态跳层或训练小型模型从头开始更为有效。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Shallow-π: Knowledge Distillation for Flow-based VLAs》的创新点分析

这篇论文针对基于流的视觉-语言-动作（VLA）模型，提出了一种系统性的知识蒸馏框架，以实现高效的模型压缩和实时部署。其核心创新点如下：

---

### 1. **首次提出针对流式VLA模型的联合深度压缩框架**
   - **改进/不同之处**：
     - 以往工作主要关注**单一组件**的压缩，例如仅减少视觉语言模型（VLM）主干的深度（如MoLE-VLA），或仅进行视觉令牌剪枝。
     - 本文**同时压缩VLM主干和基于扩散的动作头**，将总层数从18层减少到6层（减少约70%）。
   - **解决的问题/优势**：
     - 解决了流式VLA模型（如π系列）中，动作头需要逐层接收VLM特征注入的架构特点。仅压缩主干会导致特征传递不匹配。
     - 实现了**端到端的深度压缩**，在保持性能的同时显著降低计算延迟。

### 2. **设计了专门针对流式VLA架构的蒸馏目标函数**
   - **改进/不同之处**：
     - 传统蒸馏方法（如DistilBERT）通常对所有令牌进行注意力对齐，但流式VLA中只有动作令牌参与去噪生成。
     - 本文提出**三重损失组合**：
       1. **任务损失**：基于流匹配的监督。
       2. **知识蒸馏损失**：对齐学生与教师的输出速度。
       3. **注意力蒸馏损失**：**仅对齐动作令牌到视觉语言令牌的交叉注意力**（而非全部令牌）。
   - **解决的问题/优势**：
     - 避免了在非生成令牌（视觉/语言）上过度约束学生模型，防止训练不稳定。
     - 实验表明，全令牌注意力蒸馏会导致性能崩溃（成功率接近0），而本文方法稳定且有效。

### 3. **系统性地论证了层减少比令牌减少对延迟优化更有效**
   - **改进/不同之处**：
     - 现有研究多关注视觉令牌剪枝，但本文通过实验证明，在现代GPU上，**减少Transformer层数比减少令牌数对降低延迟更有效**。
     - 原因：令牌计算高度并行化，而Transformer层是顺序执行的，其延迟直接累积。
   - **解决的问题/优势**：
     - 为实时机器人部署提供了更优的压缩方向：优先减少模型深度，而非单纯减少令牌。
     - 在Jetson Orin等边缘设备上，层减少可实现近10 Hz的端到端推理速度。

### 4. **揭示了动态层跳过方法在流式VLA中的局限性，并证明蒸馏的优越性**
   - **改进/不同之处**：
     - 以往层跳过方法（如EfficientVLA）基于特征相似度或路由机制，**动态**跳过某些层，但需要完整模型驻留内存。
     - 本文通过实验发现：
       - 特征相似度随扩散噪声水平变化，固定阈值不可靠。
       - 相似度不能反映层功能重要性（高相似度的层可能至关重要）。
   - **解决的问题/优势**：
     - 证明了动态跳层在流式VLA中效果有限（跳过超过3层性能急剧下降）。
     - **知识蒸馏通过训练直接学习紧凑结构**，无需复杂启发式规则，实现了更激进且稳定的压缩。

### 5. **在复杂动态任务中进行了大规模真实世界验证**
   - **改进/不同之处**：
     - 以往效率研究多在仿真环境或简单静态任务中评估，且多在服务器GPU（如RTX 4090）上进行。
     - 本文在**Jetson Orin/Thor边缘设备**上，针对**人形机器人、双手协调、动态场景**等复杂任务进行了验证。
   - **解决的问题/优势**：
     - 证明了方法在**实际机器人部署中的有效性和鲁棒性**。
     - 低延迟（~110 ms）减少了开环执行时间，提高了在动态环境中的操作精度和成功率。

### 6. **蒸馏模型优于从头训练的小型模型**
   - **改进/不同之处**：
     - 对比小型主干模型（如SmolVLA）需要从头训练所有参数，且性能受限。
     - Shallow-π通过蒸馏**保留了教师模型的知识**，在相同容量下性能更好。
   - **解决的问题/优势**：
     - 避免了从头训练大型VLM主干的成本，兼容预训练模型。
     - 在LIBERO基准上，Shallow-π（L6）在成功率接近教师的同时，FLOPs和延迟降低超过2倍。

---

## 总结
本文的核心创新在于**针对流式VLA架构特点，设计了一套联合压缩与蒸馏框架**，解决了以往方法在架构适配性、延迟优化和实际部署方面的不足。通过**深度压缩、专用蒸馏损失、真实世界验证**，实现了在边缘设备上高性能、低延迟的机器人策略执行。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 主要实验效果总结
论文提出的 **Shallow-π** 知识蒸馏框架，通过将基于流的VLA模型的Transformer层数从18层压缩至6层，在**保持高性能**的同时，实现了**显著的推理加速**。核心效果如下：
- **性能保持**：在标准操作基准测试中，成功率绝对下降小于 **1%**。
- **推理加速**：实现了超过 **2倍** 的端到端推理加速。
- **实际部署**：在Jetson Orin和Jetson Thor等边缘设备上，实现了接近 **10 Hz** 的实时推理频率，并在复杂、动态的真实世界操作任务中验证了其鲁棒性。

### 二、 使用的数据集与评价指标

#### 1. 仿真基准测试
- **数据集**：**LIBERO** 基准测试套件。这是一个用于终身机器人学习的知识迁移基准，包含多种操作任务。
- **评价指标**：
    - **成功率**：任务执行的成功率（%）。论文报告了LIBERO中Spatial、Object、Goal和Long（10）四个子任务集的平均成功率。
    - **计算效率**：
        - **FLOPs**：浮点运算次数（T，万亿次）。
        - **CUDA推理时间**：在指定硬件（如H100 GPU）上的单次推理延迟（毫秒）。

#### 2. 真实世界实验
- **机器人平台**：
    - **ALOHA**：双臂移动操作机器人。
    - **RB-Y1**：具有手部和躯干协调能力的人形机器人平台。
- **任务场景**：设计了7个复杂、动态的操作任务（见图6），例如：
    - `Peg in hole`（动态孔洞插桩）
    - `Insert foam`（向移动书架放置泡沫块）
    - `Recycle`（垃圾分类与投掷）
    - `Open lid & peg cylinder`（开盖并插桩）
- **评价指标**：
    - **成功率**：在真实环境中执行任务的**成功次数/总尝试次数**。
    - **端到端计算时间**：在边缘设备（Jetson Orin/Thor）上完成一次模型推理的延迟（毫秒）。
    - **泛化能力**：在未见过的物体配置或环境扰动下的成功率。

### 三、 对比的基线方法
论文将Shallow-π与以下几类基线方法进行了全面对比：

| 对比组别 | 代表模型 | 核心特点 |
| :--- | :--- | :--- |
| **原始模型** | `π₀`, `π₀.₅` | 未压缩的、基于流的VLA教师模型。 |
| **Token压缩** | `CogVLA`, `LightVLA` | 通过剪枝或缓存减少视觉Token数量（与层压缩正交）。 |
| **小型骨干网络** | `SmolVLA` | 使用更小的VLM骨干网络，从头开始训练。 |
| **层跳过/动态推理** | `DeeR-VLA`, `EfficientVLA`, `MoLE-VLA` | 通过特征相似性或路由机制，在推理时动态跳过某些层。 |

### 四、 关键性能提升与结论

#### 1. 仿真实验结果（LIBERO基准）
- **性能保持**：蒸馏得到的6层学生模型（`π₀.₅-L6`）平均成功率为 **95%**，仅比其18层教师模型（`π₀.₅`，96%）下降 **1%**。
- **效率大幅提升**：
    - **FLOPs**：从教师的3.39 T降至1.30 T，减少约 **62%**。
    - **CUDA时间**：从25.5 ms降至11.3 ms，加速约 **2.26倍**。
- **对比优势**：
    - **vs. Token压缩**：在达到相近成功率（~95%）时，Shallow-π的推理延迟（11.3 ms）显著低于CogVLA（31.0 ms）和LightVLA（22.0 ms）。
    - **vs. 小型骨干网络**：Shallow-π（95%）的性能远优于从头训练的SmolVLA（87%），且推理更快（11.3 ms vs. 26.0 ms）。
    - **vs. 层跳过方法**：Shallow-π通过**结构性地移除**冗余层，实现了更彻底的压缩和更稳定的性能，避免了动态跳过方法需要驻留完整模型、难以批处理优化等问题。

#### 2. 真实世界实验结果
- **全面超越基线**：在ALOHA和RB-Y1平台的所有7个动态、复杂任务中，6层的Shallow-π模型在**成功率上均达到或超过了其教师模型和SmolVLA**（详见表3）。
- **低延迟是关键**：
    - 在Jetson Orin上，Shallow-π的端到端推理时间仅为 **110 ms**，比教师模型（364 ms）快 **2.3倍以上**。
    - 更低的延迟意味着**更短的开环执行时间**和**更频繁的观测更新**。如图10所示，延迟减少200 ms可避免约2厘米的末端执行器开环位移误差，这在精密操作（如动态插桩）中至关重要。
- **泛化能力验证**：在初始位置扰动、垃圾桶位置移动等**未见过的场景**下，Shallow-π凭借更快的推理速度，能更及时地根据新观测调整动作，表现出比教师模型更好的**鲁棒性和泛化能力**（见图11）。

### 五、 核心结论
1.  **有效性**：知识蒸馏是压缩基于流的VLA模型（其所有层都注入多模态特征）深度的**强有力方法**，能够实现**激进（70%）的层数削减**而性能损失极小。
2.  **效率优势**：与减少视觉Token相比，**减少Transformer层数对降低实际推理延迟（wall-clock time）更为有效**，因为层是顺序执行的，而Token计算在GPU上高度并行化。
3.  **实用价值**：Shallow-π成功部署于资源受限的边缘计算平台，在**复杂、动态的真实机器人任务中实现了实时（~10 Hz）高性能推理**，证明了其在实际机器人应用中的可行性和优越性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.20262v1)
- [HTML 版本](https://arxiv.org/html/2601.20262v1)
