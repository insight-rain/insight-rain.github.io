# Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies

**相关性评分**: 7.0/10

**排名**: #10


---


## 基本信息

- **arXiv ID**: [2601.21251v1](https://arxiv.org/abs/2601.21251v1)
- **发布时间**: 2026-01-29T04:17:56Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh

## 关键词

Diffusion, Inference Acceleration, fine tune, world model

## 一句话总结

该论文提出了一种基于扩散模型的混合专家策略（SMP），通过紧凑正交技能基础和自适应专家激活，实现机器人多任务操作的高效推理和快速适应。

## 摘要

Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.

## 详细分析

## 论文摘要：基于专家混合扩散策略的机器人操作技能抽象

### 1. 研究背景和动机
近年来，基于扩散模型的策略在机器人操作任务中取得了显著成功，但其扩展到多任务场景时面临挑战：单纯扩大模型规模会带来高昂的推理成本和数据需求，而现有的专家混合（MoE）方法又未能显式地解耦和表示可重用的操作技能，限制了其可解释性和迁移性。因此，如何在保持模型紧凑和推理高效的前提下，实现有效的多任务泛化，是机器人学中的一个关键开放性问题。

### 2. 核心方法和技术创新
本文提出了**技能专家混合策略（SMP）**，一个创新的扩散式专家混合策略框架。其核心技术创新包括：
- **状态自适应正交技能基**：通过QR分解构建一个随机器人状态（如位姿、接触）平滑变化的正交基，将动作空间分解为局部解耦的技能方向，确保每个专家的贡献正交且可识别。
- **粘性路由机制**：引入具有“粘性”的狄利克雷-马尔可夫先验来建模专家门控权重，鼓励技能激活在时间上保持稳定（相位一致性），减少高频切换，从而产生更稳定、可解释的行为。
- **自适应专家激活**：在推理时，根据路由器预测的权重，动态选择一小部分最相关的专家（如通过Top-k或覆盖率阈值），仅对这部分专家的系数进行去噪，显著降低了计算开销。
- **变分训练目标**：设计了一个结合了**重构损失**（在正交基中）、**门控正则化损失**（粘性先验）和**系数正则化损失**（扩散模型）的变分下界，并蒸馏出一个**仅依赖状态的路由器**，以支持高效推理。

### 3. 主要实验结果
研究在模拟环境（RoboTwin-2, RLBench-2）和真实双臂机器人平台上进行了多任务学习和迁移学习评估：
- **性能优越**：在多个双臂操作任务中，SMP取得了比大型扩散基线（如DP、RDT）和现有MoE方法（如Sparse DP）更高的平均成功率。
- **效率显著**：通过自适应激活，SMP在推理时平均仅激活约30%的参数，推理时间（~107ms）远低于大型基线模型，实现了性能与效率的更好权衡。
- **技能可重用**：学习到的技能基自然地对应了空间（左/右臂）和时间（抓取、移动、放置阶段）上的专业化分工。在迁移学习中，SMP能够通过少量样本微调路由器或整个策略，有效地将已学技能重组或适配到新任务上。

### 4. 研究意义和价值
本工作为可扩展、可迁移的多任务机器人操作提供了一条实用路径：**学习一次可重用的技能，在需要时仅激活必要的部分，并在任务变化时快速适应**。SMP不仅通过其**正交技能基和粘性路由**提升了策略的**可解释性**和**行为稳定性**，更通过**自适应专家激活**机制在保持高性能的同时大幅降低了**计算成本**，使其更适用于对实时性要求高的真实机器人控制场景。这项研究推动了高效技能抽象方法的发展，并为构建更通用、更高效的机器人策略奠定了基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文标题**
《通过专家混合扩散策略抽象机器人操作技能》

### **核心问题**
扩散策略在单任务机器人操作中表现出色，但扩展到**多任务场景**时面临两大挑战：
1.  **模型规模与数据成本高昂**：依赖扩大模型规模（如RDT）会导致推理速度慢，不适用于实时控制，且所需演示数据量随任务多样性呈指数增长。
2.  **技能可重用性与可解释性差**：现有的专家混合（MoE）方法缺乏对可重用操作技能的显式解耦和表示，导致技能纠缠、路由不稳定，限制了策略的可解释性和跨任务迁移能力。

### **核心创新点：Skill Mixture-of-Experts Policy (SMP)**
SMP是一个**基于扩散模型的专家混合策略框架**，其核心创新在于通过**状态自适应的正交技能基**与**粘性路由**，显式地学习和组合可重用的操作技能。

#### **1. 技术创新**
- **状态自适应的正交技能基**：
    - **问题**：直接混合无约束的专家输出会导致重叠和不可识别性。
    - **解决方案**：为每个状态 `s` 学习一个正交矩阵 `B(s)`，其列向量构成一组**局部解耦的技能方向**。每个技能对应一个唯一的、非重叠的动作空间方向。
    - **实现**：通过一个轻量级网络生成无约束矩阵 `W(s)`，然后通过**带符号稳定的QR收缩**投影到Stiefel流形，得到正交基 `B(s)`。这确保了技能贡献的正交性和可加性。

- **粘性路由与门控机制**：
    - **问题**：标准路由可能在每一步频繁切换，导致行为不稳定、振荡。
    - **解决方案**：引入**粘性Dirichlet-Markov动力学**作为门控 `g_t` 的先验。当前门控值 `g_t` 强烈依赖于前一步的值 `g_{t-1}`，并受到一个全局使用向量 `ϑ` 的温和牵引。
    - **效果**：这促使技能激活在时间上呈现**分段恒定**的模式，与机器人操作的“阶段”（如抓取、移动、放置）自然对齐，提高了行为的稳定性和可解释性。

- **自适应专家激活**：
    - **问题**：在每一步评估所有专家计算成本高且不必要。
    - **解决方案**：在推理时，根据状态路由器的输出均值 `ḡ_t`，计算每个专家的“质量” `m_i = (ḡ_{t,i})^2`。通过**贪心覆盖策略**（如选择最少专家使其累计质量超过总质量的95%）或**Top-k策略**，动态选择一小部分相关专家进行激活和去噪。
    - **效果**：大幅减少推理时的活跃参数和计算延迟，实现高效采样。

- **变分训练目标**：
    - 设计了一个包含四部分的损失函数：
        1.  **重构损失**：在局部白化基中重构动作，梯度会更新技能基 `B(s)`。
        2.  **系数正则化损失**：在系数空间 `z` 上应用标准扩散损失（DDPM），使用**梯度截断**的投影目标，防止该损失更新技能基。
        3.  **门控正则化损失**：强制门控后验接近粘性Dirichlet先验。
        4.  **路由器对齐损失**：将训练时依赖动作的门控后验与一个**仅依赖状态的路由器**对齐，确保部署时仅凭状态即可有效路由。

#### **2. 实际价值**
- **性能与效率的平衡**：在模拟（RoboTwin-2, RLBench-2）和真实双臂机器人平台上，SMP在**多任务学习**和**迁移学习**（少样本适应、技能重组）中取得了比大型扩散基线（如DP、RDT）**更高或相当的成功率**，同时**显著降低了推理成本和延迟**（活跃参数约为RDT的7%）。
- **可解释的技能抽象**：学习到的技能基自然地与**空间角色**（左臂/右臂）和**时间阶段**（抓取、调整、移动、释放）对齐，展示了清晰的技能解耦和跨任务重用。
- **为可扩展的多任务操作提供实用路径**：核心思想是“**学习一次可重用技能，按需激活，快速适应**”。这为在适度模型规模和采样延迟下实现有效的多任务泛化指明了方向。

### **解决方案总结**
SMP通过一个**精心设计的MoE架构**解决了多任务扩散策略的扩展性问题：
1.  **解耦表示**：利用**状态自适应正交基**在几何上解耦技能，确保每个技能贡献独立方向。
2.  **稳定组合**：通过**粘性路由**实现时间上稳定的技能激活，对应高级行为阶段。
3.  **高效推理**：通过**自适应专家激活**，在每一步仅计算任务相关的少数专家，降低计算开销。
4.  **可迁移基础**：学到的紧凑、正交的技能集构成了一个可跨任务重用和重组的“技能库”，支持高效的少样本迁移学习。

**总之，SMP的核心贡献不是简单地应用MoE，而是将MoE与扩散模型、状态依赖的正交分解、时间一致的门控以及自适应的稀疏计算深度融合，创造了一个高效、可解释且可迁移的多任务机器人操作策略框架。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对扩散策略在多任务机器人操作中因模型规模和数据需求过大而导致的推理成本高昂、泛化能力受限的核心问题，提出了一种名为**技能专家混合策略（SMP）**的创新框架。该方法通过引入**状态自适应的正交技能基**和**粘性路由**机制，将动作生成分解为一系列可重用、解耦的技能，并利用**自适应专家激活**在推理时仅调用少量相关专家，从而实现了高效的动作合成。实验结果表明，SMP在仿真和真实双臂平台上，相比大型扩散基线，在保持或提升多任务及迁移学习成功率的同时，显著降低了推理计算成本和延迟，为构建可扩展、可迁移的多任务操作策略提供了一条实用路径。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《通过专家混合扩散策略抽象机器人操作技能》提出的 **Skill Mixture-of-Experts Policy (SMP)** 在多个方面对现有的机器人操作策略学习方法进行了创新。以下是其核心创新点及其与以往方法的对比和优势：

### 1. **引入状态自适应的正交技能基**
- **改进/不同之处**：
    - **以往方法**：传统的技能抽象方法（如DIAYN、离散潜在编码）或MoE扩散策略（如Sparse Diffusion Policy）通常学习一个**全局固定**的技能表示或专家输出，这些表示在动作空间中可能是**纠缠**的，即不同技能的影响方向可能重叠。
    - **SMP的创新**：SMP学习一个**状态依赖的正交矩阵 `B(s)`** 作为技能基。通过一个轻量级网络生成无约束矩阵 `W(s)`，然后通过**带符号稳定的QR分解**将其投影到Stiefel流形上，得到一个随状态平滑变化的**正交基**。动作被解码为 `a_t = B(s_t)(g_t ⊙ z_t)`。
- **解决的问题/带来的优势**：
    - **解耦技能**：正交性确保了每个技能贡献给动作空间中一个**互不重叠的方向**，使得技能表示在局部是解耦的、可解释的。
    - **适应任务几何**：状态依赖性允许技能基根据当前的机器人姿态、接触状态等上下文进行调整，从而更准确地捕捉不同任务阶段所需的运动原语（如抓取时的旋转、移动时的平移）。
    - **稳定路由与训练**：正交基为系数 `z_t` 的监督提供了**良条件**的逆映射（`B(s)^T a_t`），避免了以往方法中由于专家输出重叠导致的**路由不稳定**和**训练不收敛**问题。

### 2. **设计“粘性”门控机制与先验**
- **改进/不同之处**：
    - **以往方法**：标准MoE中的门控网络通常在每个时间步独立地选择专家，这可能导致**高频切换**（专家激活快速变化），产生抖动的、不连贯的行为，不利于需要时序一致性的操作任务。
    - **SMP的创新**：SMP为门控 `g_t` 引入了一个**粘性狄利克雷-马尔可夫先验**。该先验建模为：`g_t ~ Dir(κ g_{t-1} + α_0 ϑ)`，其中 `ϑ` 是一个全局使用向量。这强制门控在时间上**缓慢变化**，并倾向于保持激活状态的持续性。
- **解决的问题/带来的优势**：
    - **相位一致性**：粘性先验鼓励策略将轨迹组织成**准平稳的阶段**（例如，抓取、移动、放置），在每个阶段内主要激活一个稳定的专家子集。这产生了更**平滑、更稳定**的机器人行为。
    - **减少振荡**：实验表明，SMP的**门切换率**显著低于基线（如Sparse DP），避免了动作振荡，提高了在需要精度的任务（如插入、对齐）上的成功率。
    - **可解释的技能角色**：粘性路由使得学到的专家与可解释的、持续的角色相关联（例如，“左臂平移”、“右臂抓取器控制”）。

### 3. **提出自适应专家激活机制**
- **改进/不同之处**：
    - **以往方法**：大型扩散模型（如RDT）或标准MoE策略在推理时通常需要评估**所有参数**或**所有专家**，导致**高延迟**，难以满足实时控制要求。
    - **SMP的创新**：在推理时，SMP使用一个**仅依赖状态的路由器** `p_φ(g_t|s_t)` 来预测门控均值 `ḡ_t`。然后，它根据每个专家的**质量** `m_i = (ḡ_{t,i})^2` 进行排序，并**贪婪地选择**一个最小的专家子集 `S_t`，使得其累积质量达到总质量的阈值 `τ_m`（例如，95%），或满足一个top-k约束。
- **解决的问题/带来的优势**：
    - **大幅降低计算成本**：SMP在大多数时间步只激活**少量专家**（论文中平均约2.3个）。这使得其**激活参数量**（~80M）远低于总参数量（~259M），也远低于大型基线模型（如RDT的1200M）。
    - **保持高性能**：自适应选择确保了在“简单”状态使用较少计算，在“复杂”状态调用更多专家，从而在**显著降低推理时间**（107 ms vs. RDT的183 ms）的同时，**维持甚至提升了任务成功率**。
    - **实现实时控制**：降低的延迟使得SMP能够应用于**真实机器人的实时双操作**，这是许多大型扩散模型难以实现的。

### 4. **构建基于变分推理的联合训练目标**
- **改进/不同之处**：
    - **以往方法**：许多技能抽象方法（如变分自编码器）或MoE方法使用**启发式或分离的**训练目标，可能无法协调技能学习、门控和动作生成。
    - **SMP的创新**：SMP提出了一个**统一的变分下界**作为训练目标，该目标明确地结合了：
        1.  **重构损失** (`ℒ_recon`)：在局部白化基中重建动作。
        2.  **门控正则化** (`ℒ_gate`)：将门控后验拉向粘性先验。
        3.  **系数正则化** (`ℒ_coeff`)：使用扩散模型作为系数 `z_t` 的先验，通过DDPM损失进行训练。
        4.  **对齐损失** (`ℒ_align`)：将训练时依赖状态-动作的门控后验与推理时仅依赖状态的路由器对齐。
- **解决的问题/带来的优势**：
    - **端到端可训练性**：该框架允许**技能基 `B(s)`、门控网络、扩散专家和状态路由器**在一个**联合的、理论驱动的目标**下进行端到端优化。
    - **稳定且解耦的梯度流**：通过设计**两个系数目标**——一个用于扩散损失（**停止梯度**，防止扩散损失更新技能基），一个用于重构损失（**传递梯度**以更新技能基）——确保了技能基和专家系数的学习是**解耦且稳定**的。
    - **促进技能复用**：该目标鼓励学习一个紧凑的、可跨任务复用的技能集，为迁移学习奠定了基础。

### 5. **在双操作多任务与迁移学习中验证高效技能复用**
- **改进/不同之处**：
    - **以往方法**：大规模模型（如RDT）通过**扩展数据和模型容量**来获得泛化能力，但计算成本高昂。其他技能方法在**跨任务组合和快速适应**方面表现有限。
    - **SMP的创新**：论文不仅在多任务学习上测试SMP，还专门设计了**少样本迁移**和**技能组合**实验。在技能组合实验中，**冻结**已学习的技能基和专家，**仅微调路由器**，就能将已有技能重组到新任务中。
- **解决的问题/带来的优势**：
    - **证明可扩展的多任务学习路径**：SMP展示了一条不依赖于无限扩大模型的实用路径：**学习一次可复用技能，按需激活，快速适应**。
    - **高效的迁移能力**：实验表明，SMP在少样本适应和技能重组任务上的成功率显著高于基线。这表明其学到的技能是**真正模块化且可组合**的。
    - **高计算效率**：SMP在取得更高或相当成功率的同时，**推理成本（参数量、时间）远低于大型扩散基线**，证明了其在**精度-效率权衡**上的优越性。

### 总结
SMP的核心创新在于将 **“状态自适应正交技能表示”**、**“时序一致粘性路由”** 和 **“推理时自适应稀疏激活”** 三者有机结合在一个**基于扩散的MoE框架**内。这解决了现有方法在**多任务泛化**、**技能解耦与复用**、**推理效率**和**行为稳定性**等方面的关键瓶颈，为迈向**可扩展、可迁移、实时的多任务机器人操作**提供了切实可行的方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

该论文通过一系列仿真和真实机器人实验，全面验证了所提出的 **技能专家混合策略 (SMP)** 在多任务和迁移学习场景下的有效性、高效性和技能可重用性。

### 一、 使用的数据集与实验环境

1.  **仿真环境**：
    *   **RoboTwin-2**：一个用于双臂操作的可扩展数据生成器和基准测试平台，包含多种需要双臂独立或协作的任务。
    *   **RLBench-2**：一个专注于评估双臂协调操作的基准测试套件，任务强调时空协作。

2.  **真实机器人平台**：
    *   **PiPER 双臂平台**：一个轻量级机器人臂平台，用于执行四项真实世界的双臂操作任务。

### 二、 评价指标

1.  **主要性能指标**：
    *   **任务成功率**：在仿真环境中，通过多次运行（如100次）计算任务完成的平均成功率。
    *   **进度得分**：在真实机器人实验中，根据任务完成的关键步骤定义“进度”，并计算平均进度得分，以量化执行效果。

2.  **效率与成本指标**：
    *   **模型参数量**：总参数量 (`𝒩ₚ`) 和推理时激活的参数量 (`𝒩ₚᵃᶜᵗ`)。
    *   **推理时间**：生成一个动作所需的平均时间 (`Tᵢₙf`)。
    *   **专家激活数**：平均每个时间步激活的专家数量 (`𝒩ₑᵃᶜᵗ`)。
    *   **门切换频率**：定性分析门控值随时间的变化，评估行为的阶段一致性。

### 三、 对比的基线方法

论文与多种代表性的机器人策略方法进行了对比：
*   **DP (Diffusion Policy)**： 标准的扩散策略，强大的单任务模仿学习基线。
*   **DP3 (3D Diffusion Policy)**： 融合3D感知的扩散策略变体。
*   **ACT (Action Chunking Transformer)**： 基于Transformer的动作分块策略，代表时序抽象方法。
*   **RDT (Robotics Diffusion Transformer)**： 大规模（12亿参数）扩散-Transformer基础模型，代表通过扩大模型规模实现泛化的方法。
*   **Discrete Policy**： 基于信息论和离散潜码本的技能抽象方法。
*   **Sparse DP**： 在扩散网络内部使用前馈网络专家混合 (FFN-MoE) 的方法，是现有的MoE策略代表。

### 四、 关键实验结果与性能提升

#### 1. 多任务学习性能
*   **结果**：在RoboTwin-2（6任务）和RLBench-2（4任务）的多任务联合训练中，**SMP取得了最高或接近最高的平均成功率**（见表1）。
*   **结论**：
    *   **性能领先**： SMP的成功率显著优于DP、DP3、ACT等标准基线，与参数量大10倍的RDT模型性能相当甚至更优。
    *   **解决基线瓶颈**： 基线方法（如DP、ACT）难以拟合双臂任务产生的多模态动作分布。单纯扩大模型（RDT）带来的性能提升有限且成本高昂。现有的MoE方法（Sparse DP）因缺乏明确的技能解耦和稳定路由，导致门控频繁切换和动作振荡。
    *   **SMP优势**： 通过**正交技能基`B(s)`**和**粘性门控`gₜ`**，SMP能够将动作自然地分解到空间（左/右臂）和时间（抓取、移动、放置等阶段）维度，学习到可重用、可解释的技能，从而实现了更好的多任务泛化。

#### 2. 推理效率与计算成本
*   **结果**： SMP在保持高性能的同时，**大幅降低了推理时的计算开销**（见表2）。
*   **结论**：
    *   **稀疏激活**： 得益于**自适应专家激活**机制，SMP在推理时平均只激活约30%的参数（约8020万），远低于其总参数量（2.589亿），更远低于RDT的1200亿参数。
    *   **推理速度快**： SMP的推理时间 (`107.3 ms`) 显著低于RDT (`183.1 ms`)、Discrete Policy (`153.7 ms`) 等大型扩散基线，与轻量级方法（如ACT的 `94.8 ms`) 处于同一量级，同时性能更高。
    *   **效率-精度平衡**： SMP通过动态选择任务相关的少量专家，实现了精度和延迟之间的优越权衡。

#### 3. 迁移学习能力
*   **少样本适应**：
    *   **结果**： 在4个新的RoboTwin-2任务上，仅用10条演示进行全模型微调，SMP取得了最高的平均成功率（`0.38`，见表3）。
    *   **结论**： SMP的紧凑技能集能够更有效地利用有限的微调数据，将更新集中在相关的专家上，适应速度更快。
*   **技能组合**：
    *   **设置**： **冻结**所有专家和技能基，**仅微调路由器**，让模型学习如何将已有技能（如左臂抓瓶、右臂开抽屉）重新组合成新任务（将瓶子放入抽屉）。
    *   **结果**： SMP在此设置下的成功率（`0.30`）高于Sparse DP（`0.25`，见表4）。
    *   **结论**： SMP的**正交技能表示**确保了技能的清晰分离，使得仅通过调整门控就能有效地重新组合技能，证明了其技能模块化和可组合性的优势。

#### 4. 真实机器人验证
*   **结果**： 在四项真实双臂操作任务上，SMP取得了**最高的平均进度得分**（见图5右），同时保持了较低的激活参数量和推理延迟。
*   **结论**： SMP的方法在存在传感器噪声、标定误差和执行延迟的真实世界中依然有效，其**粘性路由**和**稀疏激活**带来了稳定、高效的控制。

### 五、 核心结论

论文通过系统的实验评估表明，**SMP成功地将高性能、高效率和强可迁移性结合在一起**：
1.  **性能更优**： 在多任务和迁移学习设置下，其成功率超过或匹配强大的扩散基线。
2.  **成本更低**： 通过自适应稀疏激活，推理时的计算和内存开销显著降低，更适合实时控制。
3.  **技能可重用**： 学习到的技能在空间（左右臂）和时间（操作阶段）上具有可解释性，并能有效地跨任务重用和组合。

这些结果共同支撑了论文的核心论点：**通过显式地学习一个状态自适应的正交技能基，并结合粘性路由与稀疏激活，SMP为可扩展、可迁移的多任务操作提供了一条实用路径**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.21251v1)
- [HTML 版本](https://arxiv.org/html/2601.21251v1)
