# AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation

**相关性评分**: 8.0/10

**排名**: #7


---


## 基本信息

- **arXiv ID**: [2601.19634v1](https://arxiv.org/abs/2601.19634v1)
- **发布时间**: 2026-01-27T14:10:39Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Acceleration, Edge Deployment, fine tune

## 一句话总结

该论文提出了一种基于动作上下文感知的自适应计算框架AC^2-VLA，通过认知重用、令牌剪枝和选择性执行来加速VLA模型推理，适用于机器人操作任务。

## 摘要

Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.

## 详细分析

## 论文摘要：AC²-VLA: 面向高效机器人操作的动作-上下文感知自适应计算视觉-语言-动作模型

**1. 研究背景和动机**
视觉-语言-动作模型在机器人操作任务中展现出强大性能，但其闭环部署面临巨大挑战：需要在每个控制时间步重复运行计算量庞大的视觉-语言主干网络，导致高延迟和高计算成本，影响实时响应能力。现有高效计算方法大多基于视觉线索，忽略了**动作上下文**在具身任务中的核心作用。本文观察到VLA推理在时间、空间和深度维度上存在结构化冗余，并指出视觉复杂度与操作难度并不直接相关，因此提出应基于动作上下文进行自适应计算。

**2. 核心方法和技术创新**
本文提出了**动作-上下文感知自适应计算框架**。其核心创新在于引入一个轻量级的**动作先验路由器**，该路由器以前一时刻的动作状态、当前视觉观察和语言指令为条件，生成统一的稀疏化策略，协同控制三个互补机制：
- **认知缓存**：当动作上下文表明状态转换稳定时，跨时间步重用主干网络特征。
- **动作感知令牌剪枝**：移除与当前操作阶段无关的视觉令牌。
- **条件层跳过**：当动作上下文指示推理需求较低时，绕过冗余的Transformer模块。
为训练该自适应策略，论文设计了**动作引导的自蒸馏方案**，在实现结构化稀疏化的同时，保留了原始密集策略的行为。

**3. 主要实验结果**
在机器人操作基准SIMPLER上的广泛实验表明，AC²-VLA在保持可比任务成功率的同时，显著提升了效率：
- 相比密集基线，实现了最高**1.79倍**的加速。
- 计算量降至基线模型的**29.4%**。
- 在Google Robot的“视觉匹配”评估中，平均成功率从基线的74.8%提升至**76.8%**，部分精细操作任务（如开抽屉）提升显著。
- 消融实验证实了缓存重用、令牌剪枝和层路由三个组件的互补性与必要性。

**4. 研究意义和价值**
本研究的价值在于揭示了**基于动作上下文进行自适应计算**是比静态压缩或纯视觉驱动方法更有效的具身智能高效推理范式。AC²-VLA不仅是一个高效机制，其动作引导的稀疏化还能抑制视觉干扰、提升决策的时间一致性，起到了正则化作用。这项工作为开发可扩展的通用机器人策略指明了方向，即**自适应推理**是实现低延迟、高性能闭环控制的关键要素。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：AC²-VLA

### **一、 核心问题**
论文旨在解决**视觉-语言-动作模型在机器人闭环操控中部署效率低下**的关键瓶颈。具体表现为：
- **高延迟与高计算成本**：在每一个控制时间步，都需要重复运行庞大的视觉-语言主干网络，导致控制频率降低，难以满足动态环境的实时性要求。
- **现有效率方法的局限性**：大多数方法（如静态剪枝、量化、基于视觉线索的动态计算）忽略了**动作上下文**在具身任务中的核心作用。视觉复杂度与操控难度并不直接相关，仅依赖视觉线索进行剪枝或跳过计算可能是次优的。

### **二、 核心创新点**
论文的核心创新在于提出了一个**以动作为中心的、自适应的统一计算框架**。其创新性体现在以下三个层面：

1.  **理念创新：从“视觉中心”到“动作上下文中心”**
    - **关键洞察**：发现VLA模型的计算冗余性与**动作上下文**（如机器人当前的运动状态）的相关性，远高于与视觉复杂度的相关性。动作状态是分配计算资源的更有效信号。

2.  **方法创新：统一的动作先验路由器**
    - **统一机制**：设计了一个轻量级的**动作先验路由器**，它以前一时刻的动作状态 `a_{t-1}` 为核心，结合当前的视觉观察和语言指令的摘要，生成一个统一的稀疏化策略。
    - **三维协同优化**：该路由器在一个统一的机制下，协同控制三个维度的计算节省：
        - **时间维度（认知缓存）**：当动作上下文表明状态过渡稳定时，复用相邻时间步的主干网络特征。
        - **空间维度（动作感知的令牌剪枝）**：移除与当前操控阶段无关的视觉令牌。
        - **深度维度（条件层跳过）**：当动作上下文指示推理需求较低时，绕过冗余的Transformer层。

3.  **训练创新：动作引导的自蒸馏方案**
    - **行为保持**：通过教师-学生蒸馏框架训练路由器。教师是原始的密集策略，学生是执行路由稀疏推理的策略。
    - **动作引导**：蒸馏损失不仅匹配中间特征，更**重点匹配最终的动作预测**，确保稀疏化后的策略行为与原始策略一致，保持操控的鲁棒性。

### **三、 解决方案（AC²-VLA框架如何工作）**
解决方案是一个端到端的系统，其工作流程如下：

1.  **构建动作先验条件向量**：
    ```python
    # 核心输入：前一动作 + 视觉/语言摘要
    c_t = fuse(embed(a_{t-1}), pool(V_t), pool(E_t), ...)
    ```

2.  **路由器预测统一门控信号**：
    - **缓存重用门** `p_cache`: 标量，决定是否尝试重用缓存的特征。
    - **令牌剪枝门** `p_topk`: 向量，为每个视觉令牌分配保留分数。
    - **层跳过门** `p_lay`: 向量，为每个Transformer层分配执行概率。

3.  **门控信号驱动高效推理**：
    - **若缓存命中**：直接使用缓存的表示 `z_t`，跳过整个主干网络前向传播。
    - **若需计算**：
        - **令牌剪枝**：根据 `p_topk` 对视觉令牌进行物理压缩，缩短序列长度。
        - **层跳过**：根据 `p_lay` 动态分组，只执行被激活的层。
        - 计算得到新的 `z_t`，若之前请求了缓存但未命中，则将其写入缓存。

4.  **动作生成**：将高效计算得到的 `z_t` 输入动作头（如扩散模型），生成当前的动作轨迹。

### **四、 实际价值与效果**
- **性能**：在SIMPLER机器人操控基准测试中，在保持甚至略微提升任务成功率（如Google Robot Visual Matching平均成功率从74.8%提升至76.8%）的同时，实现了显著的效率提升。
- **效率**：计算量降至密集基线的**29.4%**，实现了 **1.79倍** 的端到端加速。
- **泛化性与鲁棒性**：方法在更具挑战性的环境变化（Variant Aggregation）和不同机器人平台（WidowX）上均表现良好，证明了其通用性。
- **启示**：论文表明，**将计算分配与任务相关的动作上下文对齐，不仅是效率机制，也是一种正则化手段**，可以抑制视觉干扰，提升闭环控制的时序一致性，为开发可扩展的通用机器人策略指明了新方向。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决视觉-语言-动作（VLA）模型在机器人闭环操控中因反复运行大型骨干网络而导致的高延迟和高计算成本问题。其核心创新在于提出了一个名为AC^2-VLA的统一框架，该框架摒弃了传统仅依赖视觉线索的效率优化方法，转而以**动作上下文**（如上一步动作状态）为核心，通过一个轻量级的“动作先验路由器”来动态协调**认知缓存重用、动作感知的视觉令牌剪枝和条件化层跳过**这三种互补的稀疏化机制。实验表明，该方法在保持任务成功率相当甚至略有提升的同时，实现了高达**1.79倍的推理加速**，并将计算量（FLOPs）降至密集基线的**29.4%**，证明了基于动作上下文的自适应计算是实现高效、鲁棒闭环机器人控制的有效范式。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation》的核心创新在于**首次将“动作上下文”作为核心信号，统一指导VLA模型在推理时的动态计算精简**。以下是其相对于已有工作的明确创新点：

---

### 1. **核心理念创新：从“视觉中心”到“动作中心”的动态计算决策**
- **改进/不同之处**：现有VLA高效化方法（如VLA-Cache、MoLe-VLA）主要依赖**视觉复杂度**或**静态启发式规则**来决定是否进行缓存、剪枝或跳层。本文指出，在具身任务中，视觉复杂度与控制难度并不直接相关（例如，视觉简单的精细操作可能需要全容量推理，而视觉复杂的移动阶段反而可以大幅精简）。因此，本文提出**基于动作上下文（如前一个动作状态）** 来指导计算分配。
- **解决的问题/优势**：解决了**计算分配与任务需求错配**的问题。通过动作上下文（如机器人正在执行的动作类型、运动连续性）来判断当前推理需求，实现了更智能、更符合控制逻辑的动态计算，避免了因过度剪枝在关键步骤出错，或因冗余计算在简单步骤浪费资源。

### 2. **方法架构创新：统一的“动作先验路由器”**
- **改进/不同之处**：以往方法通常**孤立地**应用缓存重用、令牌剪枝或层跳过等机制。本文设计了一个轻量级的**统一路由器（Action-Prior Router）**。该路由器以当前观测、语言指令和**前一时刻动作**为条件，**协同生成**用于控制缓存重用、令牌剪枝和条件层执行的三种门控信号。
- **解决的问题/优势**：
    - **解决了协调性问题**：将时间、空间和深度三个维度的冗余利用统一在一个决策框架下，避免了不同机制间的冲突或次优组合。
    - **降低了决策开销**：用一个轻量级路由器代替多个独立的决策模块，减少了引入的动态计算本身的开销。

### 3. **训练策略创新：动作引导的自蒸馏**
- **改进/不同之处**：为了在结构化稀疏化（剪枝、跳层）下保持原始密集策略的行为，本文采用了**动作引导的自蒸馏**。其特别之处在于，蒸馏损失不仅匹配最终的动作输出，还匹配骨干网络输出的**认知特征**，并且以动作预测误差作为核心监督信号。
- **解决的问题/优势**：
    - **解决了性能保持难题**：确保学生模型（执行稀疏计算）的输出与教师模型（原始密集模型）在**动作空间**上保持一致，这是机器人任务成功的直接保证。
    - **提升了泛化性**：这种基于动作的蒸馏方式有助于学到的路由策略**跨任务和场景迁移**，因为其目标是保持“行为”而非中间特征的严格一致。

### 4. **工程实现创新：面向实际加速的令牌“物理压缩”与位置编码保持**
- **改进/不同之处**：许多令牌剪枝方法仅在注意力掩码上操作，无法获得实际的序列长度缩短带来的加速。本文实施了**物理压缩**，即真正移除被剪枝的令牌，缩短Transformer序列。同时，针对使用RoPE位置编码的骨干网络，设计了**位置对齐机制**，在压缩后重新映射令牌的原始位置索引，避免了位置信息混乱。
- **解决的问题/优势**：
    - **解决了“虚假加速”问题**：实现了真实的墙钟时间加速，而不仅仅是FLOPs的减少。
    - **保持了模型完整性**：确保了位置编码与原始图像块的对应关系，避免了因位置信息错误导致的性能下降。

### 5. **综合效果创新：效率与性能的协同提升**
- **改进/不同之处**：实验表明，AC^2-VLA不仅大幅提升了效率（**最高1.79倍加速，FLOPs降至基线29.4%**），而且在多项任务上的**成功率甚至超过了原始密集模型**。这与通常“效率提升伴随性能下降”的认知相反。
- **解决的问题/优势**：
    - **揭示了动作上下文的新价值**：表明基于动作的稀疏化不仅是一个加速工具，更是一种**正则化器**。它能抑制与当前操作无关的视觉干扰物（见图4可视化），并利用缓存促进时间一致性决策（平滑动作抖动），从而**提升了闭环控制的鲁棒性和稳定性**。

---

**总结**：本文的核心创新是**范式转变**——将VLA模型的高效化焦点从“看到了什么”转移到“正在做什么”。通过一个以动作为中心的统一路由框架，它智能地协调了跨时间、空间和深度的计算精简，在实现显著加速的同时，意外地提升了任务性能，为部署实时、高效的通用机器人策略提供了新思路。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 数据集与评价指标
- **数据集**：主要使用 **SIMPLER** 仿真基准测试集。该数据集旨在缩小“仿真-现实”差距，提供高保真的机器人操作任务。
- **评价指标**：
    1.  **任务成功率（Success Rate）**：核心指标，衡量模型在指定任务中成功完成的比率。
    2.  **计算效率**：
        - **加速比（Speed-up）**：相对于原始密集模型的推理速度提升倍数。
        - **计算量（FLOPs）**：执行推理所需的浮点运算次数，以原始模型的百分比表示。

### 二、 对比的基线方法
论文将 **AC²-VLA** 与两大类基线方法进行了对比：

1.  **通用密集VLA模型（性能上限）**：
    - RT-1, RT-2-X, Octo-Base, OpenVLA, **CogACT**（作为AC²-VLA构建的基础模型）。

2.  **高效推理方法（效率对比）**：
    - **VLA-Cache**：基于时间冗余的缓存重用方法。
    - **EfficientVLA**：静态模型剪枝方法。
    - **MoLe-VLA**：基于混合专家（MoE）的条件层执行方法。
    - **FastV**：轻量级剪枝基线。

### 三、 关键性能提升与结论
实验在SIMPLER的两个机器人平台（Google Robot 和 WidowX）和两种评估协议（Visual Matching 和 Variant Aggregation）下进行。

#### 1. 任务成功率：保持甚至超越原始模型
- **Google Robot (Visual Matching)**：AC²-VLA 平均成功率为 **76.8%**，**优于** 其基础密集模型 CogACT (74.8%) 和所有其他基线模型（如RT-2-X的46.3%）。
- **Google Robot (Variant Aggregation)**：在更具挑战性的变体聚合设置下，AC²-VLA 取得 **61.6%** 的成功率，与 CogACT (61.3%) **持平**，并显著优于其他基线。
- **WidowX (Visual Matching)**：AC²-VLA 平均成功率为 **54.5%**，**优于** CogACT (51.3%) 和其他所有基线。
- **结论**：AC²-VLA 在显著降低计算成本的同时，**不仅没有牺牲性能，反而在多个任务上提升了成功率**。这表明其基于动作上下文的剪枝策略有效过滤了视觉干扰，提升了决策的鲁棒性。

#### 2. 计算效率：实现显著加速与计算量削减
- **最佳效果**：在 Google Robot Visual Matching 任务中，AC²-VLA 实现了：
    - **1.79倍** 的推理速度提升。
    - 计算量降至原始密集模型（CogACT）的 **29.4%**。
- **效率对比**：AC²-VLA 在速度提升和FLOPs削减方面均**优于所有高效推理基线方法**（如VLA-Cache的1.36倍加速，EfficientVLA的45.1% FLOPs）。
- **结论**：通过统一协调**时间（缓存重用）、空间（Token剪枝）、深度（条件层执行）** 三个维度的自适应计算，AC²-VLA 实现了当前最优的效率-精度权衡。

#### 3. 消融实验与深入分析
- **组件必要性**：移除任一效率组件（缓存、Token剪枝、层路由）都会导致性能或效率下降，验证了统一框架设计的有效性。
- **帕累托前沿**：通过超参数扫描（Token保留率、执行层数），找到了最佳操作点（如 `r_topk=0.4`, `N_lay=28`），证明了结构化稀疏化的有效性。
- **缓存的双重益处**：缓存重用不仅加速，其带来的**时间一致性**还能平滑决策，在特定缓存阈值下（`τ_cache=0.2`）将成功率提升至87.1%（比密集基线高12.3%），起到了正则化作用。
- **敏感性分析**：性能在适度稀疏下保持稳定，但过度稀疏（如Token保留率低于0.4）会导致性能崩溃，表明存在一个保证基本视觉信息的最小计算需求。

### 总结
**AC²-VLA 在 SIMPLER 基准测试上取得了卓越的综合效果**：它**同时实现了**显著的推理加速（最高1.79倍）、大幅的计算量削减（最低至29.4%），并在任务成功率上达到或超越了强大的密集基线模型。这强有力地证明了其核心创新——**以动作为中心的自适应计算**——是解决VLA模型在具身任务中部署瓶颈的有效范式，既能提升效率，又能通过过滤无关信息来增强策略的鲁棒性和稳定性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.19634v1)
- [HTML 版本](https://arxiv.org/html/2601.19634v1)
