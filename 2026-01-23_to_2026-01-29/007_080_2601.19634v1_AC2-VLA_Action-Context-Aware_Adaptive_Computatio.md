# AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation

**相关性评分**: 8.0/10

**排名**: #7


---


## 基本信息

- **arXiv ID**: [2601.19634v1](https://arxiv.org/abs/2601.19634v1)
- **发布时间**: 2026-01-27T14:10:39Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Acceleration, Edge Deployment, fine tune

## 一句话总结

该论文提出了一种基于动作上下文感知的自适应计算框架AC^2-VLA，通过认知重用、令牌剪枝和选择性执行来加速VLA模型推理，适用于机器人操作任务。

## 摘要

Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.

## 详细分析

## 论文摘要：AC²-VLA: 面向高效机器人操作的动作-上下文感知自适应计算

**1. 研究背景和动机**
视觉-语言-动作模型在机器人操作任务中展现出强大性能，但其闭环部署面临巨大挑战：需要在每个控制时间步重复运行计算量庞大的视觉-语言主干网络，导致高延迟和高计算成本，影响实时响应能力。现有高效计算方法大多仅基于视觉线索，忽略了在具身任务中起核心作用的**动作上下文**，而视觉复杂度与控制难度并不总是相关。因此，本文旨在提出一种基于动作上下文的自适应计算框架，以更高效地利用模型在时间、空间和深度维度上的结构化冗余。

**2. 核心方法和技术创新**
本文提出了**动作-上下文感知自适应计算框架**。其核心创新在于一个轻量级的**动作先验路由器**，该路由器以前一时刻的动作状态、当前视觉观测和语言指令为条件，生成统一的稀疏化策略，协同控制三个互补的高效机制：
- **认知缓存**：当动作上下文表明状态转换稳定时，复用相邻时间步的主干网络特征。
- **动作上下文感知的令牌剪枝**：移除与当前操作阶段无关的视觉令牌。
- **条件层跳过**：当动作上下文指示推理需求较低时，绕过冗余的Transformer模块。
为训练该路由策略，作者提出了**动作引导的自蒸馏方案**，在实现结构化稀疏化的同时，保留了原始密集策略的行为。

**3. 主要实验结果**
在机器人操作基准SIMPLER上的广泛实验表明，AC²-VLA在保持可比任务成功率的同时，实现了显著的效率提升：
- **性能**：在Google Robot的视觉匹配任务中，平均成功率达到了76.8%，优于其基础模型CogACT（74.8%）及其他基线模型。
- **效率**：实现了最高**1.79倍**的加速，并将浮点运算量降至密集基线的**29.4%**。
- **消融研究**：验证了缓存复用、令牌剪枝和层路由三个组件的互补性，联合启用时取得最佳速度-精度权衡。

**4. 研究意义和价值**
本研究揭示了在具身智能任务中，**基于动作上下文（而非单纯视觉线索）来引导计算分配**是更有效的效率优化范式。AC²-VLA不仅作为一种高效推理机制，其动作引导的稀疏化还能起到正则化作用，抑制视觉干扰并提升决策的时间一致性。这项工作为开发可扩展的通用机器人策略指明了方向，即**自适应推理**是实现高效、鲁棒闭环控制的关键要素。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：AC²-VLA

### **一、 核心问题**
论文旨在解决**视觉-语言-动作模型在机器人闭环操控中部署效率低下**的关键瓶颈。具体问题包括：
- **高延迟与高计算成本**：VLA模型需要在每个控制时间步重复运行庞大的视觉-语言主干网络，导致推理延迟高、控制频率低，难以满足动态环境的实时性要求。
- **现有效率方法的局限性**：大多数现有方法（如静态压缩、动态计算、缓存）在分配计算资源时，主要依赖**视觉线索**或静态启发式规则，而忽略了**动作上下文**在具身任务中的核心作用。这导致计算分配可能不最优，因为视觉复杂度与操控难度并不总是正相关。

### **二、 核心创新点**
论文的核心创新在于提出了一个**以动作为中心的、自适应的统一计算框架**，其创新性体现在以下三个层面：

1.  **理念创新：从“视觉驱动”到“动作上下文驱动”的效率优化**
    - **关键洞察**：发现VLA模型的计算冗余性与**动作上下文**（如前一时刻的动作状态、当前操控阶段）的相关性，远高于与视觉复杂度的相关性。这是本文最根本的贡献。

2.  **方法创新：统一的动作先验路由器**
    - **统一机制**：设计了一个轻量级的**动作先验路由器**，它以前一时刻的动作 `a_{t-1}` 为核心信号，结合当前视觉观察和语言指令的摘要，**统一地、协同地**预测三种计算门控策略：
        - **认知缓存门控**：决定是否重用相邻时间步的主干网络特征。
        - **动作感知的令牌剪枝**：预测并剪除与当前操控阶段无关的视觉令牌。
        - **条件层跳过**：预测并跳过冗余的Transformer层。
    - **公式化表示**：路由器输出一个统一的稀疏策略向量 `p_t = [p_t_cache; p_t_topk; p_t_lay]`，实现了跨**时间、空间、深度**三个维度的自适应计算。

3.  **训练创新：动作引导的自蒸馏方案**
    - **目标**：在引入结构化稀疏化（缓存、剪枝、跳层）的同时，保持原始密集策略的行为和鲁棒性。
    - **方法**：采用教师-学生蒸馏框架。教师模型执行密集推理，学生模型执行由路由器引导的稀疏推理。损失函数不仅蒸馏最终的动作输出，还蒸馏中间的主干网络表征，确保稀疏化后的模型决策与原始模型一致。

### **三、 解决方案概述**
AC²-VLA的解决方案是一个端到端的系统，其工作流程如下：

```mermaid
graph TD
    A[输入: 观测x_t, 指令u, 前一动作a_{t-1}] --> B[构建动作先验条件向量 c_t];
    B --> C[动作先验路由器 ℛ];
    C --> D{生成统一门控策略};
    D --> E[缓存门控 p_t_cache];
    D --> F[令牌剪枝门控 p_t_topk];
    D --> G[层跳过门控 p_t_lay];
    
    E --> H{是否请求缓存?};
    H -- 是 --> I[构建缓存键并查询];
    I --> J{缓存命中?};
    J -- 是 --> K[直接重用缓存特征 z_t];
    J -- 否 --> L[执行稀疏前向传播];
    
    H -- 否 --> L;
    
    F --> L;
    G --> L;
    
    L --> M[基于稀疏特征 z_t 生成动作 a_{t:t+H}];
    K --> M;
    
    L --> N[若缓存请求但未命中， 则写入新特征到缓存];
```

**关键技术细节**：
- **缓存机制**：使用动作增量范数和视觉特征哈希构建鲁棒的缓存键，仅在路由器请求且未命中时才执行计算并回写。
- **令牌剪枝**：不仅进行注意力掩码，还**物理压缩序列长度**以获得实际加速，并妥善处理位置编码。
- **层跳过**：通过门控残差连接实现，在推理时对低概率层进行完全旁路。

### **四、 实际价值与效果**
- **性能**：在SIMPLER机器人操控基准测试中，在保持甚至略微提升任务成功率（如Google Robot Visual Matching平均成功率从74.8%提升至76.8%）的同时，实现了显著的效率提升。
- **效率**：计算量降至密集基线的**29.4%**，实现了 **1.79倍** 的端到端加速。
- **泛化性**：框架构建在CogACT等现有VLA模型之上，仅训练轻量级路由模块，**冻结主干网络**，具有良好的可迁移性和实用性。
- **启示**：论文表明，在具身智能中，**将计算资源与任务相关的动作上下文对齐，比单纯的模型压缩更有效**，这为开发高效、实时的通用机器人策略指明了新方向。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决视觉-语言-动作（VLA）模型在机器人闭环操控中因重复运行大型多模态主干网络而导致的高延迟和高计算成本问题。其核心创新在于提出了一个名为AC²-VLA的统一框架，该框架摒弃了传统基于视觉线索的静态或启发式效率优化方法，转而引入一个轻量级的“动作先验路由器”。该路由器以前一时刻的动作状态为核心，结合当前视觉观察和语言指令，动态地、联合地决策是否跨时间步复用特征缓存、对视觉令牌进行剪枝以及跳过部分模型层，从而实现沿时间、空间和深度三个维度的自适应计算。通过一种动作引导的自蒸馏训练方案，该方法在保持原始密集策略行为的同时实现了高效推理。实验结果表明，该方法在SIMPLER机器人操控基准测试中，在任务成功率相当甚至略有提升的情况下，实现了最高1.79倍的加速，并将计算量（FLOPs）降至基线模型的29.4%，证明了基于动作上下文的自适应计算是实现高效、鲁棒闭环机器人控制的有效范式。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《AC²-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation》的核心创新在于**首次将“动作上下文”作为核心信号，来统一指导VLA模型在推理时的自适应计算**。以下是其相对于已有工作的明确创新点：

---

### 1. **提出“动作上下文感知”的自适应计算新范式**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：现有的VLA效率优化方法（如VLA-Cache的缓存复用、MoLe-VLA的层跳过、EfficientVLA的令牌剪枝）大多基于**视觉线索**或**静态启发式规则**来决定如何节省计算。例如，根据图像复杂度或固定模式来剪枝令牌或跳过层。
    - **本文方法**：AC²-VLA认为，在具身任务中，**计算冗余与“动作上下文”的相关性高于与视觉复杂度的相关性**。因此，它引入了一个轻量级的**动作先验路由器**，其决策条件不仅包括当前视觉观察和语言指令，还**显式地包含了上一时刻的动作状态**作为核心上下文。
- **解决的具体问题/带来的优势**：
    - **解决了“视觉复杂度与操控难度不匹配”的问题**。例如，视觉简单的精细操作可能需要全容量推理，而视觉复杂的移动阶段反而可以大幅剪枝。基于动作上下文的决策更符合机器人任务的实际需求。
    - **优势**：使计算资源的分配与任务的实际推理需求对齐，在保证成功率的前提下，实现了更精准、更高效的计算节省。

### 2. **设计了一个统一的路由器，协同优化时空深三个维度的计算冗余**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：现有工作通常**孤立地**针对某一维度进行优化（例如，只做缓存复用、或只做令牌剪枝、或只做层跳过），且这些决策之间缺乏协调。
    - **本文方法**：AC²-VLA提出了一个**统一的动作先验路由器**。该路由器接收动作上下文条件向量，**同时输出三组控制门**：
        1.  **缓存复用门**：决定是否复用上一时间步的骨干网络特征。
        2.  **令牌剪枝门**：预测每个视觉令牌的保留分数，进行空间剪枝。
        3.  **层执行门**：预测每个Transformer层的执行概率，进行深度跳过。
- **解决的具体问题/带来的优势**：
    - **解决了“效率机制孤立不协同”的问题**。统一路由允许模型根据同一动作上下文信号，在时间（缓存）、空间（令牌）、深度（层）三个互补维度上做出**联合最优决策**。
    - **优势**：实现了**1.79倍**的加速，并将FLOPs降至密集基线的**29.4%**，同时任务成功率相当甚至略有提升。这种协同优化带来了远超单一维度优化的效率提升。

### 3. **提出了动作引导的自蒸馏训练方案，以保持密集策略的行为**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：动态计算方法的训练常依赖于强化学习或复杂的辅助损失，可能不稳定，或难以保证稀疏化执行后的输出与原始密集策略的一致性。
    - **本文方法**：采用**教师-学生蒸馏框架**。教师是执行密集计算的原始VLA策略，学生是执行由路由器控制的稀疏计算（含缓存、剪枝、跳层）的AC²-VLA。蒸馏损失不仅匹配最终的动作输出，还匹配骨干网络输出的中间表征。
- **解决的具体问题/带来的优势**：
    - **解决了“结构化稀疏化可能破坏策略鲁棒性”的问题**。通过动作引导的蒸馏，强制稀疏化执行的学生模型在**行为上模仿**密集的教师模型。
    - **优势**：
        1.  **保持了策略的原始性能**，确保了高效推理下的任务成功率。
        2.  所学到的路由策略具有**良好的跨任务和跨设置迁移能力**。
        3.  训练稳定，仅需优化轻量级路由模块，预训练好的VLA骨干参数保持冻结。

### 4. **实现了从统一门控到实际加速的完整、可部署的推理流程**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：一些动态计算方法可能只停留在理论节省FLOPs层面，或由于实现开销（如动态 masking 带来的额外成本）无法转化为实际的端到端延迟降低。
    - **本文方法**：论文详细设计了将路由门控转化为实际速度提升的工程实现：
        - **缓存复用**：设计了结合动作增量代理和轻量视觉哈希的鲁棒缓存键，仅在路由器请求复用且缓存命中时跳过昂贵的前向传播。
        - **令牌剪枝**：不仅进行注意力Mask，还进行了**物理上的令牌压缩**，缩短Transformer序列长度，直接降低计算量。
        - **层跳过**：通过动态分组执行，将跳过层的样本完全绕过计算。
- **解决的具体问题/带来的优势**：
    - **解决了“理论计算节省无法有效转化为实际延迟降低”的部署瓶颈**。
    - **优势**：报告了**真实的端到端墙钟时间加速**（1.79倍），而不仅仅是FLOPs的减少，使其更适合对实时性要求高的闭环机器人控制。

---

### **总结：核心创新价值**
本文最根本的创新在于**视角的转变**：从“基于视觉内容决定计算量”转变为“**基于任务执行状态（动作上下文）决定计算量**”。这更符合具身AI的决策本质。通过一个**统一、轻量、可学习**的路由器架构，协同管理了计算冗余的三个主要维度，并辅以有效的蒸馏训练，最终在保持甚至提升机器人操控性能的同时，实现了显著的推理加速，为大规模VLA模型在实时机器人系统中的部署提供了切实可行的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验设置与评估指标

#### 1. 数据集
- **主要数据集**：**Open X-Embodiment** 的大规模机器人数据集，具体使用其 **Bridge 子集** 进行模型训练。
- **评估基准**：**SIMPLER**，一个旨在缩小现实与仿真差距的高保真机器人操作仿真基准。
  - **Google Robot**：评估在视觉匹配的真实世界条件下的泛化能力。
  - **WidowX**：评估精细操作任务。

#### 2. 评价指标
- **核心指标**：
  - **任务成功率**：在多个具体任务上的平均成功率。
  - **计算效率**：
    - **FLOPs**：相对于密集基线的计算量百分比。
    - **加速比**：相对于密集基线的实际推理速度提升倍数。
- **辅助分析**：通过消融实验分析各组件（缓存重用、令牌剪枝、层跳过）的贡献，并探索超参数（如令牌保留率、缓存阈值）的敏感性。

### 二、 对比的基线方法

论文将 AC²-VLA 与两大类基线方法进行了对比：

1.  **通用密集 VLA 模型**：作为性能上限的对比。
    - RT-1, RT-2-X, Octo-Base/Small, OpenVLA, **CogACT**（AC²-VLA 所基于的骨干模型）。

2.  **效率导向的方法**：作为效率-性能权衡的对比。
    - **VLA-Cache**：基于时间冗余的缓存重用方法。
    - **EfficientVLA**：静态剪枝方法。
    - **MoLe-VLA**：基于混合专家（MoE）的条件层跳过方法。
    - **FastV**：轻量级剪枝基线。

### 三、 关键性能结果与结论

#### 1. 任务成功率：保持甚至超越密集基线
- **Google Robot (视觉匹配)**：AC²-VLA 取得了 **76.8%** 的平均成功率，**优于其密集骨干模型 CogACT (74.8%)**，并显著超过了其他大型模型（如 RT-2-X 的 46.3%）。在需要精确交互的任务（如 `Drawer`）上提升尤为明显（从 71.8% 提升至 80.6%）。
- **Google Robot (变体聚合)**：在更具挑战性的环境下，AC²-VLA 取得了 **61.6%** 的平均成功率，与密集 CogACT 基线（61.3%）**基本持平**，并优于其他基线。
- **WidowX (视觉匹配)**：AC²-VLA 取得了 **54.5%** 的平均成功率，**优于其骨干模型 CogACT (51.3%)**。

**结论**：AC²-VLA 在显著降低计算成本的同时，不仅没有损失性能，在多个任务上甚至**提升了任务成功率**。这表明其基于动作上下文的稀疏化策略有效过滤了视觉干扰，提升了决策的鲁棒性。

#### 2. 计算效率：实现显著加速与计算量削减
- **最佳性能配置下**：
  - **加速比**：达到 **1.79倍** 的推理速度提升。
  - **FLOPs**：降低至密集基线的 **29.4%**。
- **与效率基线对比**：AC²-VLA 在取得可比或更高成功率的同时，实现了**更优的加速比和更低的计算量**（见表3）。例如，在视觉匹配设置下，其加速比（1.79倍）和FLOPs削减（至29.4%）均优于 VLA-Cache、EfficientVLA 和 MoLe-VLA 等方法。

**结论**：AC²-VLA 通过统一协调时间、空间和深度维度的自适应计算，实现了当前最优的效率-准确性权衡。

#### 3. 消融研究与深入分析
- **组件必要性**：移除缓存重用、令牌剪枝或层跳过任一组件都会导致性能或效率下降（见表4）。三者联合使用效果最佳。
- **帕累托前沿**：通过网格搜索发现令牌保留率 `r_topk=0.4` 和执行层数 `N_lay=28` 时达到最佳权衡点（见图5）。
- **缓存重用的双重益处**：缓存重用不仅加速，其带来的**时间一致性**还能平滑决策，提升闭环控制的稳定性，在某些配置下甚至能将成功率提升超过12个百分点（见表5分析）。
- **敏感性分析**：
  - 令牌剪枝过于激进（保留率低于0.4）会导致性能崩溃。
  - 层跳过过于激进（保留层数少于24）也会导致性能显著下降。

### 四、 总结
AC²-VLA 在 **SIMPLER** 基准上进行了全面评估，使用**任务成功率**和**计算效率（加速比、FLOPs）** 作为核心指标。实验表明，该方法：
1.  **性能强劲**：在多数任务上匹配或超越其强大的密集骨干模型（CogACT）。
2.  **效率卓越**：实现了近 **1.8倍** 的加速，并将计算量削减至约 **三分之一**。
3.  **机制有效**：验证了基于**动作上下文**进行自适应计算比仅依赖视觉线索或静态压缩更为有效，其统一路由机制能智能地在时间、空间和深度维度分配算力。
4.  **具有鲁棒性**：通过动作引导的自蒸馏训练，在实现高度稀疏化的同时保持了策略的鲁棒性和泛化能力。

因此，论文给出了明确且有力的定量结果，证明了 AC²-VLA 在高效机器人操作方面的有效性和优越性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.19634v1)
- [HTML 版本](https://arxiv.org/html/2601.19634v1)
