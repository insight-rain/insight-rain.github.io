# Safe Exploration via Policy Priors

**相关性评分**: 6.0/10

**排名**: #37


---


## 基本信息

- **arXiv ID**: [2601.19612v1](https://arxiv.org/abs/2601.19612v1)
- **发布时间**: 2026-01-27T13:45:28Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros, Andreas Krause

## 关键词

safe reinforcement learning, offline Reinforcement Learning, constrained reinforcement learning

## 一句话总结

该论文提出SOOPER方法，利用保守策略先验和概率动力学模型实现安全探索，保证学习过程中的安全性并收敛到最优策略。

## 摘要

Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.

## 详细分析

## 论文《Safe Exploration via Policy Priors》详细摘要

### 1. 研究背景和动机
强化学习（RL）在现实世界（如机器人控制）中部署的关键挑战是**安全探索**：智能体必须在学习过程中，即在环境知识不完全的情况下，始终避免导致灾难性后果的行为。现有方法要么难以扩展到复杂任务，要么无法在学习全程保证安全性。本文认为，可以利用次优但保守的策略（例如从离线数据或模拟器中获得）作为**策略先验**，来同时保障安全并引导探索。

### 2. 核心方法和技术创新
本文提出了 **SOOPER** 算法，其核心创新在于：
- **核心机制**：将安全探索问题重新表述为一个**无约束的马尔可夫决策过程**。算法构建一个“规划MDP”，其轨迹在需要调用安全先验策略时**提前终止**，并将终止奖励设置为该先验策略的悲观价值估计。这种设计激励智能体学习超越先验策略的性能，同时绕过了直接求解约束MDP的复杂性。
- **双模式操作**：
    1. **在线悲观回退**：在真实环境交互中，实时跟踪累积成本，一旦预测可能违反安全约束，则**悲观地切换**到安全的先验策略。
    2. **离线乐观探索**：在学得的概率动力学模型上进行**乐观规划**，通过内在奖励（鼓励探索和扩展安全区域）来优化策略。
- **理论贡献**：在标准假设下，证明了 **SOOPER** 能够以高概率在整个学习过程中**保证安全约束满足**，并且首次为安全探索问题建立了**累积遗憾的上界**，表明其性能在学习期间渐进最优，而非仅在训练结束时。

### 3. 主要实验结果
在RWRL、SafetyGym等标准安全RL基准测试以及真实的遥控赛车硬件平台上进行了广泛评估：
- **安全性**：在所有任务中，**SOOPER** 在整个在线学习过程中**均未违反安全约束**。
- **性能**：在满足约束的算法中，**SOOPER** 的性能提升显著优于SAILR、CRPO等前沿基线。
- **适用性**：成功应用于**从模拟到现实迁移**、**从离线到在线学习**以及**基于视觉的连续控制**任务，展示了其可扩展性和实用性。

### 4. 研究意义和价值
- **理论价值**：通过将安全约束转化为带终止的无约束MDP，并提供**累积遗憾界**，为安全探索的理论分析提供了新思路。
- **实用价值**：**SOOPER** 架构与主流深度模型预测RL框架（如MBPO）兼容，易于实现和扩展，为在物理系统中安全部署RL提供了强大且实用的工具。
- **领域贡献**：推动了安全强化学习向**可证明安全、可扩展且高性能**的方向发展，是迈向可部署自主学习系统的重要一步。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文分析：Safe Exploration via Policy Priors

### **核心问题**
论文旨在解决**强化学习（RL）在物理世界部署中的安全探索难题**。具体而言，智能体必须在**整个在线学习过程中**（而不仅仅是训练结束后）始终保持安全，避免因探索未知环境而产生灾难性后果，同时还要高效地学习到接近最优的策略。

### **核心创新点**
论文提出了 **SOOPER** 算法，其核心创新在于**将次优但保守的策略作为先验**，并巧妙地结合了**乐观探索**与**悲观回退**机制，从而在保证全程安全的同时，实现了对最优策略的收敛。

1.  **利用策略先验进行安全引导与探索**：
    - 不同于以往工作仅将先验策略用作“安全备份”，SOOPER 主张先验策略也能**积极引导探索**，指向环境中更有潜力的区域。
    - 这些先验策略可以来自有限的离线数据或存在分布偏移的模拟器，它们本质上是**悲观**的，以确保在初始不确定性下是安全的。

2.  **“乐观探索，悲观回退”的双模式框架**：
    - **在线数据收集（悲观）**：在真实环境交互中，通过在线成本跟踪（Algorithm 1），实时监控累积成本。一旦预测到继续执行当前策略可能违反安全约束，就立即**切换至安全的先验策略**。这保证了在线交互的绝对安全（Theorem 1）。
    - **模拟规划与探索（乐观）**：在学得的概率动力学模型上构建一个**带终止信号的规划MDP**。如果模拟轨迹触发了先验策略（即被视为不安全），则该轨迹**提前终止**，并赋予一个基于先验策略悲观价值的终端奖励。这种设计激励智能体在模型中寻找**能安全超越先验策略**的更好轨迹。

3.  **统一目标函数实现探索-利用-扩展的平衡**：
    - 通过一个**单一的内在奖励目标**，将鼓励探索和鼓励扩展安全区域这两个目标统一起来。
    - 公式：`π_n = argmax_π E[ Σ γ^t * r̃(s_t, a_t) + (γ^t * λ_explore + sqrt(γ^t) * λ_expand) * ‖σ_n(s_t, a_t)‖ ]`
    - 这避免了传统“先扩展安全集，再探索利用”的两阶段方法带来的效率低下问题，使得**安全集的扩展在任务学习过程中隐式地发生**。

4.  **理论贡献：新颖的累积遗憾上界**：
    - 证明了SOOPER能以高概率保证**整个学习过程中的约束满足**。
    - 更重要的是，建立了**关于累积遗憾的次线性上界**：`R(N) ≤ O(Γ_{Nlog(N)}^{7/2} * sqrt(N))`。这改进了以往许多安全探索方法只能保证最终策略的最优性（简单遗憾），而允许探索阶段性能任意差的问题。

### **解决方案的运作机制**
1.  **输入**：一个保守但安全的策略先验 `π_hat`，一个安全预算 `d`。
2.  **迭代过程**：
    a.  **安全交互**：使用当前策略 `π_n` 在真实环境中收集数据，但通过 `Algorithm 1` 进行监控，必要时切换至 `π_hat`，确保安全。
    b.  **模型更新**：用新数据更新概率动力学模型（包含均值 `μ_n` 和不确定性 `σ_n`）。
    c.  **策略优化**：在模型构建的规划MDP中，使用上述统一目标函数优化新策略 `π_{n+1}`。内在奖励鼓励访问模型不确定性的区域，既为了探索（找到高奖励），也为了扩展安全集。
3.  **输出**：学习到的策略在保持全程安全的同时，性能逐渐逼近最优。

### **实际价值与技术优势**
- **可扩展性与实用性**：算法基于深度模型（如集成神经网络）和标准深度RL方法（如MBPO），能处理连续状态-动作空间和高维任务（如图像输入）。
- **强安全保证**：理论证明与大量实验（模拟基准、视觉控制、离线到在线、真实硬件）均验证了其**在整个学习过程中维持安全的能力**。
- **卓越性能**：在多个基准测试中，SOOPER在满足安全约束的前提下，性能显著优于SAILR、CRPO、Primal-Dual等前沿基线。
- **灵活性**：能够无缝地利用来自**模拟器**或**离线数据**的保守策略作为起点，适用于sim-to-real和离线强化学习等关键场景。

**总结**：SOOPER的核心创新在于通过策略先验和双模式设计，将安全约束问题转化为一个带特殊终止信号的无约束MDP优化问题，从而**同时实现了理论上的安全保证、次线性累积遗憾以及实际部署中的高效与鲁棒性**，为安全强化学习的实际应用迈出了重要一步。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对强化学习（RL）在物理世界部署中的**安全探索**这一核心挑战，提出了一种名为**SOOPER**的模型化RL算法。该方法的核心创新在于**利用一个保守但安全的策略作为先验**，通过构建一个概率化的世界模型，在在线交互中**乐观地探索**以最大化信息获取，同时**悲观地回退**到先验策略以确保每一步的安全性。其理论贡献在于首次为这类方法提供了**学习过程中的累积遗憾上界**，保证了性能在整个学习期间而不仅仅是最终收敛时的渐进最优。大量实验（包括仿真基准和真实硬件）表明，SOOPER在保证全程安全的同时，其性能显著优于现有最先进方法，成功实现了从离线数据或存在分布偏移的仿真器中进行安全且高效的策略迁移与在线优化。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Safe Exploration via Policy Priors》的创新点分析

这篇论文提出了 **SOOPER** 算法，在安全强化学习领域做出了多项重要创新。以下是其相对于已有工作的明确创新点，逐条列出并分析其改进之处和带来的优势。

---

### 1. **将策略先验（Policy Priors）同时用于安全保证和探索引导**
- **改进/不同之处**：
    - **以往方法**：大多数安全探索工作将先验知识（如离线数据或仿真策略）**仅用于维持安全性**（例如作为“备份策略”），或将其视为一个独立的、保守的起点。探索部分通常与先验解耦，可能导致探索效率低下。
    - **SOOPER 的做法**：论文明确提出，**次优但保守的策略先验**不仅可以悲观地调用以保证安全，还可以**乐观地用于引导探索**，驱动智能体发现优于先验策略的新行为。这是通过设计一个特殊的“规划MDP”来实现的，该MDP在模拟中遇到潜在不安全状态时会**终止**，并将终止奖励设置为先验策略的悲观价值。
- **解决的具体问题/带来的优势**：
    - **解决了探索与安全利用之间的割裂问题**。传统方法中，先验策略往往被视为探索的“限制”或“安全网”，而SOOPER将其转化为**探索的引导信号**。
    - **提高了探索效率**：通过激励智能体在模拟中寻找避免触发先验策略（即避免过早终止）的轨迹，算法能更主动地探索那些可能带来更高回报且安全的区域，从而加速学习。
    - **理论优势**：这种统一的设计使得将约束MDP问题转化为无约束MDP问题进行理论分析成为可能，绕过了直接求解CMDP的复杂性。

### 2. **提出新颖的“乐观探索-悲观回退”框架，并首次为此框架提供了累积遗憾（Cumulative Regret）边界**
- **改进/不同之处**：
    - **以往方法**：许多提供安全保证的理论工作（如基于安全贝叶斯优化或Lyapunov的方法）难以扩展到复杂任务。而一些可扩展的方法（如基于优化的CMDP求解器）往往**缺乏对探索过程的优化**，或者只提供**简单遗憾**的保证。简单遗憾只保证最终策略的最优性，但**允许学习过程中的性能任意差**。
    - **SOOPER 的做法**：算法在**在线部署时悲观地**监控成本，必要时回退到安全先验策略；在**模型模拟中乐观地**规划，通过内在奖励（探索和扩展奖励）主动收集信息。最关键的是，论文**首次为该框架建立了一个关于累积遗憾的次线性上界**。
- **解决的具体问题/带来的优势**：
    - **解决了“探索期间性能无保证”的问题**。累积遗憾边界意味着算法在整个学习过程中的**长期性能损失是有界的且增长缓慢**，而不仅仅是最终策略接近最优。这提供了更强的性能保证，对实际应用至关重要。
    - **统一了探索、利用和扩展**：不同于传统的“先扩展安全集，再探索利用”的两阶段方法，SOOPER通过单一目标函数隐式地同时进行这三者，避免了第一阶段纯粹探索造成的资源浪费。

### 3. **设计了一个基于在线成本跟踪的安全回退机制，并提供了严格的安全保证**
- **改进/不同之处**：
    - **以往方法**：安全机制多种多样，如基于Lyapunov函数、安全滤波器、或简单的紧急停止。一些方法（如SAILR）也使用备份策略和终止机制，但其终止奖励是超参数，缺乏与真实回报的一致性理论依据。
    - **SOOPER 的做法**：提出了 **`Algorithm 1`**，在线实时跟踪已累积的成本 `c_{<t}`，并利用学习到的悲观成本价值函数 `Q_{c,n}^{π̂}` 来预测未来成本。当预测总成本将超过安全预算 `d` 时，立即切换到安全先验策略 `π̂`。其终止奖励被设置为先验策略在该状态下的**悲观价值** `V̲_r^{π̂}(s_t)`，这保证了价值估计的一致性。
- **解决的具体问题/带来的优势**：
    - **提供了高概率的全程安全保证**（`Theorem 1`）。只要先验策略是安全的且动力学模型校准良好，该机制能确保每一个回合都满足约束。
    - **机制更精细和可证明**：在线成本跟踪比简单的优势过滤或固定阈值更适应序列决策问题。与先验策略价值挂钩的终止奖励设计，使得模拟环境中的学习目标与真实环境中的回报结构保持一致，促进了策略的稳定改进。

### 4. **实现了高度的可扩展性和实用性，并验证了从仿真/离线到在线再到真实硬件的全链条**
- **改进/不同之处**：
    - **以往方法**：具有强理论保证的方法往往难以扩展到高维连续状态-动作空间。而可扩展的深度RL方法又常常缺乏学习过程的安全保证。
    - **SOOPER 的做法**：论文的实践实现紧密遵循理论算法，通过以下方式实现可扩展性：
        1.  使用**神经网络集成**作为概率世界模型来估计动力学和认知不确定性。
        2.  采用**模型为基础的演员-评论家架构**（类似MBPO），在构建的规划MDP `ℳ̃` 中生成模拟轨迹进行策略优化。
        3.  将安全回退机制无缝集成到数据收集环节。
    - 此外，论文在**RWRL、SafetyGym、RaceCar**等多个基准上进行了广泛测试，并进一步在**真实的高速遥控赛车硬件**上进行了验证，涵盖了从**仿真训练**、**离线训练**到**在线学习**的多种先验策略来源。
- **解决的具体问题/带来的优势**：
    - **架起了理论与实践的桥梁**。证明了具有理论保证的算法可以处理复杂的视觉控制任务和高维连续控制。
    - **提供了完整的实证验证**：不仅是在模拟环境中超越基线，更重要的是在存在动力学失配、延迟和噪声的**真实物理系统**上成功实现了安全在线学习，极大地增强了该方法的可信度和实用价值。
    - **算法易于集成**：作者指出，由于只需将模型预测包装在 `ℳ̃` MDP中，SOOPER可以轻松适配未来更先进的深度RL模型。

---

### 总结
**SOOPER** 的核心创新在于**系统性地整合了策略先验的悲观和乐观两面性**，形成了一个既安全又高效的探索框架。其理论上的突破在于**为安全在线学习提供了累积遗憾边界**，而其实践上的成功体现在**可扩展的实现和对真实硬件系统的有效验证**。这些创新共同推动了安全强化学习向更实用、更可靠的自主学习系统迈进。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

该论文通过广泛的实验评估了 **SOOPER** 算法在安全探索任务中的有效性，涵盖了模拟基准测试和真实硬件部署。实验表明，SOOPER 在保证整个学习过程安全的同时，能够显著提升策略性能，并优于现有的最先进方法。

### 1. 使用的数据集与任务环境

实验在多个具有挑战性的连续控制任务上进行，主要分为三类：

- **RWRL 基准任务** (`Dulac-Arnold et al., 2021`)：
    - `CartpoleSwingup`：控制倒立摆摆动到直立位置，同时避免超出安全区域。
    - `WalkerWalk`：控制双足机器人行走。
    - `HumanoidWalk`：控制人形机器人行走。
    - **安全约束**：关节位置/速度限制、滑块位置限制。

- **SafetyGym 任务** (`Ray et al., 2019`)：
    - `PointGoal1` 和 `PointGoal2`：一个点状机器人需要导航至目标位置，同时避免与花瓶碰撞或进入危险区域。两个任务在障碍物数量上有所不同。
    - **安全约束**：碰撞检测和危险区域侵入。

- **RaceCar 任务** (`Kabzan et al., 2020`)：
    - 控制一辆遥控赛车绕过障碍物到达目标位置。
    - **安全约束**：与障碍物碰撞（根据动能加权）以及驶出有效区域。

- **真实硬件实验**：
    - 在一辆高动态的遥控赛车（60 Hz 控制频率）上部署，任务同样是避障到达目标点。这引入了由高频控制、执行和运动捕捉延迟带来的随机性。

### 2. 评价指标

论文主要使用以下两个核心指标来评估算法：

1.  **累积奖励**：衡量策略在任务上的性能。通常报告整个训练过程中的平均未折扣累积奖励 `J_r(π)` 或其在训练结束时的值。
2.  **累积成本/约束违反**：衡量策略的安全性。关键指标是 **在整个学习过程中记录到的最大累积成本**。SOOPER 的理论保证要求在所有训练回合 `n` 中 `J_c(π_n) ≤ d`（安全预算）。实验通过绘制成本曲线并报告是否始终低于阈值 `d` 来验证这一点。
3.  **其他辅助指标**：
    - **相对于先验策略的性能提升**：训练结束后，最终策略性能相对于初始保守先验策略的改进百分比。
    - **先验策略调用次数**：在学习过程中，算法出于安全考虑切换回先验策略 `π̂` 的频率。成功的算法应随着模型不确定性降低而减少调用。

### 3. 对比的基线方法

论文将 SOOPER 与以下几类代表性基线方法进行了对比：

- **具有安全保证的先进RL算法**：
    - **SAILR** (`Wagener et al., 2021`)：一种最先进的安全RL算法，同样使用“备份”策略和基于优势的干预机制。它与 SOOPER 在利用先验策略方面有相似之处，但理论保证和实现机制不同。

- **标准的CMDP求解器（无全程安全保证）**：
    - **CRPO** (`Xu et al., 2020`)：一种基于约束策略优化的CMDP求解器，旨在优化性能的同时满足约束，但**不保证**训练过程中的每一步都安全。
    - **Primal-Dual** (`Bertsekas, 2016`)：一种常用的约束优化算法，常被用作CMDP求解的基准。
    - **Log Barriers** (`Usmanova et al., 2024`)：一种理论上能保证所有优化迭代都可行的CMDP求解器。

- **自身变体（用于消融实验）**：
    - **SOOPER (无终止信号)**：在规划MDP `ℳ̃` 中不移除终止状态 `s†`，以验证终止机制对引导策略超越先验的重要性。
    - **SafeCRPO**：将 SOOPER 的安全回退机制（算法1）与 CRPO 结合，以展示 SOOPER 框架的模块化能力及自身优化目标的优势。

### 4. 关键性能提升与结论

#### **主要结论 1：安全且高性能的迁移学习**
- **设置**：在存在动力学模型失配（sim-to-sim 或 sim-to-real gap）的情况下，使用在扰动动力学下训练的保守先验策略进行在线微调。
- **结果**：
    - **安全性**：在所有任务中，**SOOPER 是唯一一个在全部实验中都始终保持约束满足（成本不超预算）的算法**。例如，在图3中，SOOPER 在所有任务上的“最大约束”柱状图均未超出安全线。
    - **性能**：在所有满足约束的基线方法中，SOOPER 的性能提升（相对于先验策略）是最高的或与之相当。例如，在 `PointGoal2` 和 `RaceCar` 任务中，SOOPER 的性能提升显著高于 SAILR 和 CRPO。
    - **结论**：SOOPER 能够安全地利用次优但保守的先验策略，在真实的、不确定的动力学下进行探索，并找到更优的策略，实现安全迁移。

#### **主要结论 2：从离线数据到在线学习的安全衔接**
- **设置**：使用仅从离线数据（2M 个转移样本）训练的悲观策略初始化，然后进行在线学习。
- **结果**：如图5(a)所示，在 `PointGoal1` 任务中，SOOPER 在在线学习阶段不仅始终保持安全，而且性能快速提升并显著超越所有基线。CRPO 和 Primal-Dual 虽然最终性能尚可，但在学习过程中多次违反安全约束。
- **结论**：SOOPER 为安全的离线到在线强化学习提供了一个有效的框架。

#### **主要结论 3：可扩展至视觉控制与真实硬件**
- **视觉控制**：在 `CartpoleSwingup` 的视觉输入版本（64x64 灰度图像堆叠）上，SOOPER 在存在动力学失配的情况下，同样能保证安全并达到接近最优的性能（图5(b)）。
- **真实硬件部署**：在真实的遥控赛车上（图6），SOOPER 经过在线学习后，获得的策略性能约为先验策略的两倍，并且**在整个学习过程中严格满足安全约束**。这强有力地证明了其理论在实际复杂、随机环境中的有效性。

#### **主要结论 4：优于现有最先进方法**
- 与最直接的竞争对手 **SAILR** 相比，SOOPER 在大多数任务上取得了更高的最终性能，同时保持了同等或更好的安全性。论文指出，SAILR 的简单遗憾保证依赖于重置概率随时间衰减的假设，而 SOOPER 提供了更坚实的**累积遗憾**上界，保证了学习过程中的性能。
- 与 **CRPO**、**Primal-Dual** 等标准求解器相比，SOOPER 的核心优势在于**理论保证的全程安全性**。这些基线方法常常在探索阶段严重违反约束（见图3、图17），这在物理系统中是不可接受的。

#### **主要结论 5：算法组件的有效性（通过消融实验）**
- **终止机制的重要性**：移除规划MDP中的终止状态会导致智能体无法学习到减少对先验策略的依赖，从而最终性能更低，成本更高（图9）。
- **先验策略的引导作用**：与使用简单“急停”备份策略相比，使用任务相关的、经过训练的保守先验策略能极大加速学习（图8）。
- **悲观程度与乐观探索的平衡**：实验表明，适度的悲观参数 `λ_pessimism` 能确保安全，而对内在奖励（`λ_explore`, `λ_expand`）的适当缩放能有效平衡探索与利用，最小化累积遗憾（图12，图13）。

### 总结
论文通过系统性的实验，在多个标准基准和真实机器人平台上定量地证明了 **SOOPER 算法能够实现：1) 在整个在线学习过程中以高概率保证安全约束不被违反；2) 从保守的先验策略出发，安全地探索并收敛到接近最优的策略，其性能显著优于先验策略和现有的安全RL基线；3) 其理论保证（安全性、次线性累积遗憾）在实践中得到了验证。** 这些结果标志着在实现可部署的、安全的强化学习系统方面迈出了重要一步。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.19612v1)
- [HTML 版本](https://arxiv.org/html/2601.19612v1)
