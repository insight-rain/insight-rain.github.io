# PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation

**相关性评分**: 8.0/10

**排名**: #8


---


## 基本信息

- **arXiv ID**: [2601.17885v1](https://arxiv.org/abs/2601.17885v1)
- **发布时间**: 2026-01-25T15:29:32Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen, Xiao-xiao Long, Yinghao Cai, Tao Lu, Shuo Wang, Xun Cao

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

PEAfowl是一种感知增强的多视角视觉-语言-动作模型，通过深度预测和几何感知表示提升双臂操作在遮挡和场景变化下的泛化能力。

## 摘要

Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

## 详细分析

## 论文摘要：PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation

### 1. 研究背景和动机
在杂乱场景中进行**双臂协同操作**，要求策略能够在遮挡、视角和场景变化下保持稳定。现有的**视觉-语言-动作模型**通常泛化能力不足，原因有二：一是多视图特征通过**视图无关的令牌拼接**进行融合，导致对三维空间的理解较弱且不一致；二是语言指令通常作为**全局条件**注入，导致对指令的**细粒度定位**能力不足。因此，本文旨在开发一种能够增强空间感知和指令理解能力的多视图VLA策略，以应对双臂操作中的复杂挑战。

### 2. 核心方法和技术创新
本文提出了**PEAfowl**模型，其核心创新在于两个方面：
- **几何引导的多视图融合**：模型为每个视觉令牌预测**深度分布**，通过可微分的三维投影，在共享的机器人基坐标系中聚合跨视图的局部邻域信息，从而构建**几何基础扎实、跨视图一致**的空间表征。为了克服商用深度传感器的噪声问题，模型在训练时引入了**仅用于训练的深度蒸馏**，利用预训练的深度模型作为教师来监督深度分布预测头，为感知前端注入几何先验。
- **语言引导的多视图读出**：摒弃了全局语言条件注入，提出了一种**感知器风格**的文本感知读出机制。文本令牌作为潜在查询，迭代地对冻结的CLIP视觉特征进行交叉注意力计算，从而积累与指令相关的视觉证据，生成紧凑的**指令条件化视觉令牌**，实现了更精准的指令定位。

### 3. 主要实验结果
在**RoboTwin 2.0**仿真基准测试中，PEAfowl在强域随机化设置下，将最强基线（SEM）的成功率提升了**23.0个百分点**。在**真实机器人**实验中，PEAfowl也展现了可靠的**仿真到现实迁移**能力，深度蒸馏技术带来了持续的性能提升。消融实验证实了几何融合模块和语言读出模块各自的关键作用，尤其是在长时程、遮挡严重的任务上优势显著。

### 4. 研究意义和价值
PEAfowl通过**显式建模三维几何一致性**和**迭代式文本-视觉交互**，显著提升了双臂操作策略在复杂、动态环境中的**鲁棒性**和**泛化能力**。该方法为解决机器人操作中感知与决策的耦合问题提供了新思路，其**轻量级、无需额外推理开销**的深度蒸馏方案，对在现实世界中部署基于学习的机器人策略具有重要的实用价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：PEAfowl

### **一、 研究问题**
论文旨在解决**复杂场景下双臂机器人操作**的泛化难题。现有视觉-语言-动作模型在以下两方面存在局限：
1.  **空间理解弱**：多视角特征通常通过简单的、与视角无关的令牌拼接进行融合，缺乏**三维一致的空间理解**，导致模型对遮挡、视角变化和场景变化敏感。
2.  **指令接地粗糙**：语言指令通常作为全局条件注入，与视觉特征的交互不充分，导致在**多物体、高杂波场景**中，模型难以精准定位与指令相关的物体和空间关系。

### **二、 核心创新点**
PEAfowl 提出了一个感知增强的多视角 VLA 策略，其创新主要体现在两个核心模块上：

#### **1. 几何引导的多视角融合**
- **问题**：传统方法简单拼接多视角特征，忽略了它们之间的几何对应关系。
- **解决方案**：
    - **预测逐令牌深度分布**：为每个视觉令牌预测一个离散的深度概率分布，以建模深度不确定性。
    - **可微分三维提升**：利用预测的深度分布，将2D图像令牌“软性”地反投影到共享的机器人基坐标系中，生成3D锚点。
    - **跨视角三维邻域聚合**：在3D空间中，为每个查询令牌查找其在不同视角下的最近邻令牌，并根据几何距离进行加权特征聚合。这使得对应同一物理区域的跨视角特征能够对齐和增强。
    - **仅训练时深度蒸馏**：使用预训练的深度模型作为“教师”，在训练时监督深度分布预测头，为感知前端注入几何先验，而**推理时无需额外计算开销**，直接使用原始深度图。

#### **2. 语言引导的多视角读出**
- **问题**：全局语言条件化无法在复杂场景中聚焦于指令相关的视觉证据。
- **解决方案**：
    - **感知器式文本作为查询的读出机制**：将文本令牌作为一组潜在的查询向量，让它们迭代地对**冻结的CLIP视觉特征**进行交叉注意力计算。
    - **迭代证据积累**：通过多轮交叉注意力，文本查询能够逐步从多视角图像中提取和聚合与指令最相关的视觉信息。
    - **生成紧凑的指令条件化令牌**：最终输出一组紧凑的、富含视觉-语言对齐信息的上下文令牌，用于指导策略学习。

### **三、 技术实现路径**
1.  **输入**：多视角RGB-D图像、本体感知状态、自然语言指令。
2.  **处理流程**：
    - **几何流**：RGB和深度特征分别编码 → 令牌化 → 预测深度分布 → 3D提升 → 跨视角3D邻域聚合 → 生成**几何增强的视觉令牌**。
    - **语言流**：文本指令和RGB图像输入冻结的CLIP模型 → 文本令牌作为查询，迭代地从CLIP视觉特征中读出信息 → 生成**语言引导的上下文令牌**。
3.  **策略解码**：将几何增强令牌、语言上下文令牌和编码后的机器人状态一起，输入一个**以关节为中心的扩散动作解码器**，预测未来一段时间的双臂协同动作轨迹。
4.  **训练目标**：结合扩散模仿损失、前向运动学一致性损失和深度蒸馏损失进行联合优化。

### **四、 实际价值与效果**
- **性能提升显著**：在RoboTwin 2.0仿真基准测试中，PEAfowl在强域随机化设置下的平均成功率比最强基线（SEM）**高出23.0个百分点**。在真实机器人实验中，也表现出更可靠的**仿真到现实迁移能力**。
- **解决关键痛点**：
    - **提升对遮挡和视角变化的鲁棒性**：通过3D一致的跨视角融合，即使某个视角被遮挡，也能从其他视角聚合信息。
    - **增强复杂指令的遵循能力**：通过文本查询机制，能更精准地关联语言指令与场景中的特定物体和关系。
    - **兼容廉价传感器**：深度蒸馏方案有效缓解了商用深度传感器噪声大、数据不完整的问题，提升了在真实场景中的实用性。
- **轻量高效**：模型仅包含**3亿可训练参数**，并通过冻结CLIP、仅在训练时使用深度教师模型等方式，保持了较高的计算效率。

**总结**：PEAfowl 通过**显式地建模多视角几何一致性**和**深化语言-视觉的交互机制**，有效提升了双臂操作策略在复杂、动态现实场景中的感知能力、理解能力和泛化性能，为迈向开放世界的机器人操作提供了重要的技术思路。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**双手机器人在杂乱场景中执行多任务操作时，现有视觉-语言-动作模型泛化能力不足**的核心问题，提出了**PEAfowl**框架。该框架通过两个关键技术进行改进：一是**几何引导的多视图融合模块**，通过预测每个视觉令牌的深度分布并进行可微三维提升与跨视图邻居聚合，构建了三维一致的空间感知表征；二是**语言引导的多视图读出机制**，采用感知器风格、以文本为查询的迭代注意力，从冻结的CLIP特征中提取与指令紧密相关的视觉证据。最终，在仿真和真实机器人实验中，该方法显著提升了任务成功率，特别是在存在遮挡和场景变化的复杂长时程任务上，证明了其**在提升空间理解与指令落地精度方面的有效性和优越的仿真到现实迁移能力**。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## PEAfowl 论文创新点分析

这篇论文针对双手机器人操作中的视觉-语言-动作模型，提出了一个名为PEAfowl的新框架。其核心创新点在于改进了多视角感知的几何一致性和语言指令的细粒度接地能力。以下是其相对于已有工作的明确创新点：

### 1. 几何引导的多视角融合模块
- **改进/不同之处**： 与以往将多视角图像特征简单拼接或堆叠的“视角无关”方法不同，PEAfowl引入了显式的几何建模。它通过预测每个视觉令牌的深度分布，进行可微分的3D提升，并在共享的基坐标系中聚合跨视角的局部3D邻居，形成几何上接地的表示。
- **解决的问题/优势**： 解决了现有VLA模型在多视角下缺乏3D空间一致性的问题。这种方法使模型对遮挡、视角变化和相机标定误差更加鲁棒，从而在杂乱场景中提供了更稳定的空间理解。实验表明，该方法显著提升了长时程和遮挡严重任务的成功率。

### 2. 基于Perceiver架构的“文本作为查询”的视觉特征读出机制
- **改进/不同之处**： 取代了常见的将语言作为全局条件向量注入的方法。PEAfowl使用文本令牌作为潜在查询，通过一个类似Perceiver的迭代交叉注意力机制，从冻结的CLIP视觉特征中提取和聚合与指令相关的视觉证据，生成紧凑的、语言条件化的视觉令牌。
- **解决的问题/优势**： 解决了全局语言条件化在复杂多物体场景中注意力分散、指令接地能力弱的问题。这种机制能够更精确地聚焦于与任务相关的物体和空间关系，提高了模型对细粒度语言指令的理解和执行能力，尤其在涉及属性（如颜色、大小）和空间参照的任务上表现更佳。

### 3. 仅用于训练的深度蒸馏方案
- **改进/不同之处**： 为了应对实际部署中商品化深度传感器噪声大、数据不完整的问题，PEAfowl在训练阶段引入了一个预训练的深度模型作为“教师”，来监督其几何模块中的深度分布预测头。在推理时，模型仅使用原始的深度图像输入，不增加任何计算开销。
- **解决的问题/优势**： 解决了在真实机器人部署中，直接使用噪声深度数据会损害几何感知收益的难题。该方案将更精确的几何先验知识“蒸馏”到策略中，提升了模型在真实世界中对深度的感知质量，实现了更可靠的仿真到现实迁移，且没有额外的推理成本。

### 4. 局部成对的RGB-D令牌融合策略
- **改进/不同之处**： 并非简单地将RGB和深度特征在全局范围内融合。PEAfowl设计了一个轻量级的注意力模块，仅对空间上共位的RGB-D令牌对进行局部融合，且该融合主要用于深度分布预测分支。
- **解决的问题/优势**： 解决了深度噪声可能污染预训练RGB语义特征的问题。这种局部、受限的融合方式能够在利用深度信息的同时，保护主干RGB特征的语义完整性，使得几何感知模块的学习更加稳定。消融实验表明，这有助于防止深度损失在训练中发散。

### 总结
PEAfowl通过上述四个紧密耦合的创新，系统性地提升了双手机器人操作策略在**空间感知的鲁棒性**和**语言指令的精准性**两个关键维度的性能。其实验结果（在仿真中比最强基线提升23.0个百分点，并在真实机器人上展示了可靠的迁移能力）验证了这些创新点共同作用，有效解决了杂乱、多变场景下双手机器人操作泛化能力不足的核心挑战。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过仿真和真实机器人实验，全面评估了PEAfowl模型在双臂操作任务上的性能，证明了其在感知增强和指令理解方面的显著优势。

### 一、 数据集与评价指标

#### 1. 数据集
- **仿真数据集**：使用 **RoboTwin 2.0** 基准测试平台。包含9个双臂操作任务，涵盖短、中、长时域操作。每个任务包含50条演示数据。
- **真实世界数据集**：在双AgileX Piper机器人平台上，通过VR遥操作收集了6个任务（4个仿真任务改编，2个真实特有任务）的演示数据，每个任务100条。

#### 2. 评价指标
- **核心指标**：**成功率**。在仿真中，每个任务/设置运行100次试验；在真实机器人上，每个任务运行10次试验。
- **评估设置**：
    - **Clean Setting**：固定的视觉和物理参数，用于评估模型在分布内任务上的能力。
    - **Domain-Randomized (DR) Setting**：应用了强烈的结构化扰动（如背景、光照、纹理、桌面高度、指令改写、干扰物），用于压力测试模型的鲁棒性和跨场景泛化能力。

### 二、 基线方法对比

论文与两类基线方法进行了对比：

1.  **单任务视觉运动策略**：在**每个任务上单独训练**。
    - **ACT**：基于动作分块变换器的策略。
    - **DP**：扩散策略。
    - **DP3**：3D扩散策略。

2.  **多任务双臂VLA策略**：在**所有任务上联合训练**。
    - **π₀**：一个大规模预训练的视觉-语言-动作流程模型。
    - **RDT**：一个用于双臂操作的扩散基础模型。
    - **SEM**：通过空间嵌入增强理解的VLA模型，是仿真中最强的基线。

### 三、 关键性能结果与结论

#### 1. 仿真结果（RoboTwin 2.0）
- **总体性能**：在最具挑战性的**DR设置**下，PEAfowl在9个任务上的**平均成功率达到47.1%**，显著优于所有基线。
- **与最强基线对比**：PEAfowl比**SEM（24.1%）提升了23.0个百分点**。在**Clean设置**下，PEAfowl（69.6%）也比SEM（51.0%）提升了18.6个百分点。
- **任务类型分析**：
    - **长时域任务**：提升最为显著（DR下从16.4%提升至39.0%），证明了模型在持续遮挡和视角变化下维持空间感知的能力。
    - **指令理解任务**：在需要区分视觉相似但指令不同的任务（如`Stack Blocks Three` vs `Blocks Ranking RGB`）上，PEAfowl表现更稳定，表明其指令落地能力更强。
- **泛化到未见任务**：在两个未参与训练的额外任务（`Stack Blocks Two`, `Stack Bowls Two`）上，PEAfowl在DR设置下的成功率（51%, 82%）远超SEM（14%, 65%），证明了其优秀的跨任务泛化能力。

#### 2. 真实机器人结果
- **总体性能**：PEAfowl在6个真实任务上的**平均成功率为68.3%**，远超SEM的11.7%。
- **深度蒸馏的贡献**：消融实验（`PEAfowl w/o DD`）显示，移除训练时的深度蒸馏会使平均成功率从68.3%降至36.7%，尤其在依赖精确深度的任务（如`Put Bottles Dustbin`）上性能下降明显。这证明了**深度蒸馏对于处理真实世界噪声深度数据至关重要**。
- **定性观察**：PEAfowl表现出更可靠的仿真到现实迁移能力，在遮挡频繁、需要双手协调的任务上尤为稳定。

#### 3. 核心结论
1.  **几何引导的多视角融合有效**：通过预测逐令牌深度分布、可微3D提升和跨视角邻居聚合，PEAfowl建立了**3D一致的空间表征**。可视化（图5）显示，其令牌在聚合后能更准确地对应同一物理区域。
2.  **语言引导的读出机制有效**：使用Perceiver风格的“文本作为查询”的读出机制，替代全局文本条件化，实现了**更精准的指令落地**。消融实验（表4）表明，移除此模块或简化其设计会导致性能下降，尤其在DR设置下。
3.  **整体架构实现了SOTA性能**：尽管仅有**3亿可训练参数**，PEAfowl在仿真和真实世界的双臂操作任务上均取得了**最先进的成功率**，并展现出优秀的跨场景、跨任务泛化能力和仿真到现实迁移能力。

### 四、 消融分析与深入洞察
- **组件重要性**：消融研究（表4）证实了**几何引导模块（GGMVF）** 和**语言引导读出模块**都是性能提升的关键。仅添加GGMVF到一个简单基线就能稳定深度学习并提升长时域任务性能。
- **多视角必要性**：视角消融（附录表7）表明，移除前视角或仅保留头视角会显著降低性能，证明了**多视角互补信息对于应对遮挡和完成精细操作是必要的**。
- **深度预测稳定性**：PEAfowl的几何引导模块能有效**稳定深度分布的学习**（图7），而基线模型则容易出现深度损失发散，这解释了其在真实噪声深度数据上表现更优的原因。

**总结**：PEAfowl通过创新的几何感知融合和语言感知读出机制，显著提升了双臂VLA模型在杂乱、多变场景下的空间理解、指令落地和整体操作成功率，在仿真和真实环境中均验证了其有效性和先进性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.17885v1)
- [HTML 版本](https://arxiv.org/html/2601.17885v1)
