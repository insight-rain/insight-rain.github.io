# Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter

**相关性评分**: 6.0/10

**排名**: #19


---


## 基本信息

- **arXiv ID**: [2601.21297v1](https://arxiv.org/abs/2601.21297v1)
- **发布时间**: 2026-01-29T05:49:48Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Byeongjun Kim, H. Jin Kim

## 关键词

safe reinforcement learning

## 一句话总结

该论文提出了一种基于Hamilton-Jacobi可达性和无模型学习的深度QP安全过滤器，用于黑盒动态系统的安全强化学习控制。

## 摘要

We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems -- even including a hybrid system -- and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.

## 详细分析

## 论文摘要：Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter

**1. 研究背景和动机**
在机器人学和自主系统等安全关键控制领域，基于二次规划（QP）的安全滤波器因其能产生平滑控制指令而备受青睐。然而，现有方法严重依赖于精确的系统动力学模型，这在面对**黑盒系统**（模型未知或动态变化）时构成了巨大的应用障碍。尽管已有一些学习型变体，但它们通常仍需要部分模型知识。本文旨在填补这一理论与实际部署之间的鸿沟，提出一种**完全数据驱动、无需模型知识**的安全滤波器学习框架。

**2. 核心方法和技术创新**
本文提出了 **Deep QP Safety Filter**，其核心创新在于将**汉密尔顿-雅可比（HJ）可达性分析**与**无模型学习范式**相结合。
- **理论基础**：通过引入**时间折扣因子**，构建了具有收缩性质的贝尔曼算子，从而能够稳定地学习安全价值函数及其导数（即系统动力学方向导数）。
- **网络架构与学习**：训练两个神经网络分别近似安全价值函数 `V^λ(x)` 及其导数 `∂V^λ(x, u)`。设计了基于收缩性的损失函数（`ℒ_v` 和 `ℒ_∂v`），并采用**缓慢更新的目标网络**和**离线策略学习**确保稳定收敛。理论证明，在精确设定下，学习到的评论家网络会收敛到 HJ PDE 的**粘性解**，即使真实解是非光滑的。
- **安全滤波器实现**：利用学到的网络输出构造一个在线 QP 问题，将原始（可能不安全）的控制指令修正为安全指令。用户可通过一个**可调参数 α** 灵活控制滤波器的“激进”程度，在安全性和性能间取得平衡。

**3. 主要实验结果**
在多个动力学系统（包括双积分器、倒立摆、混合系统Hopper）和强化学习任务上进行了验证：
- **收敛性**：学习到的安全价值函数及其导数在不同采样间隔 `δt` 下均能**收敛到解析解**，并能捕捉非光滑特性。
- **安全性与性能**：与基线方法（如PPO、PPO-Lagrangian、基于判别超平面的方法等）相比，该方法在训练前期**显著减少了安全违规次数**，同时加速了学习过程，获得了**更高的累计回报**。
- **通用性与鲁棒性**：学到的滤波器是**任务无关的**，可与不同RL策略结合；即使在理论上不满足平滑性假设的**混合系统**上也能有效工作，展现了方法的实际鲁棒性。

**4. 研究意义和价值**
本研究为**黑盒动力系统的安全控制**提供了一条原则性强且实用的路径。其主要价值在于：
- **理论贡献**：建立了基于折扣可达性的无模型学习理论框架，并证明了其收敛性。
- **实践意义**：实现了首个完全数据驱动的 QP 安全滤波器，**无需任何先验模型知识**，降低了在真实复杂系统上部署安全控制的门槛。
- **应用潜力**：方法具有任务通用性，用户可通过简单参数调节激进程度，在确保安全的前提下最大化任务性能，为安全强化学习提供了一个强大且可靠的安全层。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Deep QP Safety Filter

### **一、 核心问题**
论文旨在解决**模型未知（黑盒）动态系统的实时安全控制**问题。现有基于二次规划（QP）的安全滤波器（Safety Filter）性能严重依赖于精确的系统模型，在模型不确定或完全未知时可靠性下降。这导致了**优雅的理论（如基于哈密顿-雅可比可达性分析的方法）与在实际未知动态系统上部署之间的巨大鸿沟**。

### **二、 核心创新点**
论文提出了一个名为 **“Deep QP Safety Filter”** 的框架，其核心创新在于：

1.  **完全数据驱动且无需模型**：首次实现了**不依赖任何先验动力学模型**的QP安全滤波器学习。与之前需要部分模型知识（如CBF结构、Lipschitz常数）或预计算安全策略的方法划清了界限。
2.  **将可达性分析与无模型学习深度融合**：
    - **理论基石**：将**时间折扣的哈密顿-雅可比（HJ）可达性**问题重新表述，定义了折扣安全值函数 `V^λ(x)` 和状态-动作安全值函数 `Q_s^λ(x, u)`。
    - **关键构造**：定义了安全“优势”函数 `q^λ(x, u)`，它量化了在特定输入下安全值的瞬时变化率。这类似于RL中的优势函数，但用于安全目标。
3.  **设计具有收缩性的贝尔曼算子**：
    - 基于理论推导，为安全值 `V^λ` 及其导数 `∂V^λ` 分别构建了贝尔曼算子 `𝒯_v` 和 `𝒯_dv`。
    - **关键定理**：证明了这两个算子在无穷范数下是 **`e^{-λ·δt}`-收缩**的。这为使用神经网络进行稳定、收敛的学习提供了**坚实的理论保证**，即使真实的安全值函数是非光滑的。
4.  **双网络架构与特定参数化**：
    - 使用两个神经网络分别近似安全值 `v^λ_θ(x)` 和安全值导数 `∂v^λ_ϕ(x, u)`。
    - 对导数网络进行**巧妙参数化**：`∂v^λ_ϕ(x, u) = a^λ_ϕ(x)u - max_{u'∈𝒰} a^λ_ϕ(x)u' + b^λ_ϕ(x)`。这使得 `max_u ∂v^λ_ϕ(x, u) = b^λ_ϕ(x)`，可以直接通过 `b^λ_ϕ(x)` 来施加HJ PDE中的最优性条件，简化了QP约束的构建。
5.  **可调攻击性的实用滤波器**：
    - 学习到的网络直接用于构建一个在线QP安全滤波器。其约束形式确保了安全值不下降（或下降速度受控）。
    - 引入用户可调的**类K函数 `α(·)`** 作为QP约束中的松弛项，从而可以**在线、直观地调节滤波器的“保守-激进”程度**，平衡安全性与任务性能。

### **三、 解决方案路径**
论文通过以下步骤实现上述创新：

1.  **问题重构**：将“保持状态约束 `c(x) ≥ 0` 永久满足”的安全问题，转化为求解**最大控制不变安全集**的HJ可达性问题。
2.  **引入时间折扣**：为解决原始HJ公式缺乏收缩性、不利于学习的问题，引入了连续时间域的折扣因子 `λ`，构建了折扣安全值函数。
3.  **构建可学习的损失函数**：
    - 利用收缩性贝尔曼算子，设计均方误差损失 `ℒ_v` 和 `ℒ_∂v`（公式10, 11），分别用于训练安全值网络和导数网络。
    - 采用**目标网络**和**经验回放**等稳定深度RL训练的技术。
4.  **在线安全滤波与数据收集**：
    - **算法1** 展示了在线学习循环：在每一步，使用当前网络参数构建并求解QP（公式12），将原始（可能不安全）输入 `u_raw` 滤波为安全输入 `u_filtered`。
    - 将执行动作后产生的状态转移数据 `(x, u, c, x^u_δt, c^u_δt)` 存入回放缓冲区，用于持续优化网络。
    - 在训练初期网络未收敛时，若QP不可行，则回退到求解一个线性规划（LP），即选择能最大程度提升估计安全值的输入。
5.  **渐进逼近精确解**：在训练过程中，**逐渐将折扣因子 `λ` 减小至接近0**，使得学到的折扣安全值函数 `v^λ_θ(x)` 收敛到原始的、未折扣的HJ可达性解 `V(x)`（粘性解）。

### **四、 实际价值与验证**
- **有效性**：在多个动力学系统（双积分器、倒立摆、倒立双摆）和RL任务中验证。学到的Critic网络能够收敛到解析解（即使解是非光滑的），作为安全滤波器能**显著减少训练过程中的失败次数**，并**加速学习以获得更高回报**。
- **泛化性与鲁棒性**：在不符合理论严格假设的**混合系统（Hopper）** 上表现依然强劲，证明了神经网络近似在实际中的有效性。
- **实用性**：滤波器是**任务无关的**，一旦训练完成，可通过调整参数 `α` 适配不同攻击性要求的任务。与RL策略协同工作时，能提供一个可靠的安全层，降低探索风险。

**总结**：本文的核心贡献是**架起了一座桥梁**，将具有强理论保证但需要模型的可达性分析，与完全数据驱动的无模型学习相结合，产生了一个**既具有理论收敛保证，又具备实际部署灵活性**的新型安全滤波器范式。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**黑盒动力系统**中安全控制的核心难题：传统基于模型的安全滤波器严重依赖精确的动力学模型，在模型未知或不确定时性能会急剧下降。为此，作者提出了 **Deep QP Safety Filter**，这是一个完全数据驱动的安全层框架。其核心创新在于**将时间折扣的汉密尔顿-雅可比可达性分析与无模型学习相结合**，通过设计具有收缩性质的贝尔曼算子，训练两个神经网络来分别逼近安全值函数及其导数，从而学习得到一个二次规划形式的安全滤波器。该方法最终实现了**无需任何模型先验知识**即可构建安全滤波器，在多种动力学系统（包括混合系统）和强化学习任务中，**显著减少了训练过程中的安全违规次数，同时加速了策略学习并获得了更高的任务回报**，在安全性与性能之间取得了优于现有基线的平衡。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter》在安全关键控制领域提出了一个完全数据驱动的安全层框架。其核心创新点在于将基于模型的Hamilton-Jacobi可达性分析与无模型学习范式相结合，以解决黑盒系统下的安全控制问题。以下是其相对于已有工作的明确创新点：

### 1. **完全无模型的安全滤波器学习框架**
- **改进/不同之处**：现有的大多数基于学习的安全方法（如基于CBF-QP或部分可达性的方法）都**依赖于部分模型知识**，例如预定义的CBF结构、Lipschitz常数或预先计算的安全策略。而本文提出的方法**完全不需要任何先验的动力学模型**，仅从离散的转移数据 `(x, u, c, x_δt^u, c_δt^u)` 中学习。
- **解决的问题与优势**：这解决了理论与实际部署之间的关键鸿沟，使得该方法能够直接应用于**动力学未知或时变的真实黑盒系统**（如复杂的机器人平台），极大地提升了方法的通用性和实用性。

### 2. **将时间折扣Hamilton-Jacobi可达性分析与无模型学习结合**
- **改进/不同之处**：论文创新性地为连续时间系统引入了**时间折扣因子λ**，定义了折扣安全值函数 `V^λ(x)` 和 `Q_s^λ(x, u)`（公式3,4）。这不同于传统的、无收缩性质的递归定义（公式1,2）。基于此，论文推导了对应的**收缩贝尔曼算子**（`𝒯_v` 和 `𝒯_dv`，公式8,9）。
- **解决的问题与优势**：
    - **解决了学习稳定性问题**：折扣因子和对应的贝尔曼算子具有 `e^{-λ·δt}`-收缩性（定理3.5, 3.6），确保了价值函数及其导数在迭代学习过程中的稳定收敛。
    - **实现了从任意策略数据中学习**：通过定义状态-动作安全值函数 `Q_s^λ`，该方法可以从非最优的、任意行为策略收集的数据中进行**离策略学习**，无需预先知道或使用最优安全策略来收集数据。

### 3. **同时学习安全值函数及其导数，并构建可微分的QP安全滤波器**
- **改进/不同之处**：现有的一些无模型安全方法可能只学习安全值或产生**开关型（bang-bang）滤波器**，导致控制指令不连续、不适合高动态系统。本文方法**同时训练两个神经网络**：一个用于近似安全值 `v_θ^λ ≈ V^λ`，另一个用于近似其导数相关的项 `a_φ^λ ≈ (∂V^λ/∂x)g` 和 `b_φ^λ ≈ max_u ∂V^λ(x,u)`。
- **解决的问题与优势**：
    - **实现平滑、持续的滤波**：利用学习到的导数信息，构建了一个**二次规划（QP）形式的安全滤波器**（公式12）。这个滤波器持续地修正原始控制输入，产生平滑的控制命令，避免了开关型滤波器带来的突变，更适合实际执行器和高动态系统。
    - **提供理论保证**：通过特定的参数化（`∂v_φ^λ(x,u) = a_φ^λ(x)u - max_u a_φ^λ(x)u + b_φ^λ(x)`），能够直接施加哈密顿-雅可比偏微分方程（HJ PDE）中的最优性条件（引理3.2），使学习目标与可达性理论一致。

### 4. **通过用户参数α实现安全性与敏捷性的可调权衡**
- **改进/不同之处**：在QP滤波器的约束条件中，引入了一个**用户定义的扩展类K函数 α(·)**（简单实现为αz）。这不同于固定保守度的滤波器或需要通过复杂机制调整保守度的学习算法。
- **解决的问题与优势**：
    - **提供直观的调节旋钮**：用户可以通过单一参数α**直接控制滤波器的“激进”程度**。α越大，允许系统更快地接近安全边界，从而在保证安全的前提下，更好地保持原始控制指令的性能（图3）。
    - **实现任务无关的通用性**：同一个训练好的安全滤波器可以**跨不同任务复用**，仅通过调整α来适应不同任务对安全性和敏捷性的不同需求。

### 5. **理论收敛性与对非光滑解及混合系统的实际鲁棒性**
- **改进/不同之处**：论文不仅提供了在精确设置下，学习到的评论家网络收敛于**粘性解（viscosity solution）** 的理论保证（这对于非光滑值函数至关重要），而且通过实验验证了该方法在**不满足理论假设的混合系统**（如Hopper机器人）上的有效性。
- **解决的问题与优势**：
    - **处理非光滑性**：哈密顿-雅可比可达性分析中产生的值函数经常是非光滑的。论文证明了其方法即使在这种情况下也能收敛。
    - **突破理论假设限制**：尽管方法基于控制仿射动力学假设，但实验表明，通过神经网络从离散数据中平滑学习，该方法能**有效处理具有不连续状态跳变的混合动力学系统**，展示了其强大的实际应用潜力。

### 总结
本文的核心创新在于**构建了一个桥梁**，将具有强理论保证但通常需要模型知识的**HJ可达性框架**，与灵活但需要稳定性保证的**无模型深度强化学习技术**相结合。其结果是一个**原理清晰、数据驱动、通用性强且性能优越的安全滤波器**，它显著减少了强化学习训练期间的失败次数，加速了学习过程，并最终能获得更高的任务回报，为实现安全的无模型控制提供了一条切实可行的路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过一系列仿真实验，系统地评估了所提出的 **Deep QP Safety Filter** 的性能。实验设计旨在验证其**理论收敛性、安全有效性、保守性调节能力以及与强化学习（RL）结合的实际价值**。

### 一、 使用的“数据集”与评价指标

论文并未使用传统意义上的静态数据集，而是通过**在线交互**或**离线仿真**的方式，在多个**动态系统环境**中生成并利用**转移数据** `(x, u, c, x^u_δt, c^u_δt)`。具体使用的仿真环境（类似于数据集）包括：

1.  **Double Integrator（双积分器）**：用于验证理论收敛性。
2.  **Inverted Pendulum（倒立摆）**：用于展示滤波器攻击性调节。
3.  **Inverted Double Pendulum-v5**：用于安全强化学习（Safe RL）性能评估。
4.  **Hopper-v5**：一个**接触驱动的混合系统**，用于验证方法在理论假设不严格满足（如非光滑动态）的复杂场景下的鲁棒性。

**核心评价指标**：
- **理论收敛性**：将学习到的安全值函数 `v^λ_θ(x)` 及其导数 `a^λ_φ(x)`, `b^λ_φ(x)` 与**解析解（Analytic Solution）**进行可视化对比，评估匹配程度。
- **安全性能**：
    - **训练前失败次数（Pre-convergence failures）**：在RL训练早期，智能体因探索导致违反安全约束的次数。**减少此类失败是核心目标之一**。
    - **约束违反**：在部署安全滤波器后，系统状态 `c(x) < 0` 的情况。
- **任务性能**：在Safe RL实验中，使用**任务回报（Return）**作为指标，衡量在保证安全的前提下完成原始任务的能力。
- **滤波器特性**：
    - **攻击性（Aggressiveness）**：通过用户参数 `α` 调节，观察其对控制指令平滑性和系统响应速度的影响。
    - **可行性（Feasibility）**：记录QP（12）求解失败（不可行）的发生频率，作为算法鲁棒性的诊断指标（例如，在Hopper上为0.2343%）。
- **保守性**：通过与其他基线方法对比，评估在相同安全水平下，所提方法允许的系统行为范围（是否过于保守）。

### 二、 对比的基线方法

论文在Safe RL实验部分与多个强基线方法进行了对比：

1.  **PPO**：标准的无安全约束的近端策略优化算法。作为**不安全基线**，预期会有大量失败。
2.  **PPO-Lagrangian**：采用拉格朗日乘子法处理约束的PPO变体，是Safe RL的常用方法。
3.  **PPO with RL-DH**：结合了**基于判别超平面的（Discriminating Hyperplane）** 模型无关安全滤波器的PPO。
4.  **PPO with Bang-Bang Safety Filter（论文自研变体）**：将本文的QP滤波器改为**开关式（Bang-Bang）滤波器**（即设置 `α → ∞`，并在 `v^λ_θ(x) < 0.2` 时切换为安全控制），用于凸显QP滤波器在**控制平滑性**上的优势。

### 三、 关键性能提升与结论

1.  **理论收敛性得到验证**：
    - **结果**：在Double Integrator系统上，对于不同的离散时间间隔 `δt`，学习到的安全值函数和导数均能**收敛到解析的HJ可达性解**，即使真实值函数是非光滑的（存在间断点）。如图2所示，学习结果与理论解高度一致。
    - **意义**：证明了所提的基于收缩贝尔曼算子的模型无关学习框架在理论上是可靠的。

2.  **安全滤波器有效且可调节**：
    - **结果**：在Inverted Pendulum中，通过调节参数 `α`，可以**连续地控制滤波器的攻击性**。`α` 越大，滤波器对原始指令的修改越小，系统响应更敏捷，但仍能保证安全（图3）。
    - **意义**：为用户提供了直观的**安全-性能权衡**调节手段，增强了方法的实用性。

3.  **显著提升Safe RL性能（核心贡献）**：
    - **减少训练期失败**：在Inverted Double Pendulum和Hopper环境中，**Deep QP Safety Filter + PPO 的组合显著减少了训练初期的安全违规次数**（图4中阴影部分宽度和高度代表失败频率和严重度，所提方法最窄）。
    - **加速学习并取得更高回报**：在存在安全约束的RL任务中，该方法不仅更安全，而且**学习速度更快，最终获得的任务回报（Return）也高于或相当于其他基线**（图4(a)(b)(c)中的学习曲线更高或相当）。
    - **超越开关式滤波器**：与自研的Bang-Bang变体相比，**QP滤波器实现了更平滑的控制和更稳定的系统行为**，验证了QP公式化在动态系统上的优势。

4.  **对复杂和混合系统具有鲁棒性**：
    - **结果**：在Hopper（一个具有不连续接触动力学的混合系统）上，该方法依然有效，且QP不可行率极低（0.2343%）。
    - **意义**：表明即使系统的真实动态不完全满足理论上的控制仿射、光滑等假设，**神经网络近似器仍能有效地学习并提供一个实用的安全滤波器**，拓展了方法的适用范围。

5.  **任务无关性**：
    - **结果**：同一个安全滤波器（仅从安全信号 `c(x)` 学习）可以用于**多个不同奖励函数的RL任务**（如Inverted Double Pendulum的两个冲突任务），并通过调整 `α` 来适配。
    - **意义**：实现了**安全模块与任务策略的解耦**，提高了安全层的通用性和可复用性。

### 总结
论文通过**系统的定量与定性实验**证明，Deep QP Safety Filter 能够：
- **无模型地**学习到理论正确的安全边界。
- 作为一个**可调节、平滑的**安全层，有效**过滤控制指令**，保证安全。
- 与RL策略结合时，能**大幅降低训练风险，同时提升或保持任务性能**。
- 其**性能在多个指标上（安全性、学习效率、最终回报）优于或匹配现有的Safe RL基线方法**，特别是在需要平滑控制的高动态系统中优势明显。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.21297v1)
- [HTML 版本](https://arxiv.org/html/2601.21297v1)
