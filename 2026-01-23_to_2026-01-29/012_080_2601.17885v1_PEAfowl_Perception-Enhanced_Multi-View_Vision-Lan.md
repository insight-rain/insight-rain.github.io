# PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation

**相关性评分**: 8.0/10

**排名**: #12


---


## 基本信息

- **arXiv ID**: [2601.17885v1](https://arxiv.org/abs/2601.17885v1)
- **发布时间**: 2026-01-25T15:29:32Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen, Xiao-xiao Long, Yinghao Cai, Tao Lu, Shuo Wang, Xun Cao

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, world model

## 一句话总结

PEAfowl是一种感知增强的多视角视觉-语言-动作模型，通过深度预测和几何感知表示提升双臂操作在遮挡和视角变化下的泛化能力。

## 摘要

Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

## 详细分析

## 论文摘要：PEAfowl：面向双手操作的感知增强多视角视觉-语言-动作模型

**1. 研究背景和动机**
在杂乱场景中进行双手操作，要求策略能够在遮挡、视角和场景变化下保持稳定。现有的视觉-语言-动作模型泛化能力不足，原因有二：(i) 多视角特征通过视角无关的令牌拼接进行融合，导致对三维空间的理解较弱且不一致；(ii) 语言通常作为全局条件注入，导致指令与视觉信息的关联（指令接地）较为粗糙。因此，本文旨在解决双手操作中多视角感知与语言指令精确理解的关键挑战。

**2. 核心方法和技术创新**
本文提出了PEAfowl模型，其核心创新在于耦合了**几何引导的多视角融合**与**语言引导的特征读出**：
*   **几何引导的多视角融合**：模型为每个视觉令牌预测深度分布，通过可微的三维提升将二维特征映射到三维空间，并在共享基坐标系中聚合跨视角的局部邻域信息，从而构建几何基础扎实、跨视角一致的空间表征。
*   **语言引导的特征读出**：摒弃全局语言条件注入，采用一种类似Perceiver的架构，将文本令牌作为潜在查询，迭代地对冻结的CLIP视觉特征进行交叉注意力计算，从而积累与指令相关的视觉证据，生成紧凑的、指令条件化的上下文令牌。
*   **深度蒸馏方案**：为克服商用深度传感器的噪声和不完整性，在训练时使用预训练的深度教师模型（Camera Depth Model）来监督深度分布预测头，为感知前端注入几何先验，且不增加推理时的计算开销。

**3. 主要实验结果**
在RoboTwin 2.0仿真基准测试中，PEAfowl在强域随机化设置下的平均成功率比最强基线（SEM）提升了**23.0个百分点**。在真实机器人实验中，PEAfowl也展现了可靠的仿真到现实迁移能力，并在所有6个任务上均优于基线。消融实验证实了几何融合模块与语言读出模块各自的有效性，深度蒸馏进一步提升了在真实噪声深度数据上的性能。

**4. 研究意义和价值**
PEAfowl通过显式建模三维几何一致性和迭代式语言-视觉交互，显著提升了双手操作策略在复杂、动态场景下的鲁棒性和泛化能力。该方法为构建更可靠、更通用的机器人视觉-语言-动作模型提供了新的技术路径，对推动开放世界中的灵巧双手操作迈向实用化具有重要价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：PEAfowl

### **一、 拟解决的核心问题**
论文旨在解决**复杂场景下双臂机器人操作**任务中，现有视觉-语言-动作模型泛化能力不足的问题。具体挑战包括：
1.  **空间理解弱**：现有方法通常简单拼接多视角的2D视觉特征，缺乏**跨视角的3D几何一致性建模**，导致策略对遮挡、视角变化和场景扰动敏感。
2.  **指令-视觉对齐粗糙**：语言指令通常作为全局条件向量注入模型，缺乏对指令中提到的**特定物体和空间关系的细粒度视觉证据检索与聚合**，在杂乱多物体场景中表现不佳。

### **二、 核心技术创新点**
PEAfowl 通过两个核心模块，分别针对上述问题提出了创新性解决方案：

#### **1. 几何引导的多视角融合**
- **方法**：
    - **预测逐令牌深度分布**：为每个视觉特征令牌预测一个离散的深度概率分布，以建模深度不确定性。
    - **可微分的3D提升**：利用预测的深度分布，将2D图像特征“软性”地反投影到共享的机器人基坐标系中，形成3D空间锚点。
    - **跨视角3D邻居聚合**：在3D空间中，为每个令牌查找其在不同视角下的K个最近邻，并根据几何距离进行加权特征聚合。这使得来自不同相机、但对应同一物理区域的视觉特征能够对齐和融合。
- **价值**：显式地建立了跨视角的几何对应关系，生成了**深度感知、3D空间一致**的场景表示，显著提升了模型对遮挡和视角变化的鲁棒性。

#### **2. 语言引导的多视角读出**
- **方法**：
    - **摒弃全局条件注入**：采用 **Perceiver 风格的“文本即查询”机制**。
    - **迭代式证据积累**：将语言指令的嵌入向量作为初始的潜在查询，通过多轮交叉注意力，迭代地从**冻结的CLIP视觉特征**中检索和聚合与指令相关的视觉证据。
    - **生成紧凑的指令条件化令牌**：最终输出一组紧凑的、富含视觉-语言对齐信息的上下文令牌，用于指导策略生成。
- **价值**：实现了**细粒度的指令-视觉对齐**，使策略能够更准确地关注与任务相关的物体和空间关系，提升了在复杂、多指令场景下的执行精度。

#### **3. 辅助技术创新：仅用于训练的深度蒸馏**
- **方法**：在训练阶段，使用一个预训练的**相机深度模型**作为“教师”，为几何融合模块中预测的深度分布提供更精确的监督信号。在推理时，模型**仅使用原始的、可能有噪声的商品深度传感器数据**，不增加任何计算开销。
- **价值**：将高质量的几何先验知识注入策略，**提升了模型在真实世界中使用廉价、有噪声深度传感器时的感知能力**，实现了更可靠的仿真到现实迁移。

### **三、 解决方案总结**
PEAfowl 的整体解决方案是一个**感知增强的多视角VLA策略框架**：
1.  **前端感知**：通过**几何引导的多视角融合**模块，从多视角RGB-D输入构建3D空间一致的场景表示。
2.  **指令理解**：通过**语言引导的多视角读出**模块，从冻结的视觉基础模型中提取与指令强相关的视觉语义信息。
3.  **策略生成**：将上述两种增强后的表示（几何增强令牌 + 语言条件化令牌）与机器人本体感知状态一起，输入到一个**基于扩散模型的关节中心化动作解码器**中，生成双臂协同动作序列。

### **四、 实际价值与效果**
- **性能提升**：在RoboTwin 2.0仿真基准测试中，在强域随机化设置下，比最强基线（SEM）的**平均成功率绝对提升了23.0个百分点**。
- **泛化能力强**：在未见过的任务和真实机器人实验中均表现出优异的泛化能力和**仿真到现实的迁移稳定性**。
- **实用性强**：模型参数量仅3亿，且深度蒸馏方案不增加推理成本，使其更易于在实际机器人系统上部署。

**总而言之，PEAfowl 的核心贡献在于，通过引入显式的几何推理和细粒度的语言-视觉交互机制，显著提升了双臂操作策略在复杂、动态现实场景中的空间理解能力和指令遵循精度。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**复杂场景下双手机器人操作任务中，现有视觉-语言-动作模型因缺乏显式的三维空间理解和细粒度指令对齐而泛化能力不足**的核心问题。为此，论文提出了 **PEAfowl** 框架，其核心创新在于：1）**几何引导的多视角融合模块**，通过预测每个视觉令牌的深度分布、可微三维提升和跨视角三维邻域聚合，构建几何一致的空间表征；2）**语言引导的多视角读出模块**，采用感知器风格、以文本为查询的迭代注意力机制，从冻结的CLIP视觉特征中提取与指令紧密相关的视觉证据。此外，框架还引入了**仅用于训练的深度蒸馏**策略，利用预训练深度模型提升感知质量而不增加推理开销。实验结果表明，该方法在仿真和真实机器人平台上均显著超越了现有基线模型，在强域随机化设置下成功率提升超过23个百分点，并实现了可靠的仿真到现实迁移，证明了其在遮挡、视角和场景变化下的卓越鲁棒性与泛化能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## PEAfowl 论文创新点分析

这篇论文针对双手机器人操作任务中，现有视觉-语言-动作模型在复杂场景下泛化能力不足的问题，提出了一个名为PEAfowl的新框架。其核心创新点在于**感知增强的多视图融合**和**语言引导的视觉特征提取**。以下是其相对于已有工作的明确创新点：

### 1. 几何引导的多视图融合模块
- **改进/不同之处**：
    - **以往方法**：大多数多视图VLA模型（如OpenVLA, SEM）将不同摄像头的视觉特征通过简单的拼接或堆叠进行融合。这种方法本质上是“视图无关的”，没有显式地建模跨视图的几何对应关系，特征空间缺乏3D一致性。
    - **PEAfowl方法**：提出了一种几何引导的多视图融合模块。该模块首先为每个视觉特征块预测一个**离散深度分布**，然后通过可微分的3D提升操作，将这些2D特征点投影到共享的机器人基坐标系中。最后，在3D空间中对每个特征点的**局部跨视图邻居**进行聚合。
- **解决的问题/带来的优势**：
    - **解决**：传统方法对相机位姿变化、标定误差和遮挡（尤其是双手机器人操作中常见的自遮挡和物体间遮挡）非常敏感的问题。
    - **优势**：显式地建立了跨视图的几何对应关系，生成了**3D空间一致**的特征表示。这使得模型对视角变化和遮挡更加鲁棒，增强了空间理解能力，特别是在杂乱和动态变化的场景中。

### 2. 感知器风格的语言引导视觉特征读出机制
- **改进/不同之处**：
    - **以往方法**：语言指令通常作为全局条件向量注入，或通过少量文本标记附加到视觉流中。注意力计算仍以视觉为中心，语言与视觉的交互较弱且粗糙。
    - **PEAfowl方法**：摒弃了全局条件注入，采用了一种**感知器风格**的“文本作为查询”的读出机制。具体而言，将文本标记作为潜在查询，让它们迭代地通过交叉注意力机制去查询**冻结的CLIP视觉特征**，从而生成一组紧凑的、经过语言条件化的视觉标记。
- **解决的问题/带来的优势**：
    - **解决**：在复杂多物体场景中，全局语言条件化容易导致注意力分散、指令关联性弱的问题，难以精确定位与指令相关的物体和空间关系。
    - **优势**：实现了**迭代的证据积累**，使模型能够更精确地“聚焦”于与任务指令相关的视觉区域。这提升了视觉-语言的对齐质量，增强了在指代模糊或多干扰物场景下的指令理解与执行能力。

### 3. 仅用于训练的深度蒸馏方案
- **改进/不同之处**：
    - **以往方法**：要么忽略深度信息，要么直接使用噪声大、不完整的商用深度传感器数据，这会影响基于几何的方法的可靠性。一些方法使用重型3D主干网络或强几何监督，增加了计算开销。
    - **PEAfowl方法**：引入了一个**训练时专用**的深度蒸馏流程。使用一个预训练的“相机深度模型”作为教师网络，在训练阶段为每个视觉特征块预测的深度分布提供监督信号。**在推理时，模型输入仍然是原始的深度图，不引入任何额外计算开销。**
- **解决的问题/带来的优势**：
    - **解决**：真实机器人部署中，低成本深度传感器数据噪声大、缺失严重，直接使用会损害几何感知模块的益处。
    - **优势**：将更精确的**几何先验知识**“蒸馏”到策略网络中，提升了模型对深度信息的理解和利用能力，从而增强了在真实世界中的空间感知鲁棒性。同时，保持了推理效率，实现了“免费”的性能提升。

### 4. 针对双手机器人操作的联合中心化策略架构集成
- **改进/不同之处**：
    - **以往方法**：许多VLA模型主要针对单臂操作设计，或未针对双手机器人高维、紧耦合的动作空间进行特别优化。
    - **PEAfowl方法**：将上述创新的感知模块与一个**联合中心化的扩散动作解码器**（基于SEM）紧密集成。该解码器以机器人关节状态为中心进行建模，并通过前向运动学确保预测动作的几何一致性。
- **解决的问题/带来的优势**：
    - **解决**：双手机器人操作任务对动作的协调性和精确度要求极高，需要策略能有效处理高维状态-动作空间。
    - **优势**：整个架构（感知增强 + 语言引导 + 动作生成）是**端到端**为双手机器人操作任务优化的。它结合了强大的3D空间感知、精确的语言 grounding 和稳定的动作生成，从而在需要双手协调的长时序、多步骤任务上表现出显著优势。

### 总结
PEAfowl的核心创新在于**系统性**地提升了VLA模型在复杂、动态、多视图环境下的**感知质量**。它通过**几何融合**解决了空间一致性问题，通过**语言引导读出**解决了指令精准关联问题，并通过**深度蒸馏**解决了真实世界感知噪声问题。这些创新共同作用，使其在仿真和真实世界的双手机器人操作任务中，相比现有基线（如SEM）取得了显著的成功率提升（在领域随机化设置下提升23.0个百分点），并实现了可靠的从仿真到现实的迁移。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过仿真和真实机器人实验，全面评估了PEAfowl模型在双臂操作任务上的性能。实验表明，PEAfowl在**泛化性、鲁棒性和成功率**方面均显著优于现有基线方法。

### 1. 数据集与评价指标
- **数据集**:
    - **仿真**: 使用 **RoboTwin 2.0** 基准测试平台，包含9个双臂操作任务（如堆叠积木、打开微波炉、悬挂杯子等）。实验在两种设置下进行：
        - **Clean Setting**: 固定的视觉和物理参数，用于评估模型在分布内任务上的能力。
        - **Domain-Randomized (DR) Setting**: 应用了**强结构化扰动**，包括背景干扰物、随机光照、纹理、桌面高度变化以及指令改写，用于压力测试模型的跨场景泛化能力。
    - **真实世界**: 在**双AgileX Piper机器人平台**上进行，包含6个任务（4个从仿真任务改编，2个真实世界特有任务）。每个任务收集了100条高质量的VR遥操作演示数据。
- **评价指标**:
    - **核心指标**: **成功率 (Success Rate)**，即在一个任务的时间限制内成功完成任务的试验比例。
    - **仿真**: 每个任务/设置下进行100次试验。
    - **真实世界**: 每个任务进行10次试验。

### 2. 对比的基线方法
论文与两类基线方法进行了对比：
- **单任务视觉运动策略 (Visuomotor Policies)**: 每个任务单独训练。
    - **ACT**: 基于动作分块变换器的策略。
    - **DP**: 扩散策略。
    - **DP3**: 3D扩散策略。
- **多任务视觉-语言-动作模型 (Multi-task VLA Models)**: 在9个任务上联合训练。
    - **π₀**: 一个大规模预训练的VLA流程模型。
    - **RDT**: 一个用于双臂操作的扩散基础模型。
    - **SEM**: 当前最强的基线，利用3D空间位置嵌入和关节中心状态编码器。

### 3. 关键性能提升与结论

#### 仿真实验结果 (RoboTwin 2.0)
- **整体性能**:
    - **Clean Setting**: PEAfowl平均成功率为 **69.6%**，比最强的基线SEM (**51.0%**) 高出 **18.6个百分点 (pp)**。
    - **DR Setting**: PEAfowl平均成功率为 **47.1%**，比SEM (**24.1%**) 高出 **23.0个百分点**。这证明了其在复杂、多变场景下的卓越泛化能力。
- **任务分析**:
    - 在**长时程、遮挡严重**的任务（如`Stack Blocks Three`, `Blocks Ranking RGB`）上提升最为显著。例如，在DR设置下的`Stack Blocks Three`任务，PEAfowl成功率为34%，而SEM仅为1%。
    - 在视觉相似但指令不同的任务上，PEAfowl表现出更可靠的**指令接地能力**，减少了任务特定偏差。
- **泛化到未见任务**:
    - 在两个未包含在训练集中的新任务 (`Stack Blocks Two`, `Stack Bowls Two`) 上，PEAfowl在DR设置下的成功率 (**51%**, **82%**) 远超SEM (**14%**, **65%**)，证明了其空间和语义信息向相关任务的有效迁移。

#### 真实世界实验结果
- **整体性能**:
    - PEAfowl在6个真实任务上的平均成功率为 **68.3%**，显著优于SEM (**11.7%**)。
    - **深度蒸馏 (DD)** 带来了额外提升，特别是在对深度敏感的任务上（如`Put Bottles Dustbin`，从30%提升至80%）。
- **核心结论**:
    - PEAfowl展示了**可靠的仿真到现实迁移能力**。
    - 其几何引导的感知模块有效克服了**商品深度传感器的噪声和不完整性**，而训练阶段的深度蒸馏在不增加推理开销的前提下，注入了更强的几何先验。

#### 消融实验分析
消融研究验证了各核心组件的贡献：
1.  **语言引导模块**: 将迭代式Perceiver读替换为OTTER式的一步相似度读 (`w/ 1-step SimAttn`)，在DR设置下性能大幅下降（从47.1%降至36.1%），证明了**迭代式、文本感知的证据积累**在杂乱场景中的必要性。
2.  **视觉接地文本令牌**: 移除视觉接地的文本令牌 (`w/o VG-Text`)，在涉及属性和指代的任务上性能下降，凸显了**文本感知的视觉特征提取**的重要性。
3.  **几何引导多视图融合 (GGMVF)**: 将该模块加入一个简单基线 (`Baseline + GGMVF`)，显著提升了长时程任务性能，并**稳定了深度分布的学习**，避免了基线模型的深度损失发散问题。

### 总结
PEAfowl通过其**几何引导的多视图融合**和**语言引导的迭代读**两大核心技术，在双臂操作任务上实现了**最先进的性能**。它不仅大幅提升了在仿真域随机化设置下的成功率，更在真实机器人实验中证明了其**卓越的仿真到现实迁移能力和对噪声深度数据的鲁棒性**，为解决杂乱场景下的通用机器人操作问题提供了有效的解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.17885v1)
- [HTML 版本](https://arxiv.org/html/2601.17885v1)
