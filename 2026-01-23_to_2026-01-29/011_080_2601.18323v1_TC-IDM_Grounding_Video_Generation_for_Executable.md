# TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion

**相关性评分**: 8.0/10

**排名**: #11


---


## 基本信息

- **arXiv ID**: [2601.18323v1](https://arxiv.org/abs/2601.18323v1)
- **发布时间**: 2026-01-26T10:06:56Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin, Kuangzhi Ge, Kai Tang, Peidong Jia, Shanghang Zhang, Jian Tang

## 关键词

Vision-Language-Action Model, VLA for Robotics, world model, fine tune, offline Reinforcement Learning, constrained reinforcement learning, safe reinforcement learning, goal conditioned reinforcement learning

## 一句话总结

TC-IDM 提出一种基于世界模型的工具中心逆动力学模型，通过视频生成和轨迹提取，将视觉规划转化为可执行的机器人动作，以解决 VLA 范式在泛化和物理执行方面的限制。

## 摘要

The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.
  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.
  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.
  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.
  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.

## 详细分析

## 论文摘要：TC-IDM: 面向可执行零样本机器人运动的视频生成落地方法

**1. 研究背景和动机**
当前，视觉-语言-动作模型虽能实现强大的机器人控制，但其泛化能力受限于大规模、高质量的机器人数据。生成式世界模型为具身智能提供了新范式，但其生成的像素级规划与物理可执行动作之间存在“最后一公里”的鸿沟。现有方法在跟踪视频中的物体状态时，面对遮挡、快速运动或可变形物体时表现脆弱。因此，亟需一种鲁棒的方法来弥合视觉规划与底层控制之间的差距。

**2. 核心方法和技术创新**
本文提出了**工具中心逆动力学模型**。其核心创新在于：**将世界模型生成的视频中“工具（如机械臂末端执行器）的想象轨迹”作为连接高层规划与底层控制的鲁棒中间表示**。TC-IDM采用“规划-翻译”的两阶段流程：
- **规划阶段**：利用世界模型根据初始RGB-D图像和文本指令生成未来视频。
- **翻译阶段**：
    - **视觉驱动状态生成**：使用冻结的视觉编码器提取视频语义特征，通过轻量级MLP头预测夹爪开合状态。
    - **几何驱动姿态生成**：利用SAM 3分割工具，结合3D点跟踪器从生成的RGB-D视频中提取末端执行器上密集点的3D轨迹，并通过求解刚性变换直接解析出6自由度末端执行器位姿动作。
这种**解耦设计**使系统对视角变化鲁棒，并能灵活处理可变形物体和长时程任务。

**3. 主要实验结果**
在真实世界机器人评估中，TC-IDM取得了显著优于基线方法的表现：
- **整体成功率**：平均达到**61.11%**。
- **任务难度分层**：在简单、中等、困难任务上的成功率分别为**77.7%**、**53.3%**和**28.9%**，显著优于端到端VLA模型及其他逆动力学模型。
- **零样本泛化**：
    - 在**可变形物体**（如布料）操作任务上，取得了**38.46%**的零样本成功率。
    - 展现出强大的**跨相机视角**、**长时程任务**和**跨 embodiment** 泛化能力。

**4. 研究意义和价值**
TC-IDM通过聚焦于工具轨迹这一稳定、定义明确的中间表示，为生成式世界模型在物理机器人上的部署提供了一个**强基准**。其“规划-翻译”范式**解耦了感知、规划与控制**，降低了学习难度，增强了系统的可解释性和泛化能力。这项工作证明了利用工具中心表示来桥接视觉想象与物理执行是一条有效且可扩展的路径，为迈向通用具身智能提供了重要的技术思路。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：TC-IDM

### **一、 核心问题**
论文旨在解决**生成式世界模型（如Sora、WoW）与机器人物理控制之间的“最后一公里”鸿沟**。具体而言：
- **问题1：规划与控制脱节**：世界模型能生成逼真的未来视频（“视觉前瞻”），但这些**像素级计划**（RGB帧序列）无法直接转换为机器人可执行的**关节空间或力矩控制指令**。
- **问题2：现有方法的局限性**：
    - **端到端VLA模型**（如RT-2、OpenVLA）依赖大规模机器人数据，泛化能力差，尤其对**长视野任务、分布外任务、可变形物体**处理不佳。
    - **传统逆向动力学模型**（IDM）尝试从视频中追踪物体状态（如AVDC、VidBot），但对遮挡、快速运动、非刚性物体（如布料）非常脆弱，因为物体状态难以定义和稳定追踪。

### **二、 核心创新点**
提出了 **“工具中心逆向动力学模型”** ，其核心思想是：**将控制策略锚定在世界模型“想象”出的机器人末端执行器（工具）的稳定运动轨迹上**，而非不稳定的物体状态或整个像素空间。

**创新架构（“计划-翻译”范式）**：
1.  **计划阶段**：使用世界模型根据初始RGB-D图像和文本指令生成未来视频。
2.  **翻译阶段**：TC-IDM将视频转换为可执行动作，关键在于**解耦的双流设计**：
    - **几何驱动的姿态生成流**：使用SAM 3分割工具，结合3D运动估计器，从生成视频中**提取末端执行器上密集点云的6自由度轨迹**。通过求解刚性变换，将这些轨迹**解析地**转换为末端执行器的6-DoF位姿动作。**（核心：利用工具的刚性先验，实现稳定、可解释的运动生成）**
    - **视觉驱动的状态生成流**：使用DINOv3从生成视频中提取语义特征，通过轻量级MLP头预测**夹爪的开合控制信号**。**（核心：利用语义信息判断接触、抓取时机）**

### **三、 解决方案与关键技术**
1.  **核心表示**：**工具轨迹**作为连接高级视觉规划和低级物理控制的**鲁棒中间表示**。它稳定、定义明确，且与视角无关（在坐标系中表示）。
2.  **关键技术流程**：
    - **时空预测**：世界模型生成视频 → 使用VGGT等进行度量深度估计与相机姿态对齐。
    - **密集轨迹提取**：SAM 3分割工具 → 3D点追踪器获取工具表面点的3D运动轨迹。
    - **刚性先验过滤**：筛选最符合刚性运动假设的轨迹点，保证几何一致性。
    - **解耦动作预测**：
        - **姿态**：通过**解析求解刚性对齐**（SVD）得到6-DoF动作。
        - **夹爪**：通过**语义特征MLP**预测1-DoF开合。
3.  **泛化能力设计**：
    - **视角不变性**：工具轨迹在世界坐标系中表示。
    - **处理可变形物体**：无需建模复杂物体状态，只需关注工具路径。
    - **长视野任务**：利用世界模型的组合规划能力，TC-IDM确保每个子动作的可行性。

### **四、 实际价值与效果**
- **性能提升**：在真实世界评估中，**平均成功率61.11%**（简单任务77.7%，零样本可变形物体任务38.46%），显著优于端到端VLA基线和其他IDM。
- **强大泛化**：实验验证了其在**相机视角变化、可变形物体操作、长视野任务、跨本体**（从单臂Franka到双臂UR5）以及**人手指向灵巧手迁移**上的零样本泛化能力。
- **范式贡献**：提供了一种**可扩展、通用**的框架，将强大的生成式世界模型“接地”到物理机器人控制中，降低了其对大规模机器人数据集的依赖，为通用具身智能提供了新思路。

**总结**：TC-IDM通过**聚焦于工具轨迹**这一创新中间表示，并采用**解耦的几何-语义双流架构**，成功弥合了生成式视觉规划与物理执行之间的鸿沟，在提升性能的同时，实现了卓越的泛化能力。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决生成式世界模型（如视频生成模型）产生的像素级视觉规划与机器人底层物理可执行动作之间的“最后一公里”鸿沟。为此，论文提出了**工具中心逆动力学模型（TC-IDM）**，其核心思想是将世界模型生成的视频中**末端执行器（工具）的想象轨迹**作为鲁棒的中间表示，通过解耦的动作预测头（分别处理6自由度末端运动与夹爪控制信号），将视觉规划“翻译”为可执行的控制指令。该方法在真实机器人实验中取得了显著效果，平均任务成功率达到61.11%，在零样本可变形物体操作任务上达到38.46%，显著优于端到端的视觉-语言-动作模型及其他逆动力学模型基线，证明了其强大的泛化能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## TC-IDM 论文创新点分析

这篇论文提出了一种名为 **TC-IDM（Tool-Centric Inverse Dynamics Model）** 的新框架，旨在解决生成式世界模型（如视频生成模型）与机器人物理控制之间的“最后一公里”鸿沟。其核心创新在于**将工具（末端执行器）的想象轨迹作为连接高层视觉规划与低层控制的关键中间表示**。以下是其相对于已有工作的明确创新点：

---

### 1. **核心范式创新：从“物体状态跟踪”转向“工具轨迹锚定”**
- **改进/不同之处**：
    - **以往方法**（如AVDC、VidBot、Novaflow）：主要依赖从生成视频中**跟踪目标物体的3D状态或密集像素对应关系**，以此推断动作。这种方法对遮挡、快速运动、非刚性物体（如布料）的状态定义和跟踪非常脆弱。
    - **TC-IDM**：**放弃了对不稳定物体状态的依赖**，转而将控制策略**锚定在世界模型生成的视频中所想象的、机器人末端执行器（工具）本身的稳定、定义明确的运动轨迹上**。
- **解决的问题/带来的优势**：
    - **解决了规划-动作的错位问题**：工具轨迹是一个更鲁棒、更直接的中间表示，因为它直接关联到机器人本体的可执行运动。
    - **增强了对非刚性物体操作的泛化能力**：无需对布料等复杂变形状态进行建模，只需关注工具本身的预期路径，从而实现了对**零样本可变形物体操作**（如叠衣服、移除布料）的泛化（成功率38.46%）。
    - **提高了视角不变性**：工具轨迹在**世界坐标系**中表示，因此对相机视角变化具有内在的鲁棒性。

### 2. **架构创新：解耦的动作预测头设计**
- **改进/不同之处**：
    - **以往方法**：通常使用单一的网络模块，将视觉特征直接映射为复合的控制信号（如位姿+夹爪开合）。
    - **TC-IDM**：采用**解耦的双流架构**，包含两个独立的学习头：
        1.  **视觉驱动的状态生成头（Vision-Driven State Generation）**：基于DINOv3提取的语义视觉特征，通过一个轻量级MLP预测**1-DoF夹爪控制信号**（开/合）。
        2.  **几何接地的姿态生成头（Geometry-Grounded Gesture Generation）**：基于从视频中分割（SAM 3）和3D运动跟踪（3D Point Tracker）得到的工具点云轨迹，通过**解析的刚体姿态恢复**（求解SE(3)变换）来预测**6-DoF末端执行器位姿轨迹**。
- **解决的问题/带来的优势**：
    - **有效利用异构信息流**：将语义任务信息（“做什么”，如抓取时机）与几何运动信息（“怎么做”，如移动路径）清晰解耦，使模型能更专注、更有效地利用不同类型的信息。
    - **提高了动作生成的解释性和物理一致性**：几何分支通过**解析求解刚体变换**来生成动作，而非完全依赖黑盒神经网络学习，使得运动生成更具物理可解释性，并减少了运动学上的不一致性。
    - **支持广泛的末端执行器**：该架构对末端执行器的具体形态是**不可知**的，只需替换相应的轨迹提取和映射模块，即可适配不同的工具（如平行夹爪、灵巧手、锤子等）。

### 3. **流程创新：“规划-翻译”的两阶段范式**
- **改进/不同之处**：
    - **以往基于世界模型的方法**：虽然使用生成模型进行“视觉前瞻”，但后续的“像素到动作”翻译模块（通常是IDM）往往很脆弱，对视觉伪影、遮挡敏感，需要频繁的重新规划（如VPA中的信任区域重规划）。
    - **TC-IDM**：明确分为两个阶段：
        1.  **规划（Plan）**：世界模型根据初始观测和文本指令，生成一个高层次的、任务达成的未来视频序列。
        2.  **翻译（Translate）**：TC-IDM模块**专门负责**从生成的视频中提取工具轨迹，并将其翻译为可执行的机器人控制指令。它作为一个**通用适配器**，可以与不同的视频生成模型（如WoW、Cosmos2、Kling）结合。
- **解决的问题/带来的优势**：
    - **弥合了“视觉规划”与“物理控制”之间的鸿沟**：提供了一个稳定、专门的模块来处理翻译问题，降低了直接部署视觉规划器的难度。
    - **实现了强大的组合与长时程泛化**：得益于世界模型的组合式规划能力，TC-IDM可以执行复杂的多步骤任务（如六步叠帽衫），同时通过确保每个子动作的可行性来保证整体执行的连贯性。
    - **建立了新的强基线**：为在物理机器人上部署生成式世界模型提供了一个有效且可复现的框架。

### 4. **应用范围创新：扩展到跨 embodiment 与灵巧手迁移**
- **改进/不同之处**：
    - **以往VLA或IDM模型**：通常针对特定机器人平台（如单臂Franka）进行训练和部署，泛化到其他机械结构需要大量重新训练或调整。
    - **TC-IDM**：展示了其表示的** embodiment 不变性**。通过简单的执行器状态重映射，可以：
        1.  **跨 embodiment 泛化**：将单臂Franka学到的运动先验，**零样本**迁移到双臂UR5机器人上执行任务（如敲击木琴）。
        2.  **人手指向灵巧手迁移**：提出一个扩展管道，使用HaMeR模型估计视频中的人手状态，然后通过一个轻量级重定向网络（Retargeting Model）将其映射到不同灵巧手（如BrainCo、Inspire-Robots）的关节指令，实现零样本迁移。
- **解决的问题/带来的优势**：
    - **提高了方法的通用性和可扩展性**：表明TC-IDM学习到的是与具体机器人形态无关的**本质运动表示**，降低了为不同机器人平台开发控制策略的成本。
    - **开辟了从人类视频学习灵巧操作的新途径**：为利用丰富的人类演示视频来指导高自由度灵巧手操作提供了可行的技术路径。

---

### **总结**
TC-IDM的核心创新在于**视角的转变**——从关注复杂多变的环境物体状态，转向关注机器人自身工具的、相对稳定且可直接控制的想象运动。通过**解耦的架构设计**和**两阶段的“规划-翻译”范式**，它系统性地解决了生成式世界模型落地机器人控制时的关键瓶颈，在**零样本泛化**（尤其是可变形物体和长时程任务）、**视角鲁棒性**和**跨平台适应性**方面带来了显著优势，为通用具身智能提供了一条新的、有前景的技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、核心实验效果
论文提出的 **TC-IDM** 框架在真实机器人实验中取得了显著效果，成功地将生成式世界模型的“视觉规划”转化为可执行的机器人动作，并在多个维度上展现出强大的泛化能力。

**核心性能指标：**
- **平均成功率**：**61.11%**（所有任务平均）
- **简单任务成功率**：**77.7%**（最高达93.3%）
- **零样本可变形物体任务成功率**：**38.46%**
- **关键结论**：TC-IDM 在各项指标上**显著优于**端到端 VLA 模型和其他逆动力学模型基线。

### 二、使用的数据集
- **训练集**：基于 **Robomind 数据集** 的子集。
    - 包含 **30,210 条演示轨迹**，覆盖 **25 个不同的操纵任务**。
    - 任务类型多样，从基础抓放到复杂的接触式精细操作。
- **测试集**：**严格留出的 9 个未见任务**，用于评估零样本泛化能力。
    - 按难度分为三级：**简单（3个）、中等（3个）、困难（3个）**。
    - 测试任务完全排除在训练分布之外，确保评估的公正性。

### 三、评价指标
核心评价指标为 **任务成功率**，即机器人根据指令成功完成指定操作的试验比例。任务根据对机器人自由度（DoF）控制和精度要求进行分类：
- **简单任务**：基础操作（如抓放、推动），容错率较高。
- **中等任务**：需要至少4-DoF控制或简单避障。
- **困难任务**：需要至少5-DoF控制，且误差容忍度低（如 `<2cm/10°`）。

### 四、对比的基线方法
论文与两大类基线方法进行了全面对比：

**1. 逆动力学模型（IDM）基线（在真实回放视频上评估）**
- **ResNet-MLPs**：基于静态视觉特征的基线。
- **AVDC**：利用合成视频帧间稠密对应关系来监督动作推断。
- **AnyPos**：在大规模随机探索数据上训练的任务无关IDM。
- **2DtrackerIDM**：结合了2D跟踪器（cotracker3）的IDM。

**2. 视频生成世界模型 + IDM 的集成方案**
- 评估了多种前沿视频生成模型作为规划器，其后接IDM（包括TC-IDM）进行动作翻译。对比模型包括：
    - `π₀`, `CogVideo-IDM`, `Wan2.1-IDM`, `Cosmos-1-IDM`, `Hailuo-IDM`, `Kling-IDM`, `Cosmos2-IDM`
    - 以及微调后的世界模型：`WoW-cosmos2-IDM*`, `WoW-wan-IDM*`

### 五、关键性能提升与结论

**1. 在真实回放评估中全面领先（表1）**
- **简单任务**：TC-IDM 达到近乎完美的成功率（如“面包放盘子”任务10/10），显著优于所有基线。
- **中等任务**：成功率（53.3%）最高，尤其在需要复合操作和避障的任务上优势明显。
- **困难任务**：在需要精细协调和长时程一致性的任务上，TC-IDM 取得了 **28.9%** 的成功率，**大幅超越**其他所有IDM方法（其他方法多数接近0%）。
- **结论**：TC-IDM 的“工具轨迹中心化”设计和解耦的架构，使其在动作回放的**稳定性、精度和长时程一致性**上具有决定性优势。

**2. 与不同世界模型结合均表现最佳（表2）**
- 无论使用哪种视频生成模型（`WoW`, `Kling`, `Cosmos2`等）作为规划器，**TC-IDM 作为翻译器始终能取得最高的成功率**。
- 这证明了 TC-IDM 框架对上游视觉规划模型的**鲁棒性**和**泛化性**，不依赖于特定模型的输出特性。

**3. 在五大泛化维度上表现出色（第5章）**
- **误差范围分析**：在“飞镖投掷”高精度任务中，TC-IDM 执行轨迹与规划轨迹偏差最小（<4cm），显著优于AVDC和AnyPos。
- **跨相机泛化**：在训练（RealSense D457）和测试（Apple Pro, RealSense D435i）相机存在显著颜色、视角差异时，无需微调即可成功执行任务。
- **可变形物体泛化**：在**零样本**的布料操作任务上，取得了 **38.46%** 的成功率，证明了其处理非刚性物体复杂状态的能力。
- **长时程泛化**：成功完成了需要**六步连续动作**的“折叠连帽衫”复杂任务，无需中间重规划。
- **跨 embodiment 泛化**：将单臂Franka机器人学到的运动先验，成功迁移到**双臂UR5机器人**执行敲击木琴任务，证明了其运动表征的 embodiment 不变性。

**4. 扩展到灵巧手（第6章）**
- 通过引入一个轻量级的重定向网络，TC-IDM 框架可以**零样本**地将视频中预测的**人手动作**迁移到不同的灵巧手（如BrainCo, Inspire-Robots）上，完成击打气球、移除布料等任务。

### 总结
TC-IDM 通过**以工具轨迹为中间表征**的“规划-翻译”范式，有效弥合了生成式视觉规划与物理控制之间的“最后一公里”鸿沟。实验证明，该框架不仅在标准任务上取得了当前最佳性能，更在**零样本泛化、跨 embodiment、处理可变形物体和长时程任务**等关键挑战上展现出卓越能力，为将通用世界模型部署到物理机器人提供了一条坚实可行的路径。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.18323v1)
- [HTML 版本](https://arxiv.org/html/2601.18323v1)
