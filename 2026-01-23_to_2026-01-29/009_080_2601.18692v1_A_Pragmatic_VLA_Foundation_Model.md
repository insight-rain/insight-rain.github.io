# A Pragmatic VLA Foundation Model

**相关性评分**: 8.0/10

**排名**: #9


---


## 基本信息

- **arXiv ID**: [2601.18692v1](https://arxiv.org/abs/2601.18692v1)
- **发布时间**: 2026-01-26T17:08:04Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Houlong Xiong, Ziyu Wang, Zechen Wang, Ran Cheng, Yong-Lu Li, Yongtao Huang, Xing Zhu, Yujun Shen, Kecheng Zheng

## 关键词

Vision-Language-Action Model, VLA for Robotics, fine tune, Edge Deployment

## 一句话总结

这篇论文介绍了一个实用的VLA基础模型LingBot-VLA，通过大规模真实世界数据训练，在机器人操作任务中展现出卓越的性能和泛化能力，并优化了代码库以实现高效部署。

## 摘要

Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.

## 详细分析

## 论文摘要：A Pragmatic VLA Foundation Model

### 1. 研究背景和动机
视觉-语言-动作（VLA）基础模型在机器人操作领域展现出巨大潜力，但现有研究缺乏对**大规模真实世界数据如何影响模型性能**的系统性实证分析。同时，社区也缺乏能够高效处理海量数据、支持快速迭代的优化训练框架。本研究旨在填补这些空白，通过构建大规模数据集和高效代码库，探究VLA模型在真实机器人场景中的**缩放规律**，并开发一个性能优越、泛化能力强且适合实际部署的实用VLA模型。

### 2. 核心方法和技术创新
- **大规模真实世界数据集**：收集了来自**9种主流双臂机器人平台**、总计约**2万小时**的遥操作数据，构建了目前规模最大、行为最多样的真实机器人操作数据集之一。
- **高效模型架构与训练**：提出了 **LingBot-VLA** 模型，其核心创新包括：
    - **混合专家Transformer架构**：采用类似BAGEL的Mixture-of-Transformers设计，将预训练的视觉语言模型（Qwen2.5-VL）与专门的动作生成专家模块结合，通过共享自注意力机制实现层级的统一序列建模，在利用VLM语义先验的同时减少跨模态干扰。
    - **空间感知增强**：通过可学习查询与专用深度模型（LingBot-Depth）的token进行对齐，将深度信息蒸馏到模型中，显著提升了模型对几何和空间的感知能力。
    - **条件流匹配**：采用Flow Matching进行连续动作建模，实现平滑、高精度的机器人控制。
- **高度优化的训练代码库**：通过**全分片数据并行（FSDP）策略**、**混合精度训练**以及**算子级优化**（如FlexAttention和算子融合），在8-GPU集群上实现了**每秒261个样本**的高吞吐量，相比现有VLA代码库有1.5~2.8倍的加速。

### 3. 主要实验结果
研究在GM-100基准（100个任务）上进行了大规模系统评估，涵盖3种机器人平台，每个任务进行130次后训练试验。
- **性能领先**：**LingBot-VLA**（含深度版本）在三个平台上的平均**成功率（SR）达到17.30%**，**进度得分（PS）达到35.41%**，显著优于 **π0.5**、GR00T N1.6和WALL-OSS等基线模型。
- **泛化能力强**：模型在未见过的任务和平台上表现出色，测试集中约50%的原子动作未在训练集高频动作中出现，证明了其强大的泛化能力。
- **数据缩放规律**：实证研究表明，随着预训练数据量从3000小时增加到20000小时，下游任务的成功率和进度得分持续提升，且未出现饱和迹象，验证了**扩大真实世界数据规模对VLA模型性能有持续积极影响**。
- **数据高效性**：仅使用80个演示进行后训练，**LingBot-VLA** 的性能即可超越使用130个演示的 **π0.5**，展示了卓越的数据效率。

### 4. 研究意义和价值
- **学术价值**：首次对VLA模型在真实世界数据上的**缩放规律**进行了系统性实证研究，为未来大规模数据收集和模型开发提供了关键见解。同时，提出了一个包含空间感知增强的高效VLA模型架构。
- **工程与实践价值**：发布的**高效开源代码库**大幅降低了大规模VLA模型训练的计算成本和时间。构建的**大规模、高质量真实世界数据集和基准**为社区提供了严谨的评估标准。
- **产业应用前景**：模型展现出的**强大性能、广泛泛化能力和训练效率**，使其非常适合于**现实世界的机器人部署**。研究团队开源了代码、模型和基准数据，旨在推动机器人学习领域的开放科学发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：LingBot-VLA

### **一、 核心要解决的问题**
论文旨在解决当前**视觉-语言-动作（VLA）基础模型**在迈向实际机器人部署时面临的三个关键瓶颈：
1.  **规模化效应不明**：缺乏关于VLA模型性能如何随**海量真实世界机器人数据**规模扩展的实证研究。
2.  **评估标准不健全**：社区缺乏能够系统、严谨评估模型在**多任务、多机器人平台**上泛化能力和鲁棒性的基准与方法。
3.  **训练效率低下**：大规模VLA模型训练面临**数据I/O瓶颈和通信开销**，导致计算成本高昂，迭代周期长，阻碍了规模化探索。

### **二、 核心创新点**
论文的创新是系统性的，涵盖了**数据、模型、评估框架和训练基础设施**四个方面：

#### **1. 数据层面的创新：超大规模、多平台真实数据集**
- **规模与多样性**：构建了约 **20,000小时** 的真实世界双手机器人操作数据，来自 **9种** 不同的主流机器人平台（如Agibot G1, AgileX等）。
- **价值**：首次提供了研究VLA模型在真实数据上**缩放定律（Scaling Laws）** 的实证基础。实验表明，模型性能（成功率和进度得分）随预训练数据量（从3,000到20,000小时）**持续提升且未见饱和**，证明了数据规模对机器人学习的持续增益。

#### **2. 模型架构的创新：高效融合语义与空间感知**
- **基础架构**：采用 **Mixture-of-Transformers (MoT)** 架构（类似BAGEL），将预训练好的视觉语言模型（Qwen2.5-VL）与专门的动作生成专家模块结合，通过共享自注意力实现层间统一表征，**兼顾语义引导与模态隔离**。
- **空间感知增强**：创新性地通过**可学习查询（Learnable Queries）** 对齐机制，将VLM的视觉特征与一个专门的深度模型（LingBot-Depth）的深度令牌进行蒸馏对齐。
    - **技术细节**：使用投影层和蒸馏损失（公式5），将几何（深度）信息隐式注入VLA模型，**显著提升了模型对复杂操作任务的空间理解和执行精度**（带深度版本模型全面优于基线）。
- **动作建模**：采用 **Flow Matching** 进行连续动作建模，实现平滑、高精度的机器人控制。

#### **3. 评估体系的创新：大规模、系统性真实世界评测**
- **基准与规模**：在 **GM-100** 基准（100个精心设计的任务）上，跨 **3个** 机器人平台（AgileX, Agibot G1, Galaxea R1Pro），每个任务进行 **130次** 后训练轮次和大量测试试验，总计产生海量评估数据。
- **严谨性**：设定了严格的评测协议，包括**机器-任务配对**、**环境随机化**、**标准化训练流程**，并引入**进度得分（PS）** 以衡量部分任务完成度，提供了比单一成功率（SR）更细致的性能洞察。
- **价值**：为VLA领域建立了新的、**更可靠的评估标准**，证明了模型在**跨平台泛化**和**任务多样性**上的强大能力。

#### **4. 工程效率的创新：高性能训练代码库**
- **优化策略**：
    - **分布式策略**：采用 **FSDP（Fully Sharded Data Parallel）** 并结合针对动作专家模块的**分组分片策略**，在减少内存占用的同时优化通信开销。
    - **算子级优化**：利用 **FlexAttention** 优化稀疏注意力计算，并通过 `torch.compile` 进行算子融合，减少内核启动开销。
- **成果**：在8-GPU训练设置下，实现了 **每秒261样本/GPU** 的吞吐量，相比现有VLA代码库（StarVLA, Dexbotic, OpenPI）有 **1.5~2.8倍的加速**，且具有良好的线性扩展效率。

### **三、 解决方案的总体思路**
论文通过一个**“数据-模型-评估-效率”四位一体**的闭环方案解决问题：
1.  **收集与标注海量多平台真实数据** -> 为规模化研究提供燃料。
2.  **设计并训练一个融合语义与空间信息的高效VLA模型（LingBot-VLA）** -> 提升模型核心能力。
3.  **构建一个大规模、跨平台、任务多样的系统性评估基准** -> 科学验证模型性能与泛化能力。
4.  **开发一个高度优化的训练代码库** -> 极大降低大规模实验的计算成本与时间，使前述研究可行。

### **四、 实际价值与影响**
- **学术价值**：首次揭示了VLA模型在真实机器人数据上的**有利缩放规律**，为未来研究方向和数据构建提供了关键依据；设立了更严格的评估范式。
- **工程与应用价值**：提供的 **LingBot-VLA 模型** 在真实世界和仿真基准上均达到**最先进性能**，其**优异的泛化性、数据效率（见图6）和训练高效性**使其非常接近于实际部署的要求。
- **社区贡献**：**开源**了代码、模型和基准数据，旨在降低领域门槛，推动更富挑战性的任务研究和更健全的评估标准发展。

**总结**：LingBot-VLA 不仅仅是一个性能更强的VLA模型，更是一项**涵盖数据、算法、评测和系统工程的全栈式创新**，旨在系统性地推动VLA基础模型从实验室研究走向现实世界机器人应用。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决当前视觉-语言-动作（VLA）基础模型在**真实世界机器人操控**中面临的三大挑战：**1）缺乏大规模真实机器人数据下的性能扩展规律实证研究；2）现有训练框架计算效率低下，阻碍大规模探索；3）评估标准不够系统全面，难以衡量模型的泛化能力**。为此，论文提出了 **LingBot-VLA** 模型及配套的高效训练系统，其核心方法是：**基于约2万小时、来自9种不同双臂机器人平台的真实操控数据进行预训练**；模型架构上，**采用混合专家（Mixture-of-Transformers）设计，将预训练视觉语言模型（Qwen2.5-VL）与动作生成专家模块通过共享注意力机制耦合，并引入基于可学习查询的深度信息蒸馏以增强空间感知**；同时，**开发了高度优化的训练代码库，通过全分片数据并行（FSDP）、算子融合等技术显著提升训练吞吐量**。最终，论文通过在三类机器人平台上对100项任务进行大规模系统评估（每任务130次测试），证明了该模型在**成功率和任务进度得分上均显著优于现有主流VLA模型**，展现了**卓越的性能和跨平台泛化能力**；其训练代码库实现了**最高每秒261样本/GPU的吞吐量，相比现有方案有1.5至2.8倍的加速**；此外，**缩放实验首次揭示了VLA模型性能随真实数据规模（从3000到20000小时）持续提升且未饱和的有利规律**。这些成果共同表明，LingBot-VLA是一个**性能强大、泛化性好且训练高效**的实用化VLA基础模型。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《A Pragmatic VLA Foundation Model》的创新点分析

这篇论文提出的 **LingBot-VLA** 模型在多个维度上对现有的视觉-语言-动作（VLA）基础模型研究做出了明确且重要的创新。其核心创新点不仅体现在模型性能上，更在于其**系统性、实用性和工程效率**方面的突破。

以下是其相对于已有工作的主要创新点，逐条列出并分析：

### 1. **大规模、多平台、真实世界数据集的构建与系统性缩放研究**
- **改进/不同之处**：
    - **数据规模与来源**：构建了约 **20,000小时** 的真实世界机器人操作数据，来自 **9种不同的双臂机器人平台**。这远超以往VLA模型通常使用的数据规模（如数千小时）和平台多样性。
    - **系统性缩放分析**：论文首次对VLA模型在**真实世界数据**上的缩放规律进行了系统性实证研究（从3,000小时到20,000小时），并观察到性能随数据量增加而持续提升，且未出现饱和迹象。
- **解决的问题/带来的优势**：
    - **解决了数据瓶颈与泛化性不足的问题**：大规模、多平台数据是模型获得强大跨任务、跨平台泛化能力的基础。论文证明，持续增加真实数据量是提升VLA模型性能的有效途径，为未来大规模数据收集提供了实证依据。
    - **建立了首个真实世界VLA缩放定律的实证证据**：填补了社区在“VLA模型如何随真实数据规模扩展”这一核心问题上的认知空白，具有重要的指导意义。

### 2. **高效、可扩展的专用训练代码库**
- **改进/不同之处**：
    - **吞吐量显著提升**：提出的代码库在8-GPU训练设置下，达到了 **261样本/秒/GPU** 的吞吐量，相比现有主流VLA代码库（如StarVLA、Dexbotic、OpenPI）实现了 **1.5倍到2.8倍的加速**。
    - **系统性优化**：并非单一优化，而是实施了**分布式策略**（改进的FSDP/HSDP）、**算子级优化**（FlexAttention, `torch.compile`融合）和**混合精度策略**的组合优化。
- **解决的问题/带来的优势**：
    - **解决了大规模VLA训练的计算效率瓶颈**：数据I/O和通信开销是训练超大规模模型的常见瓶颈。此高效代码库大幅缩短了训练周期，降低了计算成本，使得在有限资源下探索更大规模模型和数据成为可能。
    - **促进了研究的可重复性与迭代速度**：开源的高效代码库为社区提供了强大的工具，降低了进入VLA大规模训练的门槛，加速了整个领域的技术迭代。

### 3. **融合空间感知（深度信息）的模型架构设计**
- **改进/不同之处**：
    - **轻量级深度信息融合**：采用**可学习查询**与预训练的 **LingBot-Depth** 模型的深度令牌进行**对齐**（通过蒸馏损失 `ℒ_distill`），而非直接将深度图作为输入或进行复杂的多模态融合。
    - **集成于MoT框架**：该设计嵌入在**混合专家Transformer**架构中，保持了语义主干（VLM）与动作专家路径的分离，通过共享注意力进行交互，减少了模态干扰。
- **解决的问题/带来的优势**：
    - **增强了模型的几何与空间推理能力**：解决了传统VLA模型在需要精确深度感知和空间关系的复杂操作任务（如堆叠、插入）中表现不佳的问题。
    - **保持了模型的高效性**：相比显式处理深度图，这种基于对齐的蒸馏方法是一种更轻量、更高效的隐式空间信息注入方式，在提升性能（实验显示SR平均提升约4.28%）的同时，未给模型带来过大的计算负担。

### 4. **大规模、系统性、严谨的真实世界评估基准**
- **改进/不同之处**：
    - **评估规模与严谨性**：在 **3个不同的真实机器人平台** 上，对 **100个任务** 进行了系统评估，每个任务进行了 **130次训练后测试**。采用了严格的**机器-任务配对**和**控制变量法**，确保比较的公平性。
    - **评估指标**：不仅使用**成功率**，还引入了**进度分数**，能更细致地衡量部分成功和失败模式。
- **解决的问题/带来的优势**：
    - **解决了VLA评估碎片化、小规模的问题**：以往研究多在仿真中或仅用少量真实任务进行评估。此工作建立了一个**大规模、标准化**的真实世界评估协议，为衡量模型的**实际部署能力**和**跨平台泛化性**提供了可靠、全面的基准。
    - **推动了可靠的性能比较**：严格的实验控制（相同数据、超参数、硬件）确保了性能差异源于模型本身，而非实验配置，提升了结果的公信力，为领域设立了新的评估标准。

### 5. **卓越的数据效率与泛化性能**
- **改进/不同之处**：
    - **数据效率**：实验表明，**LingBot-VLA** 仅用 **80个示范/任务** 进行后训练，其性能即可超越竞争对手 **π_0.5** 使用 **130个示范/任务** 的性能。
    - **跨平台泛化**：在未经专门预训练的机器人平台上（如对GR00T N1.6模型而言的AgileX和Agibot G1），**LingBot-VLA** 表现出了显著优于基线模型的性能，证明了其通过大规模多平台数据学习到的**通用表征**的有效性。
- **解决的问题/带来的优势**：
    - **降低了机器人技能学习的成本**：优异的数据效率意味着在实际部署中，为新任务收集示范数据的成本和时间大大降低。
    - **验证了基础模型的实用价值**：强大的跨平台泛化能力是VLA“基础模型”的核心承诺。**LingBot-VLA** 的实证结果证明了构建大规模通用机器人策略的可行性，使其更贴近现实应用场景。

### 总结
**LingBot-VLA** 的核心创新在于其 **“务实”** 的特性：它通过**工程上高效**的方法（创新点2），利用**规模上空前**的真实数据（创新点1），构建了一个**性能上强大且泛化性好**的模型（创新点3,5），并最终通过一个**极其严谨和标准化**的评估体系（创新点4）证明了其价值。这套组合拳不仅提升了模型的SOTA性能，更重要的是为整个机器人学习领域提供了**可扩展的数据洞察、高效的训练工具和可靠的评估基准**，具有很高的实际应用价值和推动领域发展的意义。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 主要实验效果
论文通过大规模、系统化的真实机器人实验，证明了 **LingBot-VLA** 模型在**性能**、**泛化能力**和**训练效率**三个方面均实现了显著提升，展现出作为实用化VLA基础模型的潜力。

### 二、 使用的数据集
1.  **预训练数据集**：
    - **规模**：约 **20,000小时** 的真实世界机器人操作数据。
    - **来源**：来自 **9种** 不同的主流双机械臂机器人平台（如AgiBot G1, AgileX, Galaxea R1Pro等）。
    - **特点**：数据通过遥操作收集，并利用大模型（Qwen3-VL）进行视频片段分割和任务指令标注，确保了数据的规模、多样性和语义质量。

2.  **评估基准数据集**：
    - **GM-100**：包含 **100个** 精心设计的桌面操作任务，每个任务在3个机器人平台上各收集 **130条** 高质量专家演示轨迹，用于**真实世界评估**。
    - **RoboTwin 2.0**：一个大规模仿真基准，包含50个代表性任务，提供“干净”和“高度随机化”两种场景设置，用于**仿真评估**。

### 三、 评价指标
1.  **成功率**：任务在规定时间（3分钟）内被完全正确执行的比例。这是衡量部署可行性的**核心指标**。
2.  **进度得分**：衡量部分任务完成度。根据完成的子任务步骤数占总步骤数的比例计算，能有效诊断失败模式并奖励部分成功。
3.  **训练吞吐量**：以**每秒每GPU处理的样本数** 来衡量，用于评估代码库的**训练效率**。

### 四、 对比的基线方法
论文与三个当时最先进的VLA开源基础模型进行了对比：
1.  **π₀.₅**
2.  **GR00T N1.6**
3.  **WALL-OSS**

### 五、 关键性能提升与结论

#### 1. 真实世界性能与泛化性（核心结论）
在GM-100基准上，对三个机器人平台（Agibot G1, AgileX, Galaxea R1Pro）进行了总计 **22,500次** 试验评估。

- **总体优势**：`LingBot-VLA`（无论是否集成深度信息）在**所有三个平台**上的平均成功率和进度得分均**显著优于** WALL-OSS 和 GR00T N1.6。
- **与最强基线对比**：集成深度信息的 `LingBot-VLA (w/ depth)` 相比最强的基线 `π₀.₅`，在三个平台上的**平均成功率提升了4.28%**，**平均进度得分提升了7.76%**。
- **具体数据**（来自Table 1）：
    - **平均成功率**：`LingBot-VLA (w/ depth)` 达到 **17.30%**，高于 `π₀.₅` 的 13.02%。
    - **平均进度得分**：`LingBot-VLA (w/ depth)` 达到 **35.41%**，高于 `π₀.₅` 的 27.65%。
- **深度信息价值**：集成深度信息的版本 (`w/ depth`) 相比不集成的版本 (`w/o depth`)，在大多数情况下性能更优，验证了空间感知对复杂操作任务的有效性。
- **泛化性证据**：测试集中约50%的原子动作未在训练集高频动作中出现，但模型仍能取得良好表现，证明了其强大的泛化能力。同时，模型在三个结构不同的机器人平台上均表现优异，体现了**跨平台泛化能力**。

#### 2. 仿真环境鲁棒性
在RoboTwin 2.0仿真基准上：
- `LingBot-VLA (w/o depth)` 在干净和随机化场景下的成功率分别比 `π₀.₅` 高 **3.76%** 和 **8.58%**。
- `LingBot-VLA (w/ depth)` 的提升更显著，分别达到 **5.82%** 和 **9.92%**。
- **结论**：模型在充满干扰的随机化环境中表现出了更强的**鲁棒性**。

#### 3. 训练效率突破
- **吞吐量**：论文提出的优化代码库，在8-GPU训练设置下，达到了 **261 samples/s/GPU** 的吞吐量。
- **加速比**：相比现有的VLA专用代码库（StarVLA, Dexbotic, OpenPI），实现了 **1.5倍 到 2.8倍** 的加速（具体倍数取决于所基于的VLM骨干模型）。
- **意义**：这大幅缩短了训练周期，降低了大规模机器人学习的研究与开发成本，使在超大规模真实数据上进行迭代成为可能。

#### 4. 数据规模与效率的实证研究（重要发现）
- **数据规模定律**：通过将预训练数据从3,000小时逐步增加到20,000小时，发现下游任务的**成功率和进度得分持续、显著提升**，且**在20,000小时时仍未出现饱和迹象**。这为“VLA模型性能随真实世界数据规模扩大而持续提升”提供了首个实证依据。
- **数据效率**：在少量数据微调场景下，`LingBot-VLA` 仅用每个任务 **80条** 演示数据微调后，其性能即可超越使用 **130条** 完整数据微调的 `π₀.₅`，显示出**卓越的数据效率**和从大规模预训练中迁移知识的能力。

### 总结
论文通过严谨、大规模的真实机器人实验，定量证明了 `LingBot-VLA` 在**绝对性能**、**跨任务与跨平台泛化性**、**训练与数据效率**方面均处于领先地位。其开源的高效代码库和验证的“数据规模收益未饱和”结论，对推动整个机器人学习领域向更大规模真实数据训练发展具有重要的实践指导价值。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.18692v1)
- [HTML 版本](https://arxiv.org/html/2601.18692v1)
