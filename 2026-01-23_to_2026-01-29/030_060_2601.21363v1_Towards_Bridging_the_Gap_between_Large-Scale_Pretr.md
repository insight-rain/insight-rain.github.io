# Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control

**相关性评分**: 6.0/10

**排名**: #30


---


## 基本信息

- **arXiv ID**: [2601.21363v1](https://arxiv.org/abs/2601.21363v1)
- **发布时间**: 2026-01-29T07:43:24Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su, Jingwen Zhang

## 关键词

fine tune, offline Reinforcement Learning, world model

## 一句话总结

该论文提出一种结合大规模预训练和基于模型微调的方法，用于人形机器人控制，以提高样本效率和适应性。

## 摘要

Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.

## 详细分析

## 论文摘要：《Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control》

### 1. 研究背景和动机
强化学习（RL）是人形机器人控制的主流方法，但现有方法在**大规模预训练**与**数据高效微调**之间存在显著鸿沟。主流方法如PPO虽能通过大规模并行仿真实现快速训练和零样本部署，但其**同策略**特性导致样本效率低下，难以在新环境中安全、高效地适应。尽管离策略RL和基于模型的方法样本效率更高，但直接应用于人形机器人时，仍面临**训练时间长、探索风险高、模型泛化差**等挑战。本文旨在弥合这一鸿沟，提出一个统一的预训练-微调框架。

### 2. 核心方法和技术创新
本文提出了 **LIFT** 框架，包含三个阶段：
- **（i）大规模策略预训练**：采用**离策略算法SAC**，并利用JAX实现大规模并行仿真。通过**大批量更新**和**高更新数据比（UTD）**，在单块RTX 4090 GPU上**一小时内**即可完成鲁棒训练，并实现零样本部署到真实机器人。
- **（ii）物理信息世界模型预训练**：利用SAC预训练阶段收集的数据，离线训练一个**物理信息世界模型**。该模型将已知的拉格朗日刚体动力学与神经网络预测的**接触力等残差项**相结合，提升了预测精度和泛化能力。
- **（iii）高效策略与模型微调**：在新环境中，**仅执行确定性策略**收集数据，而将**随机探索完全限制在预训练的世界模型内**进行。这种“环境内确定，模型内随机”的分离设计，在保证探索覆盖的同时，**极大提升了微调的安全性和样本效率**。

### 3. 主要实验结果
- **预训练**：LIFT在多个仿真任务中达到或超越了PPO、FastTD3等基线的性能，且收敛速度更快，成功实现了从仿真到真实室外环境的**零样本部署**。
- **微调**：在仿真到仿真（Sim2Sim）迁移任务中，LIFT能够在**仅约800秒（4万步）** 的真实交互数据内，成功适应**分布内、长尾及分布外**的新速度跟踪任务，而SAC、PPO等基线方法则出现性能退化或完全失败。
- **真实世界微调**：在Booster T1机器人上，仅用**数分钟的真实数据**，即可将一个不稳定的仿真策略微调为姿态更直立、步态更平滑的可行走策略。
- **消融实验**：验证了**SAC预训练**和**物理信息世界模型**均为框架成功的关键组件；纯神经网络世界模型（如MBPO）在微调中会因预测失真而失败。

### 4. 研究意义和价值
LIFT框架首次系统性地将**大规模离策略预训练**与**物理信息模型的高效微调**相结合，为人形机器人的持续学习提供了一条实用路径。其核心价值在于：
- **工程价值**：提供了从大规模仿真预训练、零样本部署到安全数据高效微调的**完整开源 pipeline**，降低了人形机器人强化学习的研究门槛。
- **算法创新**：提出的“环境内确定，模型内随机”微调范式，为解决机器人**安全适应**这一核心难题提供了新思路。
- **应用前景**：显著减少了微调所需的真实世界数据量和风险，推动了人形机器人从“一次训练”到“持续学习”的范式转变，为在多样、动态的真实环境中部署鲁棒、可适应的人形控制器奠定了基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究问题**
论文旨在解决**人形机器人控制**领域的一个关键矛盾：**大规模预训练的高效性与新环境下微调的样本效率之间的鸿沟**。
- **现状**：主流方法（如PPO）能利用大规模并行仿真进行快速预训练，并实现零样本部署到真实机器人。但其**同策略**特性导致样本效率低，难以在收集数据昂贵或危险的新环境中进行安全、高效的适应。
- **挑战**：直接在新环境中进行随机探索（如标准离策略RL）对人形机器人而言风险极高（易摔倒）。而从头开始训练基于模型的RL方法则耗时漫长且容易陷入局部最优。

### **核心创新点：LIFT框架**
论文提出了 **LIFT（Large-scale pretraIning and efficient FineTuning）** 框架，这是一个**三阶段**的预训练-微调流程，核心创新在于**将不同范式的优势进行耦合**。

1.  **阶段一：大规模、高效率的离策略预训练**
    - **技术创新**：提供了一个**基于JAX的高效SAC（Soft Actor-Critic）实现**，支持**大批量更新**和**高更新数据比（UTD）**。
    - **解决什么问题**：克服了传统观点中SAC不适合大规模并行训练的偏见，实现了**快速（单GPU一小时）且鲁棒**的策略收敛，并能零样本部署到真实人形机器人。这为后续微调提供了高质量的初始策略。

2.  **阶段二：物理信息世界模型的预训练**
    - **技术创新**：在预训练SAC收集的数据上，离线训练一个**物理信息世界模型**。该模型**结合了已知的拉格朗日动力学与神经网络预测的残差项**（如接触力、未建模的耗散扭矩）。
    - **解决什么问题**：相比纯神经网络的世界模型（如MBPO），该模型具有更强的**物理先验**和**外推能力**，能生成更准确、更稳定的合成轨迹，为安全微调奠定基础。

3.  **阶段三：安全、高效的模型化微调**
    - **技术创新**：提出了 **“环境内确定性执行，世界模型内随机探索”** 的微调范式。
        - **在真实/新环境中**：仅执行策略的**确定性动作（均值）** 收集数据，极大降低了随机探索带来的风险。
        - **在世界模型中**：使用**随机策略**进行探索，生成丰富的合成数据用于策略优化。
    - **解决什么问题**：**解耦了探索的风险与策略改进的需求**。既保证了数据收集过程的安全性（对人形机器人至关重要），又通过模型内探索保持了足够的探索覆盖率以实现策略提升，从而实现了**高样本效率的适应**。

### **解决方案的概括**
论文通过一个**统一的框架**，巧妙地串联了三种技术：
- **利用离策略RL（SAC）** 实现**快速大规模预训练**。
- **利用物理信息模型** 构建**高保真、可泛化的世界模型**。
- **利用模型化RL** 实现**安全、样本高效的微调**。

**最终效果**：LIFT框架将预训练阶段的**挂钟时间效率**与微调阶段的**样本效率**结合起来，为人形机器人提供了一条从大规模仿真快速获得基础技能，并能用极少量的真实数据安全适应新任务或新环境的可行路径。实验表明，该方法在仿真到仿真、仿真到真实的适应任务中，均优于PPO、FastTD3、MBPO等基线方法。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决人形机器人控制中**大规模预训练与高效微调之间的鸿沟**。针对现有方法（如PPO）样本效率低、难以安全适应新环境的问题，论文提出了一个名为 **LIFT** 的三阶段框架。该框架首先利用**大规模并行仿真和经过优化的SAC算法**进行快速、鲁棒的策略预训练，实现零样本部署；然后，利用预训练数据离线训练一个**融合拉格朗日动力学先验的物理信息世界模型**；最后，在新环境中进行微调时，**仅在真实环境中执行确定性策略以收集数据，而将随机探索限制在世界模型内部**进行策略优化。实验表明，该方法在预训练阶段实现了与主流基线相当的性能和快速的零样本仿真到现实迁移，在微调阶段则能以极少的真实交互数据（仅需数分钟）高效、安全地适应新任务和分布外目标，显著提升了人形机器人策略的适应性和样本效率。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文提出了名为 **LIFT** 的框架，旨在弥合人形机器人控制中大规模预训练与高效微调之间的鸿沟。其核心创新点可归纳为以下几条：

---

### 1. **提出“大规模预训练 + 物理信息世界模型 + 安全微调”的三阶段统一框架**
- **改进/不同之处**：
    - **以往方法**：主流方法（如PPO）主要依赖大规模并行仿真进行端到端训练，但样本效率低，难以安全适应新环境。而基于模型的RL方法（如SSRL）虽样本效率高，但从零开始训练人形机器人耗时极长且不稳定。
    - **本文方法**：将流程明确分为三个阶段：(i) 使用SAC进行大规模策略预训练；(ii) 利用预训练数据离线预训练一个**物理信息世界模型**；(iii) 在新环境中进行**确定性环境交互 + 世界模型内随机探索**的微调。
- **解决的问题/优势**：
    - **解决了** 大规模预训练（追求时钟效率）与安全、样本高效微调（追求数据效率）之间的目标冲突。
    - **带来了** 一个端到端的解决方案：预训练阶段利用大规模仿真快速获得一个可零次部署的稳健策略；微调阶段则能以极少的真实交互数据（几分钟），安全地将策略适配到新环境或分布外任务。

### 2. **开发了支持大规模并行仿真和高效微调的JAX版SAC实现**
- **改进/不同之处**：
    - **以往方法**：在并行仿真中，PPO是主流，而SAC等离策略算法因稳定性问题较少被用于大规模训练。已有工作（如FastTD3, PQL）需要额外的技巧（如混合高斯噪声）来稳定训练。
    - **本文方法**：实现了基于JAX的SAC，通过**固定张量形状、内核融合**等技术，支持**大批量更新和高更新数据比（UTD）**，且无需复杂的稳定器（如周期重置、集成评论家）。
- **解决的问题/优势**：
    - **解决了** 离策略算法在大规模并行仿真中收敛不稳定的问题，并充分发挥了GPU的并行计算能力。
    - **带来了** **极高的时钟效率**：在单块RTX 4090 GPU上，**1小时内**即可完成人形机器人行走策略的预训练，并实现零次部署到真实机器人。这为后续微调提供了高质量的初始策略。

### 3. **设计了“环境内确定性执行 + 世界模型内随机探索”的安全微调机制**
- **改进/不同之处**：
    - **以往方法**：标准微调（包括SAC、MBPO）通常需要在真实环境中进行随机探索（注入动作噪声），这对人形机器人（支撑面小、易摔倒）是高风险行为。ASAP等方法学习动作修正网络，但输出可能无界，且需要大量真实数据。
    - **本文方法**：在微调阶段，**在真实环境中只执行确定性策略（动作均值）**以收集数据；而**随机探索被限制在预训练的物理信息世界模型内进行**。策略通过在世界模型内生成的合成轨迹进行更新。
- **解决的问题/优势**：
    - **解决了** 微调阶段因随机探索导致的**安全性问题**和**执行器损坏风险**。
    - **带来了** **更高的样本效率和安全性**：只需收集少量确定性交互数据，即可在世界模型内进行充分且安全的探索来改进策略。实验表明，仅需约800秒（4万步）的交互，就能成功微调策略适应新的目标速度。

### 4. **构建并优化了适用于人形机器人的物理信息世界模型**
- **改进/不同之处**：
    - **以往方法**：SSRL为四足机器人设计了物理信息世界模型，但直接应用于人形机器人时存在不稳定问题。纯神经网络世界模型（如MBPO）在分布外动作下预测精度差。
    - **本文方法**：在SSRL基础上进行了关键改进：(i) 修正了特权状态到仿真器广义状态的映射；(ii) 在状态中明确加入了**基座高度**，这对人形平衡至关重要；(iii) 使用**自回归多步损失**进行训练，提升模型精度。
- **解决的问题/优势**：
    - **解决了** 人形机器人动力学复杂（特别是接触力）导致的世界模型难以训练和滚动不稳定的问题。
    - **带来了** **更高保真度的长时程预测**：该模型结合了已知的拉格朗日动力学与学习的残差（接触力、耗散力），为策略在模型内的探索提供了**可靠且物理一致的模拟环境**，这是微调成功的关键。

### 5. **首次系统验证了SAC预训练策略结合模型微调在人形机器人上的全流程有效性**
- **改进/不同之处**：
    - **以往工作**：要么只关注大规模预训练和零次部署（如PPO），要么只关注从零开始的模型学习（如SSRL），要么在微调能力上验证不足。缺乏一个从“快速预训练”到“安全高效微调”再到“真实机器人验证”的完整范例。
    - **本文方法**：通过系统的实验（仿真到仿真、仿真到真实）完整验证了LIFT框架：
        1. **预训练**：性能与PPO/FastTD3相当，但收敛更快。
        2. **微调**：在分布内、长尾、分布外任务上均能稳定适应，而基线方法（SAC、PPO、FastTD3、SSRL）出现发散或性能下降。
        3. **真实世界**：用仅**80-590秒的真实数据**，成功将一个零次部署失败的策略微调为稳定行走的策略。
- **解决的问题/优势**：
    - **解决了** 该领域缺乏一个**兼顾效率、安全性与通用性**的完整基准方案的问题。
    - **带来了** 一个**可复现的实践基线**：作者开源了全部代码，为社区提供了一个从大规模仿真预训练到真实机器人高效适配的端到端工具链。

---

**总结**：本文的核心创新在于**系统性**地整合并改进了多个技术模块（高效离策略预训练、物理信息建模、安全探索），形成了一个**统一、高效且安全**的人形机器人学习框架LIFT。它不仅在**时钟时间**上实现了快速预训练，更在**样本效率**和**安全性**上实现了高效的后续适应，为人形机器人的持续学习提供了一个切实可行的技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

该论文通过一系列实验，系统地评估了所提出的 **LIFT** 框架在**人形机器人控制**任务上的性能，涵盖了从大规模预训练到高效微调的全流程。

### 一、 主要实验效果

1.  **大规模预训练 (Pretraining)**：
    *   **效果**：在多个仿真环境中，LIFT（基于SAC）实现了与主流基线（PPO, FastTD3）**相当或更优**的收敛速度和最终性能。特别是在复杂地形上，LIFT能更快地达到稳定峰值。
    *   **关键成果**：预训练策略成功实现了**零样本（Zero-Shot）** 从仿真到真实机器人（Booster T1）的部署，能在草地、斜坡、泥地等多种室外未知地形上稳定行走。

2.  **高效微调 (Finetuning)**：
    *   **效果**：在仿真到仿真（Sim-to-Sim）和真实世界微调任务中，LIFT框架展现出**卓越的样本效率和稳定性**。
        *   **仿真微调**：在目标速度跟踪任务中（包括分布内、长尾和分布外目标），LIFT能快速收敛并精确跟踪，而SAC、PPO、FastTD3等基线方法则出现性能退化、振荡或完全失败。
        *   **真实世界微调**：仅用**80-590秒**的真实机器人交互数据，就能将一个在零样本转移中失败的策略，微调为具有更直立姿态、更平滑步态和更稳定速度的控制器。

### 二、 使用的数据集与评价指标

1.  **数据集**：
    *   **预训练数据**：在 **MuJoCo Playground** 仿真器中，通过大规模并行采样（1024或4096个并行环境）自行生成。涉及多种地形（平坦、粗糙）和机器人平台（Booster T1低维/全维模型、Unitree G1）。
    *   **微调数据**：
        *   **仿真**：在 **Brax** 仿真器中收集的**确定性策略交互数据**。
        *   **真实世界**：在 **Booster T1** 真实机器人上收集的**短时交互数据**（约几分钟）。

2.  **评价指标**：
    *   **核心指标**：**平均无折扣回报**。在 `E=1024` 个评估回合、每回合 `T_ep=1000` 步的设置下计算：`J^(π) = (1/E) * Σ Σ r_t`。
    *   **任务特定指标**：
        *   **速度跟踪精度**：机器人基座沿X轴的前向速度与目标速度的匹配程度。
        *   **收敛样本数/时间**：微调阶段达到稳定性能所需的环境交互步数或挂钟时间。
        *   **定性评估**：通过视频展示步态稳定性、身体振荡减少、行为改善等。

### 三、 对比的基线方法

论文与以下五类代表性基线进行了全面对比：

1.  **FastTD3**：高效的模型无关（Model-free）离线策略算法，专为并行仿真设计。
2.  **PPO**：主流的模型无关在线策略算法，是人形控制领域的常用基线。
3.  **SAC**：标准的模型无关离线策略算法（作为LIFT预训练的核心组件，也在微调中作为对比）。
4.  **SSRL**：基于物理信息世界模型的从头训练方法。
5.  **MBPO**：使用神经网络集成作为世界模型的经典模型基方法。

### 四、 关键性能提升与结论

1.  **预训练效率与鲁棒性**：
    *   **结论**：经超参数调优后，LIFT的SAC预训练模块在单块RTX 4090 GPU上**仅需约30分钟**即可完成训练，并实现零样本仿真到真实转移。其性能与PPO、FastTD3相当，证明了大规模离线策略预训练的可行性。

2.  **微调的样本效率与安全性**：
    *   **主要提升**：在最具挑战性的**分布外目标速度跟踪任务**（如1.5 m/s）中，LIFT是**唯一能稳定收敛并成功跟踪**的方法。
    *   **与基线对比**：
        *   **SAC/PPO/FastTD3**：在仅使用确定性环境交互的有限数据下，迅速过拟合或发散，无法有效利用数据。
        *   **SSRL**：由于缺乏预训练，在复杂人形任务中容易陷入局部最优（如仅学会站立）。
        *   **MBPO**：纯神经网络世界模型在分布外动作下产生物理不可信的预测，导致训练崩溃。
    *   **核心结论**：**“预训练 + 物理信息世界模型 + 模型内探索”** 的三段式设计，是实现**安全、高效微调**的关键。它结合了大规模仿真的时间效率与模型基学习的样本效率。

3.  **消融实验的发现**：
    *   **预训练阶段至关重要**：移除SAC预训练（即退化为SSRL）会导致微调失败；移除世界模型预训练会显著降低样本效率。
    *   **物理先验不可或缺**：将LIFT的物理信息世界模型替换为MBPO的神经网络集成模型，微调完全失败，凸显了物理结构对于预测泛化和训练稳定性的价值。
    *   **超参数影响**：较高的更新数据比（UTD）、合适的熵系数 `α` 和多步自回归训练损失，对微调成功有重要影响。

**总结**：LIFT框架通过创新的算法与系统设计，**实质性地弥合了大规模预训练与高效微调之间的鸿沟**。它在保持甚至提升预训练阶段效率的同时，显著提高了在有限、高风险数据下进行策略适应的能力，为人形机器人的持续学习提供了一条切实可行的技术路径。论文给出的定量结果（回报曲线、速度跟踪曲线）和定性视频证据充分支撑了上述结论。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.21363v1)
- [HTML 版本](https://arxiv.org/html/2601.21363v1)
