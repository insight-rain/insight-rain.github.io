# Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter

**相关性评分**: 6.0/10

**排名**: #31


---


## 基本信息

- **arXiv ID**: [2601.21297v1](https://arxiv.org/abs/2601.21297v1)
- **发布时间**: 2026-01-29T05:49:48Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Byeongjun Kim, H. Jin Kim

## 关键词

safe reinforcement learning

## 一句话总结

该论文提出了一种基于模型无关学习和可达性分析的安全过滤器，用于增强强化学习中的安全性，但未直接涉及视觉-语言-动作模型、扩散方法或边缘部署等技术。

## 摘要

We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems -- even including a hybrid system -- and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.

## 详细分析

## 论文摘要：Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter

**1. 研究背景和动机**
在机器人学和自主系统等安全关键控制领域，基于二次规划（QP）的安全滤波器因其能实时修改不安全参考输入而广受欢迎。然而，现有滤波器严重依赖于精确的系统模型知识，在模型未知或存在不确定性的“黑盒”系统中，其可靠性会急剧下降。尽管已有一些基于学习的方法尝试缓解此问题，但它们通常仍需要部分模型知识（如预定义的CBF结构），这限制了其在真实、未知动态系统上的部署。本文旨在填补这一理论与实际部署之间的鸿沟，提出一种完全数据驱动的、无需模型知识的安全滤波器学习框架。

**2. 核心方法和技术创新**
本文提出了 **Deep QP Safety Filter**，其核心创新在于将**时间折扣的Hamilton-Jacobi (HJ)可达性分析**与**无模型学习范式**相结合，直接从数据中学习一个QP安全滤波器。
*   **理论基础**：通过引入折扣因子，定义了折扣安全值函数及其优势函数，并证明了对应的贝尔曼算子是收缩算子，确保了学习的稳定收敛性。
*   **网络架构与损失函数**：训练两个神经网络分别近似安全值函数 `V^λ(x)` 及其沿动态的导数 `∂V^λ(x, u)`。设计了基于收缩性的损失函数（`ℒ_v` 和 `ℒ_∂v`），利用目标网络和回放缓冲区进行稳定的离策略学习。
*   **安全滤波器形式**：学习到的导数网络参数被用于构建一个在线QP，该QP在最小化与原始指令偏差的同时，强制执行基于学习到的安全值的约束，从而生成平滑、安全的控制指令。用户可通过一个标量参数 `α` 灵活调节滤波器的“激进”程度。

**3. 主要实验结果**
在多个动力学系统（包括双积分器、倒立摆、倒立双摆乃至接触驱动的混合系统Hopper）上进行了验证：
*   **收敛性**：学习到的评论家网络能够收敛到解析的HJ可达性解，即使该解是非光滑的，并且对不同的离散时间间隔 `δt` 具有鲁棒性。
*   **安全性与性能**：与多种基线方法（如PPO、PPO-Lagrangian、基于判别超平面的方法等）相比，本文方法在强化学习训练期间**显著减少了安全违规次数**，同时**加速了学习过程并获得了更高的累计奖励**。
*   **任务无关性与可调性**：滤波器是任务无关的，一旦训练完成，可通过调整 `α` 参数在不同任务中复用，平衡安全性与任务性能。

**4. 研究意义和价值**
本研究为**完全无模型的安全关键控制**提供了一条原则性强且实用的路径。其主要价值在于：
*   **理论贡献**：建立了折扣HJ可达性与无模型学习之间的理论桥梁，并提供了收敛性保证。
*   **实践价值**：方法不依赖于任何系统动力学模型，仅需可观测的状态和约束信号，极大地提升了在真实“黑盒”系统（如复杂机器人）上部署安全滤波器的可行性。
*   **性能优势**：相比开关式（bang-bang）滤波器或其他学习基线，本文的QP滤波器能产生更平滑的控制，且在保证安全的同时表现出更小的保守性，实现了安全与任务性能的有效平衡。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
当前基于二次规划（QP）的安全滤波器严重依赖精确的系统动力学模型，在模型未知或存在不确定性的“黑盒”系统中难以可靠部署。这造成了理论方法（如基于哈密顿-雅可比可达性分析的方法）与实际应用（如强化学习中的安全控制）之间的鸿沟。

### **核心创新点**
本文提出了一种**完全数据驱动、无需模型**的安全滤波器学习框架，名为 **Deep QP Safety Filter**。其核心创新在于：

- **模型无关的安全滤波器学习**：首次将**哈密顿-雅可比可达性**与**无模型学习**范式相结合，直接从系统转移数据中学习一个QP安全滤波器，无需任何先验动力学知识（`f(x)`, `g(x)`）。
- **基于收缩的贝尔曼算子**：针对**折扣安全值函数** `V^λ(x)` 及其**导数** `∂V^λ/∂x`，分别设计了具有**收缩性质**的贝尔曼算子（`𝒯_v` 和 `𝒯_dv`）。这保证了即使对于非光滑的真实值函数，神经网络的近似也能稳定收敛到其粘性解。
- **双网络架构与损失函数**：训练两个神经网络分别近似安全值 `v_θ^λ(x)` 和其导数相关的量 `a_φ^λ(x)`, `b_φ^λ(x)`。通过精心构造的损失函数（`ℒ_v`, `ℒ_∂v`），利用时间折扣和离线策略数据实现稳定学习。
- **可调攻击性的QP滤波器**：将学习到的安全值导数信息嵌入到一个标准QP中，通过一个用户定义的类K函数 `α(·)` 来**在线调节滤波器的保守程度**，实现了安全性与任务性能之间的灵活权衡。

### **解决方案与技术路径**
1.  **问题重构**：将安全定义为**时间折扣的哈密顿-雅可比可达性问题**。定义了折扣安全值函数 `V^λ(x)` 和状态-动作安全值函数 `Q_s^λ(x, u)`，并推导出其满足的PDE和优势函数 `q^λ(x, u)`。
2.  **无模型学习框架**：
    - **数据**：仅使用离散时间间隔 `δt` 采样的转移数据 `(x, u, c, x_δt^u, c_δt^u)`，其中 `c(x)` 是可观测的安全约束信号（类似RL中的奖励信号）。
    - **理论保证**：证明了在固定 `q^λ` 或 `V^λ` 的情况下，对应的贝尔曼算子是 `e^{-λ·δt}`-收缩的，这为迭代学习提供了收敛性基础。
    - **实践算法**：采用**目标网络**和**经验回放**等稳定深度RL技术，通过最小化（10）和（11）的损失函数来交替更新值网络和导数网络。
3.  **安全滤波器部署**：
    - 学习完成后，对于任何原始控制指令 `u_raw`，通过求解以下QP实时计算安全控制指令 `u_filtered`：
        ```math
        minimize ‖u - u_raw‖²
        subject to a_φ^λ(x)u - max_{u'∈𝒰} a_φ^λ(x)u' + b_φ^λ(x) + α(v_θ^λ(x)) ≥ 0
                 u ∈ 𝒰
        ```
    - 约束条件确保了安全值函数不会过快下降。当QP不可行时，回退到使安全值导数最大化的贪婪策略。
4.  **实验验证**：
    - **准确性**：在双积分器系统上，学习到的安全值与解析解高度吻合，即使真实值函数非光滑。
    - **泛化性**：在倒立摆、双倒立摆乃至**混合动力系统**（Hopper）上均有效，表明其对理论假设（如控制仿射性）具有实践鲁棒性。
    - **实用性**：与PPO等RL算法结合，在训练初期**显著减少了安全违规次数**，并加速了策略学习，最终获得更高的任务回报。滤波器可通过参数 `α` 调节攻击性，且**任务无关**，可跨任务复用。

### **实际价值**
- **为黑盒系统安全控制提供了新工具**：使得在模型未知或难以建模的复杂系统（如新型机器人、生物系统）上部署具有理论保障的安全层成为可能。
- **桥接了理论可达性与实际数据驱动方法**：将最优控制中强大的HJ可达性框架成功“移植”到无模型学习领域，兼具理论严谨性和实践灵活性。
- **提升了安全强化学习的实用性和效率**：通过任务无关的安全滤波器，大幅降低了RL训练过程中的探索风险，加速了收敛，为解决安全关键领域的RL应用（如自动驾驶、医疗机器人）提供了更可靠的方案。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**黑盒动力学系统在完全未知模型情况下的实时安全控制问题**，其核心是弥合传统基于模型的安全滤波器理论与实际部署之间的差距。为此，论文提出了 **Deep QP Safety Filter** 框架，该方法**将时间折扣的Hamilton-Jacobi可达性分析与无模型学习范式相结合**，通过设计具有收缩性的贝尔曼算子，训练两个神经网络来分别逼近安全值函数及其导数，从而构建一个纯数据驱动的二次规划安全滤波器。最终，该方法在多种动力学系统（包括混合系统）和强化学习任务中验证有效，**显著减少了训练过程中的安全违规次数，同时加速了策略学习并获得了更高的任务回报**，实现了在保证安全性的前提下，比现有基线方法更少保守、更实用的无模型控制。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter》针对黑盒动力系统的安全控制问题，提出了一种全新的、完全数据驱动的安全层方法。其核心创新点在于将**哈密顿-雅可比可达性分析**与**无模型学习**相结合，以学习一个二次规划安全滤波器。以下是其相对于已有工作的明确创新点：

---

### 1. **完全无模型的安全滤波器学习框架**
- **改进/不同之处**： 现有基于学习的安全方法（如基于CBF-QP或部分可达性的方法）通常需要部分模型知识，例如预定义的CBF结构、系统的Lipschitz常数或一个预先计算的安全策略。本文提出的方法**完全不需要任何先验的动力学模型知识**，仅从离散的转移数据 `(x, u, c, x_δt^u, c_δt^u)` 中学习。
- **解决的问题/优势**： 解决了在**真实、未知或动态变化的黑盒系统**上部署安全控制的理论与实践鸿沟。这使得该方法能够应用于模型难以精确获取或快速变化的实际平台（如复杂机器人），提升了安全控制方法的普适性和实用性。

### 2. **引入时间折扣可达性并构建收缩性损失**
- **改进/不同之处**： 论文将传统的HJ可达性价值函数重新定义为**时间折扣形式**（公式3, 4），并基于此推导出用于学习安全价值函数 `V^λ(x)` 及其导数 `∂V^λ/∂x` 的**收缩性贝尔曼算子**（`𝒯_v` 和 `𝒯_dv`，定理3.5, 3.6）。这不同于以往直接使用非收缩的递归定义（公式1, 2）或依赖模型求解HJ PDE的方法。
- **解决的问题/优势**：
    1.  **解决了学习稳定性问题**： 收缩性保证了在固定目标网络下，通过贝尔曼算子的重复应用，学习过程能够稳定收敛到唯一不动点。
    2.  **实现了无模型学习**： 基于折扣形式的贝尔曼更新（公式10, 11），可以直接从任意策略收集的离线或在线数据中进行**离策略学习**，无需预先知道或依赖最优安全策略来收集数据。
    3.  **处理非光滑解**： 理论证明，在精确设定下，学习到的评论家能够收敛到HJ PDE的**粘性解**，即使真实的安全价值函数是非光滑的。

### 3. **同时学习安全价值函数及其导数，并用于构造QP滤波器**
- **改进/不同之处**： 现有许多无模型安全学习方法仅学习一个安全价值或成本函数，并常导致**开关型滤波器**（仅在安全值低于阈值时干预），这可能产生不连续的控制指令。本文同时训练两个神经网络分别近似 `V^λ(x)` 和 `∂V^λ(x, u)`，并利用其导数项直接构造一个**连续的QP安全滤波器**（公式12）。
- **解决的问题/优势**：
    1.  **产生平滑控制**： QP公式通过最小化与原始指令的偏差来生成安全指令，确保了控制行为的平滑性和稳定性，特别适用于高度动态或不稳定系统（如双足机器人）。
    2.  **降低保守性**： 基于HJ可达性框架的滤波器被证明能产生**最小保守性**的安全控制，因为它精确地刻画了在输入约束下的最大可控安全集。
    3.  **提供理论保障**： 学习到的导数 `a_φ^λ(x) ≈ (∂V^λ/∂x)g(x)` 直接对应于动力学模型，使得QP约束具有明确的控制理论解释（类似于CBF的导数条件）。

### 4. **设计可调的攻击性参数与任务无关的部署**
- **改进/不同之处**： 在QP滤波器约束中引入了一个用户定义的**扩展类K函数 α(·)**（简化为 `α·v_θ^λ(x)`）。通过调整增益 `α`，用户可以**在线、连续地调节滤波器的“攻击性”**，而无需重新训练安全评论家网络。
- **解决的问题/优势**：
    1.  **平衡安全与性能**： 用户可以根据硬件极限或任务需求，在安全范围内灵活调整控制器的激进程度（如图3所示，更大的 `α` 允许更快的加速/减速）。
    2.  **任务无关性**： 安全滤波器仅从安全信号 `c(x)` 学习，与任何具体的任务奖励函数无关。因此，**同一个训练好的滤波器可以即插即用地与不同的RL策略或任务结合**，提高了方法的通用性和复用性（如图4所示，同一滤波器用于两个奖励冲突的倒立摆任务）。

### 5. **在混合动力系统上的实证验证与理论假设松弛**
- **改进/不同之处**： 论文明确地将方法应用于**接触驱动的混合系统**（Hopper-v5），该系统具有不连续的状态跃迁，并不严格满足理论中控制仿射和光滑性的假设。
- **解决的问题/优势**：
    1.  **证明了方法的实践鲁棒性**： 尽管存在理论假设的违背，但通过神经网络从离散转移数据中学习，该方法依然取得了强大的性能，且QP不可行率极低（0.2343%）。这表明**神经网络近似器在实践中能够有效地平滑和泛化**，放宽了严格的理论前提。
    2.  **拓展了应用范围**： 验证了该方法不仅能用于简单的连续系统，也能有效处理更复杂、更贴近现实机器人学的混合动力学问题。

---

**总结**： 本文的核心创新在于**提出并实现了一个从理论（折扣HJ可达性）到算法（收缩性贝尔曼学习）再到应用（可调QP滤波器）的完整、完全无模型的安全层解决方案**。它系统性地解决了以往方法对模型知识的依赖、学习过程的不稳定性、控制指令的非平滑性以及保守性与性能难以权衡等问题，为在未知动力学系统上实现安全、高性能的控制提供了一条有原则且实用的路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过一系列仿真实验，全面评估了所提出的 **Deep QP Safety Filter** 的性能。实验设计旨在验证其**理论收敛性、安全有效性、任务通用性**以及与强化学习（RL）结合的性能提升。

### 一、 使用的“数据集”与评价指标

由于论文研究的是**模型无关（Model-free）** 的安全滤波器学习，其“数据”并非静态数据集，而是通过与仿真环境的**在线交互**动态生成的**状态-动作转移轨迹**。实验在多个标准控制环境中进行：

1.  **基础验证系统**：
    *   **Double Integrator（双积分器）**：用于验证学习到的安全价值函数与解析解的收敛性。
    *   **Inverted Pendulum（倒立摆）**：用于展示滤波器攻击性参数 `α` 的可调性。

2.  **复杂RL任务环境**（来自Gymnasium）：
    *   **Inverted Double Pendulum-v5（倒立双摆）**：高度不稳定的欠驱动系统。
    *   **Hopper-v5（单足跳跃机器人）**：具有接触动力学的**混合系统**，不严格满足理论中的光滑性假设。

3.  **核心评价指标**：
    *   **收敛精度**：学习到的安全价值函数 `v_θ^λ(x)` 及其导数与**解析解**的视觉对比（定性）和误差（隐含）。
    *   **安全性能**：
        *   **训练期间的安全违规次数（Failures）**：在RL策略收敛前，系统违反状态约束（`c(x) < 0`）的回合数。这是衡量安全滤波器**预收敛保护能力**的关键指标。
        *   **QP不可行率**：在数百万次滤波调用中，二次规划问题无解的次数比例，用于评估学习稳定性。
    *   **任务性能**：与RL结合时，最终达到的**累计奖励（Return）** 和**学习速度（收敛曲线）**。
    *   **控制行为**：滤波后控制指令的**平滑性**（与Bang-Bang方法对比）和**攻击性**（通过参数 `α` 调节）。

### 二、 对比的基线方法

论文将Deep QP Safety Filter与多个强基线方法在RL任务上进行了对比：

1.  **PPO（Proximal Policy Optimization）**：标准的无安全约束的RL算法。
2.  **PPO-Lagrangian**：一种将安全约束作为惩罚项融入RL目标的常用安全RL方法。
3.  **PPO with RL-DH**：结合了**基于判别超平面（Discriminating Hyperplane）** 的模型无关安全滤波器的方法。
4.  **PPO with Bang-Bang Safety Filter（作者自建变体）**：将本文方法退化为一个**开关式滤波器**（当 `v_θ^λ(x) < 0.2` 时进行干预），用于凸显QP滤波器的平滑性优势。

### 三、 关键性能提升与结论

1.  **理论收敛性得到验证**：
    *   在Double Integrator系统上，学习到的安全价值函数 `v_θ^λ(x)` 及其梯度项 `a_φ^λ(x)`, `b_φ^λ(x)` 在不同采样间隔 `δt`（0.1, 0.05, 0.01, 0.005）下，均能**准确收敛到非光滑的解析解（粘性解）**。这证明了所提损失函数和收缩算子的有效性。

2.  **卓越的安全性能与低保守性**：
    *   **极低的预收敛失败率**：在Inverted Double Pendulum和Hopper等复杂、不稳定系统中，与所有基线相比，**本文方法在RL策略训练早期几乎完全避免了安全违规**（见图4）。而PPO和PPO-Lagrangian在探索阶段发生了大量失败。
    *   **高可行性**：在超过百万次的在线滤波调用中，QP不可行率极低（Double Integrator等系统为0%，混合系统Hopper也仅为0.2343%），表明学习稳定可靠。
    *   **基于可达性的最小保守性**：论文指出，由于方法根植于Hamilton-Jacobi可达性理论，能够计算出**最大控制不变安全集**，因此相比其他基于累积代价或启发式的方法（如RL-DH），其行为是**理论上最不保守**的，在保证安全的前提下最大化了对原始指令的跟踪。

3.  **提升RL任务性能**：
    *   **更快学习，更高回报**：在安全约束与任务奖励存在冲突的挑战性场景下（例如，倒立双摆需要平衡但又不能移动太快），配备Deep QP Safety Filter的PPO不仅更安全，而且**学习速度更快，最终获得的累计奖励也显著高于所有基线**（见图4a, 4b, 4c）。
    *   **任务无关与可调性**：安全滤波器的训练**完全独立于任务奖励**，仅依赖于安全信号 `c(x)`。因此，同一个训练好的滤波器可以通过调整一个参数 `α` 来适应不同攻击性要求的任务，展示了强大的通用性和实用性。

4.  **工程实用优势得到体现**：
    *   **平滑控制 vs. 开关控制**：与自建的Bang-Bang变体对比表明，QP公式能产生**平滑的控制指令**，避免了开关式滤波带来的突变，这对高动态系统（如机器人）的执行器更友好。
    *   **对假设的鲁棒性**：在理论上不满足光滑性假设的**混合系统（Hopper）** 上，方法依然表现强劲，证明了神经网络近似在实际中的有效性和鲁棒性。

### 总结

论文通过系统的实验表明，Deep QP Safety Filter成功实现了 **“理论严谨”** 与 **“实践有效”** 的统一。它不仅能在模型未知的情况下，从数据中学习到具有理论保证的安全边界，更能作为一层通用的“安全盔甲”，显著提升强化学习在安全关键场景中的**训练效率、最终性能和安全可靠性**，其效果全面优于现有的主流安全RL方法。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.21297v1)
- [HTML 版本](https://arxiv.org/html/2601.21297v1)
