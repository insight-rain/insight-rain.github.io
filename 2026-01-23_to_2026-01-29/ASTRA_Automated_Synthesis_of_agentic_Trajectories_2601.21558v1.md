# ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas

**相关性评分**: 6.0/10

**排名**: #14


---


## 基本信息

- **arXiv ID**: [2601.21558v1](https://arxiv.org/abs/2601.21558v1)
- **发布时间**: 2026-01-29T11:22:23Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Xiaoyu Tian, Haotian Wang, Shuaiting Chen, Hao Zhou, Kaichi Yu, Yudian Zhang, Jade Ouyang, Junxi Yin, Jiong Chen, Baoyan Guo, Lei Zhang, Junjie Tao, Yuansheng Song, Ming Cui, Chengwei Liu

## 关键词

fine tune, offline Reinforcement Learning, world model

## 一句话总结

ASTRA是一个自动化框架，通过合成轨迹和环境来训练工具增强的语言模型代理，结合监督微调和强化学习以提升多步决策能力。

## 摘要

Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.

## 详细分析

## ASTRA 论文详细摘要

### 1. 研究背景和动机
大型语言模型（LLM）作为工具增强型智能体，在多步决策任务中的应用日益广泛。然而，训练鲁棒、通用的工具使用智能体仍面临诸多挑战：现有方法通常依赖人工干预、基于不可验证的模拟环境、仅采用监督微调（SFT）或强化学习（RL）单一训练范式，且难以实现稳定的长视野、多轮次学习。为解决这些问题，本文提出了 **ASTRA**，一个全自动、端到端的框架，旨在通过可扩展的数据合成和可验证的强化学习来训练工具增强的语言模型智能体。

### 2. 核心方法和技术创新
ASTRA 的核心创新在于整合了两个互补的组件，并设计了一套统一的训练方法：
- **基于工具调用图静态拓扑的轨迹合成**：提出一个自动化流水线，利用工具调用图的静态结构，合成多样化、结构化的多轮次工具使用轨迹。该过程结合了真实 MCP 服务器和工具模拟器，并引入自动化的轨迹质量评估（奖励建模），为高质量的 SFT 提供数据，无需人工标注。
- **基于人类语义推理组合拓扑的环境合成**：提出一个环境合成框架，将分解后的问答对（QA）轨迹转换为独立的、可代码执行的、规则可验证的环境。这为多轮次、长视野的在线 RL 提供了确定性的状态转移和可靠的奖励信号。
- **统一的 SFT+RL 训练方法**：采用两阶段训练策略。首先，利用合成的轨迹进行 SFT，获得一个适应多轮次工具交互的更强初始策略。然后，在合成的可验证环境中进行在线多轮次 RL，并引入**无关工具混合**和**F1风格轨迹级奖励**，以共同优化任务完成度和交互效率。

### 3. 主要实验结果
在多个智能体工具使用基准测试（BFCL-v3 Multi-Turn, τ²-Bench, ACEBench）上的实验表明：
- ASTRA 训练的模型（14B 和 32B 参数规模）在同等规模的开源模型中达到了**最先进的性能**，其表现接近甚至部分超越了一些闭源系统（如 Claude-Sonnet-4.5）。
- 训练过程分析显示，SFT 和 RL 阶段均带来了性能提升，其中 RL 阶段贡献了最大的增益。
- 在专注于核心推理能力的非智能体基准（AIME2024/2025）上，ASTRA 模型保持了与原始模型相当的水平，表明其增强工具使用能力的同时，**未损害核心推理能力**。
- 消融实验证实了无关工具混合和 F1 奖励设计对于稳定训练和提升工具选择判别力的关键作用。

### 4. 研究意义和价值
ASTRA 的研究具有重要的理论意义和实际价值：
- **方法论贡献**：首次系统性地将**工具静态拓扑**与**语义推理组合拓扑**相结合，为自动化生成高质量训练数据和可验证训练环境提供了完整解决方案，显著减少了对人工标注和手动设计环境的依赖。
- **工程价值**：提出的全自动端到端框架、改进的训练基础设施（如自适应批次填充）以及开源的完整流水线、环境和模型，极大地促进了该领域研究的可复现性和进一步发展。
- **应用前景**：ASTRA 能够训练出在复杂、多轮次场景下表现鲁棒的工具使用智能体，为实际部署（如信息检索、数据分析、交互式对话系统）提供了更可靠的解决方案。其方法有望缓解实际部署中对静态、场景特定标注数据的依赖，通过合成多样化的可执行环境进行迭代交互训练来提升智能体的鲁棒性。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## ASTRA 论文核心分析

### **一、论文想解决的核心问题**
当前训练**工具增强型大语言模型（LLM）代理**面临四大挑战：
1.  **依赖人工干预**：数据构建和验证需要大量人工。
2.  **环境不可验证**：许多方法依赖LLM模拟环境，其状态转移和奖励信号**非确定性、不可验证**，导致强化学习（RL）不稳定，尤其不利于长视野、多轮次学习。
3.  **训练范式单一**：要么只使用监督微调（SFT），要么只使用强化学习（RL），无法兼顾**广度与深度**。
4.  **长视野决策能力弱**：现有方法常将多轮轨迹分解为单步实例训练，破坏了**连贯的多轮决策学习**。

### **二、核心技术创新：ASTRA框架**
ASTRA是一个**全自动、端到端**的框架，通过**可扩展的数据合成**和**可验证的强化学习**来训练工具代理。其核心创新在于整合了两个互补的组件，分别对应两种“拓扑结构”：

#### **1. 用于SFT的轨迹合成管道（利用“静态工具调用图拓扑”）**
- **目标**：自动化生成高质量、多样化的多轮工具使用轨迹，用于监督微调，赋予模型**广泛、可迁移的工具使用能力**。
- **关键步骤**：
    - **工具收集与规范化**：从开放MCP注册中心等来源收集工具，统一为OpenAI工具调用格式。
    - **工具链构建**：基于工具文档，使用LLM为每个MCP服务器合成可能的“任务-工具链”对，并构建**工具转移图**，通过随机游走采样候选链。
    - **任务构建与增强**：结合“链条件生成”和“仅服务器生成”两种模式，并通过**多样性、复杂性、角色条件**三种方式进行增强，确保任务真实、多样。
    - **轨迹收集与奖励建模**：使用混合执行（真实MCP服务器 + 文档模拟器）进行多轮交互，并设计**自动化轨迹质量评估管道**（包含查询理解、规划、工具响应理解、工具调用状态、简洁性、最终答案质量等7个维度），为每条轨迹生成标量奖励，用于筛选高质量SFT数据。

#### **2. 用于RL的环境合成框架（利用“人类语义推理的组合拓扑”）**
- **目标**：将人类解决问题的**语义推理过程**转化为**独立、代码可执行、规则可验证**的环境，为多轮在线RL提供**确定性**的训练场。
- **关键步骤**：
    - **Q-A实例合成**：将复杂问题分解为主问题和一系列具有依赖关系的子问题-答案对 `(q_i, a_i, d_i)`，显式建模解决问题的**语义拓扑**（如链式或DAG）。
    - **质量验证**：从**依赖一致性、子问题原子性、顺序合理性、任务完整性**四个维度过滤和评分Q-A实例，确保逻辑严密。
    - **环境合成**：为每个通过验证的子任务节点合成：
        - **工具规范文档**
        - **Python工具实现代码**
        - 在沙箱中执行验证，确保代码能返回目标答案 `a_i`。
    - **子环境合并**：识别功能相同的子问题，将其工具实现合并到单个代码中，避免动作空间膨胀。

### **三、统一的训练方法学**
ASTRA提出**两阶段训练法**，将SFT与RL有机结合：
1.  **SFT阶段**：使用上述合成的轨迹数据对基础模型进行微调，获得一个**更适应多轮工具交互的初始策略**。
2.  **RL阶段**：在合成的可验证环境上进行**在线、多轮RL**，关键设计包括：
    - **无关工具混合**：在训练时，为每个环境额外添加**高、中、低**三个语义相似度等级的无关工具，迫使模型学习**工具判别能力**，避免过拟合。
    - **轨迹级F1奖励**：奖励设计兼顾**任务完成度（召回率）** 和**交互效率（精确率）**，公式为 `F1 = 2pr/(p+r)`。这有效平衡了探索（多调用工具）与利用（避免冗余调用），防止训练崩溃。
    - **自适应批次填充**：一种高效的批次构建策略，确保每个优化步骤都使用能提供有效学习信号（奖励方差非零）的样本，提升训练稳定性。

### **四、实际价值与实验结果**
- **性能领先**：在BFCL-v3 Multi-Turn、τ²-Bench、ACEBench等多个代理工具使用基准测试中，ASTRA训练的模型（14B/32B）在**同等规模下达到最优性能**，甚至接近部分闭源系统（如Claude Sonnet 4.5）。
- **保持核心推理能力**：在非代理的复杂数学推理基准（AIME2024/2025）上，性能**未出现退化**，证明其专精于工具使用而不损害模型根本的推理能力。
- **开源与可复现**：论文完整开源了**数据合成管道、合成环境及训练好的模型**，极大促进了该领域的研究和实际应用。
- **解决部署瓶颈**：提供了一种**自动化、低成本**生成大量可验证训练环境的方法，减少了对人工标注数据和特定场景模拟的依赖，有助于训练更**鲁棒、通用**的实用化AI代理。

### **总结**
ASTRA的核心创新在于**系统性地将两种“拓扑”——工具调用的静态工作流拓扑和人类语义推理的动态逻辑拓扑——转化为自动化、可验证的训练数据与环境**，并通过精心设计的**两阶段训练范式与RL奖励机制**，实现了对工具代理**广度（泛化性）与深度（复杂场景鲁棒性）** 的协同提升。这为构建下一代实用、可靠的大模型智能体提供了重要的方法论和工程实践基础。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决训练**工具增强型大语言模型智能体**时面临的三大核心挑战：**1）依赖人工干预和不可验证的模拟环境**；**2）训练方法单一（仅用SFT或RL）**；**3）难以实现稳定、长视野的多轮决策学习**。

为此，论文提出了一个名为 **ASTRA** 的**全自动端到端训练框架**。该框架的核心创新在于整合了两个互补的自动化数据合成组件：**1）基于工具调用图静态拓扑的轨迹合成管道**，用于生成高质量、多样化的多轮工具使用轨迹，以进行监督微调（SFT）；**2）基于人类语义推理组合拓扑的环境合成框架**，将分解的问题-答案对转化为**独立、可代码执行、规则可验证**的强化学习环境，从而支持确定性的多轮在线RL。

通过结合SFT与在线RL的统一训练方法，ASTRA训练的模型在多个智能体工具使用基准测试中，**在同等参数规模下达到了最先进的性能**，接近闭源系统的水平，同时保持了模型的核心推理能力。实验表明，该方法有效平衡了任务完成度与交互效率，显著提升了智能体在复杂、长视野场景下的鲁棒性和泛化能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## ASTRA论文的核心创新点分析

ASTRA论文提出了一种用于训练工具增强型语言模型智能体的全自动端到端框架。其核心创新点主要体现在**数据合成方法**和**训练范式**上，旨在解决现有方法在**可验证性**、**长程决策**和**训练稳定性**方面的关键瓶颈。

以下是其相对于已有工作的明确创新点：

### 1. **双拓扑驱动的数据合成框架**
   - **改进之处**： 现有方法通常依赖单一的数据来源（如静态工具链或LLM模拟环境）。ASTRA创新性地提出了两种互补的、基于不同“拓扑”的数据合成路径：
     - **静态工具调用图拓扑**： 用于**监督微调（SFT）数据合成**。通过分析工具间的调用关系图，合成多样化、结构化的多轮轨迹。
     - **人类语义推理的组合拓扑**： 用于**强化学习（RL）环境合成**。将复杂的问答对分解为具有依赖关系的子任务图，并转化为可执行的代码环境。
   - **解决的问题与优势**：
     - **解决了数据多样性与结构性的平衡问题**。工具图拓扑确保了数据在工具使用逻辑上的合理性（广度），而语义推理拓扑则捕获了复杂任务背后的认知步骤（深度）。
     - **为后续的两阶段训练（先SFT后RL）提供了高质量、目标明确的数据基础**，使模型既能广泛掌握工具用法，又能深入处理复杂场景。

### 2. **可验证、代码可执行的环境合成**
   - **改进之处**： 以往许多RL训练依赖**LLM模拟的环境**，其中工具执行、状态转移和反馈均由语言模型生成，缺乏明确的规则，是**不可验证的**。ASTRA将分解后的QA轨迹转化为独立的、**代码可执行的Python环境**。
   - **解决的问题与优势**：
     - **解决了RL训练中确定性奖励和状态转移的难题**。代码环境提供了确定性的、可重复的执行结果，为稳定的多轮在线RL提供了基础。
     - **带来了训练稳定性和可复现性的巨大优势**。避免了因LLM模拟器的不确定性或不一致性导致的训练波动，使得长期、多步的决策学习成为可能。

### 3. **面向长程决策的多轮在线RL范式**
   - **改进之处**： 许多现有方法将多轮轨迹分解为孤立的单步训练样本，或者仅在工具调用节点进行更新。ASTRA采用**真正的在线、多轮智能体RL范式**。模型在完整的环境中进行多步交互，收集包含历史观察、行动和奖励的完整轨迹，并用于策略优化。
   - **解决的问题与优势**：
     - **解决了模型难以学习连贯长程决策的问题**。通过在整个交互轨迹上进行优化，模型学会了在长期视野下进行规划和状态跟踪，而不是优化孤立的单步动作。
     - **使智能体能够学习从错误中恢复和重新规划的策略**，提升了在复杂多步任务中的鲁棒性。

### 4. **统一的两阶段训练方法论：SFT + 轨迹级奖励RL**
   - **改进之处**：
     - **结合而非割裂**： 现有工作往往只侧重SFT或RL之一。ASTRA明确将两者结合，先用合成的SFT数据获得一个**更强的初始策略**，再在此基础上进行在线RL微调。
     - **创新的奖励设计**： RL阶段采用**F1风格的轨迹级奖励**，同时优化任务完成率（召回率）和交互效率（精确率），公式为 `2pr/(p+r)`，其中 `p = 成功子任务数 / 工具调用次数`, `r = 成功子任务数 / 总子任务数`。
   - **解决的问题与优势**：
     - **解决了单一训练范式的局限性**。SFT提供了良好的起点和工具使用规范，RL则通过环境交互进一步优化决策质量和效率。
     - **解决了奖励稀疏和短视问题**。轨迹级奖励为整个解决过程提供密集信号，鼓励模型不仅要做对，还要高效地做对，避免了工具滥用或过度保守的行为。

### 5. **“无关工具混合”技术**
   - **改进之处**： 在RL训练时，ASTRA不会只提供完成任务必需的工具，而是会主动加入一批**语义相似度各异的无关工具**（分为高、中、低相似度三个区间）。
   - **解决的问题与优势**：
     - **解决了模型在“干净”工具集中过拟合、缺乏工具判别能力的问题**。现实部署中，智能体面对的是包含大量无关API的工具池。
     - **迫使模型学习“负向判断”**，即不仅要知道调用哪个工具，还要学会忽略不相关的工具。论文中的消融实验证明，该技术对最终性能提升至关重要。

### 6. **工程优化：自适应批次填充**
   - **改进之处**： 针对GRPO等策略优化算法在奖励方差为零时梯度信号消失的问题，ASTRA设计了**自适应批次填充**策略。它持续进行环境交互，但只将能产生有效学习信号（奖励方差大于阈值）的轨迹放入训练批次。
   - **解决的问题与优势**：
     - **解决了在线RL因无效样本导致的训练不稳定和低效问题**。
     - **确保了每个训练步骤都基于“信息量最大”的样本**，提高了训练数据的利用效率和整体稳定性。

### 总结
ASTRA的核心创新在于**系统性**地构建了一个**自动化、可验证、面向长程决策**的智能体训练闭环。它通过**双拓扑数据合成**解决了数据来源的质量和多样性问题；通过**代码环境**和**多轮在线RL**解决了训练稳定性和长程能力问题；通过**SFT+RL两阶段训练**和**无关工具混合**等策略，确保了智能体既具备广泛的基础能力，又在复杂、嘈杂的环境中表现出精准和鲁棒性。其实验结果在多个智能体基准测试上达到同规模SOTA，并逼近闭源系统，验证了这套框架的有效性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

根据论文内容，ASTRA 在多个基准测试上进行了全面的实验评估，旨在验证其自动化框架在训练工具增强语言模型代理方面的有效性。

### 一、 使用的数据集与评价指标

#### 1. 代理能力基准测试（Agentic Benchmarks）
论文使用以下三个广泛认可的交互式基准来评估模型的**多轮工具使用能力**：
- **BFCL-v3 Multi-Turn (BFCL-MT)**：评估模型在功能缺失、参数缺失、长上下文等复杂场景下的多轮工具调用能力。
- **τ²-Bench**：在一个双重控制环境中评估对话代理，包含零售和电信领域，并集成了用户模拟器以测试交互鲁棒性。论文排除了其航空子集。
- **ACEBench**：专注于工具学习的基准，同样包含用户模拟器，评估多轮、多步骤的交互能力。

**评价指标**：这些基准主要使用**任务完成准确率**作为核心指标。具体来说：
- **BFCL-MT**：报告了“基础任务”、“缺失功能”、“缺失参数”、“长上下文”四个子类别的得分以及综合“总体”得分。
- **τ²-Bench**：报告了“零售”、“电信”领域得分及“总体”得分。
- **ACEBench**：报告了“多轮”、“多步”场景得分及“总体”得分。

#### 2. 非代理推理能力基准测试（Non-agentic Benchmarks）
为了验证ASTRA训练方法不会损害模型的核心推理能力，论文补充评估了：
- **AIME2024** 与 **AIME2025**：专注于数学问题解决的基准。
- **评价指标**：**通过率**。

### 二、 对比的基线方法

论文将ASTRA训练的模型与以下两类基线进行了广泛对比：

#### 1. 闭源模型（Closed-source）
- Claude-Opus-4.5, Claude-Sonnet-4.5, Claude-Haiku-4.5
- Gemini-3-Pro, Gemini-2.5-Pro
- GPT-4.1

#### 2. 开源模型（Open-source）
- Kimi-K2-Instruct
- GLM-4.6
- LoopTool-32B (一个专注于工具使用的先进方法)
- **基础模型**：Qwen3-14B 和 Qwen3-32B (作为ASTRA训练的起点)

### 三、 关键性能结果与结论

#### 1. 总体性能表现（见表1和表2）
- **SOTA级性能**：在**相同参数量级**的开源模型中，ASTRA训练的模型（Astra-14B/32B）在BFCL-MT、τ²-Bench和ACEBench上均取得了**最先进的性能**。
- **逼近闭源系统**：ASTRA-32B的性能与部分更大规模的闭源模型（如Claude-Sonnet-4.5）相当，甚至在BFCL-MT的“基础任务”上超越了Claude-Opus-4.5。
- **两阶段训练均有效**：
    - **SFT阶段**：通过工具链合成的轨迹进行监督微调，显著提升了模型对多轮工具交互的适应性，性能明显优于原始基础模型（Qwen3）。
    - **RL阶段**：在可验证环境上进行在线多轮强化学习，带来了**最大的性能提升**。例如，Astra-32B在BFCL-MT上的总体得分相比基础模型提升了16.38个百分点，在τ²-Bench上提升了14个百分点。

#### 2. 核心推理能力保持（见表3）
- **无显著退化**：在AIME2024/2025数学推理基准上，ASTRA训练后的模型（Astra-14B/32B）与原始Qwen3模型相比，性能**持平或略有波动**，但未出现显著下降。
- **结论**：这表明ASTRA专注于提升**代理工具使用能力**，同时成功**保留了模型的核心推理能力**。

#### 3. 关键消融实验与深入分析
论文通过消融实验验证了其核心设计选择的有效性：

- **无关工具混合策略**（图6）：
    - **无无关工具**：性能最差，模型过拟合于狭窄的工具选择模式。
    - **随机混合无关工具**：有所改善，但不如ASTRA的完整策略。
    - **ASTRA策略（按语义相似度分波段混合）**：效果最佳。这表明让模型学习在包含**高相似度、中相似度、低相似度**的无关工具池中进行判别，对于学习“负向工具判断”能力至关重要。

- **奖励函数设计**（图7, 8）：
    - **仅召回奖励**：导致交互轮数爆炸性增长，训练不稳定。
    - **仅精确率奖励**：导致模型过于保守，工具调用次数锐减，性能低下。
    - **F1风格奖励（召回与精确率的调和平均）**：取得了最佳且稳定的训练效果。该奖励同时鼓励**完成任务**和**提升交互效率**，引导模型在探索（使用工具解决问题）和利用（避免冗余调用）之间取得平衡。

- **分阶段行为分析**（表4）：
    - **交互步数**：SFT和RL阶段并未系统性改变对话深度，表明性能提升并非源于简单地拉长或缩短交互。
    - **输出长度**：原始模型输出最长；SFT模型输出被压缩为简洁的示范风格；RL模型输出长度介于两者之间。这反映了RL鼓励模型进行更广泛的轨迹空间探索，而非模仿固定模式。

### 四、 总结

**最终效果**：ASTRA框架通过**全自动的轨迹合成**和**可验证的环境合成**，结合**SFT+RL的两阶段训练方法**，成功训练出了在**多轮工具使用任务上达到开源SOTA水平**的语言模型代理。其性能逼近顶尖闭源模型，且不损害核心推理能力。

**核心结论**：
1.  **自动化数据合成可行且高效**：基于工具链拓扑和人类语义推理拓扑的自动化合成管道，能够产生高质量的训练数据与环境。
2.  **可验证的RL环境是关键**：代码可执行、规则可验证的环境为稳定的多轮在线RL提供了基础，这是性能大幅提升的关键。
3.  **精心设计的训练策略至关重要**：无关工具混合和F1风格轨迹级奖励等设计，有效塑造了模型高效、鲁棒的工具使用行为。

论文给出了明确的定量结果，并在多个维度上与强基线进行了对比，充分证明了ASTRA方法的有效性和先进性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.21558v1)
- [HTML 版本](https://arxiv.org/html/2601.21558v1)
