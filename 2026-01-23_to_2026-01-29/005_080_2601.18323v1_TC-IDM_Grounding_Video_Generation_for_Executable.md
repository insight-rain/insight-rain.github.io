# TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion

**相关性评分**: 8.0/10

**排名**: #5


---


## 基本信息

- **arXiv ID**: [2601.18323v1](https://arxiv.org/abs/2601.18323v1)
- **发布时间**: 2026-01-26T10:06:56Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin, Kuangzhi Ge, Kai Tang, Peidong Jia, Shanghang Zhang, Jian Tang

## 关键词

Vision-Language-Action Model, VLA for Robotics, world model, fine tune, offline Reinforcement Learning

## 一句话总结

TC-IDM通过工具中心逆动力学模型，将生成世界模型的像素级计划转化为可执行的机器人动作，弥补了视觉语言动作模型在泛化和物理执行方面的不足。

## 摘要

The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.
  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.
  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.
  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.
  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.

## 详细分析

## 论文摘要：TC-IDM：面向可执行零样本机器人运动的视频生成落地方法

**1. 研究背景和动机**
当前，视觉-语言-动作模型虽能实现强大的机器人控制，但其严重依赖大规模、高质量的机器人数据，限制了泛化能力。生成式世界模型为通用具身智能提供了新范式，但其生成的像素级规划与物理可执行动作之间存在“最后一公里”的鸿沟。现有方法在跟踪视频中的物体状态时，面对遮挡、快速运动或可变形物体时往往表现脆弱。因此，亟需一种鲁棒的方法来弥合高层视觉规划与底层物理控制之间的差距。

**2. 核心方法和技术创新**
本文提出了**工具中心化逆动力学模型**。其核心创新在于：**将世界模型生成的视频中末端执行器的想象轨迹作为连接视觉规划与物理控制的鲁棒中间表示**。TC-IDM采用“规划-翻译”的两阶段流程：
- **规划阶段**：利用世界模型根据初始RGB-D图像和文本指令生成未来视频。
- **翻译阶段**：首先，使用SAM 3分割末端执行器，并结合3D运动估计器从生成视频中提取其密集点云的6自由度轨迹。然后，通过**解耦的动作预测头**，分别将这些轨迹和融合的视觉-文本特征，映射为可执行的机器人末端位姿轨迹和夹爪控制信号。

**3. 主要实验结果**
在真实世界评估中，TC-IDM展现出卓越性能：
- **整体性能**：平均任务成功率达到**61.11%**，在简单任务上高达**77.7%**。
- **零样本泛化**：在未训练过的可变形物体（如布料）操作任务上，成功率达**38.46%**，显著优于端到端VLA基线及其他逆动力学模型。
- **泛化能力**：在相机视角变化、长时程任务（如叠衣服）、跨 embodiment（从单臂迁移到双臂机器人）等多个维度上均表现出强大的零样本泛化能力。

**4. 研究意义和价值**
TC-IDM通过聚焦于末端执行器这一稳定、定义明确的实体，为生成式世界模型在物理机器人上的部署提供了一个**可扩展、泛化性强且易于解释**的解决方案。它成功地将前沿视频生成模型的“视觉预见”能力转化为精确的机器人动作，为解决机器人学中规划与控制脱节的长期挑战提供了新思路，推动了通用具身智能的发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：TC-IDM

### **一、 研究背景与核心问题**

**背景：**
1.  **现有范式的局限**：主流的“视觉-语言-动作”模型严重依赖大规模、高质量的机器人演示数据，导致其泛化能力受限，难以处理未见过的任务、长时程任务或与可变形物体交互。
2.  **生成式世界模型的兴起**：以Sora、Kling、Cosmos等为代表的视频生成模型，展现出强大的“视觉预见”能力，可以作为高级规划器。然而，其生成的**像素级视频计划**与机器人**可执行的物理动作**之间存在巨大鸿沟，即“规划-执行鸿沟”。

**核心问题：**
如何将生成式世界模型输出的、不精确的、像素级的视频计划，**稳健、精确地翻译**成机器人可执行的、低级别的控制指令（如关节空间轨迹或力矩指令），从而实现**零样本、可执行的机器人运动**？

### **二、 核心创新点：TC-IDM框架**

论文提出了 **“工具中心逆动力学模型”** ，其核心创新在于**思路的转变**和**模块化设计**。

#### **1. 核心思路转变：从“跟踪物体”到“锚定工具”**
- **传统方法**：尝试从视频中跟踪和估计**目标物体**的状态（如位置、姿态）。这种方法在面对遮挡、快速运动、特别是**可变形物体**（如布料）时非常脆弱，因为物体的状态难以精确定义和跟踪。
- **TC-IDM的创新**：**不依赖不稳定的物体状态，而是将控制策略锚定在机器人末端执行器（工具）在生成视频中“想象”出的稳定、定义明确的运动轨迹上**。工具轨迹成为了连接高级视觉规划和低级物理控制的**鲁棒中间表示**。

#### **2. “规划-翻译”两阶段范式**
TC-IDM采用解耦的“规划-翻译”两阶段流程，清晰地将高级语义规划与低级几何控制分离。

- **第一阶段：视觉规划**
    - **输入**：初始RGB图像、深度图、文本指令。
    - **过程**：使用生成式世界模型（如WoW）预测出完成任务所需的未来视频序列。这提供了高级的“视觉预见”。

- **第二阶段：动作翻译（TC-IDM的核心）**
    - **步骤1：提取工具点云轨迹**
        - 使用分割模型（如SAM 3）从生成的每一帧视频中分割出末端执行器（工具）。
        - 使用3D运动跟踪器，从对齐的RGB-D视频序列中，**提取工具表面上密集点集的6自由度（6-DoF）三维运动轨迹**。这些轨迹是在世界坐标系下的。
    - **步骤2：解耦的动作预测头**
        - **几何驱动的姿态生成头**：基于提取的密集工具点轨迹，通过**解析的刚体对齐算法**（如最小二乘拟合），直接计算出末端执行器TCP（工具中心点）从一个时间步到下一个时间步的6-DoF相对位姿变换 `(R, t)`。**这部分不依赖学习，保证了运动的物理可解释性和精确性。**
        - **视觉驱动的状态生成头**：使用冻结的视觉编码器（如DINOv3）从生成视频中提取高级语义特征（如物体接触状态、任务阶段），通过一个轻量级MLP预测**工具的内部状态**（如夹爪的开合度）。
    - **输出**：将两个头的输出拼接，得到完整的机器人控制向量：`[末端执行器6-DoF位姿动作， 夹爪开合动作]`。

### **三、 技术优势与解决的实际问题**

通过上述创新，TC-IDM有效解决了多个关键挑战：

1.  **解决“规划-执行鸿沟”**：通过将不精确的像素计划转化为精确的、基于刚体运动学的工具轨迹，实现了从视觉到动作的可靠翻译。
2.  **强大的泛化能力**：
    - **视角不变性**：工具轨迹在世界坐标系下表示，对相机视角变化不敏感。
    - **处理可变形物体**：无需对布料等复杂状态建模，只需关注工具本身的运动路径，实现了**零样本可变形物体操作**（成功率38.46%）。
    - **长时程任务**：利用世界模型的组合式规划能力，并通过TC-IDM确保每个子动作的可行性。
    - **跨本体泛化**：方法可迁移到不同的机器人平台（如从单臂Franka到双臂UR5）和不同的灵巧手上，只需调整逆运动学或重定向网络。
3.  **提升执行精度与成功率**：在真实世界评估中，TC-IDM配合世界模型取得了**平均61.11%**的成功率（简单任务77.7%），显著优于端到端VLA基线和其他逆动力学模型。

### **四、 实际价值总结**

TC-IDM为**具身智能**领域提供了一条切实可行的新路径：
- **降低对机器人数据的依赖**：利用互联网预训练的世界模型作为“通用规划器”，减少了对昂贵、特定任务机器人演示数据的需求。
- **解锁复杂技能**：使得机器人能够执行需要长时程推理、精细操作以及与可变形物体交互的复杂任务。
- **推动生成式模型落地**：为Sora等强大的生成式世界模型在物理机器人上的部署提供了一个关键且有效的“最后一公里”解决方案，架起了虚拟想象与物理执行之间的桥梁。

**核心贡献一句话概括**：提出了一种以**工具轨迹**为中间表示的“规划-翻译”框架，通过**解耦的几何与语义推理**，首次实现了基于生成式世界模型的、高泛化能力的零样本机器人运动控制。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决生成式世界模型（如视频生成模型）与机器人物理控制之间的“最后一英里”鸿沟，即如何将像素级的视觉规划稳健地转化为可执行的机器人动作。为此，论文提出了**工具中心逆动力学模型（TC-IDM）**，其核心思想是**以世界模型生成的视频中末端执行器（工具）的想象轨迹作为中间表示**，通过解耦的架构（分别处理语义特征和几何运动）将工具的点云轨迹转换为6自由度末端执行器位姿和夹爪控制信号。该方法在真实机器人实验中取得了显著效果，平均任务成功率（61.11%）远超端到端视觉-语言-动作模型基线，并在零样本可变形物体操作（38.46%成功率）、长时程任务和跨视角跨本体泛化方面展现出强大能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## TC-IDM 论文创新点分析

这篇论文提出了一种名为 **TC-IDM（Tool-Centric Inverse Dynamics Model）** 的新框架，旨在解决生成式世界模型（如视频生成模型）与机器人底层物理控制之间的“最后一公里”鸿沟。其核心创新点在于**将工具（末端执行器）的想象轨迹作为连接视觉规划与物理控制的稳健中间表示**。以下是其相对于已有工作的明确创新点：

### 1. **核心范式创新：从“物体状态跟踪”转向“工具轨迹跟踪”**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**（如AVDC、VidBot、Novaflow）：主要依赖从生成视频中估计和跟踪**目标物体**的3D状态（如位置、姿态、形变）。这种方法在处理遮挡、快速运动或非刚性物体（如布料）时非常脆弱，因为物体的状态难以精确定义和稳定跟踪。
     - **TC-IDM方法**：将关注点从**不稳定的物体状态**转移到**机器人末端执行器（工具）自身在生成视频中的想象轨迹**。它不试图精确建模复杂的环境物体状态，而是专注于恢复工具本身一个**密集点云集合的6自由度轨迹**。
   - **解决的具体问题/带来的优势**：
     - **解决了“规划-动作”的错位问题**：生成模型输出的像素序列（视频）缺乏对机器人自身形态和运动学约束的 grounding。TC-IDM 通过工具轨迹这个中间表示，直接将高层次的视觉规划与低层次、物理一致的控制指令联系起来。
     - **提升了鲁棒性与泛化能力**：工具自身的运动通常比复杂多变的物体状态（尤其是可变形物体）更稳定、更好定义。这使得系统对**遮挡、快速运动、非刚性物体**等场景具有更强的鲁棒性。论文中在零样本可变形物体任务上达到38.46%的成功率，显著优于基线。
     - **实现了更好的视角不变性**：由于工具轨迹是在世界坐标系下表示的（通过3D运动估计和度量对齐），因此对相机视角的变化不敏感。

### 2. **架构设计创新：解耦的动作预测头**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：许多逆动力学模型（IDM）或端到端VLA模型使用单一网络或耦合的架构，同时预测末端执行器的位姿和夹爪开合等控制信号。这可能导致异构信息（几何运动与语义任务）相互干扰。
     - **TC-IDM方法**：采用**解耦的双流架构**，包含两个独立的学习头：
       1.  **视觉驱动的状态生成头（Vision-Driven State Generation）**：基于冻结的视觉编码器（如DINOv3）提取生成视频的语义特征，通过一个轻量级MLP预测**1-DoF的夹爪控制信号**（如开合）。这主要依赖**语义视觉特征**（如接触状态、任务阶段）。
       2.  **几何接地的姿态生成头（Geometry-Grounded Gesture Generation）**：基于从生成视频中提取的、经过度量对齐的RGB-D序列和工具掩码，使用3D点跟踪器获取工具表面的密集3D点轨迹，然后通过**解析的刚体对齐**（如普氏分析）恢复出**6-DoF的末端执行器位姿轨迹**。这完全基于**几何运动信息**。
   - **解决的具体问题/带来的优势**：
     - **清晰分离异构信息**：将依赖于高层任务语义的“做什么”（夹爪开合）与依赖于底层几何运动的“怎么做”（末端移动）解耦，使模型能更有效、更专注地利用不同类型的信息。
     - **提升可解释性与精度**：姿态生成流不依赖学习的动作解码器，而是通过解析的刚体运动恢复动作，这使得运动生成具有物理可解释性，并有助于提高轨迹精度。在飞镖投掷的误差分析实验中，TC-IDM展示了更高的执行保真度。
     - **支持多样的末端执行器**：该架构对末端执行器的具体形态是无关的（Agnostic），只需更换相应的工具掩码和轨迹提取方式即可适配不同工具（如平行夹爪、灵巧手、锤子）。

### 3. **系统流程创新：“规划-翻译”两阶段范式**
   - **相比以往方法的改进/不同之处**：
     - **以往“规划即生成”方法**：虽然利用世界模型进行视觉前瞻规划，但后续的“像素到动作”翻译模块（通常是IDM）往往很脆弱，对视觉伪影、物理幻觉和运动学不一致敏感，需要频繁重规划（如VPA）。
     - **TC-IDM的“规划-翻译”范式**：
       1.  **规划阶段**：利用任何先进的视频生成世界模型（如WoW、Cosmos），根据初始观测和文本指令，生成一个描绘任务完成过程的未来视频序列。这提供了**高层次、组合式的视觉规划**。
       2.  **翻译阶段**：TC-IDM 作为独立的模块，接收生成的视频，通过上述解耦的架构，将其“翻译”成可执行的机器人控制轨迹。它充当了**通用的、稳健的规划执行器**。
   - **解决的具体问题/带来的优势**：
     - **利用了世界模型的强大规划能力，同时规避了其控制弱点**：世界模型擅长于长视野、组合式任务的视觉模拟，但不擅长生成精确的控制指令。TC-IDM 专门负责将这种“想象”可靠地落地。
     - **实现了强大的零样本和长视野泛化**：通过利用世界模型的组合规划能力，TC-IDM 能够处理训练中未见过的长序列任务（如叠连帽衫）和分布外任务。实验表明，其在简单、中等、困难任务上的平均成功率分别达到77.7%、53.3%和28.9%，综合平均成功率为61.11%。
     - **降低了对大规模机器人数据集的依赖**：与严重依赖大规模、高质量机器人演示数据的端到端VLA模型（如RT系列、OpenVLA）相比，TC-IDM 只需要相对较小的机器人数据集来训练其“翻译”模块，而其“规划”能力则来自在互联网规模数据上预训练的世界模型。

### 4. **应用范围创新：支持跨 embodiment 迁移与灵巧手操控**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：大多数机器人策略模型与特定的机器人本体（ embodiment ）紧密耦合，迁移到不同形态的机器人（如从单臂到双臂，从夹爪到灵巧手）需要重新收集数据或进行大量微调。
     - **TC-IDM方法**：通过其工具中心的表示，天然支持跨 embodiment 迁移。论文额外展示了一个**人手指向灵巧手的迁移管道**：
       - 使用预训练的手部模型（如HaMeR）从生成视频中估计**人手状态**。
       - 通过一个轻量级的、可学习的**重定向网络**，将人手状态映射到目标灵巧手的关节角度命令。
       - **SE(3)轨迹估计分支**保持不变，仍从视频中恢复末端执行器的6-DoF运动。
   - **解决的具体问题/带来的优势**：
     - **实现了零样本的跨本体知识迁移**：在实验中，成功将单臂Franka机器人学到的运动先验，零样本迁移到真实双UR5机器人上完成敲击木琴任务。
     - **为高自由度灵巧手操控提供了新思路**：通过“人手状态估计 + 重定向”的方式，可以利用丰富的人类视频数据来驱动复杂的灵巧手操作，而无需为每个灵巧手收集大量机器人数据。实验在气球击打和布料移除任务上，成功迁移到了两种不同的灵巧手（BrainCo, Inspire-Robots）。

### 总结
TC-IDM 的核心创新在于**范式、架构和流程**上的系统性设计，其**工具中心的中间表示**是关键突破口。它并非试图让世界模型直接输出动作，也不是简单地跟踪视频中的物体，而是**巧妙地利用世界模型对工具自身运动的“想象”作为桥梁**，再通过一个解耦、稳健的翻译模块将其转化为物理控制。这种方法在**泛化性（视角、物体、任务长度、机器人本体）、鲁棒性（对遮挡、非刚性物体）和执行精度**方面，相比现有的端到端VLA模型和传统IDM都带来了显著提升，为生成式世界模型在物理机器人上的可靠部署提供了一个强有力的新基线。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、核心实验效果
论文通过真实机器人实验验证了 **TC-IDM** 框架的有效性，在多个维度上显著优于现有基线方法。核心成果包括：
- **整体成功率**：在涵盖不同难度的任务上，TC-IDM 结合世界模型（如 WoW）取得了 **61.11%** 的平均成功率。
- **任务难度分层表现**：
  - **简单任务**：成功率 **77.7%**（部分任务如“面包放盘子”达到 10/10 完美成功率）。
  - **中等任务**：成功率 **53.3%**。
  - **困难任务**：成功率 **28.9%**，显著优于所有基线。
- **零样本泛化能力**：
  - **可变形物体操作**（如叠衣服、取布料）：在未训练过的任务上达到 **38.46%** 的成功率。
  - **长时程任务**（如六步叠卫衣）：能够完成完整多步序列，无需中途重规划。
  - **跨相机泛化**：在 Apple Pro、RealSense D435i 等未见相机上直接迁移成功。
  - **跨 embodiment 泛化**：从单臂 Franka 机器人迁移到双臂 UR5 机器人，成功完成击打木琴任务。

### 二、使用的数据集
1. **训练集**：
   - 来源：**Robomind 数据集** 的子集。
   - 规模：**30,210 条轨迹**，覆盖 **25 种不同任务**（包括抓取、放置、推拉等）。
   - 目的：训练 TC-IDM 的逆动力学模型部分（从视频到动作的映射）。

2. **测试集**：
   - **9 个全新任务**，严格从训练集中排除，分为三个难度等级（简单、中等、困难各 3 个）。
   - 任务示例：
     - 简单：面包放盘子、关抽屉、移动牛奶。
     - 中等：面包放入抽屉、从杯架取杯子、开抽屉。
     - 困难：翻转按钮、挂杯子到杯架、将筷子插入竹筒。

### 三、评价指标
- **主要指标**：**任务成功率**（Success Rate），即机器人按指令完成任务的次数占总尝试次数的比例。
- **辅助分析**：
  - **误差范围分析**：通过飞镖投掷任务衡量轨迹执行精度（如距靶心距离）。
  - **泛化维度**：跨相机、跨 embodiment、可变形物体、长时程任务的零样本成功率。

### 四、对比的基线方法
论文与两大类基线方法进行了对比：

1. **逆动力学模型（IDM）类**：
   - **AVDC**：基于视频稠密对应的逆动力学方法。
   - **AnyPos**：基于大规模随机探索训练的 task-agnostic IDM。
   - **2DtrackerIDM**：基于 2D 跟踪器（如 CoTracker）的 IDM。
   - **ResNet-MLPs**：基于 ResNet 特征直接预测动作的基线。

2. **端到端 VLA 模型及视频生成模型 + IDM**：
   - **VLA 模型**：`π₀`、OpenVLA、RT-2 等。
   - **视频生成模型 + IDM**：CogVideo-IDM、Cosmos-1-IDM、Wan2.1-IDM、Kling-IDM、Cosmos2-IDM、WoW-cosmos2-IDM 等。

### 五、关键性能提升与结论
1. **显著优于传统 IDM 方法**：
   - 在真实回放视频测试中（表1），TC-IDM 在简单、中等、困难任务上全面领先。
   - 例如，在困难任务上，TC-IDM 成功率（28.9%）远超 AnyPos（约 8.9%）和 AVDC（0%）。

2. **大幅超越视频生成模型 + 普通 IDM**：
   - 在相同世界模型（如 WoW）下，TC-IDM 相比其他 IDM 适配方法有巨大优势（表2）。
   - 例如，在 WoW-wan-IDM 上，TC-IDM 在简单任务达到 100%（9/9），而其他 IDM 方法普遍低于 50%。

3. **核心优势体现**：
   - **工具轨迹作为中间表示**：通过分割和 3D 运动估计提取工具点云轨迹，提供了稳定、视角不变的表示，避免了直接处理物体状态（尤其是可变形物体）的脆弱性。
   - **解耦设计**：分离的“手臂策略头”（几何轨迹）和“工具策略头”（语义特征）使系统能同时利用几何运动信息和高级语义信息。
   - **强泛化能力**：在相机变化、可变形物体、长时程、跨 embodiment 等零样本场景下均表现出色，证明了其作为通用 embodied AI 桥梁的潜力。

### 六、未明确给出的定量结果
论文对所有主要实验均给出了定量成功率结果。未明确量化的部分主要是部分定性展示的泛化实验（如跨 embodiment 击打木琴），但论文通过轨迹对比图和成功执行视频证明了其有效性。

**总结**：TC-IDM 通过 **“工具轨迹”** 这一创新中间表示，有效弥合了生成式世界模型的视觉规划与机器人底层控制之间的“最后一公里”鸿沟，在多项真实机器人任务中实现了显著优于现有方法的性能和强大的零样本泛化能力。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.18323v1)
- [HTML 版本](https://arxiv.org/html/2601.18323v1)
