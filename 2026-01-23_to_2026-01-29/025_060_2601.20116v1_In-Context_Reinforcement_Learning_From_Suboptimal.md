# In-Context Reinforcement Learning From Suboptimal Historical Data

**相关性评分**: 6.0/10

**排名**: #25


---


## 基本信息

- **arXiv ID**: [2601.20116v1](https://arxiv.org/abs/2601.20116v1)
- **发布时间**: 2026-01-27T23:13:06Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Juncheng Dong, Moyang Guo, Ethan X. Fang, Zhuoran Yang, Vahid Tarokh

## 关键词

offline Reinforcement Learning, fine tune, world model

## 一句话总结

该论文提出了一种基于Transformer的上下文强化学习框架，利用离线数据集中的次优历史数据，通过加权最大似然估计来提升策略性能。

## 摘要

Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

## 详细分析

## 论文摘要

**论文标题**：基于次优历史数据的上下文强化学习

### 1. 研究背景和动机
Transformer模型因其卓越的上下文学习能力，在监督学习领域取得了巨大成功。受此启发，研究者开始探索将自回归Transformer用于**上下文强化学习**。该范式旨在通过离线数据集（包含来自不同RL任务的轨迹）预训练一个Transformer模型，然后将其固定并用于解决**全新的、未见过的RL任务**。然而，现有方法（如AD和DPT）要求预训练数据集中包含**最优动作标签**或**完整的算法学习过程**，这在实践中往往难以获取。相比之下，**次优的历史数据**（例如，由非专家用户产生的轨迹）则更容易收集。因此，本文致力于解决一个更具挑战性但更实际的问题：**如何仅使用次优历史数据来预训练一个具备上下文强化学习能力的Transformer模型？**

### 2. 核心方法和技术创新
本文提出了**决策重要性Transformer**框架。其核心创新在于，它模仿了**演员-评论家算法**的框架，但以**上下文学习**的方式实现：
- **第一步：上下文优势函数估计**。首先训练一个基于Transformer的**优势函数估计器**。该估计器能够根据给定的上下文轨迹，**跨任务插值**，从而为次优行为策略产生的状态-动作对估计其优势值。
- **第二步：加权最大似然估计**。然后，基于估计出的优势值，为预训练数据集中的每个动作计算一个**指数权重**（优势值高的动作权重更大）。最后，使用这个**加权的最大似然估计损失**来训练策略Transformer。这种加权机制能够引导模型从次优数据中学习到更优的策略，而非简单地模仿次优行为。

### 3. 主要实验结果
在**老虎机问题**和**马尔可夫决策过程**（包括Dark Room、Miniworld导航任务以及Meta-World、Half-Cheetah连续控制任务）上进行了广泛实验。结果表明：
- DIT在在线和离线测试中均表现优异，在老虎机问题上性能与理论最优算法（如汤普森采样）相当。
- 在MDP任务中，**尽管DIT仅使用次优数据预训练，但其性能与需要最优动作标签的DPT方法相当，甚至在某些情况下更优**。
- 消融实验表明，**加权训练目标**（对比于不加权的行为克隆）带来了显著的性能提升，验证了其核心设计的有效性。

### 4. 研究意义和价值
DIT框架**显著降低了上下文强化学习的实践门槛**，使其能够利用现实中更易获得的次优历史数据。其技术贡献在于：
- **理论保证**：提供了策略改进的理论保证，确保学习到的策略优于行为策略。
- **实际可行性**：摆脱了对最优数据或完整学习历史的依赖，使得在大规模、真实的历史数据上预训练通用决策模型成为可能。
- **性能卓越**：在多个具有挑战性的基准测试中达到了先进水平，展示了从次优数据中提取并泛化最优决策的强大能力。这项工作为构建更通用、更数据高效的决策智能体迈出了重要一步。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文标题**
《基于次优历史数据的上下文强化学习》

### **核心问题**
论文旨在解决一个关键挑战：**如何仅使用来自不同任务的、由次优行为策略收集的历史轨迹数据，来预训练一个Transformer模型，使其具备强大的上下文强化学习能力**。

- **现有方法的局限**：之前的ICRL方法（如DPT、AD）在预训练时，要么需要**最优动作标签**，要么需要**完整的算法学习过程数据**。这些要求在实际中（例如，从非专家用户的历史数据中）往往难以满足。
- **本文的突破点**：本文探索在**仅使用次优历史数据**（即没有最优动作标签，也没有学习过程）的情况下，实现有效的上下文强化学习。这大大提升了ICRL的实用性和可行性。

### **核心创新点：决策重要性变换器框架**
论文提出了 **“决策重要性变换器”** 框架来解决上述问题。其核心创新在于一个**两阶段的、基于重要性加权的监督预训练流程**，旨在从次优数据中“提炼”出接近最优的策略。

#### **1. 技术创新**
- **加权最大似然估计**： 核心思想是**对次优数据中的“好动作”赋予更高权重**。具体来说，使用行为策略的优势函数来构造权重。对于一个状态-动作对 `(s, a)`，其权重为 `w = exp(A_b(s, a) / η)`，其中 `A_b` 是行为策略的优势函数，`η` 是温度参数。
    - **理论依据**：论文证明了优化此加权目标等价于优化一个同时追求**策略改进**和**防止策略崩溃**的目标（公式3）。这确保了学习到的策略在理论上优于行为策略。
- **上下文优势函数估计器**： 在ICRL中，每个任务的行为策略优势函数 `A_b(s, a)` 是未知且**任务依赖**的。直接估计单个轨迹的优势函数非常困难且不可靠。
    - **解决方案**： DIT训练了两个Transformer模型（`Q^` 和 `V^`）作为**上下文价值函数估计器**。它们能够根据提供的上下文轨迹（历史状态、动作、回报），**即时地**估计当前任务中任意状态-动作对的价值和优势。这解决了跨任务、小样本情况下的优势估计难题。
- **端到端框架**：
    1.  **第一阶段**： 使用离线数据集训练上下文优势估计器 `A^_b`。
    2.  **第二阶段**： 使用 `A^_b` 计算权重 `w`，然后通过加权最大似然估计目标训练策略Transformer `T_θ`。

#### **2. 方法流程**
```mermaid
graph TD
    A[输入: 多任务次优历史轨迹数据集] --> B[阶段一: 训练上下文优势估计器]
    B --> C{使用Transformer Q^和V^<br>进行上下文插值估计}
    C --> D[得到估计的优势函数 A^_b]
    D --> E[阶段二: 加权策略预训练]
    E --> F[基于A^_b计算权重 w = exp(A^_b/η)]
    F --> G[使用加权损失训练策略Transformer T_θ]
    G --> H[输出: 预训练的DIT模型]
    H --> I[部署: 用于新任务的在线/离线强化学习]
```

### **实际价值与实验验证**
- **降低数据门槛**： DIT仅需易于获取的次优历史数据，无需精心设计的最优演示或完整学习日志，极大扩展了ICRL的应用场景（如企业用户行为数据、机器人历史操作记录）。
- **性能卓越**： 在**赌博机问题**和**复杂的MDP问题**上进行了广泛实验。
    - **赌博机**： DIT在线性能匹配理论最优算法（如汤普森采样），离线性能显著优于基线。
    - **MDP任务**： 在Dark Room、Miniworld、Meta-World、Half-Cheetah四个挑战性环境中，DIT表现优异。
    - **关键对比**： **尽管预训练时没有使用最优动作标签**，DIT的性能在多数场景下与需要最优标签的DPT相当，甚至在某些在线学习设置中**优于DPT和AD**。同时，DIT显著优于单纯的行为克隆和Prompt-DT。
- **理论保障**： 论文提供了**策略改进定理**，从理论上保证了DIT学习到的策略在期望上优于生成数据的行为策略。

### **总结**
这篇论文的核心贡献在于提出了一个**实用且高效**的ICRL框架 **DIT**。它通过**创新的加权预训练目标**和**上下文优势估计技术**，成功地从**易于获取的次优多任务数据**中，学习到了一个能够快速适应新任务的强大Transformer策略模型。这项工作在**理论**（提供了策略改进保证）、**方法**（设计了新颖的两阶段框架）和**实践**（在多个基准上验证了卓越性能）三个层面都做出了重要贡献，推动了强化学习向更实用、更通用的方向发展。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**利用次优历史数据进行上下文强化学习**的核心挑战，即如何在仅使用由次优行为策略生成的历史轨迹（无需最优动作标签）预训练Transformer模型后，使其能泛化到新的、未见过的决策任务中。为此，论文提出了**决策重要性Transformer**框架，其核心创新在于设计了一个两阶段训练流程：首先训练一个基于Transformer的上下文优势函数估计器来评估历史数据中动作的优劣；然后利用估计出的优势值对标准的最大似然预训练目标进行指数加权，从而在模仿学习的基础上引导策略向更优的方向改进。实验结果表明，该方法在多种赌博机和马尔可夫决策过程任务上，即使仅使用次优数据预训练，其性能也能达到甚至超越需要最优动作标签的基线方法，显著提升了上下文强化学习在实际场景中的可行性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《In-Context Reinforcement Learning From Suboptimal Historical Data》针对**基于Transformer的上下文强化学习**领域，提出了一个名为**决策重要性变换器**的新框架。其核心创新在于**仅使用次优历史数据**进行预训练，而无需依赖最优动作标签或完整的算法学习历史。以下是其相对于已有工作的明确创新点：

---

### 1. **提出了一个仅需次优历史数据的ICRL预训练框架**
   - **相比以往方法的改进/不同之处**：
     - **先前工作**：如**算法蒸馏**需要预训练数据集包含RL算法从随机策略到接近最优策略的**完整学习过程轨迹**；**决策预训练变换器**则需要为查询状态提供**最优动作标签**。两者都对数据质量有严格要求。
     - **本文方法**：DIT框架**仅需要由次优行为策略在不同任务上收集的历史轨迹**，无需最优动作标签或学习过程元数据。
   - **解决的具体问题/带来的优势**：
     - **大幅提升了ICRL的可行性**：在现实场景中（如大型公司的用户历史数据），收集最优演示或完整学习历史极其困难，而次优轨迹则容易得多。DIT降低了对预训练数据的要求，使得ICRL技术更易于落地应用。

### 2. **引入了基于优势函数的加权最大似然估计预训练目标**
   - **相比以往方法的改进/不同之处**：
     - **标准模仿学习/行为克隆**：在次优数据上直接进行最大似然估计（即模仿行为策略），会导致模型性能天花板被限制在次优水平。
     - **本文方法**：DIT提出一个**加权最大似然估计**损失函数。该损失为离线数据集中的每个状态-动作对 `(s, a)` 分配一个权重 `w = exp(A_b(s, a)/η)`，其中 `A_b(s, a)` 是该行为策略的优势函数估计值。
   - **解决的具体问题/带来的优势**：
     - **实现了策略提升**：通过为**高优势值（即“好”）的动作**分配更高权重，DIT在模仿学习的过程中，能够**引导策略从次优行为策略向更优策略改进**，而不仅仅是模仿。论文中的**命题4.2**从理论上保证了这种策略提升。
     - **克服了次优数据的性能瓶颈**：使得模型能够“去芜存菁”，从混杂的次优数据中识别并强化那些相对更好的决策。

### 3. **设计了基于Transformer的上下文优势函数估计器**
   - **相比以往方法的改进/不同之处**：
     - **标准离线RL中的优势加权回归**：通常在单个任务上进行，优势函数的估计可以利用该任务下的大量轨迹。
     - **本文的ICRL场景挑战**：预训练数据包含来自**大量不同任务**的轨迹，且每个任务可能只有少量轨迹。优势函数的估计必须是**任务依赖的**，但任务的真实身份（索引）未知，无法按任务分组数据。
     - **本文方法**：DIT训练了两个Transformer模型（`V^` 和 `Q^`）作为**上下文价值函数和动作价值函数估计器**。它们以当前轨迹的历史片段为上下文，**跨任务进行插值**，从而为预训练数据集中的每个轨迹片段**在线地、上下文地**估计其优势值 `A^_b`。
   - **解决的具体问题/带来的优势**：
     - **解决了多任务、少样本下的优势估计难题**：这是将优势加权回归思想推广到ICRL场景的**关键技术创新**。它使得在未知任务来源的混合数据中，为每个状态-动作对生成有意义的权重成为可能。
     - **支撑了加权预训练框架**：为创新点2中的加权损失函数提供了可靠且可计算的权重来源。

### 4. **实现了与需要更多信息的先进方法相媲美的性能**
   - **相比以往方法的改进/不同之处**：
     - **实验对比**：在多项实验中，DIT与需要**最优动作标签**的DPT性能相当，甚至在某些在线学习场景下表现更优；同时显著优于仅进行行为克隆的变体以及需要**完整学习历史**的AD。
   - **解决的具体问题/带来的优势**：
     - **证明了框架的有效性**：尽管DIT在预训练阶段利用的信息更少（只有次优数据），但其最终学到的策略在**泛化到新任务**时，能达到与利用“特权信息”（最优标签）的方法相近的水平。这凸显了DIT框架**数据效率高**和**实用性强**的优势。
     - **为更经济的ICRL模型训练提供了实证依据**：表明无需耗费大量资源生成最优演示，也能训练出高性能的上下文决策模型。

---

## 总结

这篇论文的核心创新在于**系统性地解决了如何仅使用易于获取的次优历史数据来预训练高性能的上下文强化学习模型**这一难题。它通过**理论驱动的加权目标**和**精巧的上下文优势估计器**，将离线策略改进的思想成功引入多任务、少样本的ICRL设定中。这不仅**放宽了ICRL对数据质量的严苛要求**，具有重要的实际应用价值，也为后续研究如何从非完美数据中进行元学习提供了新的思路和技术基础。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果总结

该论文通过一系列实验，全面评估了所提出的**决策重要性变换器**框架在**上下文强化学习**任务中的性能。实验旨在验证DIT仅使用**次优历史数据**进行预训练后，在未见任务上的泛化能力。

### 一、 使用的数据集与任务
实验涵盖了从简单到复杂的多种决策问题，以验证方法的通用性：

1.  **老虎机问题**：
    *   **任务**：线性老虎机。
    *   **数据集**：生成100k个上下文数据集。每个任务的特征函数和参数独立采样自高斯分布。行为策略是随机生成的次优策略（混合随机分布和单臂点质量分布），**不强制覆盖最优动作**。

2.  **马尔可夫决策过程问题**：
    *   **导航任务（稀疏奖励）**：
        *   **Dark Room**：10x10网格导航，目标位置未知。使用80个目标训练，20个未见目标测试。
        *   **Miniworld**：基于视觉的3D导航，到达指定颜色的盒子。使用4种颜色训练和测试。
    *   **连续控制任务**：
        *   **Meta-World (reach-v2)**：机械臂到达目标位置。使用15个任务训练，5个任务测试。
        *   **Half-Cheetah (velocity)**：控制机器人达到目标速度。使用35个任务训练，5个任务测试。
    *   **数据集构建**：为确保策略可学习性，数据集中以概率 `p` 使用最优策略，以概率 `1-p` 使用随机策略生成轨迹，使得数据集的平均回报低于最优轨迹的30%，模拟现实中的次优数据。对于连续控制任务，使用SAC训练过程中的历史轨迹构建数据集。

### 二、 评价指标
根据任务类型不同，采用以下指标：
*   **老虎机问题**：
    *   **在线测试**：累积遗憾。
    *   **离线测试**：次优性（最优臂与所选臂的期望奖励之差）。
*   **MDP问题**：
    *   **在线/离线测试**：回合累积回报。

### 三、 对比的基线方法
论文与多种先进的ICRL和RL方法进行了对比，这些方法对预训练数据的要求各不相同：

1.  **需要最优动作标签的方法**：
    *   **决策预训练变换器**：需要查询状态及其对应的**最优动作标签**。
2.  **需要完整学习历史的方法**：
    *   **算法蒸馏**：需要包含从随机策略到接近最优策略的**完整RL算法学习历史**。
3.  **仅使用相同次优数据的方法（公平对比）**：
    *   **提示决策变换器**：基于相同次优数据集，使用提示进行少样本适应。
    *   **行为克隆**：DIT的变体，**去除指数加权**，直接模仿数据集中的动作。
4.  **无预训练的在线RL方法**：
    *   **软演员-评论家**：在每个新任务上从头开始训练，作为性能基准。
5.  **老虎机理论最优算法**：
    *   **汤普森采样**、**上置信界**、**下置信界**、**经验均值**。

### 四、 关键性能结果与结论
实验结果表明，**DIT在仅使用次优历史数据的情况下，达到了与需要更多信息（最优标签或学习历史）的先进方法相媲美甚至更优的性能**。

1.  **老虎机问题**：
    *   **在线测试**：DIT的性能与理论最优算法（TS， UCB）相当，能快速识别最优臂并保持低遗憾。
    *   **离线测试**：
        *   当上下文数据由**随机策略**收集时，DIT**显著优于**TS和LCB。
        *   当上下文数据由**专家策略**收集时，DIT能**进一步改进**专家策略的性能。
    *   **结论**：DIT能有效从次优数据中推断出接近最优的策略。

2.  **MDP问题（以Dark Room为例）**：
    *   **在线测试**：DIT初始回报低于DPT和AD，但**快速超越并持续改进**，而SAC和BC由于探索困难性能不佳。
    *   **离线测试**：
        *   使用**随机轨迹**作为上下文：DIT表现优异。
        *   使用**专家轨迹**作为上下文：DIT表现出与DPT**相竞争的性能**。
    *   **关键结论**：尽管预训练时没有最优动作标签，DIT在大多数场景下与DPT表现相当。

3.  **消融实验与有效性验证**：
    *   **加权预训练的有效性**：DIT的性能**显著优于**其未加权的行为克隆变体，证明了基于优势函数加权的关键作用。
    *   **与“理论上界”的对比**：在Miniworld任务中，与一个使用**相同观测状态但附有真实最优动作标签**的“Oracle版DPT”对比，DIT在在线设置中表现更优，在离线设置中性能相当。这表明DIT的加权预训练框架已**接近其性能上限**。
    *   **上下文轨迹对齐的重要性**：实验表明，提供的上下文数据集必须与当前测试任务的目标一致，否则性能会显著下降，验证了DIT确实依赖上下文进行任务推断和决策。

### 五、 总结
论文通过系统的实验证明：
*   **DIT成功解决了仅使用次优历史数据进行ICRL预训练的挑战**。
*   在多种任务上，其性能**匹配或超越了**需要更强假设（最优标签、学习历史）的先进方法。
*   核心的**加权最大似然估计**框架（基于上下文估计的优势函数进行加权）被证明是有效的，能够引导策略从次优行为向最优策略改进。
*   该方法**大幅提升了ICRL的可行性**，因为现实世界中更容易收集大量次优的历史轨迹，而非最优演示或完整的学习过程数据。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2601.20116v1)
- [HTML 版本](https://arxiv.org/html/2601.20116v1)
