# Learning Nash Equilibrial Hamiltonian for Two-Player Collision-Avoiding Interactions

**相关性评分**: 6.0/10

**排名**: #37


---


## 基本信息

- **arXiv ID**: [2503.07013v2](https://arxiv.org/abs/2503.07013v2)
- **发布时间**: 2025-03-10T07:52:30Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Lei Zhang, Siddharth Das, Tanner Merry, Wenlong Zhang, Yi Ren

## 关键词

Robot control, quadruped robot locomotion, multi-agent RL, CTDE methods, Edge Deployment, mujoco, local observational information

## 一句话总结

这篇论文提出了一种更数据高效的方法来学习两玩家碰撞避免交互中的纳什均衡策略，通过简化均衡共态结构和理论驱动的主动学习来改进策略泛化性和降低碰撞概率。

## 摘要

We consider the problem of learning Nash equilibrial policies for two-player risk-sensitive collision-avoiding interactions. Solving the Hamilton-Jacobi-Isaacs equations of such general-sum differential games in real time is an open challenge due to the discontinuity of equilibrium values on the state space. A common solution is to learn a neural network that approximates the equilibrium Hamiltonian for given system states and actions. The learning, however, is usually supervised and requires a large amount of sample equilibrium policies from different initial states in order to mitigate the risks of collisions. This paper claims two contributions towards more data-efficient learning of equilibrium policies: First, instead of computing Hamiltonian through a value network, we show that the equilibrium co-states have simple structures when collision avoidance dominates the agents' loss functions and system dynamics is linear, and therefore are more data-efficient to learn. Second, we introduce theory-driven active learning to guide data sampling, where the acquisition function measures the compliance of the predicted co-states to Pontryagin's Maximum Principle. On an uncontrolled intersection case, the proposed method leads to more generalizable approximation of the equilibrium policies, and in turn, lower collision probabilities, than the state-of-the-art under the same data acquisition budget.

## 详细分析

## 论文摘要

**研究背景和动机**
在自动驾驶、机器人交互等场景中，智能体间的碰撞规避交互可建模为一般和微分博弈，其纳什均衡策略由 Hamilton-Jacobi-Isaacs (HJI) 方程描述。然而，由于碰撞规避导致均衡值函数在状态空间上不连续，实时求解 HJI 方程极具挑战。现有基于监督学习的方法通常需要从大量初始状态采样均衡策略，数据效率低下。本文旨在提出一种更数据高效的方法，以学习泛化性能更好的均衡策略近似。

**核心方法和技术创新**
本文提出了两项主要技术创新：
1.  **学习低维协态而非价值函数**：论文证明，在系统动力学为线性且碰撞规避主导智能体损失函数的情况下，纳什均衡的协态轨迹具有简单的分段线性结构，本质上是低维的。因此，直接学习协态网络比学习价值函数网络（其梯度可能放大误差）更具数据效率。
2.  **理论驱动的主动学习**：提出了一种基于庞特里亚金极大值原理的主动学习算法。其采集函数通过比较预测协态与根据系统动力学反向推算得到的协态之间的差异，来衡量预测对理论约束的符合程度。该算法能高效地采样对当前模型最具挑战性的初始状态，从而提升学习效率。

**主要实验结果**
在一个无信号灯交叉口的双车交互案例中，论文验证了所提方法的有效性。实验表明：
*   在相同训练数据量下，使用**协态网络**进行闭环控制导致的碰撞概率，显著低于使用**价值网络**。
*   在中等数据规模下，**主动学习**的协态网络进一步降低了碰撞概率，优于静态采样的协态网络。
*   协态网络的预测误差方差更小，表现出更好的稳定性和泛化能力。

**研究意义和价值**
本研究为数据高效地学习微分博弈的纳什均衡策略提供了新思路。通过揭示并利用均衡协态的内在低维结构，以及引入理论指导的主动学习机制，该方法能够在有限的数据预算下，学习到更安全、更可靠的交互策略。这对于需要实时意图推断、运动规划和信号交互的自动驾驶等实际应用具有重要价值。未来的工作可探索该方法在非线性动力学和多智能体场景中的扩展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究问题**
论文旨在解决**双智能体风险敏感避碰交互**中，纳什均衡策略的**实时计算与泛化**难题。具体挑战在于：
- 避碰需求导致均衡值函数在状态空间上**不连续**，使得直接求解 Hamilton-Jacobi-Isaacs (HJI) 方程极其困难。
- 现有基于学习的近似方法（如监督学习价值网络）需要**大量标注数据**（通过求解大量初始状态的边值问题BVP获得），数据效率低，且价值函数预测误差通过梯度算子放大后，可能导致控制策略不安全（碰撞风险高）。

### **核心创新点**
论文提出了两个主要创新，以**更高效的数据利用方式**学习泛化能力更强的均衡策略近似模型：

1.  **学习低维共态而非价值函数**
    - **理论洞察**：在**系统动力学为线性**且**避碰损失占主导**的双人微分博弈中，纳什均衡的**共态轨迹具有简单的低维结构**。它仅由终端边界条件和智能体进入/离开碰撞区域的时间点决定，可参数化为少量关键参数（如 `(λ_i(T), t_i^in, t_i^out, q_i)`）。
    - **技术优势**：直接学习共态网络，避免了学习复杂、高变化的价值函数及其梯度带来的误差放大问题。共态的低维性使其**更容易学习、数据效率更高**。已知系统动力学和损失函数下，可直接从预测的共态计算出哈密顿量，进而导出控制策略。

2.  **理论驱动的主动学习采样策略**
    - **方法创新**：提出一种新的**获取函数**，用于指导应采集哪些初始状态的数据。该函数衡量**预测的共态轨迹对庞特里亚金极大值原理的符合程度**。
    - **计算优势**：获取函数的计算仅需一次**逆向动力学推演**（求解一个初值问题），成本远低于求解一个完整的BVP来获取真实标签。因此，能高效地识别出当前模型预测误差较大的状态区域，并针对性地进行采样和标注。
    - **目标**：在相同的数据采集预算下，获得比随机或静态采样（如拉丁超立方采样）**泛化性能更好、碰撞概率更低**的策略模型。

### **解决方案路径**
1.  **问题建模**：将双人交互建模为一般和微分博弈，其开环纳什均衡由PMP给出的一组BVP描述。
2.  **数据生成**：使用BVP求解器，对一批采样初始状态离线求解，得到均衡的状态、控制、共态轨迹。为避免碰撞损失不连续导致的求解困难，对碰撞惩罚进行了平滑处理。
3.  **模型学习**：
    - **基线方法**：监督学习价值网络 `V(x,t)`，然后通过自动微分求梯度得到共态。
    - **本文方法**：监督学习共态参数网络 `y(x,t)`。网络输出低维参数，可重构完整共态轨迹。
    - **主动学习增强**：在共态学习框架中嵌入算法1。迭代地：用当前模型评估候选状态池的获取函数值，选择最不满足PMP的状态，求解其BVP获得新标签，加入训练集更新模型。
4.  **策略执行**：在线运行时，对于给定状态 `(x0, t0)`，通过训练好的网络预测共态，然后**局部最大化哈密顿量** `Hi` 来实时计算控制输入 `u_i*`，实现闭环控制。
5.  **实验验证**：在**无控制十字路口**场景中进行案例研究。对比了价值网络、静态学习的共态网络、主动学习的共态网络。结果表明，在相同训练数据量下，共态网络（尤其是主动学习版本）能实现**更低的闭环控制碰撞概率**，验证了其更高的数据效率和泛化能力。

### **实际价值**
- **为实时人机/机机交互提供新工具**：该方法能够学习并泛化复杂的博弈均衡策略，可用于自动驾驶（交叉路口、换道）、机器人协作等需要实时意图推断和运动规划的场景。
- **提升安全性与数据效率**：通过更高效地利用昂贵的BVP求解数据，在有限数据预算下训练出更安全的策略模型，降低了在未知状态下的碰撞风险。
- **提供新的学习范式**：展示了在具有已知物理约束（动力学、PMP）的问题中，**学习对偶变量（共态）的流形**可能比学习原始变量（价值函数）更有效，特别是当该流形具有简单结构时。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**实时求解双人风险敏感型避碰交互纳什均衡策略的数据效率低下问题**。针对传统方法（如直接求解HJI方程或监督学习价值函数）计算成本高、泛化性能差的问题，论文提出了一个**基于共态（co-state）学习和理论驱动主动学习的混合框架**。其核心创新在于：1）理论证明了在系统动力学线性且避碰占主导时，纳什均衡的共态具有低维结构，因此比价值函数更易于学习；2）设计了一种基于庞特里亚金极大值原理（PMP）符合性的主动学习采集函数，以高效选择能最大程度提升模型性能的训练数据。在无信号灯交叉口案例上的实验表明，该方法在相同数据预算下，相比最先进的基准方法（如价值网络学习），能学习到泛化能力更强的策略，并最终实现**更低的碰撞概率**。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文针对**双玩家风险敏感避碰交互的纳什均衡策略学习**问题，提出了两项核心创新，旨在解决现有方法数据效率低、泛化性能差的问题。

### 创新点一：学习低维共态而非价值函数

- **相比以往方法的改进/不同之处**：
    - **传统方法**：主流方法（如基于值函数近似的强化学习）通常学习**纳什均衡价值函数** `V*(x, t)` 或其对应的动作价值函数（哈密顿量）。这需要神经网络去拟合一个在状态空间上可能**高度非线性且不连续**的函数（由避碰惩罚引起）。
    - **本文方法**：论文证明，在**系统动力学为线性**且**避碰损失占主导**的设定下，纳什均衡的**共态轨迹** `λ*(t)` 具有**简单的低维结构**。具体而言，共态轨迹可以由少数几个参数（如终端共态值 `λ*(T)`、进入/离开碰撞区域的时间 `t_in`, `t_out` 以及碰撞相关向量 `q`）来表征。因此，论文提出直接学习一个从初始状态 `(x0, t0)` 映射到这些低维共态参数的神经网络 `ŷ(x0, t0)`。

- **解决的具体问题/带来的优势**：
    1. **更高的数据效率**：共态的参数化表示本质上比价值函数更平滑、更易于学习。价值函数在碰撞边界附近变化剧烈（见图3c,e），而共态参数（如 `q`）的变化更具分类特性（见图3d,f）。因此，用相同规模的数据集训练，学习共态比学习价值函数能获得**更好的泛化性能**。
    2. **更稳定的策略推导**：通过学得的共态参数，可以结合已知的系统动力学和损失函数，**精确地重构出哈密顿量**，进而通过最大化哈密顿量得到控制策略。这避免了传统方法中先学习价值函数再通过梯度 `∇V` 计算共态所带来的**误差放大问题**，从而得到更稳定、更安全的策略。
    3. **计算优势**：学得低维共态参数后，在实时控制中计算最优动作 `u*` 是一个简单的凸优化问题（如文中公式5所示），计算成本低。

### 创新点二：基于理论的主动学习用于数据采样

- **相比以往方法的改进/不同之处**：
    - **传统主动学习**：在监督学习中，当标注成本高时，传统的主动学习采集函数通常基于模型预测的**统计不确定性**（如方差）来选择样本。这类方法**不利用问题底层的物理或数学约束**。
    - **本文方法**：论文提出了一种**理论驱动的主动学习**方法。其采集函数 `a(x0, t0; w)` 衡量的是当前神经网络预测的共态轨迹 `λ̂` 对**庞特里亚金极大值原理** 的**符合程度**。具体做法是：用预测的共态 `λ̂` 前向推演得到状态和动作轨迹 `(x̂*, û*)`，然后利用终端条件反向求解一个**逆值问题**，得到另一条共态轨迹 `λ̃`。采集函数即为 `λ̂` 与 `λ̃` 之间的差异范数。差异越大，说明预测的共态越不满足PMP条件，该样本越有价值。

- **解决的具体问题/带来的优势**：
    1. **针对性采样**：该方法能**主动识别出当前模型预测误差较大的区域**（即那些PMP合规性差的初始状态），并优先为这些状态求解真实的BVP（边界值问题）以获取标注数据。这比随机采样或基于不确定性的采样更能**高效地提升模型在关键区域的性能**。
    2. **降低计算成本**：评估采集函数只需要一次前向推演和一次反向的逆值问题求解，其计算成本**远低于**为每个候选样本从头求解一个完整的BVP（这是获取真实标签所需的成本）。因此，该方法能以可承受的成本实现智能采样。
    3. **提升安全性能**：实验表明，在相同的数据采集预算下，采用此主动学习方法训练的共态网络，在闭环控制中能实现**更低的碰撞概率**（见图4a），尤其是在中等规模训练数据时，其优势相对于静态采样具有统计显著性。

### 总结

| 创新点 | 对比对象 | 核心改进 | 解决的关键问题/优势 |
| :--- | :--- | :--- | :--- |
| **学习低维共态** | 学习价值函数/哈密顿量 | 利用线性动力学下共态的结构化、低维特性，直接学习其参数化表示。 | 1. **数据效率更高**：模型更易学习，泛化更好。<br>2. **策略更稳定**：避免梯度误差放大。<br>3. **实时计算更高效**。 |
| **理论驱动的主动学习** | 基于统计不确定性的主动学习 | 采集函数基于庞特里亚金极大值原理的合规性，而非预测方差。 | 1. **采样更高效**：精准定位模型薄弱环节。<br>2. **标注成本相对低**：评估采集函数的计算远低于求解BVP。<br>3. **最终策略更安全**：在相同数据量下获得更低的碰撞率。 |

这两项创新共同致力于在**数据受限**的场景下，学习出**泛化能力更强、安全性更高**的纳什均衡策略近似模型，对于自动驾驶、机器人交互等需要实时安全决策的领域具有重要的实际价值。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验评估效果分析

该论文通过一个**无信号灯交叉路口两车交互案例**，系统评估了所提出的**协同态网络学习**与**理论驱动的主动学习**方法的性能。以下是详细的实验设置、评价指标、对比方法和关键结论。

### 一、 实验设置与数据集

1.  **系统模型**：
    -   采用**线性动力学**模型（双积分器模型），状态为车辆的位置和速度，控制输入为加速度。
    -   损失函数包含**控制努力成本**（加速度平方）和**碰撞惩罚项**。碰撞惩罚项经过**软化处理**（使用Sigmoid函数），以确保边界值问题（BVP）求解器能够收敛。

2.  **数据集构建**：
    -   **状态空间**：初始位置 `d_i ∈ [15, 20] m`，初始速度 `v_i ∈ [18, 25] m/s`，初始时间 `t_0 ∈ [0, 3] s`。
    -   **数据划分**：使用拉丁超立方采样独立生成三个数据集：
        -   **训练集 (`𝒳_t`)**：1573个初始状态。
        -   **主动学习采集集 (`𝒳_a`)**：1000个初始状态。
        -   **测试集 (`𝒳_e`)**：575个初始状态。
    -   **标签生成**：对每个初始状态，使用BVP求解器离线计算纳什均衡的**状态轨迹、控制轨迹、价值函数和协同态轨迹**，作为监督学习的真实标签。

### 二、 评价指标

论文采用两个层次的指标来评估学习模型的泛化性能：

1.  **协同态预测误差**：在测试集上，计算预测的协同态轨迹与真实协同态轨迹之间的**均方误差 (MSE)**。这是一个**中间指标**，用于衡量模型对博弈解结构的近似能力。
2.  **碰撞概率**：在测试集上进行**闭环控制仿真**，统计使用学习到的策略时发生碰撞的测试案例比例。这是论文关注的**核心性能指标**，直接反映了学习策略的安全性和实际价值。

### 三、 对比的基线方法

论文与以下两种基线方法进行了对比：

1.  **价值网络**：标准的监督学习方法。学习一个神经网络来近似每个玩家的**均衡价值函数 `V_i*(x, t)`**。在控制时，通过自动微分计算价值梯度得到协同态 `λ_i = ∇_x V_i`，再通过最大化哈密顿量得到控制策略。
2.  **静态学习的协同态网络**：论文提出的**核心方法**。直接学习一个神经网络来近似**均衡协同态的参数化表示 `y(x, t)`**。由于在碰撞避免占主导的线性动力学下，协同态具有低维结构（由终端值、进出碰撞区时间等少数参数决定），因此学习效率更高。

### 四、 主要性能结果与结论

实验通过改变训练数据量（从50到300个样本），重复20次随机实验，得到以下关键结论：

1.  **协同态网络 vs. 价值网络**：
    -   **在相同数据量下，协同态网络能实现显著更低的碰撞概率**。例如，在250个训练样本时，协同态网络的碰撞概率约为5%，而价值网络的碰撞概率超过15%。统计检验（单边t检验）表明，这种性能提升在多个数据规模下具有统计显著性（p值 < 0.05）。
    -   **原因分析**：协同态本身具有**低维、分类式的结构**（如图3所示，`q_i` 参数随状态变化呈现“加速”、“减速”、“匀速”等类别变化），而价值函数在状态空间中变化剧烈、非线性强（如图3c,e所示）。因此，学习协同态比学习价值函数**更简单、更数据高效**，预测方差更小，从而在闭环控制中导致更安全的策略。

2.  **主动学习 vs. 静态学习**：
    -   在**中等数据规模（如250个样本）** 下，采用理论驱动主动学习的协同态网络，其碰撞概率**进一步低于**静态采样的协同态网络，且差异具有统计显著性。
    -   **主动学习的机制**：其采集函数 `a(x, t; w)` 衡量了预测协同态对庞特里亚金极大值原理（PMP）的**符合程度**。通过选择PMP符合度最差的状态进行采样和标注，主动学习能更有效地发现和修正模型的预测盲区。
    -   **局限性**：当数据量非常小或非常大时，主动学习的优势不明显。原因在于：1）小数据下模型本身不准确；2）大数据下，静态的拉丁超立方采样本身已是高效采样方法，主动学习的边际效益下降。

3.  **定性结果**：
    -   图5展示了在特定测试案例下的轨迹对比。使用价值网络和静态协同态网络可能导致车辆轨迹异常（如不必要的剧烈减速或转向），甚至发生碰撞（轨迹进入灰色碰撞区）。而使用主动学习的协同态网络得到的轨迹与真实均衡轨迹最为接近，安全通过交叉口。

### 总结

论文通过严谨的实验表明：
-   **技术创新有效**：利用协同态的**低维结构**进行学习，相比学习价值函数，是一种**更数据高效、更安全**的纳什均衡策略近似方法。
-   **方法具有实际价值**：在**无信号灯交叉口**这一典型高风险交互场景中，所提方法能用**更少的标注数据**，学习到**碰撞概率更低**的交互策略，对于自动驾驶等安全关键应用具有重要价值。
-   **主动学习提供额外增益**：在数据预算有限的情况下，理论驱动的主动学习能进一步优化数据使用效率，提升策略的泛化安全性能。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2503.07013v2)
- [HTML 版本](https://arxiv.org/html/2503.07013v2)
