# Continuous Control of Diverse Skills in Quadruped Robots Without Complete Expert Datasets

**相关性评分**: 7.0/10

**排名**: #12


---


## 基本信息

- **arXiv ID**: [2503.03476v1](https://arxiv.org/abs/2503.03476v1)
- **发布时间**: 2025-03-05T13:12:49Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Jiaxin Tu, Xiaoyi Wei, Yueqi Zhang, Taixian Hou, Xiaofei Gao, Zhiyan Dong, Peng Zhai, Lihua Zhang

## 关键词

Robot control, quadruped robot locomotion, mujoco, local observational information

## 一句话总结

该论文提出了一种无需完整专家数据集的四足机器人技能学习方法PASIST，通过自模仿学习和技能选择模块实现多样技能的连续控制和平滑过渡。

## 摘要

Learning diverse skills for quadruped robots presents significant challenges, such as mastering complex transitions between different skills and handling tasks of varying difficulty. Existing imitation learning methods, while successful, rely on expensive datasets to reproduce expert behaviors. Inspired by introspective learning, we propose Progressive Adversarial Self-Imitation Skill Transition (PASIST), a novel method that eliminates the need for complete expert datasets. PASIST autonomously explores and selects high-quality trajectories based on predefined target poses instead of demonstrations, leveraging the Generative Adversarial Self-Imitation Learning (GASIL) framework. To further enhance learning, We develop a skill selection module to mitigate mode collapse by balancing the weights of skills with varying levels of difficulty. Through these methods, PASIST is able to reproduce skills corresponding to the target pose while achieving smooth and natural transitions between them. Evaluations on both simulation platforms and the Solo 8 robot confirm the effectiveness of PASIST, offering an efficient alternative to expert-driven learning.

## 详细分析

## 论文摘要

**论文标题**：无需完整专家数据集的四足机器人多样化技能连续控制

### 1. 研究背景和动机
传统的强化学习（RL）方法依赖精心设计的手动奖励函数，而模仿学习（IL）方法虽然能有效复现专家行为，但需要依赖昂贵且难以获取的完整专家数据集。对于四足机器人学习多样化技能（如行走、爬行、高跷步、双足站立）及其之间的平滑过渡，现有方法面临**数据集获取成本高**和**过渡阶段数据缺失**的挑战。受人类“内省学习”（通过关注目标姿态进行试错模仿）的启发，本研究旨在开发一种无需完整专家数据集，仅基于**目标姿态**即可自主探索并学习多样化技能及过渡的方法。

### 2. 核心方法和技术创新
本文提出了 **渐进式对抗自模仿技能过渡（PASIST）** 框架，其核心创新点包括：
- **基于GASIL的自模仿学习**：在生成对抗自模仿学习（GASIL）框架下，机器人通过探索收集自身的高质量轨迹，并存入自模仿（SIL）缓冲区，随后模仿这些轨迹而非外部专家数据。
- **高质量轨迹评估标准**：提出一种结合**任务奖励**和**动态时间规整（DTW）相似度**的评估指标（公式1），用于从自主探索中筛选出既高效又贴近目标姿态的轨迹。
- **防模式崩溃的技能选择器**：设计了一个自适应技能选择模块（公式4, 5），通过评估各技能当前学习进度，动态调整训练重点，平衡不同难度技能的学习，有效防止策略陷入局部最优（模式崩溃）。
- **动态奖励权重**：总奖励由SIL奖励、任务奖励和正则化奖励组成，其权重（公式7, 8）可根据当前轨迹与目标姿态的DTW相似度及任务奖励动态调整，自主平衡模仿与探索。

### 3. 主要实验结果
在仿真（Isaac Gym）和真实Solo 8机器人上的实验验证了PASIST的有效性：
- **技能模仿**：PASIST成功学习了行走、爬行、高跷步和双足站立四种技能，其生成的动作与各自目标姿态高度相似（图4）。
- **模块有效性**：消融实验表明，移除DTW评估、技能选择器或仅模仿目标姿态都会导致性能下降，特别是在学习困难技能（如双足站立）时，证明了各模块的必要性（图5, 6）。
- **缓解模式崩溃**：与无技能选择器的版本相比，PASIST能持续为困难技能收集高质量模仿轨迹，确保了均衡学习（图7）。
- **零样本仿真到现实迁移**：训练好的策略能够**零样本**部署到真实Solo 8机器人上，实现技能间的平滑切换，并能灵活组合技能以穿越复杂障碍（图2）。

### 4. 研究意义和价值
本研究提出了一种**数据高效**的技能学习新范式。其核心价值在于：
- **降低数据依赖**：仅需目标姿态（而非完整运动序列）即可引导学习，大幅降低了对昂贵专家数据集的依赖。
- **实现自然过渡**：通过自模仿和技能选择机制，自主发现并学习技能间的平滑过渡策略，解决了过渡数据缺失的难题。
- **提升实用性与泛化性**：方法在仿真和现实中均表现鲁棒，为零样本sim-to-real迁移和机器人学习复杂、多样的运动技能提供了高效且可行的解决方案。未来可扩展至集成更多传感器数据以处理更复杂的导航等任务。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究目标与待解决问题**
本文旨在解决**四足机器人学习多样化技能**时面临的两个核心挑战：
1.  **对完整专家数据集的依赖**：传统模仿学习（IL）方法（如GAIL、AMP）需要昂贵、难以获取的完整专家轨迹数据（来自动作捕捉或轨迹优化），且通常缺乏**技能间过渡**的数据。
2.  **模式崩溃与技能平衡**：在生成对抗框架下学习多技能时，策略容易陷入局部最优，过度拟合于某些简单子技能（模式崩溃），导致难以掌握所有技能及它们之间的自然过渡。

### **核心创新点：PASIST框架**
论文提出了 **“渐进式对抗性自模仿技能过渡”（PASIST）** 框架。其核心创新在于**摒弃了对完整专家数据集的依赖**，转而让机器人通过**自探索**和**自模仿**来学习技能和过渡。

#### **1. 方法论创新：基于目标姿态的自模仿学习**
- **核心理念**：受人类“内省学习”启发，仅向机器人提供**目标姿态**（如站立、爬行等关键身体构型），而非完整动作序列。机器人通过试错探索，自主发现并模仿自身产生的高质量轨迹。
- **关键技术组件**：
    - **高质量轨迹选择器**：定义了一个新的评估指标 `A(τ)`，用于筛选探索中的高质量轨迹存入自模仿学习缓冲区。
        ```python
        A(τ) = 累计任务奖励 + DTW(策略轨迹, 目标姿态轨迹)
        ```
        - **任务奖励**：鼓励实现技能功能（如移动速度）。
        - **动态时间规整距离**：衡量探索轨迹与目标姿态的**形态相似性**，防止机器人为了高奖励而做出怪异动作。
    - **自模仿判别器**：在生成对抗自模仿学习框架下，训练一个判别器来区分当前策略产生的轨迹和缓冲区中的高质量轨迹。策略的目标是“欺骗”判别器，从而模仿自己的成功经验。
    - **动态奖励加权**：总奖励 `r` 是 SIL奖励、任务奖励和正则化奖励的加权和。权重 `ω^SIL` 和 `ω^T` 根据当前轨迹与目标姿态的DTW距离以及任务奖励动态调整，实现了**模仿与探索的自动平衡**。

#### **2. 机制创新：技能选择器应对模式崩溃**
- **问题**：在同时学习多个难度不同的技能时，策略容易偏向于更容易获得高奖励的技能（如行走），而忽略困难技能（如双足站立）。
- **解决方案**：引入**技能选择器模块**。
    - **工作原理**：根据每个已学习技能的当前性能（通过 `p(x=m) = 当前平均奖励 / 最优奖励` 计算），**自适应地选择下一个需要训练的技能**。性能越接近最优的技能，被选中的概率越低。
    - **实际价值**：确保了所有技能都能得到充分训练，有效缓解了模式崩溃。同时，在切换训练技能命令时，**自然地引导策略学习技能间的过渡动作**，无需额外的过渡数据集。

### **解决方案总结**
论文通过以下方式系统性地解决了问题：
1.  **数据获取**：用**单一目标姿态**替代**完整专家轨迹**，大幅降低数据需求。
2.  **学习机制**：结合**生成对抗自模仿学习**，让机器人成为自己的“老师”，从自身探索的成功经验中学习。
3.  **训练稳定性**：通过**融合任务奖励与DTW相似性的轨迹评估指标**，以及**技能选择器**，确保多技能学习的平衡性与鲁棒性。
4.  **验证**：在仿真（Isaac Gym）和真实机器人（Solo 8）上验证了框架能学会行走、爬行、高跷步、双足站立四种技能及其间的平滑过渡，并实现了零样本的**仿真到现实迁移**。

### **实际价值**
- **降低开发门槛与成本**：减少了对昂贵动作捕捉系统和复杂轨迹优化算法的依赖，使机器人技能学习更易实施。
- **提升灵活性与泛化性**：机器人能自主发现并学习技能间的自然过渡，增强了应对复杂、非结构化环境的能力。
- **为更广泛的机器人学习提供新范式**：展示了仅凭少量“目标”指导，通过自模仿进行高效技能学习的可行性，可扩展至导航、操作等更复杂任务。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决四足机器人学习多样化技能时，对完整专家数据集依赖性强、技能间过渡不自然的核心难题。为此，作者提出了**渐进式对抗性自模仿技能过渡（PASIST）**框架，该方法仅需目标姿态作为引导，通过结合任务奖励与动态时间规整（DTW）值来评估并筛选机器人自主探索产生的高质量轨迹，以此构建自模仿学习（SIL）数据集，并引入技能选择器来平衡不同难度技能的学习，防止模式崩溃。实验结果表明，PASIST能在无需完整专家数据的情况下，有效学习多种离散技能（如行走、爬行、高跷步、双足站立）并实现它们之间的平滑自然过渡，且训练出的策略能够零样本迁移到真实Solo 8机器人上，展现出良好的泛化性与鲁棒性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文提出了一个名为 **PASIST** 的新框架，用于四足机器人的连续多样化技能学习。其核心创新在于**无需完整的专家数据集**，而是通过模仿机器人自身探索出的高质量轨迹来学习。以下是其相对于已有工作的明确创新点：

### 1. **基于目标姿态的自模仿学习框架**
   - **改进/不同之处**：
     - **以往方法**：传统的模仿学习（如GAIL、AMP）严重依赖**完整、高质量的专家数据集**。这些数据集通常通过昂贵的动作捕捉系统或复杂的轨迹优化算法生成，且难以覆盖技能间的过渡。
     - **本文方法**：PASIST摒弃了对完整专家轨迹的依赖，仅需要为每个技能定义一个**目标姿态**（如站立、爬行的关键身体构型）。机器人通过自主探索，生成轨迹，并从中筛选出高质量的片段存入“自模仿学习缓冲区”，作为后续模仿的“专家数据”。
   - **解决的问题/带来的优势**：
     - **解决了数据获取瓶颈**：大幅降低了对昂贵、耗时专家数据集的依赖，使技能学习更易启动和扩展。
     - **实现了自然过渡**：由于模仿数据来自机器人自身的连续探索，其生成的技能间过渡（如从行走切换到双足站立）更加平滑自然，而传统方法缺乏专门的过渡数据。
     - **更具通用性**：框架适用于任何能定义目标姿态的技能，扩展性强。

### 2. **结合任务奖励与动态时间规整（DTW）的高质量轨迹评估指标**
   - **改进/不同之处**：
     - **以往方法**：在自模仿学习中，通常仅使用**任务奖励**作为筛选高质量轨迹的标准。这容易导致策略陷入局部最优，产生怪异但高奖励的行为，且无法有效评估不同长度轨迹与目标姿态的相似性。
     - **本文方法**：提出一个新的评估值 `A(τ)`，它是**任务奖励总和**与**轨迹和目标姿态之间的DTW距离期望值**的加权和。DTW能有效度量变长序列的相似性。
   - **解决的问题/带来的优势**：
     - **平衡了“最优性”与“相似性”**：不仅鼓励获得高奖励的行为，也鼓励其运动模式与期望的目标姿态相似，避免了为追求奖励而产生的不自然、抖动剧烈的动作。
     - **提供了更可靠的筛选标准**：结合了人类专家对姿态的期望（通过DTW体现），使选入缓冲区的轨迹质量更高，模仿学习效果更好。

### 3. **用于防止模式崩溃的自适应技能选择器**
   - **改进/不同之处**：
     - **以往问题**：在生成对抗网络（GAN）框架下训练多技能策略时，常出现**模式崩溃**——策略过度专注于学习某个简单或高奖励的子技能（如只学会行走），而忽略其他更难技能（如双足站立）。
     - **本文方法**：设计了一个**技能选择器模块**。它根据每个已学习技能的当前性能（通过其任务奖励与理论最优奖励的比值 `p` 来度量），动态调整下一个训练时间步的技能命令。性能越好的技能，被选中的概率越低。
   - **解决的问题/带来的优势**：
     - **平衡了多技能学习**：确保策略在所有技能上都能得到充分训练，特别是那些更难、奖励更稀疏的技能，有效缓解了模式崩溃。
     - **自动化了课程学习**：无需手动设计训练课程，系统能自适应地判断哪些技能需要更多训练关注。
     - **促进了过渡策略学习**：当选择器切换到不同技能时，策略为了完成切换，会自然地学习技能间的过渡动作。

### 4. **动态奖励权重调整机制**
   - **改进/不同之处**：
     - **以往方法**：模仿奖励、任务奖励和正则化奖励的权重通常是固定的或需要手动调整。
     - **本文方法**：PASIST中，模仿奖励权重 `ω^SIL` 和任务奖励权重 `ω^T` 是**动态变化**的。它们分别根据当前缓冲区轨迹与目标姿态的DTW距离、以及当前任务奖励的水平进行计算（见公式7、8）。
   - **解决的问题/带来的优势**：
     - **实现了探索与利用的自动平衡**：当策略离目标姿态较远或任务奖励较低时，系统会调整权重，鼓励更多探索；当接近目标时，则加强模仿。这解决了RL中经典的探索-利用困境。
     - **减少了超参数调优**：自动化了奖励塑形过程，降低了算法对超参数调优的依赖，使训练更稳定、高效。

### 总结
PASIST的核心创新在于**将“仅需目标姿态”的自模仿学习思想系统化**，并通过**创新的轨迹评估指标**和**自适应技能选择机制**，解决了由此带来的模式崩溃和训练不平衡问题。相比需要完整专家数据的传统模仿学习方法，它**数据需求低、能学习自然过渡、且训练更自动化**，为四足机器人的多样化技能学习提供了一种高效、实用的新范式。实验在仿真和真实机器人Solo 8上的成功验证了其有效性和“零样本”仿真到现实的迁移能力。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过仿真和实物实验，验证了所提出的 **PASIST** 框架在四足机器人多技能学习与平滑过渡上的有效性。其实验设计旨在回答三个核心问题，并最终实现了**无需完整专家数据集即可学习多种技能并实现自然技能过渡**的目标。

### 一、 使用的数据集与评价指标

#### 1. 数据集
论文**没有使用传统意义上完整的、时序的专家运动轨迹数据集**。这是其核心创新点。取而代之的是：
*   **目标姿态集**：为每个技能（行走、爬行、高跷步、双足站立）仅提供一个**静态的目标姿态**。这些姿态来源于：
    *   基于轨迹优化的数据集生成。
    *   理论上也可来自真实动作捕捉数据或根据物理参数手动计算。
*   **自生成的高质量轨迹缓冲区**：在训练过程中，算法通过自主探索，根据评估标准筛选出高质量的轨迹，存入 **SIL缓冲区**，作为自我模仿学习的“专家数据”。

#### 2. 主要评价指标
1.  **总奖励**：训练过程中策略获得的总奖励 `r`（公式6），用于衡量整体学习进度和性能。
2.  **动态时间规整值**：用于量化学习到的技能与目标姿态之间的相似度。
    *   **DTW值**：直接计算，值越低表示越相似（表II，图6）。
    *   **J_DTW**（公式9）：将DTW值归一化到[0,1]区间，值越高表示轨迹质量越高、与目标姿态越接近（图7）。
3.  **t-SNE可视化**：对策略生成的状态和动作空间进行降维可视化，直观展示不同技能在特征空间中的分布差异（图3）。
4.  **定性实物演示**：在Solo 8实物机器人上进行零样本仿真到现实迁移，展示技能执行和过渡的平滑性与鲁棒性（图2）。

### 二、 对比的基线方法

论文进行了系统的消融实验，将完整的PASIST与以下三种变体进行对比，以验证其各个模块的必要性：
1.  **IL by tp**：使用PASIST框架，但用简单复制目标姿态生成的“伪专家数据集”进行模仿学习。用于对比**自主筛选高质量轨迹**与**使用简陋固定数据**的效果差异。
2.  **PASIST w/o DTW**：在筛选高质量轨迹进入SIL缓冲区时，**仅依据任务奖励**，而不使用DTW值评估与目标姿态的相似性。用于验证**结合任务奖励与姿态相似性**这一评估准则的有效性。
3.  **PASIST w/o selector**：**移除了技能选择器模块**，在每个时间步随机选择要训练的技能。用于验证**自适应技能选择器**在平衡不同难度技能学习、防止模式崩溃方面的作用。

### 三、 关键性能提升与结论

#### 1. 多技能学习效果
*   **定量结果**：如图4所示，PASIST学习到的四种技能（行走、爬行、高跷步、双足站立）在形态上与各自的目标姿态高度相似。
*   **定量对比**：在图6的消融实验中，对于相对简单的“行走”技能，所有方法都能达到较低的DTW值（高相似度）。但对于更困难的“双足站立”技能，**只有完整的PASIST能实现与目标姿态的高相似度**，其他基线方法产生的姿态与目标相差甚远。这证明了PASIST在处理技能难度差异上的优越性。

#### 2. 训练效率与稳定性
*   **总奖励曲线**：如图5所示，PASIST的总奖励 `r` 随着训练持续增长，显示出其能够不断从SIL缓冲区中学习并提升。而其他基线方法（尤其是IL by tp和PASIST w/o selector）很快收敛到局部最优，奖励停止增长，表明其探索能力受限或陷入模式崩溃。
*   **模式崩溃缓解**：图7通过 `J_DTW` 指标展示了SIL缓冲区中高质量轨迹的质量变化。对于“行走”技能，所有方法都能收集到高质量轨迹。但对于“双足站立”技能，**移除技能选择器（PASIST w/o selector）的方法几乎无法收集到有效的轨迹**，而完整的PASIST则可以。这直接证明了技能选择器对于平衡不同难度技能的学习、防止策略过度偏向简单技能（模式崩溃）至关重要。

#### 3. 技能过渡与仿真到现实迁移
*   **定性结论**：论文通过实物实验（图2）展示了学习到的策略可以成功进行**零样本的仿真到现实迁移**。机器人能够根据指令，在行走、爬行、高跷步、双足站立等技能间进行**平滑、自然的切换**，并利用这些技能组合灵活地通过现实世界中的障碍物。这证明了PASIST所学习策略的**强泛化能力和鲁棒性**。

### 总结
论文通过系统的实验表明，PASIST框架在**仅依赖目标姿态而非完整专家数据集**的前提下，能够：
1.  **有效学习**多种形态各异的运动技能。
2.  **通过创新的轨迹评估准则（任务奖励+DTW）和技能选择器**，显著提升训练效率，避免模式崩溃，确保所有技能都能被充分学习。
3.  **自主发现并实现平滑的技能间过渡**。
4.  **学习到的策略具备良好的仿真到现实迁移能力**。

其核心性能提升体现在**与使用简陋或固定数据的基线方法相比，在困难技能学习效果、训练稳定性和最终策略的泛化能力上均有显著优势**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2503.03476v1)
- [HTML 版本](https://arxiv.org/html/2503.03476v1)
