# Chance-constrained Linear Quadratic Gaussian Games for Multi-robot Interaction under Uncertainty

**相关性评分**: 6.0/10

**排名**: #36


---


## 基本信息

- **arXiv ID**: [2503.06776v2](https://arxiv.org/abs/2503.06776v2)
- **发布时间**: 2025-03-09T21:03:53Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Kai Ren, Giulio Salizzoni, Mustafa Emre Gürsoy, Maryam Kamgarpour

## 关键词

Robot control, multi-agent RL, CTDE methods, local observational information

## 一句话总结

该论文提出了一种基于机会约束线性二次高斯博弈的方法，用于处理多机器人交互中的不确定性和安全性问题，并通过算法确保反馈广义纳什均衡。

## 摘要

We address safe multi-robot interaction under uncertainty. In particular, we formulate a chance-constrained linear quadratic Gaussian game with coupling constraints and system uncertainties. We find a tractable reformulation of the game and propose a dual ascent algorithm. We prove that the algorithm converges to a feedback generalized Nash equilibrium of the reformulated game, ensuring the satisfaction of the chance constraints. We test our method in driving simulations and real-world robot experiments. Our method ensures safety under uncertainty and generates less conservative trajectories than single-agent model predictive control.

## 详细分析

## 论文摘要

**论文标题：** 不确定性下多机器人交互的机会约束线性二次高斯博弈

### 1. 研究背景和动机
在仓库自动化、无人机集群和自动驾驶车队等多机器人交互应用中，确保安全至关重要。现有研究要么缺乏对随机不确定性的考虑，要么无法提供安全性和收敛性的理论保证。本文旨在解决**不确定性下多智能体交互的安全问题**，通过博弈论框架建模智能体间的相互影响，并保证机会约束的满足。

### 2. 核心方法和技术创新
本文提出了一个**机会约束线性二次高斯（LQG）博弈**框架，并做出了以下核心贡献：
- **问题建模与转化**：将具有耦合约束和系统噪声（高斯分布）的多智能体交互建模为机会约束LQG博弈。通过风险分配和仿射近似，将非凸的联合机会约束（如防碰撞）**保守地转化为确定性的线性状态约束**，从而得到一个可处理的LQG博弈。
- **算法设计与理论保证**：提出了一种**对偶上升算法**（Algorithm 1）来求解转化后博弈的反馈广义纳什均衡。在强对偶性等假设下，**证明了算法以O(1/k)的次线性速率收敛**到反馈GNE，该均衡策略能确保满足转化后的线性约束，从而满足原始的机会约束。
- **策略形式**：算法最终输出一个**线性状态反馈策略**，可离线计算、在线高效执行。

### 3. 主要实验结果
方法在自动驾驶仿真和真实机器人实验中得到验证：
- **仿真对比**：在十字路口场景中，与将其他车辆视为动态障碍物的**单智能体模型预测控制（MPC）** 相比，本文的博弈方法能生成**更高效（成本更低、通行时间更短）且同样安全（碰撞率低于设定阈值）** 的轨迹。单智能体MPC因保守而等待，导致效率较低。
- **实物实验**：在四机器人平台上，算法成功实现了实时、安全、无碰撞的交互导航。
- **计算效率**：博弈方法虽需较长的离线计算时间，但在线仅需微不足道的计算量进行状态反馈；而MPC需要在每个时间步在线重新规划。

### 4. 研究意义和价值
本研究为**不确定环境下安全的多智能体交互**提供了一个兼具**理论保证**和**实践可行性**的框架。其价值在于：
- **理论层面**：首次在机会约束LQG博弈中给出了收敛到反馈GNE并保证机会约束满足的严格证明，弥补了相关工作的空白。
- **应用层面**：通过博弈建模显式考虑智能体间的策略互动，相比传统的单智能体规划方法，能减少保守性，提升系统整体效率，并支持实时应用。
- **局限性**：当前方法依赖于线性高斯假设和中心化规划，未来工作可致力于放松这些假设并发展分布式方法。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究问题**
论文旨在解决**不确定环境下多机器人交互的安全性问题**。具体而言，在存在系统噪声（高斯不确定性）和耦合约束（如碰撞避免）的情况下，如何为多个自私的智能体（机器人）规划出安全、高效且非保守的轨迹。

### **核心创新点**
1.  **问题建模创新**：
    - 首次将**机会约束**与**线性二次高斯博弈**相结合，提出了**机会约束LQG博弈**模型。该模型同时考虑了：
        - **随机不确定性**：系统动态受高斯噪声影响。
        - **耦合机会约束**：要求状态（如机器人位置）以一定概率满足安全集约束（如避免碰撞、保持在车道内）。
    - 该模型比传统的确定性博弈或单智能体规划更能反映现实世界交互中的不确定性和智能体间的策略耦合。

2.  **理论方法创新**：
    - **可处理的约束重构**：通过**风险分配**和**仿射内近似**，将非凸、难处理的机会约束（特别是二次型的碰撞避免约束）保守地近似为一组**确定性的线性状态约束**。这保证了新问题的任何解都满足原机会约束。
    - **对偶上升算法与收敛性证明**：针对重构后的带线性约束的LQG博弈，提出了一个**对偶上升算法**，并**首次证明了该算法能以O(1/k)的次线性速率收敛到反馈广义纳什均衡**。
        - **关键引理**：证明了在LQG博弈设定下，对偶函数的梯度是Lipschitz连续的，这是收敛性证明的基础。
        - **实际价值**：该证明为算法在安全关键应用中的可靠性提供了理论保障，弥补了先前类似工作缺乏理论保证的空白。

3.  **实际应用验证**：
    - 在**自动驾驶仿真**和**真实多机器人实验**中验证了方法的有效性。
    - 结果表明，与**单智能体模型预测控制**相比，本文的博弈论方法能生成**更高效（更少保守）** 的轨迹，同时通过机会约束保证安全概率。
    - 算法能离线计算**线性状态反馈策略**，在线应用时计算开销极低，支持实时交互。

### **解决方案概述**
1.  **建模**：将多机器人系统建模为受高斯噪声驱动的线性时变系统，每个机器人（玩家）的目标是最化自己的二次型代价，并共同满足耦合的机会约束。
2.  **重构**：利用概率论工具（高斯分布性质）和保守近似，将机会约束转化为关于状态期望值的确定性线性约束，得到一个新的、可处理的带线性约束的LQG博弈。
3.  **求解**：
    - 利用**广义纳什均衡**和**对偶理论**，将约束博弈的求解转化为一系列无约束LQG博弈的求解。
    - 提出**对偶上升算法**：在每次迭代中，固定对偶变量，求解一个无约束LQG博弈的反馈纳什均衡；然后利用约束违反程度更新对偶变量。
    - **输出**：算法返回一个线性状态反馈策略，该策略是原机会约束博弈的反馈GNE的近似。
4.  **理论保证**：在假设（强对偶性）成立的前提下，严格证明了算法的收敛性及其生成的策略能满足原机会约束。
5.  **实验验证**：通过交叉路口驾驶仿真和四机器人实物实验，证明了方法在确保安全、提升效率和支持实时计算方面的优势。

### **技术亮点总结**
```plaintext
- **模型**：CC-LQG Game = 随机动态 + 策略交互 + 概率安全。
- **重构**：Chance Constraints -(保守近似)-> Deterministic Linear Constraints。
- **算法**：Dual Ascent on Lagrange Multipliers + Solving Unconstrained LQG NE。
- **理论**：O(1/k) Convergence to Feedback GNE with Safety Guarantees。
- **实践**：Less Conservative than Single-Agent MPC, Real-Time Feedback Policy。
```

**简言之，这篇论文的核心价值在于为不确定环境下的多智能体安全交互提供了一个兼具理论严谨性（收敛性与安全性证明）和实际可行性（可计算、已验证）的博弈论框架。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**多机器人系统在不确定性下的安全交互问题**，核心是处理随机噪声和耦合约束（如避碰）下的协同决策。为此，作者提出了一个**机会约束线性二次高斯博弈**模型，并通过一个**保守的线性化近似**将复杂的概率约束转化为确定性线性约束，从而将问题重构为一个可处理的约束LQG博弈。论文的主要贡献是设计了一个**对偶上升算法**，并证明了该算法能以O(1/k)的次线性收敛速度收敛到重构博弈的一个**反馈广义纳什均衡**，该均衡能保证满足原始的机会约束。仿真和实物机器人实验表明，该方法能在保证安全的前提下，生成比单智能体模型预测控制**更高效、更不保守**的轨迹。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇题为《不确定性下多机器人交互的机会约束线性二次高斯博弈》的论文，在解决多智能体交互的安全与效率问题上，提出了几个关键的创新点。

### 1. **问题建模的创新：将机会约束与反馈广义纳什均衡结合**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：先前关于约束动态博弈的研究主要集中于**开环广义纳什均衡**（Open-loop GNE）或**确定性环境下的反馈GNE**。例如，文献[3,4,5]研究了开环GNE的计算，而[11,12]在确定性假设下推导了反馈GNE。虽然文献[5]考虑了随机不确定性和机会约束，但未提供收敛性或安全性保证。
    - **本文方法**：本文首次在**随机不确定性**（高斯噪声）和**机会约束**的设定下，**系统性地建模并求解一个反馈广义纳什均衡问题**。它将单智能体风险约束LQR问题[13,14]的框架扩展到了多智能体博弈场景。
- **解决的具体问题/带来的优势**：
    - 解决了现实世界中多机器人交互（如自动驾驶、无人机集群）必须同时应对**系统不确定性**和**耦合安全约束**（如防碰撞）的核心挑战。
    - 提供了在随机环境下进行**安全、交互式决策**的严格数学框架，为后续的算法设计和理论分析奠定了基础。

### 2. **算法设计的创新：提出具有收敛性保证的对偶上升算法**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：文献[5]使用增广拉格朗日法求解机会约束动态博弈，但**缺乏收敛性证明**。文献[14]针对单智能体风险约束LQR问题证明了次线性收敛，但需要“对偶变量有界”的强假设。
    - **本文方法**：本文提出了一个**对偶上升算法**（算法1），并**在更弱的假设下，证明了其以O(1/k)的速率次线性收敛到反馈GNE**。关键在于证明了在对偶空间中，每个智能体的对偶函数梯度是Lipschitz连续的（引理2）。
- **解决的具体问题/带来的优势**：
    - **提供了可验证的收敛保证**，这对于安全关键应用至关重要。算法确保最终得到的策略满足经保守近似后的机会约束。
    - 算法计算的是**线性状态反馈策略**，形式为 `u_t^i = -K_t^i x_t - α_t^i`。这种策略可以**离线计算**，在线应用时仅需简单的状态反馈，**计算开销极低**，支持实时部署。

### 3. **理论贡献的创新：将非凸机会约束转化为线性约束的保守近似**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：机会约束，特别是涉及非凸二次约束的防碰撞约束（公式5），通常是**难以直接处理的**。
    - **本文方法**：通过**引理1**，论文给出了一种将原机会约束LQG博弈（6）**保守近似为一个带有线性状态约束的LQG博弈（7）** 的显式方法。对于防碰撞约束，该方法在给定的参考距离向量处进行一阶近似，并利用高斯分布的性质进行确定性重构。
- **解决的具体问题/带来的优势**：
    - **将难以处理的随机非凸博弈问题，转化为一个结构清晰、可求解的线性二次高斯博弈问题**。这是整个方法能够实现收敛性证明和高效计算的前提。
    - 保证了任何近似后博弈（7）的反馈GNE，都**必然满足原问题的机会约束（3）**，从而为安全性提供了理论背书。

### 4. **实际验证的创新：通过对比实验凸显博弈方法的优势**
- **相比以往方法的改进/不同之处**：
    - **以往方法**：许多多机器人规划工作采用**集中式或单智能体模型预测控制**，将其他智能体视为动态障碍物。这种方法本质上是**保守的**，因为它假设其他智能体不会响应自己的行为。
    - **本文方法**：在自动驾驶十字路口场景中，将本文的LQG博弈规划器与**单智能体机会约束MPC**[16,20]进行了对比。
- **解决的具体问题/带来的优势**：
    - 实验表明，博弈方法通过考虑智能体间的**策略性互动**，能够生成**更高效（更低成本、更短通行时间）** 的轨迹。例如，博弈中上方车辆会策略性地先行通过，而单智能体MPC中的车辆则会保守地等待。
    - 在**满足相同安全水平**（碰撞率低于5%）的前提下，博弈方法在效率上显著优于传统的单智能体规划方法，证明了在多智能体系统中显式建模交互的价值。

### 总结
本文的核心创新在于**建立了一个兼具理论严谨性和实用性的框架**，用于解决不确定环境下多智能体的安全交互问题。它通过**创新的问题转化**，将复杂的机会约束博弈简化为可处理的形态；通过**设计并证明一个收敛算法**，提供了可靠的求解工具；最终通过**实验验证**，证明了其在提升系统效率方面的实际优势。这些工作为后续在更复杂场景（如非线性、分布式信息）下的研究奠定了基础。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过**仿真实验**和**真实世界机器人实验**验证了所提出的机会约束线性二次高斯（LCC-LQG）博弈方法的有效性。以下是详细的实验设置、对比方法和主要结论。

### 一、 实验设置与评价指标

#### 1. 实验场景
- **自动驾驶交叉路口仿真**：模拟三辆车同时到达一个十字路口，目标是尽快通过路口并避免碰撞。
- **多机器人真实世界实验**：在 `7m × 3.5m` 的区域内，使用多个 Nvidia JetBot 机器人进行交互导航实验，目标是到达各自目的地并避免碰撞。

#### 2. 系统模型与参数
- **动力学模型**：基于独轮车模型的线性化，得到线性时变系统（公式1）。状态包括2D位置、朝向和线速度，控制输入为线加速度和角速度。
- **成本函数**：包含到达目标状态的终端成本和平滑驾驶的输入成本。
- **约束**：
    - **车道保持与速度限制**：采用解耦的箱型约束（公式4）。
    - **碰撞避免**：采用成对的耦合状态约束（公式5）。
- **风险容忍度**：`ϵ = 0.05`，采用均匀风险分配。
- **规划时域**：10秒，时间间隔0.2秒，共 `T=50` 个时间步。

#### 3. 评价指标
- **最优性**：
    - **平均总成本**：所有智能体成本（公式6a）之和的平均值。
    - **总旅行时间**：所有车辆到达目标所需的总时间。
- **安全性**：
    - **碰撞率**：在多次仿真运行中发生碰撞的比率。
- **计算效率**：
    - **计算时间**：算法求解策略所需的时间（离线计算或在线重规划）。

### 二、 对比的基线方法

论文将提出的 **LQG-Game 方法** 与以下基线方法进行了对比：

1.  **单智能体机会约束模型预测控制**：
    - **方法描述**：将多智能体问题简化为单智能体问题。仅控制一辆车（顶部车辆），其他车辆被视为具有已知状态协方差传播的动态障碍物。基于对障碍物轨迹的预测，控制车辆通过机会约束MPC规划自身轨迹。
    - **实现**：基于文献 [16, 20] 的算法，采样2000条障碍物轨迹来估计其不确定性参数的高斯矩。

2.  **LQG-Game 方法的两种变体**：
    - **状态反馈**：离线计算线性状态反馈策略（公式9），在线仅需状态评估，计算开销可忽略。
    - **滚动时域**：在线每个时间步重新运行算法1进行规划。

### 三、 主要性能结果与结论

基于100次随机噪声下的仿真结果（汇总于表1），主要结论如下：

#### 1. 最优性与效率
- **LQG-Game 方法显著降低了保守性**：
    - **成本更低**：LQG-Game（两种变体）的平均总成本（~105）显著低于单智能体MPC（125.4）。
    - **通行更快**：LQG-Game规划下，顶部车辆更主动，率先通过路口，总旅行时间（4.8秒）远短于单智能体MPC的保守等待策略（9.6秒）。
- **结论**：博弈论框架通过建模智能体间的相互策略依赖，能够生成更高效、更符合实际交互行为的轨迹。

#### 2. 安全性
- **所有方法均满足安全阈值**：在 `ϵ=5%` 的风险容忍度下，所有规划器的碰撞率均低于该阈值。
    - LQG-Game 方法：碰撞率 **2%**。
    - 单智能体MPC：碰撞率 **0%**（最为保守）。
- **结论**：论文提出的方法在显著提升效率的同时，依然能保证机会约束所要求的安全水平。

#### 3. 计算效率
- **计算时间对比**：
    - **LQG-Game (状态反馈)**：**离线计算** 3.61秒。在线应用策略的时间可忽略，**非常适合实时控制**。
    - **LQG-Game (滚动时域)**：**在线重规划** 每步0.66秒（需在0.2秒的步长内完成）。
    - **单智能体MPC (滚动时域)**：**在线重规划** 每步1.33秒。
- **结论**：虽然LQG-Game的离线计算或单步在线规划时间较长，但其**状态反馈形式**实现了计算负担的转移，在线计算效率极高，具备实际部署的潜力。

#### 4. 真实世界实验验证
- **实验效果**：在50次随机目标点的四机器人实验中，机器人均成功到达目标且**未发生碰撞**。
- **实际价值**：证明了算法在存在真实传感器噪声和执行器不确定性的物理系统上的**可行性与鲁棒性**。线性状态反馈策略便于在ROS2系统中实现实时控制。

### 四、 总结

论文通过系统的仿真与实物实验，定量和定性地证明了所提方法的优势：
- **核心创新价值得以验证**：提出的机会约束LQG博弈框架及对偶上升算法，能够有效处理不确定性下的多机器人安全交互问题。
- **关键性能提升**：与传统的、将其他智能体视为动态障碍物的单智能体MPC相比，该方法通过显式建模博弈交互，在**保证可比安全性**的前提下，显著**减少了行为的保守性**，提升了整体效率。
- **实用潜力**：算法能提供可离线计算的**线性状态反馈策略**，确保了在线执行的实时性，并通过真实机器人实验展示了其工程应用的可行性。

**局限性说明**：论文未在更大规模、更复杂场景的数据集（如标准自动驾驶数据集）上进行测试，也未与更广泛的多智能体规划算法（如基于学习的或完全分布式的方法）进行对比。这可能是未来工作的方向。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2503.06776v2)
- [HTML 版本](https://arxiv.org/html/2503.06776v2)
