# Data-Efficient Learning from Human Interventions for Mobile Robots

**相关性评分**: 6.0/10

**排名**: #32


---


## 基本信息

- **arXiv ID**: [2503.04969v1](https://arxiv.org/abs/2503.04969v1)
- **发布时间**: 2025-03-06T21:02:02Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Zhenghao Peng, Zhizheng Liu, Bolei Zhou

## 关键词

Robot control, quadruped robot locomotion, mujoco, local observational information

## 一句话总结

该论文提出一种结合模仿学习和强化学习的在线人机交互方法PVP4Real，用于提高移动机器人在真实世界任务中的数据效率和训练安全性，并在四足机器人和轮式机器人上验证。

## 摘要

Mobile robots are essential in applications such as autonomous delivery and hospitality services. Applying learning-based methods to address mobile robot tasks has gained popularity due to its robustness and generalizability. Traditional methods such as Imitation Learning (IL) and Reinforcement Learning (RL) offer adaptability but require large datasets, carefully crafted reward functions, and face sim-to-real gaps, making them challenging for efficient and safe real-world deployment. We propose an online human-in-the-loop learning method PVP4Real that combines IL and RL to address these issues. PVP4Real enables efficient real-time policy learning from online human intervention and demonstration, without reward or any pretraining, significantly improving data efficiency and training safety. We validate our method by training two different robots -- a legged quadruped, and a wheeled delivery robot -- in two mobile robot tasks, one of which even uses raw RGBD image as observation. The training finishes within 15 minutes. Our experiments show the promising future of human-in-the-loop learning in addressing the data efficiency issue in real-world robotic tasks. More information is available at: https://metadriverse.github.io/pvp4real/

## 详细分析

## 论文摘要

**论文标题**：Data-Efficient Learning from Human Interventions for Mobile Robots

### 1. 研究背景和动机
移动机器人（如自主配送和服务机器人）的广泛应用依赖于高效、安全的学习方法。传统的模仿学习（IL）需要大量演示数据且存在分布偏移问题，而强化学习（RL）则依赖于精心设计的奖励函数并面临模拟到现实的鸿沟（sim-to-real gap）。这两种方法在现实世界部署中通常数据效率低下且存在安全风险。因此，亟需一种能够**高效利用人类反馈、确保训练安全**的新方法。

### 2. 核心方法和技术创新
本文提出了 **PVP4Real**，一种**数据高效、人在回路（Human-in-the-Loop）的在线学习方法**。其核心创新在于：
- **奖励函数无关**：无需手动设计奖励函数或任何预训练。
- **IL与RL的融合**：算法结合了模仿学习与强化学习的优势。
    - **IL组件**：通过行为克隆（BC）损失，使策略模仿人类干预时演示的最优动作。
    - **RL组件**：利用时序差分（TD）学习从非干预数据中学习环境动态和因果关联，提升泛化能力；同时，通过一个代理价值函数，明确地为被人类干预的智能体动作赋予低价值，引导策略避开不良行为。
- **双缓冲区设计**：将人类干预数据与智能体自主交互数据分别存储，平衡训练样本，防止数据不平衡。
- **现实世界部署导向**：方法直接面向真实机器人，处理原始RGB-D图像等复杂感官输入和真实的动力学延迟。

### 3. 主要实验结果
研究在两个现实世界的移动机器人任务上验证了PVP4Real：
- **安全导航任务**：机器人使用原始RGB-D图像进行端到端学习，在走廊中前进并避障（包括静态障碍、动态行人和紧急制动）。
- **人体跟随任务**：机器人根据检测到的人体边界框进行跟随。
- **实验平台**：在Unitree Go2（四足机器人）和Coco（轮式配送机器人）上进行了测试。
- **关键结果**：
    - **极高数据效率**：仅需**约15分钟**（2000步交互）的在线人类监督，即可从零开始训练出有效策略。
    - **性能优越**：在安全导航和人体跟随的所有子任务上，PVP4Real的成功率均显著超过传统行为克隆（BC）基线。例如，在“急转弯”这种训练中未见过的情况下，PVP4Real仍能100%成功，而BC完全失败。
    - **仿真验证**：在模拟驾驶环境中，PVP4Real在达到相近性能时，所需的人类干预次数少于其他先进的人在回路方法。

### 4. 研究意义和价值
本工作的价值主要体现在：
- **实践价值**：首次系统性地证明了**复杂人在回路在线学习算法在真实移动机器人上的可行性与高效性**，为机器人的快速、安全技能学习提供了切实可行的方案。
- **方法创新**：提出的IL与RL融合框架，在无需奖励函数的设定下，巧妙地利用人类干预信号作为学习信号，同时兼顾了模仿的准确性和强化学习的泛化与因果推理能力。
- **推动领域发展**：为克服机器人学习中的数据效率与安全性瓶颈提供了新思路，推动了人机交互与机器人学习交叉领域向更复杂、更真实的场景发展。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究背景与核心问题**
这篇论文旨在解决**移动机器人**在现实世界中应用**学习型方法**（如模仿学习IL和强化学习RL）时面临的三大核心挑战：
1.  **数据效率低下**：传统IL需要大量专家演示数据，RL需要大量环境交互试错。
2.  **安全性风险**：RL在现实世界中的探索可能导致危险行为（如碰撞）。
3.  **奖励函数设计困难与仿真到现实的鸿沟**：RL依赖精心设计的奖励函数，且仿真训练的策略难以直接迁移到充满噪声和不确定性的真实环境。

### **核心创新点：PVP4Real方法**
论文提出了名为 **PVP4Real** 的**在线人机交互学习框架**，其核心创新在于**将模仿学习与强化学习在无奖励设置下进行高效融合**，直接从人类的在线干预和演示中学习策略。

**主要创新体现在以下三个层面：**

1.  **方法论创新：奖励无关的IL与RL融合**
    *   **无需奖励函数**：完全摆脱了对人工设计奖励函数的依赖。
    *   **双目标函数设计**：
        *   **价值网络更新 (`J^Ours(θ)`)**：结合了基于干预的Q值学习 (`J^Q`) 和时序差分学习 (`J^TD`)。
            ```python
            # 公式 (5) 简化表示
            J^Ours(θ) = J^Q(θ) + J^TD(θ)
            # J^Q: 使被干预的机器人动作Q值为负，人类替代动作Q值为正。
            # J^TD: 在所有数据上进行无奖励的TD学习，学习环境动态和状态关联。
            ```
        *   **策略网络更新 (`J^Ours(φ)`)**：结合了策略优化目标和行为克隆损失 (`J^BC`)。
            ```python
            # 公式 (6) 简化表示
            J^Ours(φ) = -E[Q(s,a)] + J^BC(φ)
            # 前半部分：最大化价值（RL目标）。
            # 后半部分：模仿人类演示动作（IL目标），起到正则化作用，防止策略偏离数据分布太远。
            ```

2.  **系统与实验创新：在复杂现实任务中验证**
    *   **真实世界验证**：在**两个不同的机器人平台**（Unitree Go2四足机器人和Coco轮式配送机器人）和**两个复杂任务**（安全导航、人体跟随）上成功部署。
    *   **处理高维原始感知**：在安全导航任务中，策略直接以**原始RGB-D图像**作为输入，进行端到端学习，证明了方法对噪声传感器数据的鲁棒性。
    *   **极高的数据与时间效率**：仅需**约2000步交互（约15分钟）** 的人类监督，即可从零开始训练出有效的策略。

3.  **算法流程创新：高效利用人机交互数据**
    *   **双回放缓冲区**：将数据按是否有人类干预分别存入 `𝒟_h`（人类缓冲区）和 `𝒟_n`（智能体缓冲区），并均衡采样，解决了数据分布不平衡的问题。
    *   **在线实时学习**：算法（见Algorithm 1）在机器人与环境、人类交互的同时，实时更新策略和价值网络，实现了“边做边学”。

### **解决方案总结**
论文通过以下方式系统性地解决了前述问题：

- **解决数据效率问题**：利用人类的**在线干预**作为高质量、针对性的数据来源，替代了RL的大量随机探索和IL的大量离线演示收集。TD学习帮助模型**泛化**人类偏好，减少了对重复演示的依赖。
- **解决安全问题**：人类作为安全监督者，可以实时中断危险行为并提供正确演示，确保了**训练过程本身的安全**。
- **规避奖励设计与仿真鸿沟**：采用**无奖励学习**，直接从人类行为中推断“偏好”。通过在**真实机器人上进行在线学习**，彻底避免了仿真到现实的迁移问题。

### **实际价值与意义**
- **为机器人快速技能部署提供了新范式**：使得在真实、动态、非结构化的环境中，为移动机器人快速教授新技能成为可能，且成本（时间、数据）极低。
- **提升了人机交互的实用性与安全性**：将人类从“数据标注者”或“奖励函数设计师”转变为**实时的、主动的教学伙伴**，使人机协作训练更自然、更安全。
- **展示了方法通用性**：成功应用于不同形态的机器人（轮式、足式）、不同任务（导航、跟随）、以及不同输入模态（原始图像、边界框），显示了其作为**通用机器人学习框架**的潜力。

**结论**：PVP4Real的核心创新在于提出并验证了一套**数据高效、安全、且无需奖励**的在线人机协作学习框架，通过巧妙的算法设计将IL的稳定性和RL的泛化能力相结合，为移动机器人在现实世界中的快速、安全学习提供了切实可行的解决方案。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决移动机器人在现实世界中应用学习算法时面临的数据效率低下和训练安全性问题。传统模仿学习（IL）和强化学习（RL）方法通常需要大量数据、精心设计的奖励函数，并且存在仿真到现实的鸿沟。为此，论文提出了 **PVP4Real** 方法，这是一种**在线人机交互学习框架**，它巧妙地将IL和RL相结合，无需预定义奖励函数或进行预训练。其核心创新在于，通过人类在训练过程中的实时干预和演示，系统性地利用这些交互数据：一方面通过行为克隆（BC）学习人类演示，另一方面通过构建代理价值函数（Proxy Value Function）来惩罚被人类干预的次优动作，并利用时序差分（TD）学习来提升策略的泛化能力。该方法在两个具有挑战性的真实世界任务（安全导航和人体跟随）和两种不同的机器人平台上进行了验证，结果表明，仅需**15分钟**的在线训练，机器人就能从零开始高效、安全地学会任务，显著超越了传统行为克隆方法的性能，证明了其人机交互学习范式在提升数据效率和保障训练安全方面的巨大潜力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Data-Efficient Learning from Human Interventions for Mobile Robots》提出的 **PVP4Real** 方法，在“人在回路”机器人学习领域做出了多项明确的创新。其核心在于将模仿学习与强化学习在无奖励、在线交互的设定下进行高效结合，以解决移动机器人在真实世界中数据效率低下和训练不安全的问题。

以下是其相对于已有工作的主要创新点：

### 1. **提出了一种新颖的、无奖励的在线人机协同学习算法框架**
   - **改进/不同之处**：
     - **无奖励设计**：与传统的强化学习需要精心设计奖励函数，或基于偏好的学习需要事后的人类反馈不同，PVP4Real完全**无需任何环境奖励信号或人工定义的奖励函数**。
     - **在线干预与演示的联合利用**：算法同时利用人类**在线干预**（接管控制）时产生的“负面”信号（被干预的动作是不好的）和**人类演示**（被干预时执行的动作是好的）的“正面”信号，以及智能体未被干预时产生的“中性”数据。
     - **RL与IL的深度融合**：并非简单交替使用IL和RL，而是通过一个统一的**代理价值函数**来融合三种信号。具体地，通过组合三个损失函数来更新策略和价值网络：1) 对演示数据的模仿学习损失；2) 对干预数据的价值惩罚/奖励损失；3) 对所有数据（无奖励）的时间差分学习损失。
   - **解决的问题/带来的优势**：
     - **解决了奖励工程难题和偏好反馈的延迟问题**：避免了设计复杂任务奖励函数的困难，也无需等待交互结束后再收集人类偏好，实现了实时、高效的学习。
     - **提升了数据利用效率和策略泛化能力**：TD学习部分使策略能理解状态间的时序因果关系，相比纯步进式模仿学习，能更好地泛化到未见过的情况，减少复合误差。
     - **确保了训练过程的安全性**：人类可以实时干预，防止机器人执行危险动作，这是离线数据收集或纯RL探索无法保证的。

### 2. **首次在复杂真实机器人任务中系统验证了在线“人在回路”方法的有效性与高效性**
   - **改进/不同之处**：
     - **任务与输入的复杂性**：此前大多数“人在回路”研究局限于仿真环境或简单任务（如直线行驶、固定赛道）。本文在**真实世界**中验证了两个更具挑战性的任务：**安全导航**（以原始RGB-D图像为输入，端到端学习）和**人体跟随**（以边界框为输入）。这涉及处理**真实的传感器噪声、视觉外观变化、复杂的机器人动力学和人体行为**。
     - **跨平台与跨模态的泛化**：实验在两种动力学截然不同的机器人（四足机器Unitree Go2和轮式配送机器人Coco）上进行，并处理了**原始图像**和**处理后的特征**两种不同模态的输入，证明了方法的通用性。
     - **极致的训练效率**：在仅**约2000步交互**（约7分钟人类监督时间，总耗时15分钟内），即可从零开始训练出有效的策略。
   - **解决的问题/带来的优势**：
     - **证明了方法对“仿真到现实”鸿沟的克服能力**：通过在线人类干预直接应对真实世界的噪声和不确定性，无需依赖可能不准确的仿真模型进行预训练或微调。
     - **展示了极高的数据效率和实用性**：将训练时间缩短到分钟级，这对于在真实环境中快速部署和适应移动机器人至关重要，大大降低了数据收集成本和部署门槛。
     - **为复杂视觉-运动策略的在线学习提供了可行路径**：特别是在安全导航任务中，成功训练了一个从高维RGB-D图像到速度命令的端到端策略，这在以往的在线人机交互学习中很少见。

### 3. **设计了更高效的数据利用与策略正则化机制**
   - **改进/不同之处**：
     - **双缓冲区分与平衡采样**：创建两个经验回放缓冲区，分别存储**人类干预的转移数据**和**智能体自主行动的转移数据**。训练时从两个缓冲区**平衡采样**，避免了因干预数据可能较少而导致的数据不平衡和过拟合问题。
     - **策略正则化结合**：在RL目标（最大化价值）中**加入了模仿学习损失**作为正则项。这约束了策略不要过度偏离已观察到的人类行为分布，在积极利用数据（通过RL）和保持行为保守性（通过IL）之间取得了平衡。
     - **对未干预数据的TD学习利用**：与EIL等方法对未干预数据也做模仿学习不同，PVP4Real对这些数据采用**无奖励的TD学习**。这能更好地学习环境动力学和状态价值传播，有助于策略在长视野任务中做出更优决策。
   - **解决的问题/带来的优势**：
     - **提升了学习稳定性和样本效率**：平衡采样确保了各类数据对训练的均衡贡献；策略正则化防止了策略在探索中崩溃或产生过于激进的行为。
     - **更好地从成功与失败中学习**：算法明确区分了“人类认为好的动作”（演示）、“人类认为坏的动作”（被干预的智能体动作）和“人类未反对的动作”（可能可行），并分别用不同机制学习，对人类意图的利用更加充分和精细。
     - **减少了对人类干预的依赖**：相比之前的PVP等工作，本文方法在仿真实验中达到了相近或更好的性能，但**所需的人类干预次数更少**，进一步减轻了人类的负担。

### 总结
PVP4Real的核心创新在于**将无奖励在线RL与IL在理论框架和工程实现上深度融合**，并**首次在复杂、多模态的真实移动机器人任务上实现了高效、安全的“人在回路”学习**。它直接针对传统IL（需要大量数据、存在分布偏移）和RL（需要奖励函数、仿真到现实鸿沟、探索不安全）在现实部署中的痛点，提供了一种**数据效率极高、安全性好、且无需奖励设计**的实用化解决方案，为移动机器人的快速自适应学习开辟了新的途径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果分析

该论文通过**仿真实验**和**真实世界实验**，全面验证了所提出的 **PVP4Real** 方法在数据效率、安全性和泛化能力方面的优越性。

### 一、 仿真实验 (Simulation Experiments)

*   **任务与环境**：
    *   **任务**：在 **MetaDrive Safety Environment** 中驾驶车辆到达目的地。
    *   **环境**：包含随机障碍物（车辆、交通锥等）的驾驶场景。训练集和测试集各包含50个不同的场景，以评估泛化性能。
*   **评价指标**：
    *   **成功率 (Success Rate)**：成功到达目的地的回合比例。
    *   **碰撞率 (Crash Rate)**：因碰撞而终止的回合比例。
    *   **出界率 (Out Rate)**：驶出道路的回合比例。
    *   **专家干预次数**：训练过程中人类（模拟专家）需要干预的次数。
*   **基线方法对比**：
    *   **人机交互基线**：HACO, EIL, PVP。
    *   **强化学习基线**：PPO, TD3。
    *   **模仿学习基线**：行为克隆 (BC)。
*   **主要结果与结论**：
    *   **数据效率极高**：在仅 **40k** 次环境交互（远少于传统RL）后，PVP4Real 等人在环方法已能有效学习，而RL方法（PPO, TD3）几乎未学到有效行为。
    *   **性能优越**：PVP4Real 在任务完成度上**优于 EIL 和 HACO**。
    *   **减少人类负担**：与之前的 PVP 方法相比，PVP4Real 在达到相近性能的同时，**需要的人类专家干预更少**。
    *   **结论**：人机交互方法在样本效率上显著优于传统RL和BC方法，而PVP4Real在其中表现最佳。

### 二、 真实世界实验 (Real-world Experiments)

论文在两个具有挑战性的移动机器人任务上进行了验证，使用了两种不同的机器人平台。

*   **任务1：安全导航 (Safe Navigation)**
    *   **设置**：机器人在走廊环境中前进，需避开静态障碍物、动态行人，并在必要时紧急停止。
    *   **输入**：**原始RGB-D图像**（端到端策略）。
    *   **机器人**：Coco 四轮配送机器人。
    *   **评价指标**：四个子任务的**成功率**（各20次试验的平均值）：
        1.  直线前进
        2.  静态障碍物避让
        3.  动态障碍物避让
        4.  紧急停止
    *   **基线方法**：**行为克隆 (BC)**，使用与PVP4Real相同数量（~2000步）的人类演示数据进行训练。
    *   **主要结果 (Table I)**：
        *   PVP4Real 在所有子任务上均**大幅超越BC**。
        *   在最具挑战性的“动态障碍物避让”和“紧急停止”任务上，PVP4Real 分别取得了 **90%** 和 **100%** 的成功率，而BC仅为45%和60%。
        *   **关键结论**：在仅使用原始像素的端到端设置中，BC容易对无关特征（如光照）过拟合，且在遇到训练数据分布外的状态时容易“卡住”。PVP4Real 通过在线人机交互纠正，能更好地处理未见状态，实现更高的安全性和效率。

*   **任务2：人跟随 (Human Following)**
    *   **设置**：机器人跟随一个行人，保持一定距离。
    *   **输入**：由现成检测器和跟踪器提供的**行人2D边界框**（模块化感知）。
    *   **机器人**：Coco 配送机器人 和 **Unitree Go2 四足机器人**。
    *   **评价指标**：对不同人类行为（直线、左转曲线、右转曲线、急转弯、突然停止）的**跟随成功率**（各20次试验）。
    *   **基线方法**：**行为克隆 (BC)**。
    *   **主要结果 (Table II)**：
        *   **在Coco机器人上**：PVP4Real 在曲线跟随和突然停止等任务上表现优于BC。
        *   **在Unitree Go2机器人上**：结果更具说服力。PVP4Real 在**所有测试行为上均达到100%成功率**。而BC在训练中未出现过的“急转弯”行为上**完全失败（0%）**，因为一旦跟踪丢失（分布外状态），BC无法恢复。
        *   **关键结论**：PVP4Real 能够从干预中学习，并利用时序差分（TD）学习来**推断人类意图**（例如，从历史信息中推断行人方向），从而在遇到**未见过的、具有挑战性的状态**（如跟踪丢失）时，仍能鲁棒地执行任务。这证明了其强大的**泛化能力**。

### 三、 核心效果总结

1.  **极高的数据与时间效率**：在两个真实机器人任务上，仅需 **~2000步交互**（约**7分钟人类监督时间**，总实验在**15分钟内完成**），即可从零开始训练出有效的策略。
2.  **无需奖励函数与预训练**：在整个训练过程中，**完全不需要设计奖励函数**，也无需任何模拟器预训练或先验知识，降低了部署门槛。
3.  **卓越的泛化与安全性**：方法能处理**原始视觉输入**，并能泛化到**训练中未见过的人类行为**（如急转弯），同时通过在线干预保障了训练过程的安全。
4.  **多平台、多任务通用性**：成功在**轮式**和**足式**两种不同动力学特性的机器人上，验证了方法在**端到端视觉导航**和**模块化感知跟随**两种不同范式任务中的有效性。

**最终结论**：PVP4Real 通过巧妙结合模仿学习（IL）和强化学习（RL）框架，并充分利用在线人机交互数据，实现了**快速、安全、高效**的移动机器人策略学习，为解决现实世界机器人学习的数据效率与安全问题提供了极具前景的方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2503.04969v1)
- [HTML 版本](https://arxiv.org/html/2503.04969v1)
