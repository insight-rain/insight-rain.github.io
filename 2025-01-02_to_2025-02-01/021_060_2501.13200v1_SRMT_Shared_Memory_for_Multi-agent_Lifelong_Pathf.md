# SRMT: Shared Memory for Multi-agent Lifelong Pathfinding

**相关性评分**: 6.0/10

**排名**: #21


---


## 基本信息

- **arXiv ID**: [2501.13200v1](https://arxiv.org/abs/2501.13200v1)
- **发布时间**: 2025-01-22T20:08:53Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Alsu Sagirova, Yuri Kuratov, Mikhail Burtsev

## 关键词

reinforcement learning (RL), multi-agent RL, CTDE methods, local observational information

## 一句话总结

该论文提出了一种基于共享循环记忆变换器的多智能体强化学习方法，用于解决部分可观测环境下的路径规划问题，与强化学习、多智能体强化学习、CTDE方法和局部观测信息相关，但与机器人控制、四足/人形机器人、MuJoCo和边缘部署等关键词无关。

## 摘要

Multi-agent reinforcement learning (MARL) demonstrates significant progress in solving cooperative and competitive multi-agent problems in various environments. One of the principal challenges in MARL is the need for explicit prediction of the agents' behavior to achieve cooperation. To resolve this issue, we propose the Shared Recurrent Memory Transformer (SRMT) which extends memory transformers to multi-agent settings by pooling and globally broadcasting individual working memories, enabling agents to exchange information implicitly and coordinate their actions. We evaluate SRMT on the Partially Observable Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that requires agents to pass through a narrow corridor and on a POGEMA benchmark set of tasks. In the Bottleneck task, SRMT consistently outperforms a variety of reinforcement learning baselines, especially under sparse rewards, and generalizes effectively to longer corridors than those seen during training. On POGEMA maps, including Mazes, Random, and MovingAI, SRMT is competitive with recent MARL, hybrid, and planning-based algorithms. These results suggest that incorporating shared recurrent memory into the transformer-based architectures can enhance coordination in decentralized multi-agent systems. The source code for training and evaluation is available on GitHub: https://github.com/Aloriosa/srmt.

## 详细分析

## 论文摘要：SRMT：用于多智能体终身路径规划的共享记忆

**1. 研究背景和动机**
多智能体强化学习（MARL）在解决协作与竞争问题方面取得了显著进展，但如何实现智能体间的有效协调仍是一大挑战。特别是在部分可观测多智能体路径规划（PO-MAPF）问题中，智能体仅能获取局部观测，传统的显式通信或集中式规划方法存在易陷入死锁、泛化能力差等局限。受大脑“全局工作空间理论”启发，本研究旨在探索一种通过**隐式信息交换**来提升去中心化多智能体系统协调能力的新方法。

**2. 核心方法和技术创新**
本文提出了**共享循环记忆变换器（SRMT）**。其核心创新在于将记忆变换器扩展至多智能体场景：
- **共享记忆机制**：每个智能体维护一个个人循环记忆向量。在每个时间步，所有智能体的记忆被汇集并**全局广播**，形成一个共享记忆空间。
- **基于注意力的信息融合**：智能体通过自注意力处理自身历史观测和当前状态，再通过交叉注意力机制与共享记忆进行交互。这使得每个智能体在决策时能隐式地融入全局上下文信息，而无需显式通信。
- **完全去中心化**：SRMT在训练和执行阶段均保持去中心化，智能体共享策略但独立决策，更适用于许多现实场景。

**3. 主要实验结果**
研究在POGEMA基准的多个环境中进行了验证：
- **瓶颈导航任务**：在需要两个智能体协调通过狭窄走廊的任务中，SRMT在稀疏奖励等挑战性设置下，性能显著优于MAMBA、QPLEX等多种MARL基线及记忆模型（如ATM、RATE）。其学习到的策略能有效泛化到比训练时长**长数十倍**的走廊。
- **终身MAPF任务**：在迷宫、随机地图等复杂环境中，SRMT与最新的MARL、混合及基于规划的方法（如Follower、RHCR）相比具有竞争力。在拥堵的仓库环境中，结合启发式规划的SRMT变体取得了最佳吞吐量。
- **关键指标**：SRMT在路径寻优、可扩展性、分布外泛化等关键指标上表现优异，尤其在**可扩展性**方面优势明显。

**4. 研究意义和价值**
本研究证明了在基于变换器的架构中引入**共享记忆**，能有效增强去中心化多智能体系统的隐式协调与决策能力。SRMT提供了一种不依赖显式通信协议或集中控制器的、可扩展的协作新范式。其在稀疏奖励下的鲁棒性、对长序列任务的泛化能力，以及在实际复杂地图上的竞争性表现，为解决现实世界中大规模、部分可观测的多智能体协同问题（如机器人集群、自动驾驶）提供了有前景的技术方向。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
论文旨在解决**去中心化、部分可观测的多智能体路径规划（PO-MAPF）** 中的核心挑战：**智能体间如何在不依赖显式通信协议的情况下，实现有效的隐式协调与合作**，以避免死锁并高效完成目标。

### **核心创新点**
论文提出了 **共享循环记忆变换器（SRMT）** 架构。其核心创新在于将**全局共享记忆工作区**的概念引入基于Transformer的多智能体强化学习框架，以实现隐式、高效的智能体间协调。

具体技术创新包括：
- **共享记忆池与全局广播机制**：每个智能体维护一个个人循环记忆向量。在每个时间步，所有智能体的记忆向量被汇集到一个**全局共享记忆序列**中。
- **基于交叉注意力的信息交换**：智能体通过一个**交叉注意力层**，使其当前的隐藏状态与全局共享记忆序列进行交互。这使得每个智能体在决策时都能融入**全局环境上下文和其他智能体的意图信息**，而无需发送显式消息。
- **完全去中心化的训练与执行**：与许多需要集中式训练（如QMIX）或显式通信（如MAMBA）的MARL方法不同，SRMT在**训练和执行阶段都保持去中心化**。智能体共享的是记忆的“读取”权限，而非决策权。
- **对现有记忆变换器的扩展**：将原本为单智能体长序列处理设计的**循环记忆变换器（RMT）** 扩展至多智能体场景，通过共享机制实现了记忆从“私有”到“共享”的关键转变。

### **解决方案与实现**
SRMT通过以下方式解决协调问题：

1.  **架构设计**：
    ```mermaid
    智能体观测 → 空间编码器（ResNet） → SRMT核心模块 → 行动解码器/评价器
                    ↑                           ↑
                历史观测序列               全局共享记忆池
                    ↑                           ↑
                个人记忆向量 ← 记忆更新头 ← 交叉注意力
    ```
    - **输入**：智能体的局部观测（包含障碍物、其他智能体、目标）。
    - **SRMT核心**：处理个人记忆、历史观测和当前观测，并通过交叉注意力与共享记忆交互。
    - **输出**：更新后的个人记忆（用于下一时间步）和当前行动策略。

2.  **隐式协调机制**：共享记忆充当了一个**全局工作区**（灵感来源于全局工作区理论）。智能体通过向其中写入自己的记忆状态，并读取他人的状态，来感知整体局势、预测他人行为，从而自发地协调行动（如在狭窄走廊中交替通行）。

3.  **实验验证**：
    - **瓶颈任务**：在稀疏奖励、长走廊等挑战性设置下，SRMT显著优于无通信（IQL, VDN）、显式通信（MAMBA, QPLEX）及非共享记忆（ATM, RATE）的基线模型，并展现出对**远超训练长度的走廊的卓越泛化能力**（长达1000格）。
    - **终身MAPF基准（POGEMA）**：在迷宫、随机地图、MovingAI等多种复杂环境中，SRMT与最新的MARL、混合方法和规划算法竞争，表现优异。在拥堵的仓库环境中，结合启发式规划（SRMT-FlwrPlan）后性能甚至超过部分集中式规划器（如RHCR）。

### **实际价值**
- **为去中心化多智能体系统提供了一种可扩展的协调范式**：无需设计复杂的通信协议，降低了系统复杂性和通信开销。
- **提升了在稀疏奖励和部分可观测环境下的学习效率与鲁棒性**：共享记忆提供了更丰富、更全局的学习信号。
- **具有良好的泛化性和可扩展性**：模型能够泛化到未见过的、规模更大的地图和更多智能体的场景，这对机器人集群、自动驾驶车队等实际应用至关重要。
- **开源实现**：提供了完整的代码，便于研究和应用社区复现与拓展。

**总结**：SRMT通过创新性地将**共享循环记忆**与**Transformer架构**结合，为去中心化多智能体协调问题提供了一个强大、优雅且高效的解决方案，其核心在于利用**全局共享的隐式信息空间**来替代复杂且脆弱的显式通信。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**去中心化多智能体路径规划（PO-MAPF）中智能体间协调困难、易陷入死锁**的核心问题，提出了一种名为**共享循环记忆变换器（SRMT）** 的新框架。该方法的核心创新在于为每个智能体引入可循环更新的个人记忆向量，并通过一个**全局共享的“工作空间”** 对这些记忆进行池化和广播，使智能体能够通过跨注意力机制隐式地交换信息，从而实现无需显式通信的协调。实验表明，SRMT在需要紧密协作的“瓶颈”导航任务中，尤其是在**稀疏奖励**的困难场景下，显著优于多种强化学习基线方法，并能有效**泛化**到比训练时更长的走廊环境。在更复杂的终身路径规划（LMAPF）基准测试中，SRMT与最新的基于学习、规划和混合方法的主流算法相比，展现出**有竞争力的性能**，证明了共享记忆机制在提升去中心化多智能体系统协调能力方面的有效性和潜力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《SRMT: Shared Memory for Multi-agent Lifelong Pathfinding》的创新点分析

这篇论文针对多智能体强化学习（MARL）中的协调难题，提出了**共享循环记忆变换器（SRMT）**。其核心创新在于将“全局工作空间”理论与变换器架构结合，为去中心化多智能体系统设计了一种隐式、高效的协调机制。以下是其相对于已有工作的明确创新点：

### 1. **架构创新：引入全局共享的循环记忆空间**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：现有的多智能体记忆方法（如ATM、RATE、RRNN）主要关注为每个智能体维护**独立的、私有的记忆状态**。通信方法（如MAMBA、DCC）则需要智能体之间进行**显式的、直接的**信息交换（如通过离散通信协议或选择性信息共享）。
     - **SRMT的创新**：SRMT在变换器架构中引入了一个**全局共享的记忆序列**。每个智能体在每一步都将自己的个人记忆向量写入这个共享池，并通过**交叉注意力（cross-attention）** 机制读取所有其他智能体的记忆。这创造了一个**隐式的、持续的全局信息广播通道**。
   - **解决的具体问题/带来的优势**：
     - **解决隐式协调问题**：无需设计复杂的显式通信协议，智能体通过共享记忆间接感知全局状态，自然学会了协调行动（如在狭窄走廊中互相避让、排队通过）。
     - **提升稀疏奖励下的学习效率**：在奖励信号极少（如仅在到达目标时获得奖励）的困难场景下，共享记忆成为关键的信息来源，帮助智能体理解长期任务和同伴意图，从而显著提升了学习效果和成功率（如图3中Sparse奖励下的卓越表现）。
     - **保持完全去中心化**：与VDN、QMIX等需要集中式训练（CTDE）的方法不同，SRMT在**训练和执行阶段都是完全去中心化的**，更适用于对通信带宽或中心节点有限制的现实应用。

### 2. **方法创新：基于变换器的记忆池化与广播机制**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：经典的记忆变换器（如RMT）是为处理长序列而设计，其记忆在**单个智能体内部**跨时间步循环。多智能体通信变换器（如SCRIMP）可能使用变换器处理智能体间消息，但未必与循环记忆深度整合。
     - **SRMT的创新**：SRMT将RMT的循环记忆概念**扩展至多智能体领域**。其关键操作是“池化（pooling）”和“广播（broadcasting）”：在每一步，将所有智能体的个人记忆向量聚合成一个有序的全局序列，供所有智能体通过交叉注意力查询。个人记忆则通过自注意力和与共享记忆的交互进行循环更新。
   - **解决的具体问题/带来的优势**：
     - **实现长期依赖与全局上下文感知**：循环记忆使智能体能记住过去的交互历史，而共享广播机制使每个智能体都能获得**全局的、实时的上下文信息**。这有效解决了部分可观测环境下的“视野局限”问题。
     - **增强泛化与可扩展性**：该机制使学习到的策略具有强大的泛化能力。例如，在瓶颈任务中，模型在训练时只见过长度3-30的走廊，却能成功泛化到**长达1000个单元的未见过的长走廊**（如图4所示）。这表明模型学会了通用的“轮流通过”协调原则，而非记忆特定地图。

### 3. **训练与性能创新：在稀疏奖励和复杂终身MAPF任务中表现卓越**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：许多MAPF的RL方法（如PRIMAL）依赖于精心设计的**人工奖励函数（reward shaping）** 或外部演示来引导学习。在终身MAPF（LMAPF）中，基于搜索的规划器（如RHCR）有理论保证但可能缺乏灵活性，而纯RL方法在复杂拥堵场景中可能表现不佳。
     - **SRMT的创新**：
       1. **在极端稀疏奖励下有效学习**：论文验证了SRMT在“仅目标奖励”（Sparse）和“移动负奖励”（Moving Negative）这类极难探索的奖励设置下，性能显著优于所有基线（MAMBA, QPLEX, ATM等）。
       2. **与启发式规划有效结合**：在终身MAPF实验中，SRMT与**Follower的启发式路径搜索**相结合（SRMT-FlwrPlan）。这种“学习+规划”的混合方法，在高度拥堵的Warehouse地图上，其吞吐量甚至超过了部分集中式规划方法（如RHCR）（如图5所示）。
   - **解决的具体问题/带来的优势**：
     - **降低对奖励工程的需求**：SRMT强大的表示和协调能力使其对奖励函数的设计不那么敏感，降低了应用门槛。
     - **提升复杂动态环境的实战性能**：在终身MAPF的POGEMA基准测试中，SRMT在**泛化性（Out-of-Distribution）、可扩展性（Scalability）和寻路能力（Pathfinding）** 等多个关键指标上都具有竞争力（如图6所示）。这表明它不仅能解决简单的协调问题，还能适应大规模、动态、未知的复杂环境。

### 4. **设计细节创新：记忆的适时生成初始化**
   - **相比以往方法的改进/不同之处**：
     - **以往方法**：论文指出，RRNN、RATE、ATM等方法通常使用**固定值（如单位向量）或随机分布**来初始化智能体的记忆状态。
     - **SRMT的创新**：SRMT的智能体记忆状态是在**每个回合的第一步，根据其初始观察动态生成的**。
   - **解决的具体问题/带来的优势**：
     - **提供与当前情境相关的记忆起点**：动态初始化使记忆从一开始就包含了与当前任务（如起点、目标位置）相关的信息，为后续的协调提供了更好的基础。论文通过实验（图4中的RATE_gen）证明，这一改进能**显著提升基线方法的性能**，凸显了其重要性。

### 总结
SRMT的核心创新在于**将全局共享记忆作为一个隐式的协调基础设施，并成功地将其嵌入到基于变换器的循环架构中**。它主要解决了去中心化多智能体系统中**因局部观测和缺乏通信导致的协调效率低下、稀疏奖励难以学习、以及泛化能力弱**等问题。其优势体现在**更强的协调能力、优异的稀疏奖励鲁棒性、出色的泛化与可扩展性，以及完全去中心化的部署便利性**，为复杂的多智能体路径规划问题提供了一个强大且实用的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 使用的数据集与任务
1.  **Bottleneck Navigation Task (玩具任务)**：
    *   **环境**：两个房间通过一个单格宽度的狭窄走廊连接。
    *   **目标**：两个智能体从各自房间出发，需协调通过走廊到达对面的目标点。
    *   **训练**：在走廊长度3-30格的16张地图上进行。
    *   **泛化测试**：在走廊长度5-1000格（远超训练范围）的地图上评估。

2.  **POGEMA 基准测试集**：
    *   包含多种复杂地图类型，用于评估**终身多智能体路径规划 (LMAPF)** 性能。
    *   **地图类型**：迷宫 (`Mazes`)、随机障碍 (`Random`)、城市地图 (`MovingAI`)、拼图 (`Puzzle`)、仓库 (`Warehouse`)。
    *   **训练**：主要在 `65x65` 的迷宫地图上进行。
    *   **评估**：在所有类型地图（包括未见过的地图）上测试泛化能力。

### 二、 使用的评价指标
1.  **Bottleneck 任务**：
    *   **合作成功率 (Cooperative Success Rate, CSR)**：所有智能体在 episode 结束前到达目标的比例。
    *   **个体成功率 (Individual Success Rate, ISR)**：成功到达目标的智能体比例。
    *   **总成本 (Sum-of-Costs, SoC)**：所有智能体到达目标所需的总时间步数（越低越好）。

2.  **LMAPF 任务 (POGEMA 基准)**：
    *   **核心指标**：**平均吞吐量 (Average Throughput)**，即每个时间步所有智能体平均完成的目标数。
    *   **高级综合指标**（用于基准比较）：
        *   **性能 (Performance)**：在 `Random` 和 `Mazes` 地图上的平均吞吐量相对于最佳方法的比例。
        *   **路径规划 (Pathfinding)**：在大型地图 (`256x256`) 上，单个智能体路径是否接近最优的二元指标。
        *   **拥堵处理 (Congestion)**：在 `Warehouse` 地图上，智能体观测范围内平均智能体密度与全局密度的比值（反映处理拥堵能力）。
        *   **合作能力 (Cooperation)**：在小型手工 `Puzzle` 地图上，解决复杂协作场景的能力（相对于经典规划器的吞吐量比例）。
        *   **分布外泛化 (Out-of-Distribution)**：在未见过的 `MovingAI` 地图切片上的 `Performance` 指标。
        *   **可扩展性 (Scalability)**：在 `Warehouse` 地图上，运行时随智能体数量增长的比率。

### 三、 对比的基线方法
论文与多类前沿方法进行了广泛对比：

1.  **通信类 MARL 方法**：
    *   `MAMBA`：基于模型的MARL，使用离散通信。
    *   `QPLEX`：使用联合价值函数分解的MARL方法。

2.  **个体记忆类方法**：
    *   `ATM`：为每个智能体维护独立工作记忆的Transformer。
    *   `RATE`：为决策Transformer加入记忆模块。
    *   `RRNN`：使用注意力更新记忆的关系循环神经网络。

3.  **SRMT 的消融变体 (Ablations)**：
    *   `RMT`：**无共享**的循环记忆Transformer（关键对比，凸显“共享”的价值）。
    *   `Attention`：无记忆的标准注意力模型。
    *   `Empty`：直接将空间编码器连接到行动解码器。
    *   `RNN/GRU`：基于RNN的观测处理器。

4.  **LMAPF 专用方法**：
    *   `Follower`：结合A*搜索和学习的无通信方法。
    *   `MATS-LP`：结合学习策略和蒙特卡洛树搜索的无通信方法。
    *   `RHCR`：**集中式**搜索规划器（作为性能上限参考）。

### 四、 关键性能提升与结论

#### 1. Bottleneck 任务：卓越的协调与泛化能力
*   **核心结论**：SRMT 在**稀疏奖励**和**移动负奖励**等极具挑战性的场景下，**显著且一致地优于所有基线**。
*   **性能表现**：
    *   在稀疏奖励下，SRMT 保持了高成功率，而其他方法（包括 RMT）性能大幅下降。
    *   在移动负奖励下，SRMT 和 RMT 表现优异，但 SRMT 通常更优，证明了共享记忆对协调的额外增益。
*   **泛化能力**：
    *   训练于走廊长度≤30的地图，但能有效泛化到长度**高达1000格**的走廊。
    *   在长达400格的走廊上，稀疏奖励下的 CSR 仍接近1.0，远超其他方法。
    *   在移动负奖励下，SRMT 在长达1000格的走廊上所有指标均保持**第一**。

#### 2. LMAPF 任务：强大的竞争性与泛化性
*   **核心结论**：SRMT 在复杂的终身路径规划任务中，与最先进的 MARL、混合和规划方法**具有竞争力**，且在泛化方面表现突出。
*   **性能表现**：
    *   在 `Mazes`, `Random`, `MovingAI` 地图上，SRMT 的**平均吞吐量均优于 MARL 基线 (MAMBA, QPLEX)**。
    *   在高度拥堵的 `Warehouse` 环境中，结合了 Follower 启发式路径搜索的 `SRMT-FlwrPlan` 变体，其吞吐量**超过了 MAMBA, MATS-LP, QPLEX 甚至集中式规划器 RHCR**。
    *   在**可扩展性 (Scalability)** 和**路径规划 (Pathfinding)** 指标上，SRMT 及其变体表现优异。
*   **高级指标分析**：
    *   **合作与分布外泛化**：虽然集中式规划器 RHCR 在这些指标上领先（接近100%），但 SRMT 作为**完全去中心化**的方法，表现出了**强大的泛化能力**和**合理的协作水平**，优于其他去中心化方法（除 MATS-LP 外）。
    *   **拥堵处理**：MAMBA 略优于基础 SRMT，但 `SRMT-FlwrPlan` 在拥堵管理上表现最佳，表明结合启发式规划能有效提升在拥堵环境中的性能。

### 总结
论文通过系统的实验给出了**明确的定量结果**，证明了 SRMT 的有效性：
1.  **技术创新价值**：通过**共享循环记忆**机制，SRMT 实现了智能体间隐式、高效的信息交换，解决了去中心化系统中协调难的问题。
2.  **实际性能**：在需要紧密协调的 Bottleneck 任务中，尤其在奖励稀疏的困难设定下，SRMT **显著领先**；在复杂的终身路径规划基准测试中，SRMT **具备与最先进方法竞争的实力**，并在泛化到未知环境和处理拥堵方面展现出优势。
3.  **关键对比**：与不共享记忆的 `RMT` 的对比，直接证明了**共享记忆**（而非仅仅是记忆）对于提升多智能体协调能力的核心作用。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2501.13200v1)
- [HTML 版本](https://arxiv.org/html/2501.13200v1)
