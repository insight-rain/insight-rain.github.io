# Centralized Cooperative Exploration Policy for Continuous Control Tasks

**相关性评分**: 6.0/10

**排名**: #16


---


## 基本信息

- **arXiv ID**: [2301.02375v1](https://arxiv.org/abs/2301.02375v1)
- **发布时间**: 2023-01-06T04:45:50Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Chao Li, Chen Gong, Qiang He, Xinwen Hou, Yu Liu

## 关键词

reinforcement learning (RL), continuous control tasks, exploration policy, centralized policy framework, multi-agent RL, value functions

## 一句话总结

该论文提出了一种名为CCEP的集中式合作探索策略，通过低估和高估价值函数来增强连续控制任务中的探索能力，并利用集中式框架促进多策略间的信息传递。

## 摘要

The deep reinforcement learning (DRL) algorithm works brilliantly on solving various complex control tasks. This phenomenal success can be partly attributed to DRL encouraging intelligent agents to sufficiently explore the environment and collect diverse experiences during the agent training process. Therefore, exploration plays a significant role in accessing an optimal policy for DRL. Despite recent works making great progress in continuous control tasks, exploration in these tasks has remained insufficiently investigated. To explicitly encourage exploration in continuous control tasks, we propose CCEP (Centralized Cooperative Exploration Policy), which utilizes underestimation and overestimation of value functions to maintain the capacity of exploration. CCEP first keeps two value functions initialized with different parameters, and generates diverse policies with multiple exploration styles from a pair of value functions. In addition, a centralized policy framework ensures that CCEP achieves message delivery between multiple policies, furthermore contributing to exploring the environment cooperatively. Extensive experimental results demonstrate that CCEP achieves higher exploration capacity. Empirical analysis shows diverse exploration styles in the learned policies by CCEP, reaping benefits in more exploration regions. And this exploration capacity of CCEP ensures it outperforms the current state-of-the-art methods across multiple continuous control tasks shown in experiments.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.02375v1)
- [HTML 版本](https://arxiv.org/html/2301.02375v1)
