# Train Hard, Fight Easy: Robust Meta Reinforcement Learning

**相关性评分**: 6.0/10

**排名**: #13


---


## 基本信息

- **arXiv ID**: [2301.11147v2](https://arxiv.org/abs/2301.11147v2)
- **发布时间**: 2023-01-26T14:54:39Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Ido Greenberg, Shie Mannor, Gal Chechik, Eli Meirom

## 关键词

reinforcement learning (RL), meta reinforcement learning (MRL), robustness, navigation, continuous control

## 一句话总结

这篇论文提出了一种鲁棒元强化学习算法（RoML），通过过采样困难任务来优化元策略，以应对环境变化和任务难度差异，提升系统可靠性。

## 摘要

A major challenge of reinforcement learning (RL) in real-world applications is the variation between environments, tasks or clients. Meta-RL (MRL) addresses this issue by learning a meta-policy that adapts to new tasks. Standard MRL methods optimize the average return over tasks, but often suffer from poor results in tasks of high risk or difficulty. This limits system reliability since test tasks are not known in advance. In this work, we define a robust MRL objective with a controlled robustness level. Optimization of analogous robust objectives in RL is known to lead to both *biased gradients* and *data inefficiency*. We prove that the gradient bias disappears in our proposed MRL framework. The data inefficiency is addressed via the novel Robust Meta RL algorithm (RoML). RoML is a meta-algorithm that generates a robust version of any given MRL algorithm, by identifying and over-sampling harder tasks throughout training. We demonstrate that RoML achieves robust returns on multiple navigation and continuous control benchmarks.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.11147v2)
- [HTML 版本](https://arxiv.org/html/2301.11147v2)
