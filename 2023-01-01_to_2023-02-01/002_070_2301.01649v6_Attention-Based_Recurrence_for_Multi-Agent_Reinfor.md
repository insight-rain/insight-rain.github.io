# Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability

**相关性评分**: 7.0/10

**排名**: #2


---


## 基本信息

- **arXiv ID**: [2301.01649v6](https://arxiv.org/abs/2301.01649v6)
- **发布时间**: 2023-01-04T14:48:25Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Thomy Phan, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Nüßlein, Michael Kölle, Thomas Gabor, Claudia Linnhoff-Popien

## 关键词

multi-agent RL, CTDE methods, reinforcement learning (RL)

## 一句话总结

该论文提出了一种基于注意力的多智能体强化学习方法，用于处理随机部分可观测性，并在改进的SMAC基准上进行了评估。

## 摘要

Stochastic partial observability poses a major challenge for decentralized coordination in multi-agent reinforcement learning but is largely neglected in state-of-the-art research due to a strong focus on state-based centralized training for decentralized execution (CTDE) and benchmarks that lack sufficient stochasticity like StarCraft Multi-Agent Challenge (SMAC). In this paper, we propose Attention-based Embeddings of Recurrence In multi-Agent Learning (AERIAL) to approximate value functions under stochastic partial observability. AERIAL replaces the true state with a learned representation of multi-agent recurrence, considering more accurate information about decentralized agent decisions than state-based CTDE. We then introduce MessySMAC, a modified version of SMAC with stochastic observations and higher variance in initial states, to provide a more general and configurable benchmark regarding stochastic partial observability. We evaluate AERIAL in Dec-Tiger as well as in a variety of SMAC and MessySMAC maps, and compare the results with state-based CTDE. Furthermore, we evaluate the robustness of AERIAL and state-based CTDE against various stochasticity configurations in MessySMAC.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.01649v6)
- [HTML 版本](https://arxiv.org/html/2301.01649v6)
