# Distributed Control of Partial Differential Equations Using Convolutional Reinforcement Learning

**相关性评分**: 6.0/10

**排名**: #14


---


## 基本信息

- **arXiv ID**: [2301.10737v2](https://arxiv.org/abs/2301.10737v2)
- **发布时间**: 2023-01-25T17:55:30Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Sebastian Peitz, Jan Stenner, Vikas Chidananda, Oliver Wallscheid, Steven L. Brunton, Kunihiko Taira

## 关键词

reinforcement learning (RL), multi-agent RL, distributed control, computational effort reduction, PDEs

## 一句话总结

该论文提出了一种基于卷积强化学习的分布式控制框架，用于降低偏微分方程系统控制的复杂性，但未直接涉及机器人控制或特定机器人类型。

## 摘要

We present a convolutional framework which significantly reduces the complexity and thus, the computational effort for distributed reinforcement learning control of dynamical systems governed by partial differential equations (PDEs). Exploiting translational invariances, the high-dimensional distributed control problem can be transformed into a multi-agent control problem with many identical, uncoupled agents. Furthermore, using the fact that information is transported with finite velocity in many cases, the dimension of the agents' environment can be drastically reduced using a convolution operation over the state space of the PDE. In this setting, the complexity can be flexibly adjusted via the kernel width or by using a stride greater than one. Moreover, scaling from smaller to larger systems -- or the transfer between different domains -- becomes a straightforward task requiring little effort. We demonstrate the performance of the proposed framework using several PDE examples with increasing complexity, where stabilization is achieved by training a low-dimensional deep deterministic policy gradient agent using minimal computing resources.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.10737v2)
- [HTML 版本](https://arxiv.org/html/2301.10737v2)
