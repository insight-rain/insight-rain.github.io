# Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning

**相关性评分**: 6.0/10

**排名**: #17


---


## 基本信息

- **arXiv ID**: [2301.12593v2](https://arxiv.org/abs/2301.12593v2)
- **发布时间**: 2023-01-30T00:37:06Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

James Queeney, Mouhacine Benosman

## 关键词

reinforcement learning (RL), safe decision making, uncertain environments, distributionally robust, model-free implementation, continuous control tasks, safety constraints

## 一句话总结

这篇论文提出了一种基于风险规避模型不确定性的分布鲁棒安全强化学习框架，用于在不确定环境中实现安全决策，并通过实验验证了其在连续控制任务中的鲁棒性能。

## 摘要

Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test environments.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.12593v2)
- [HTML 版本](https://arxiv.org/html/2301.12593v2)
