# Opponent-aware Role-based Learning in Team Competitive Markov Games

**相关性评分**: 6.0/10

**排名**: #9


---


## 基本信息

- **arXiv ID**: [2301.05873v1](https://arxiv.org/abs/2301.05873v1)
- **发布时间**: 2023-01-14T09:50:48Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Paramita Koley, Aurghya Maiti, Niloy Ganguly, Sourangshu Bhattacharya

## 关键词

multi-agent RL, CTDE methods, reinforcement learning (RL)

## 一句话总结

该论文提出了一种基于角色编码和对手角色预测的多智能体强化学习方法，用于团队竞争场景中的动态角色学习。

## 摘要

Team competition in multi-agent Markov games is an increasingly important setting for multi-agent reinforcement learning, due to its general applicability in modeling many real-life situations. Multi-agent actor-critic methods are the most suitable class of techniques for learning optimal policies in the team competition setting, due to their flexibility in learning agent-specific critic functions, which can also learn from other agents. In many real-world team competitive scenarios, the roles of the agents naturally emerge, in order to aid in coordination and collaboration within members of the teams. However, existing methods for learning emergent roles rely heavily on the Q-learning setup which does not allow learning of agent-specific Q-functions. In this paper, we propose RAC, a novel technique for learning the emergent roles of agents within a team that are diverse and dynamic. In the proposed method, agents also benefit from predicting the roles of the agents in the opponent team. RAC uses the actor-critic framework with role encoder and opponent role predictors for learning an optimal policy. Experimentation using 2 games demonstrates that the policies learned by RAC achieve higher rewards than those learned using state-of-the-art baselines. Moreover, experiments suggest that the agents in a team learn diverse and opponent-aware policies.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.05873v1)
- [HTML 版本](https://arxiv.org/html/2301.05873v1)
