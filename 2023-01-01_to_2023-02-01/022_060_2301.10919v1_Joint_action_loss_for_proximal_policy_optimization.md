# Joint action loss for proximal policy optimization

**相关性评分**: 6.0/10

**排名**: #22


---


## 基本信息

- **arXiv ID**: [2301.10919v1](https://arxiv.org/abs/2301.10919v1)
- **发布时间**: 2023-01-26T03:42:29Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Xiulei Song, Yizhao Jin, Greg Slabaugh, Simon Lucas

## 关键词

reinforcement learning (RL), PPO, compound actions, sample efficiency, MuJoCo, Gym-μRTS

## 一句话总结

这篇论文提出了一种针对PPO算法的联合动作损失方法，通过改进子动作的概率处理来提高样本利用效率，并在强化学习环境中进行了实验验证。

## 摘要

PPO (Proximal Policy Optimization) is a state-of-the-art policy gradient algorithm that has been successfully applied to complex computer games such as Dota 2 and Honor of Kings. In these environments, an agent makes compound actions consisting of multiple sub-actions. PPO uses clipping to restrict policy updates. Although clipping is simple and effective, it is not efficient in its sample use. For compound actions, most PPO implementations consider the joint probability (density) of sub-actions, which means that if the ratio of a sample (state compound-action pair) exceeds the range, the gradient the sample produces is zero. Instead, for each sub-action we calculate the loss separately, which is less prone to clipping during updates thereby making better use of samples. Further, we propose a multi-action mixed loss that combines joint and separate probabilities. We perform experiments in Gym-$μ$RTS and MuJoCo. Our hybrid model improves performance by more than 50\% in different MuJoCo environments compared to OpenAI's PPO benchmark results. And in Gym-$μ$RTS, we find the sub-action loss outperforms the standard PPO approach, especially when the clip range is large. Our findings suggest this method can better balance the use-efficiency and quality of samples.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.10919v1)
- [HTML 版本](https://arxiv.org/html/2301.10919v1)
