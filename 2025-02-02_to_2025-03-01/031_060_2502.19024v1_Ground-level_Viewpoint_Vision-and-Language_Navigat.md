# Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments

**相关性评分**: 6.0/10

**排名**: #31


---


## 基本信息

- **arXiv ID**: [2502.19024v1](https://arxiv.org/abs/2502.19024v1)
- **发布时间**: 2025-02-26T10:30:40Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Zerui Li, Gengze Zhou, Haodong Hong, Yanyan Shao, Wenqi Lyu, Yanyuan Qiao, Qi Wu

## 关键词

quadruped robot locomotion, Edge Deployment

## 一句话总结

该论文提出一种地面视角视觉语言导航方法，针对四足机器人在低视角下的视觉障碍和感知不匹配问题，通过加权历史观测和空间先验增强，提升在模拟和真实环境中的导航性能。

## 摘要

Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, generalization remains a persistent challenge, particularly when dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a low-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Ground-level Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots.

## 详细分析

## 论文摘要：Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments

**1. 研究背景和动机**
视觉与语言导航（VLN）旨在让智能体根据自然语言指令在环境中导航。然而，现有研究大多基于模拟离散环境和高视点（如人类身高）的视觉观察，导致模型在部署到**低视点四足机器人**（如机器狗）时面临严重泛化挑战。主要问题包括：**视点高度不匹配**（人类指令基于全局视野，而机器人视野受限）、**单目视觉传感器限制**以及**复杂真实环境中路径点预测性能下降**。本文首次系统性地研究了VLN中因视觉观察高度差异导致的泛化鸿沟。

**2. 核心方法和技术创新**
本文提出了**地面视点导航（GVNav）** 方法，包含两大核心技术：
- **大规模路径点预测网络训练**：为解决低视点下路径点预测性能骤降问题，作者利用HM3D、Gibson等公开3D扫描数据集，构建了规模扩大**22倍**的低视点路径点预测训练数据集，显著提升了模型在复杂真实环境中的泛化能力。
- **多视角信息自适应聚合模块**：针对低视点导致的视觉遮挡和局部观察信息不足，设计了一个**可训练的Transformer编码器层**。该模块能够自适应地对历史不同视角的视觉特征进行加权融合，从而为当前决策提供更丰富、更准确的时空上下文信息，有效缓解了遮挡问题。

**3. 主要实验结果**
- **模拟环境**：在VLN-CE R2R基准测试中，GVNav在低视点条件下全面超越现有SOTA方法（如ETPNav），在未见过的验证集上，**成功率（SR）提升了3%-8%**，导航误差（NE）显著降低。
- **消融实验**：证明了**路径点预测器**的性能提升对整体导航效果的贡献远大于导航策略本身。使用大规模数据集重新训练路径点预测器，将“开放空间”预测准确率提升了约**8%**。
- **真实世界部署**：在小米Cyberdog机器狗上的实地测试表明，GVNav在游戏室、厨房、实验室和办公区四种复杂场景中均能有效导航，**成功率显著高于其他对比方法**，验证了其从模拟到现实的强大泛化能力。

**4. 研究意义和价值**
本研究首次明确并系统解决了VLN任务中因**视点高度差异**引发的“指令-感知”不匹配这一关键瓶颈。所提出的GVNav框架通过**数据增强**和**自适应信息融合**，有效弥合了模拟与真实部署、高视点与低视点之间的性能鸿沟。这项工作为将高性能VLN模型成功应用于各类形态（尤其是低矮形态）的实体机器人迈出了关键一步，对辅助机器人、自主导航等实际应用具有重要的推动价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **一、 研究问题**
论文旨在解决**视觉语言导航在真实机器人部署中，因视点高度差异导致的泛化难题**。具体问题包括：
1.  **视点高度不匹配**：现有VLN模型基于人类身高（~1.7米）的视觉数据进行训练，而许多实用机器人（如四足机器狗）的摄像头高度很低（~0.3米）。这导致**视觉信息不对称**——人类指令基于全局视野，而机器人只能看到局部、被遮挡的地面级视图。
2.  **传感器模式不匹配**：VLN研究通常假设智能体拥有全景视觉，但真实机器人多配备**单目RGBD相机**，视野受限。
3.  **路径点预测泛化能力不足**：在复杂真实环境中，尤其是在低视点下，现有的路径点预测模型性能显著下降。

### **二、 核心创新点**
论文提出了 **“地面级视点导航”** 框架，包含三项核心技术创新：

1.  **大规模路径点预测网络训练数据扩展**
    - **方法**：利用HM3D、Gibson等公开3D扫描数据集，在模拟器中以低视点（80厘米）渲染，构建了比原有数据量大**22倍**的路径点预测训练数据集。
    - **解决**：针对低视点下路径点预测性能骤降的问题，通过海量数据提升模型对地面级视角下空间结构和可行路径的理解与泛化能力。

2.  **多视图信息自适应聚合模块**
    - **方法**：引入一个**可训练的Transformer编码器层**，对智能体历史轨迹中的多个视点特征进行加权融合。该模块能学习为不同视点的相同特征分配合适的权重，从而**自适应地选择信息最丰富的视觉表征**。
    - **解决**：地面级视点下常见的**视觉遮挡问题**。即使当前位置（A点）视野被挡，模型也能利用之前未被遮挡的视点（B点）信息，做出更鲁棒的导航决策。

3.  **全景视觉输入的系统性重建与评估**
    - **方法**：对小米Cyberdog机器人进行硬件改造，通过可编程电机旋转单目RGBD相机，合成**360度全景视觉输入**，从而在真实部署中弥合了VLN模型对全景输入的需求与机器人单目传感器之间的差距。
    - **解决**：首次系统性地评估并量化了**视点高度差异**对VLN性能的影响，并提供了从模拟到真实、从高视点到低视点的完整解决方案。

### **三、 解决方案架构**
整体方案是一个**两阶段训练**的导航系统：
1.  **第一阶段（路径点预测预训练）**：使用扩展的大规模低视点数据集，训练路径点预测网络，使其能在地面视角下准确预测可行的导航路径点。
2.  **第二阶段（导航策略训练）**：在R2R等VLN任务数据集上，结合**拓扑地图**和**多视图信息聚合模块**，训练完整的导航策略。该策略能利用加权的历史观测作为时空上下文，跟随语言指令进行决策。

### **四、 实际价值与意义**
- **技术价值**：首次明确指出并系统解决了VLN从模拟到真实、从人到机器人的“**视点高度泛化鸿沟**”，推动了VLN技术走向实用化。
- **应用价值**：为**低矮服务机器人、四足机器人**等地面移动平台提供了可靠的视觉语言导航解决方案，使其能在复杂、动态的真实室内环境中理解人类指令并执行任务。
- **方法论贡献**：提出的**数据扩展**和**自适应信息聚合**方法具有通用性，可迁移至其他存在视觉域差异或遮挡问题的具身AI任务中。

**结论**：该论文的核心创新在于构建了一个针对地面级视点的VLN系统，通过**数据扩展、算法创新（多视图聚合）和系统重建**三位一体的方式，有效解决了因视点高度降低带来的视觉信息缺失、遮挡和泛化能力下降等关键挑战，在模拟和真实机器人实验中均取得了显著性能提升。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**视觉语言导航（VLN）在真实四足机器人部署中存在的视角高度不匹配问题**，提出了**地面视角导航（GVNav）方法**。核心问题是：基于人类高视角指令训练的VLN模型，在部署到低视角（如30cm高的机器狗）时，因视觉信息不对称（遮挡、地标识别差异）和单目传感器限制，导致导航性能严重下降。

论文的主要解决方案包含两个关键技术：1) **大规模路径点预测网络训练**：利用HM3D、Gibson等公开3D扫描数据集的连通图，构建了22倍于原数据量的低视角路径点预测数据集，显著提升了模型在复杂真实环境中的泛化能力。2) **多视角信息自适应聚合模块**：通过一个可训练的Transformer编码层，对历史观测中的视觉特征进行加权融合，使机器人能利用过去未遮挡视角的信息来弥补当前视角的局限，从而做出更鲁棒的导航决策。

实验结果表明，该方法在模拟环境和真实四足机器人（小米Cyberdog）部署中均取得了显著提升。在VLN-CE基准测试上，多项导航指标（如SR、SPL）超越现有最优方法；在四个真实场景的测试中，其成功率也明显高于其他对比方法，有效证明了其**在低视角条件下提升VLN模型泛化性和实际性能的有效性**。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments》针对视觉语言导航（VLN）在真实机器人部署中存在的泛化挑战，特别是**低视角四足机器人**的导航问题，提出了GVNav方法。其核心创新点如下：

---

### 1. **首次系统性地提出并量化了“视觉视角高度差异”问题**
- **相比以往方法的改进/不同之处：**
    - 以往VLN研究大多假设智能体（或仿真环境）的视觉观察视角与人类身高（约1.7米）相近，使用全景或单目RGBD输入，但**未考虑机器人（如机器狗）因低视角（如0.3-0.8米）导致的视觉信息差异**。
    - 本文首次将“**视点高度不匹配**”作为一个明确的、影响Sim-to-Real性能的关键问题提出，并通过实验量化了其影响（如表I所示，性能下降显著）。
- **解决的具体问题/带来的优势：**
    - **明确了问题根源**：揭示了在真实机器人部署中，由于指令基于人类高视角，而机器人执行基于低视角，导致**信息不对称**和**感知错配**。
    - **为后续研究指明了方向**：强调了为低视角机器人设计VLN系统的必要性，推动了该领域向更实际、更多样化机器人形态的拓展。

### 2. **提出并构建了大规模低视角路径点预测数据集**
- **相比以往方法的改进/不同之处：**
    - 以往路径点预测网络（Waypoint Predictor）的训练数据量有限（如基于R2R数据集），且渲染视角为人类高度。
    - 本文**显著扩展了训练数据**：利用HM3D、Gibson、MP3D数据集的连通图，在Habitat仿真器中以**80厘米的离地高度**渲染，生成了212,924个训练样本，数据量是之前工作的**22.02倍**（见表IV）。
- **解决的具体问题/带来的优势：**
    - **提升了路径点预测的泛化能力**：大规模、低视角的专门化数据训练，使路径点预测器能更好地适应真实、复杂环境中机器人的低视角观察。
    - **解决了低视角下路径点预测性能严重下降的问题**：实验表明（表II和IV），重新训练路径点预测器对性能提升的贡献远大于仅重新训练导航器，验证了该模块在低视角导航中的核心重要性。

### 3. **设计了多视角信息自适应聚合模块**
- **相比以往方法的改进/不同之处：**
    - 以往方法（如ETPNav）在构建拓扑地图时，对“幽灵节点”的视觉表征通常采用**历史观测特征的简单平均**。
    - 本文引入了一个**可训练的Transformer编码器层**，通过自注意力机制为来自不同历史视角的相同特征**动态分配权重**，自适应地选择信息最丰富的视觉表征（见图2和公式描述）。
- **解决的具体问题/带来的优势：**
    - **缓解了低视角下的遮挡问题**：低视角机器人更容易被家具、杂物等遮挡视线。该模块能利用历史轨迹中未被遮挡的视角信息，弥补当前视角的不足。
    - **增强了导航决策的鲁棒性**：使导航策略不仅能基于当前（可能受限）的观测，还能智能地融合过去更清晰的观测，从而做出更可靠的路径规划。

### 4. **提出了一个完整的、面向真实低视角机器人的VLN系统框架与验证**
- **相比以往方法的改进/不同之处：**
    - 以往VLN-CE研究多在仿真中验证，或仅将高视角模型简单迁移到真实机器人，未针对低视角进行系统性硬件改造和算法适配。
    - 本文以小米Cyberdog为平台，**重构了全景视觉输入系统**（使用可编程电机旋转RGBD相机），并**将整个GVNav流水线（包括路径点预测、多视角聚合、拓扑导航）部署到真实机器人进行端到端验证**。
- **解决的具体问题/带来的优势：**
    - **实现了从仿真到真实世界的闭环验证**：在四个不同的真实场景（游戏室、厨房、实验室、办公区）中测试，证明了方法的实用性和有效性（表III）。
    - **提供了一套可行的工程方案**：展示了如何通过硬件（旋转相机）和软件（GVNav算法）协同，解决低视角机器人获取全景视觉信息并执行语言指令导航的完整问题。

---

**总结**：本文的核心创新在于**从一个被忽视的实际约束（机器人低视角）出发**，系统地定义了问题，并通过**数据扩充（创新点2）**、**算法改进（创新点3）** 和**系统集成与验证（创新点4）** 三个层面给出了解决方案。其最大价值在于**将VLN研究推向更贴近真实机器人应用的场景**，为解决Sim-to-Real中的视角域差异提供了新的思路和基准。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过模拟环境和真实机器人部署，全面评估了所提出的**地面视角视觉语言导航（GVNav）方法**的效果。以下是详细的实验设置、对比方法和性能提升总结。

### 一、 实验数据集与评价指标

#### 1. **主要数据集**
- **训练与验证（模拟环境）**：
    - **R2R-CE**：在连续环境中的VLN基准数据集，用于导航策略的最终训练和评估。
    - **大规模路径点预测数据集**：为提升路径点预测器的泛化能力，构建了一个包含 **HM3D（800个场景）、Gibson（491个场景）、MP3D（61个场景）** 的大规模数据集，共生成 **212,924个训练样本**。渲染视角高度设置为**80厘米**以模拟地面机器人视角。
- **真实世界测试**：
    - 使用**小米CyberDog机器人**在四个真实室内场景进行测试：**游戏室、厨房、实验室、办公区**。每个场景提供25条独特的自然语言指令。

#### 2. **核心评价指标**
论文采用了VLN-CE任务的标准指标：
- **导航误差（NE, Navigation Error）**：最终位置与目标位置之间的欧氏距离（米）。**越低越好**。
- **成功率（SR, Success Rate）**：在终点3米范围内视为成功。**越高越好**。
- **路径长度加权成功率（SPL, Success weighted by Path Length）**：综合考虑成功率和路径效率。**越高越好**。
- **整体成功率（OSR, Overall Success Rate）**：不考虑最终距离，只要靠近目标即成功。**越高越好**。
- **归一化动态时间规整（nDTW）**：衡量路径与参考路径的相似度。**越高越好**。
- **轨迹长度（TL, Trajectory Length）**：实际行走的总路径长度。

### 二、 对比的基线方法
论文与多个当前先进的VLN-CE方法进行了对比：
1.  **CMA (mono)**：经典的连续环境VLN基线。
2.  **RecurrentBert**：使用递归BERT进行跨模态对齐的方法。
3.  **BEVBert**：基于鸟瞰图特征进行地图预训练的方法。
4.  **ETPNav**：当前在VLN-CE上的**State-of-the-Art (SoTA)** 方法，使用演化的拓扑图进行规划。

### 三、 关键性能提升与结论

#### 1. **模拟环境实验结果（R2R-CE）**
- **核心发现：视角高度差异导致巨大性能鸿沟**。
    - 将训练于**高视角（1.7米，人类视线）** 的模型直接用于**低视角（地面视线）** 测试时，所有基线模型性能均**大幅下降**。例如，ETPNav的SR从66%暴跌至22%（下降44个百分点），BEVBert的SR从71%降至28%（下降43个百分点）。这证实了**人类视角与机器人地面视角之间存在严重的领域差异**。
- **GVNav的性能优势**：
    - 如表I（M#13）所示，GVNav在**低视角测试**下，在`Val Seen`和`Val Unseen`分割上均**全面超越了所有重新训练后的基线模型**。
    - **关键指标提升示例（Val Unseen分割）**：
        - **vs 重新训练的ETPNav (M#12)**：SR从 **52%** 提升至 **55%**，SPL从 **43%** 提升至 **45%**。
        - **vs 重新训练的BEVBert (M#10)**：SR从 **47%** 提升至 **55%**，NE从 **5.61** 降低至 **4.89**。
    - **结论**：GVNav通过**大规模路径点预测数据增强**和**多视角信息自适应聚合**，有效缓解了地面视角带来的视觉信息缺失和遮挡问题，实现了更鲁棒的导航。

#### 2. **消融实验与组件分析**
- **路径点预测器 vs 导航器**（表II）：
    - **核心结论：在低视角条件下，提升路径点预测器的性能比优化导航策略更为关键**。
    - 仅重新训练**路径点预测器**（冻结导航器），能将ETPNav的SR从21%大幅提升至39%。
    - 仅重新训练**导航器**（冻结路径点预测器），SR仅从21%提升至32%。
    - **大规模数据增强的有效性**（表IV）：将路径点预测训练数据扩大**22.02倍**后，预测路径点位于开放空间的比率（%Open）从约82%显著提升至约**88-90%**，这直接提升了导航的安全性和可行性。

#### 3. **真实世界机器人部署结果**
- **实验设置**：在小米CyberDog（搭载旋转RGB-D相机以模拟全景输入）上于四个真实场景测试。
- **性能对比**（表III）：
    - **Seq2Seq和CMA(mono)方法完全失败**（SR=0），凸显了从模拟到真实、从全景到单目/地面视角的极端挑战。
    - **GVNav在四个场景中的三个取得了最高的SR和OSR**，尤其在相对开放的厨房和办公区表现优异。
    - 例如，在**厨房**场景，GVNav的SR达到**40%**，显著优于ETPNav的28%和BEVBert的16%。
- **结论**：GVNav是唯一一个在**所有测试的真实场景中均能成功完成导航任务**的方法，证明了其方案在解决**模拟到真实（Sim-to-Real）迁移**和**地面视角适配**问题上的有效性。

### 总结
论文通过系统的实验给出了**明确的定量结果**，并得出以下核心结论：
1.  **视角高度不匹配是VLN从模拟走向真实部署的关键瓶颈之一**，直接使用高视角数据训练的模型性能会严重退化。
2.  所提出的**GVNav框架**通过**大规模地面视角路径点预测数据增强**和**基于注意力的多视角历史信息聚合**，显著提升了模型在地面视角下的导航性能。
3.  在模拟环境（R2R-CE）和真实机器人（小米CyberDog）上的实验均表明，GVNav**超越了现有的先进方法**，为解决低视角四足机器人的视觉语言导航问题提供了有效的方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2502.19024v1)
- [HTML 版本](https://arxiv.org/html/2502.19024v1)
