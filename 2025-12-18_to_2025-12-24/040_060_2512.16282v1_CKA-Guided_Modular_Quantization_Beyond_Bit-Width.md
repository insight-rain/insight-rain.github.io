# CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity

**相关性评分**: 6.0/10

**排名**: #40


---


## 基本信息

- **arXiv ID**: [2512.16282v1](https://arxiv.org/abs/2512.16282v1)
- **发布时间**: 2025-12-18T08:01:19Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Jinhao Zhang, Yunquan Zhang, Daning Chen

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种无需微调的模块化量化框架，通过算法多样性优化大型语言模型的推理效率，适用于轻量级架构和边缘部署。

## 摘要

Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.16282v1)
- [HTML 版本](https://arxiv.org/html/2512.16282v1)
