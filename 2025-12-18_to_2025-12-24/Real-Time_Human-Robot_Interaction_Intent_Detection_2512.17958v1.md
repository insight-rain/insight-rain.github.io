# Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization

**相关性评分**: 7.0/10

**排名**: #14


---


## 基本信息

- **arXiv ID**: [2512.17958v1](https://arxiv.org/abs/2512.17958v1)
- **发布时间**: 2025-12-18T08:44:22Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Farida Mohsen, Ali Safa

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

该论文提出了一种基于RGB视频的实时人机交互意图检测框架，融合姿态和情感特征，在资源受限的嵌入式硬件上运行，具有跨相机和环境泛化能力，适用于边缘部署。

## 摘要

Service robots in public spaces require real-time understanding of human behavioral intentions for natural interaction. We present a practical multimodal framework for frame-accurate human-robot interaction intent detection that fuses camera-invariant 2D skeletal pose and facial emotion features extracted from monocular RGB video. Unlike prior methods requiring RGB-D sensors or GPU acceleration, our approach resource-constrained embedded hardware (Raspberry Pi 5, CPU-only). To address the severe class imbalance in natural human-robot interaction datasets, we introduce a novel approach to synthesize temporally coherent pose-emotion-label sequences for data re-balancing called MINT-RVAE (Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation). Comprehensive offline evaluations under cross-subject and cross-scene protocols demonstrate strong generalization performance, achieving frame- and sequence-level AUROC of 0.95. Crucially, we validate real-world generalization through cross-camera evaluation on the MIRA robot head, which employs a different onboard RGB sensor and operates in uncontrolled environments not represented in the training data. Despite this domain shift, the deployed system achieves 91% accuracy and 100% recall across 32 live interaction trials. The close correspondence between offline and deployed performance confirms the cross-sensor and cross-environment robustness of the proposed multimodal approach, highlighting its suitability for ubiquitous multimedia-enabled social robots.

## 详细分析

## 论文摘要

**论文标题**：基于RGB姿态与情感线索及跨相机模型泛化的实时人机交互意图检测

### 1. 研究背景和动机
在公共服务场景中，机器人需要实时理解人类的行为意图以实现自然交互。现有方法通常依赖昂贵的RGB-D传感器或GPU加速，限制了其在低成本、资源受限的嵌入式机器人平台上的部署。此外，真实人机交互数据存在严重的类别不平衡问题，且多数研究仅在受控实验室环境下评估，缺乏对跨传感器、跨环境泛化能力的验证。本研究旨在解决这些挑战，提出一种仅使用单目RGB视频、可在CPU上实时运行的实用化意图检测框架。

### 2. 核心方法和技术创新
- **多模态RGB感知框架**：提出一个可部署的系统，从单目RGB视频中提取**相机无关的2D人体姿态**（使用YOLOv8-Pose）和**面部情感特征**（使用DeepFace），并将二者融合。
- **轻量级时序建模**：采用GRU、LSTM和轻量级Transformer等时序模型，进行**帧级**的意图概率预测，实现高时间分辨率的早期意图检测。
- **创新的数据平衡方法**：针对交互意图数据稀缺的问题，提出了**MINT-RVAE**（用于意图序列生成的多模态循环变分自编码器）。该生成模型能合成具有时间一致性和跨模态对齐的“姿态-情感-标签”序列，有效缓解类别不平衡。
- **跨相机泛化设计**：通过**基于边界框的骨骼关键点归一化**处理，使姿态特征对相机视角和硬件不敏感，实现了在训练（USB摄像头）与部署（机器人板载RGB摄像头）使用不同传感器时的零样本泛化。

### 3. 主要实验结果
- **离线评估**：在跨被试者五折交叉验证中，最佳模型（多模态+Transformer+MINT-RVAE）在帧级和序列级的AUROC均达到**0.95**。在跨场景（未见过的多人环境）评估中，AUROC仍保持**0.932**（帧级）和**0.957**（序列级），证明了强大的泛化能力。
- **真实世界部署**：将训练好的模型**不经微调**部署到搭载Raspberry Pi 5（仅CPU）的MIRA机器人头部。在32次真实交互试验中，系统实现了**91%的准确率**和**100%的召回率**，验证了其在跨相机、跨环境下的鲁棒性和实时性。

### 4. 研究意义和价值
本研究证明了**仅使用低成本RGB摄像头和嵌入式CPU**即可实现高性能、实时的人机交互意图检测。其核心价值在于：
- **实用性**：大幅降低了感知系统的硬件成本和计算需求，为普及型社交机器人的部署扫清了障碍。
- **泛化性**：通过创新的特征归一化和生成式数据增强，系统对不同的相机、环境和参与者行为表现出优异的适应能力。
- **前瞻性**：实现了帧级精度的早期意图检测，使机器人能够更自然、更主动地发起交互，提升了人机交互的流畅性与用户体验。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文拟解决的核心问题**
这篇论文旨在解决**服务机器人在公共空间进行自然、流畅的人机交互（HRI）时，实时、准确地检测人类交互意图**这一关键挑战。具体而言，它针对现有方法的几个主要局限性：
1.  **硬件成本与计算限制**：现有方法多依赖昂贵的RGB-D深度传感器和GPU加速，难以部署在低成本、资源受限的嵌入式机器人平台上。
2.  **泛化能力不足**：模型通常在受控实验室环境下训练和评估，缺乏在真实、非结构化环境以及不同摄像头硬件间的泛化能力验证。
3.  **数据类别严重不平衡**：真实HRI数据集中，表示“有交互意图”的帧（正样本）远少于“无意图”的帧（负样本），导致模型容易偏向预测多数类。
4.  **时间粒度粗糙**：许多方法仅进行片段级或事件级分类，无法实现**帧级精确**的意图检测，从而无法支持机器人做出前瞻性响应。

### **核心技术创新点**
论文提出了一个集成的、可部署的解决方案，其创新主要体现在以下三个方面：

#### **1. 纯RGB多模态意图检测框架**
- **技术路径**：摒弃昂贵的RGB-D传感器，仅使用**单目RGB摄像头**。
- **特征提取**：
    - **姿态**：使用YOLOv8-Pose提取2D人体骨骼关键点，并通过**边界框归一化** (`(u_n - u_min)/w, (v_n - v_min)/h`) 生成**摄像头不变的特征**，使模型能泛化到不同摄像头和视角。
    - **情绪**：使用DeepFace从面部区域提取七类基本情绪的概率向量。
- **时序建模与融合**：将姿态和情绪特征拼接后，输入轻量级时序模型（**GRU、LSTM、轻量化Transformer**）进行帧级意图概率预测。
- **实际价值**：大幅降低了硬件成本（仅需普通RGB摄像头）和计算需求，使系统得以在**仅使用CPU的树莓派5**上实时运行，为低成本社交机器人的普及扫清了关键障碍。

#### **2. 用于数据重平衡的MINT-RVAE生成模型**
- **问题针对性**：直接解决HRI数据中严重的类别不平衡问题。
- **技术创新**：提出了 **MINT-RVAE（用于意图序列生成的多模态循环变分自编码器）**。
    - **架构**：基于GRU的编码器-解码器结构，将包含姿态、情绪和意图标签的**多模态序列**编码到潜空间，并从中解码生成新的序列。
    - **定制化损失函数**：设计了联合损失，分别优化姿态重建（使用**置信度加权的Huber损失**以应对遮挡和噪声）、情绪重建（KL散度）和意图标签重建（二元交叉熵），并采用**自由比特（free-bits）策略**防止后验坍塌。
    - **生成方式**：训练后，从标准高斯先验中采样潜向量，通过自回归解码器生成**时间连贯、多模态对齐**的合成序列，专门用于增强稀有的“有意图”样本。
- **实际价值**：通过生成高质量合成数据，有效平衡了训练集，显著提升了模型（尤其是Transformer）对少数类的**召回率**和整体泛化能力，避免了简单的过采样或欠采样带来的问题。

#### **3. 全面的跨域与跨传感器验证范式**
- **评估协议创新**：论文没有止步于常规的交叉验证，而是设计了三个层层递进的严格评估协议：
    1.  **跨主体**：5折交叉验证，评估对未见个体的泛化能力。
    2.  **跨场景**：在单/双人简单场景训练，在**多人、动态背景的食堂场景**测试，评估环境泛化能力。
    3.  **跨摄像头部署**：**核心贡献**。将在USB网络摄像头数据上训练的模型，**不经任何微调**，直接部署到使用不同内置RGB摄像头的MIRA机器人头部（树莓派5，CPU-only），在完全未见的真实非受控环境中进行实时测试。
- **实际价值**：证明了所提方法（特别是摄像头不变的姿态特征）具有强大的**跨硬件、跨环境鲁棒性**。离线评估（AUROC 0.95）与真实部署（准确率91%，召回率100%）性能高度一致，**有力证实了该框架的实际可用性和部署可行性**，这是许多现有工作所缺乏的。

### **解决方案总结**
论文通过一个**端到端的系统**解决了上述问题：
1.  **感知层**：用YOLOv8-Pose和DeepFace从RGB视频提取归一化2D姿态和情绪特征。
2.  **数据层**：用MINT-RVAE生成合成序列，解决数据不平衡。
3.  **模型层**：用轻量级时序模型（最优为Transformer）进行帧级意图概率预测。
4.  **验证层**：通过跨主体、跨场景、尤其是**跨摄像头真实部署**的完整实验链条，全面证明了系统的有效性、鲁棒性和可部署性。

**总而言之，这篇论文的核心价值在于将一个高精度的意图检测系统，从依赖昂贵硬件的实验室原型，成功推进到了能在低成本、资源受限的嵌入式平台上稳定运行的实用阶段，并通过严谨的跨域验证为其实际应用提供了坚实依据。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决服务机器人在公共空间中**实时、低成本地理解人类交互意图**的难题，尤其针对现有方法依赖昂贵RGB-D传感器、高算力GPU，且难以处理真实场景中数据类别严重不平衡和跨设备泛化的问题。为此，作者提出了一个**纯RGB视觉的多模态框架**，其核心创新在于：1）融合了基于YOLOv8提取的、经过归一化处理的相机无关2D姿态特征和基于DeepFace提取的面部情绪特征；2）设计了一种名为**MINT-RVAE**的新型循环变分自编码器，用于生成时序连贯的合成数据，以有效平衡训练集中的罕见交互意图样本。该框架在轻量级时序模型（GRU、LSTM、Transformer）上实现帧级意图预测。实验表明，该系统在离线交叉验证中取得了高达**0.95的AUROC**；更重要的是，在未经微调的情况下，成功部署到仅使用CPU的树莓派5和不同摄像头的MIRA机器人头上，在32次真实世界交互试验中实现了**91%的准确率和100%的召回率**，证明了其强大的跨相机、跨环境泛化能力和实际部署可行性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization》针对服务机器人的人机交互意图检测，提出了一套完整且实用的解决方案。其核心创新点明确，旨在解决该领域长期存在的成本、泛化性和部署可行性问题。

以下是其相对于已有工作的主要创新点，逐条分析如下：

### 1. **纯RGB视觉、多模态融合的意图检测框架**
   - **相比以往方法的改进/不同之处**：
     - **传感器差异**：绝大多数现有工作（如Zimmermann et al., 2018; Liu et al., 2019; Arreghini et al., 2024）依赖于**RGB-D（深度）相机**或结合眼动仪、可穿戴设备等多模态传感器来获取3D姿态或更精确的空间信息。
     - **本文方法**：**仅使用单目RGB相机**，从中提取**2D人体姿态**和**面部情绪**两种视觉线索进行融合。它摒弃了昂贵的深度传感器（如Kinect, RealSense）和额外的专用硬件。
   - **解决的具体问题/带来的优势**：
     - **显著降低成本**：将传感器成本从数百美元（RGB-D）降低到数十美元（普通RGB摄像头），极大提升了方案的可扩展性和在预算敏感型机器人平台上的部署潜力。
     - **降低部署复杂度**：避免了RGB-D传感器所需的**逐设备校准**问题，简化了安装和维护流程。
     - **保护隐私**：仅处理提取的抽象特征（关键点坐标、情绪概率），而非原始视频流，更符合在公共空间部署的隐私要求。
     - **证明RGB信息的充分性**：通过精心设计的特征（相机无关的2D姿态）和融合模型，证明了在嵌入式平台上仅凭RGB信息也能达到与RGB-D方案相媲美的性能。

### 2. **针对HRI数据类别不平衡的创新生成式数据增强方法：MINT-RVAE**
   - **相比以往方法的改进/不同之处**：
     - **传统方法局限**：面对HRI数据中“有交互意图”的正样本远少于“无意图”负样本的严重不平衡问题，先前研究通常采用简单的**过采样（如SMOTE）**、**欠采样**或**类别权重调整**。这些方法对于**时序多模态数据**效果不佳，因为它们会破坏时间连贯性或跨模态一致性（例如，简单复制片段会导致不自然的动作序列）。
     - **本文方法**：提出了 **MINT-RVAE（用于意图序列生成的多模态循环变分自编码器）**。这是一个**生成式模型**，专门用于合成**时间上连贯**且**跨模态（姿态-情绪）对齐**的意图序列，以增强稀少的正类样本。
   - **解决的具体问题/带来的优势**：
     - **有效缓解类别不平衡**：通过生成高质量、多样化的合成正样本序列，从根本上改善了训练数据的分布，使模型不会偏向于预测多数类（“无意图”）。
     - **提升模型泛化与召回率**：实验表明，使用MINT-RVAE增强后，所有时序模型的性能均有提升，特别是在**召回率（Recall）** 上表现突出（在真实部署中达到100%）。这意味着系统能更可靠地捕捉到真实的交互意图，减少了漏报，这对于提升用户体验至关重要。
     - **保持时序与语义真实性**：生成的序列不是随机拼凑，而是学习了真实数据中姿态、情绪和意图标签之间的联合动态演化，因此合成的数据更“逼真”，有助于模型学习到更鲁棒的意图模式。

### 3. **帧级精度的实时意图检测与轻量化时序建模**
   - **相比以往方法的改进/不同之处**：
     - **检测粒度**：许多现有方法（如Trick et al., 2023; Arreghini et al., 2024）进行的是**片段级（clip-level）或事件级（event-level）** 分类，即对一个2-3秒的视频片段给出一个整体标签。这无法精确定位意图**何时开始**。
     - **本文方法**：实现了**帧级（frame-level）** 意图概率预测。系统连续处理视频流，为每一帧输出一个交互意图的概率。
   - **解决的具体问题/带来的优势**：
     - **实现早期、主动的意图检测**：能够**在物理交互发生之前**就探测到意图的萌发，使机器人可以**提前、主动地**调整行为、发起问候或做好准备，从而实现更流畅、更自然的“预期式”交互。
     - **更高的时间分辨率**：为机器人控制提供了更细粒度的反馈信号。
     - **轻量化模型设计**：为了在嵌入式设备上实现实时帧级预测，论文对比并优化了**GRU、LSTM和一个极简Transformer**（仅一个编码器块，四个注意力头）等轻量级时序模型，在精度和效率间取得了平衡。

### 4. **面向嵌入式部署的系统性设计与跨相机/跨环境验证**
   - **相比以往方法的改进/不同之处**：
     - **部署可行性**：绝大多数相关研究仅在配备GPU的工作站上进行离线评估，**未考虑在真实机器人嵌入式平台上的部署可行性**。计算开销和硬件兼容性常被忽略。
     - **泛化性验证**：许多实验在受控的实验室环境下进行，使用脚本化的行为，对模型在**未知环境、不同传感器**下的泛化能力验证不足。
     - **本文方法**：
         1. **硬件目标明确**：整个系统设计以在**资源受限的嵌入式平台（树莓派5，仅CPU）** 上实时运行为目标。
         2. **相机无关特征**：通过**基于边界框的2D关键点归一化**（公式3），使姿态特征对相机视角、距离和分辨率变化不敏感。
         3. **严格的跨域验证协议**：
             - **跨主体**：5折交叉验证。
             - **跨场景**：在单/多人、不同光照和背景的未见过的环境中测试。
             - **跨相机部署**：**核心创新验证**——将在普通USB摄像头数据上训练的模型，**不经任何微调**，直接部署到使用不同型号板载RGB相机的MIRA机器人头上，并在完全未在训练中出现的“野外”非受控环境（如餐厅）中进行实时测试。
   - **解决的具体问题/带来的优势**：
     - **证明了实际可部署性**：系统在树莓派5（CPU-only）上实现了实时推理，验证了其在低成本、低功耗服务机器人上运行的可行性，这是迈向实际应用的关键一步。
     - **证明了强大的跨域泛化能力**：跨相机部署实验取得了91%的准确率和100%的召回率。这强有力地证明了所提方法（特别是相机无关特征和MINT-RVAE增强）能够有效应对**传感器差异**和**环境变化**带来的域偏移问题。
     - **建立了从离线训练到在线部署的完整闭环**：论文不仅提出了算法，还完成了从数据收集、模型训练到真实机器人集成和评估的全流程，研究完整度很高，结论可信。

### 总结
总而言之，这篇论文的创新是一个**系统性**的创新，它并非只改进某个单一模块，而是从**感知模态（RGB-only）、数据层面（MINT-RVAE）、模型粒度（帧级）、到最终部署（嵌入式跨相机验证）** 等多个维度，共同构建了一个**低成本、高泛化、可实时部署**的HRI意图检测解决方案。其最突出的贡献在于，它成功地将一个性能优异的意图检测系统从依赖昂贵硬件的实验室环境，“降维”到了仅需普通摄像头和嵌入式CPU的实用场景，并经过了严苛的跨域泛化测试，为社交机器人的普及化应用提供了重要的技术路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

该论文通过一系列严谨的离线与在线实验，全面验证了所提出的RGB-only人机交互意图检测框架的有效性、泛化能力和实际部署可行性。

### 一、 使用的数据集
论文**未使用**公开的基准数据集，而是**自行采集**了一个专用于人机交互意图检测的数据集。该数据集的特点如下：
*   **采集平台**：使用配备标准USB网络摄像头的Elephant Robotics MyCobot 320协作机械臂。
*   **场景与规模**：在3个不同的室内环境（图书馆学习区、建筑走廊、公共餐厅）中，由10名参与者共采集了88个RGB视频序列，总计约12,615帧。
*   **数据特点**：
    *   **类别不平衡**：真实反映了现实HRI场景，约70%的帧为“无意图”（路过），30%为“有意图”（接近并交互）。
    *   **精细标注**：参与者通过无线演示器按钮，在**形成交互意图的精确时刻**进行标注，实现了**帧级**的意图起始点标注。
    *   **隐私保护**：仅提取并保留去身份化的2D姿态坐标和情感概率向量，不保留原始视频。

### 二、 评价指标
论文采用了多层次的评价指标，以全面评估系统性能：

1.  **帧级指标**：评估模型对每一帧的意图预测能力。
    *   **宏平均F1分数**：平等对待“意图”和“无意图”两类，适用于类别不平衡场景。
    *   **平衡准确率**：补偿类别先验概率的偏差。
    *   **AUROC**：综合评估模型在所有阈值下的分类性能。

2.  **序列级指标**：评估模型对整个时间窗口（15帧）的意图判断能力。一个序列被判定为“有意图”的条件是：模型预测的意图概率在连续至少7帧内超过阈值 `τ`。
    *   同样报告**宏平均F1分数**、**平衡准确率**和**AUROC**。

3.  **实时部署指标**：在真实机器人（MIRA）上进行在线测试时，计算了基于试验次数的标准分类指标：
    *   **准确率**、**精确率**、**召回率**、**F1分数**。

### 三、 基线方法与对比
论文从多个维度与现有工作进行了对比，并设置了内部消融实验：

1.  **内部消融实验（核心对比）**：
    *   **特征模态**：对比了**仅姿态**、**仅情感**、**姿态+情感（多模态）** 三种输入特征的效果。
    *   **数据平衡策略**：对比了**不使用数据增强**与使用提出的 **MINT-RVAE进行数据重平衡** 的效果。
    *   **时序模型**：对比了**GRU**、**LSTM**和**轻量级Transformer**三种时序骨干网络。

2.  **与现有工作的横向对比**：
    论文在讨论部分（表IV）系统性地与6篇代表性文献进行了对比，突出了本工作的创新点：
    *   **传感器**：对比了依赖**RGB-D（深度）** 传感器的方法（如Zimmermann等人， Liu等人， Trick等人， Abbate等人， Arreghini等人， Kedia等人）。
    *   **计算平台**：对比了仅在**GPU工作站**上进行离线评估的方法。
    *   **时序粒度**：对比了仅进行**序列级**或**事件级**预测的方法。
    *   **类别不平衡处理**：指出大多数现有工作**未明确处理**严重的类别不平衡问题。

### 四、 关键性能结果与结论

1.  **离线跨主体评估（5折交叉验证）**：
    *   **最佳性能**：**多模态（姿态+情感） + MINT-RVAE + Transformer** 模型取得了最优性能。
        *   **帧级AUROC**: **0.950 ± 0.015**
        *   **序列级AUROC**: **0.951 ± 0.017**
    *   **核心结论**：
        *   **多模态融合有效**：姿态与情感特征的融合显著优于任何单一模态（例如，情感单独使用效果很差）。
        *   **MINT-RVAE提升显著**：引入MINT-RVAE进行数据增强后，所有模型的宏平均F1分数和AUROC均有稳定提升（约1-4%），**召回率曲线整体上移**（见图7），表明其对少数类（意图）的检测能力更强。
        *   **Transformer优势**：轻量级Transformer在捕获时序依赖和区分意图线索方面表现最佳。

2.  **跨场景评估（泛化能力）**：
    *   **设置**：在单人环境（环境1、2）上训练，在未见过、背景更复杂、包含多人的环境（环境3）上测试。
    *   **结果**：性能相比跨主体评估略有下降，但**MINT-RVAE的增强作用更加明显**。最佳模型（Transformer + MINT-RVAE）仍能保持：
        *   **帧级AUROC**: **0.932 ± 0.007**
        *   **序列级AUROC**: **0.957 ± 0.002**
    *   **结论**：证明了模型对**新环境、新人群配置**具有良好的泛化能力，MINT-RVAE合成的数据增强了模型的鲁棒性。

3.  **跨相机实时部署评估（实际价值核心）**：
    *   **设置**：将离线训练的最佳模型**不经任何微调**，部署到**MIRA机器人头部**。该机器人使用**不同的板载RGB摄像头**，并在**完全未受控的真实环境**（如餐厅）中运行，计算仅依赖**Raspberry Pi 5的CPU（无GPU）**。
    *   **结果**：在32次真人实时交互试验中（15次有意图，17次无意图），系统取得了：
        *   **准确率**: **91%**
        *   **召回率**: **100%** （所有15次交互意图均被成功检测）
        *   **精确率**: 83%
        *   **F1分数**: 91%
    *   **核心结论与价值**：
        *   **强大的跨域泛化**：在训练（USB摄像头）与部署（机器人板载摄像头）存在显著**域偏移**的情况下，性能与离线评估结果高度吻合，验证了所提方法（特别是基于边界框归一化的姿态特征）的**跨传感器、跨环境鲁棒性**。
        *   **嵌入式可行性验证**：首次在**低成本、仅CPU的嵌入式平台**上实现了实时（帧级）的、高性能的HRI意图检测，证明了该框架在**资源受限的普及型服务机器人**上的实际部署潜力。
        *   **完美召回率**：对于服务机器人场景至关重要，避免了错过用户的交互意图，从而提升用户体验。

**总结**：该论文通过系统的实验设计，不仅证明了其提出的多模态框架在离线指标上达到SOTA水平（AUROC > 0.95），更通过**严格的真实世界跨相机部署实验**，验证了其**核心技术（相机无关特征、MINT-RVAE数据平衡）的实际有效性和泛化能力**，最终在仅使用RGB摄像头和CPU计算的嵌入式机器人上实现了高精度（91%准确率）和高可靠性（100%召回率）的实时意图检测。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.17958v1)
- [HTML 版本](https://arxiv.org/html/2512.17958v1)
