# Scaling Laws for Energy Efficiency of Local LLMs

**相关性评分**: 6.0/10

**排名**: #22


---


## 基本信息

- **arXiv ID**: [2512.16531v4](https://arxiv.org/abs/2512.16531v4)
- **发布时间**: 2025-12-18T13:40:33Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Ander Alvarez, Alessandro Genuardi, Nilotpal Sinha, Antonio Tiene, Mikail Okyay, Bakbergen Ryskulov, David Montero, Samuel Mugel, Román Orús

## 关键词

Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

该论文通过系统基准测试和量子启发压缩技术，探索了本地语言和视觉语言模型在CPU上的计算扩展规律，旨在提高边缘设备上的推理效率和能源效率。

## 摘要

Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

## 详细分析

## 论文摘要

**1. 研究背景和动机**
随着对隐私、延迟和能耗的关注，在边缘设备上本地部署大型语言模型（LLM）和视觉语言模型（VLM）的需求日益增长。然而，大多数消费级硬件（如笔记本电脑、嵌入式系统）依赖中央处理器（CPU），而现有研究主要关注GPU加速的推理，对纯CPU推理的计算规律和能耗特性缺乏系统性探索。本研究旨在填补这一空白，量化本地LLM/VLM在CPU上的能效扩展规律。

**2. 核心方法和技术创新**
- **统一评测框架**：在两类代表性CPU硬件（MacBook Pro M2和Raspberry Pi 5）上，系统性地对LLM和VLM进行基准测试。
- **资源度量方法**：采用基于曲线下面积（AUC）的集成度量方法，连续采样CPU、内存使用率和能耗，以量化推理过程中的总计算负载。
- **量子启发压缩**：应用名为**CompactifAI**的量子启发张量网络压缩方法，对模型进行结构化剪枝，以减少参数冗余。
- **输入复杂性控制**：通过构建累积增长的文本提示序列和系统性地改变图像分辨率，探究输入复杂度对资源消耗的影响。

**3. 主要实验结果**
- **LLM扩展定律**：纯CPU推理的计算成本（CPU/内存AUC）与输入**令牌长度呈近似线性关系**。模型压缩（Gilda）能显著降低该线性关系的截距和斜率，在树莓派5上最高减少71.9%的内存使用和60.5%的CPU负载，同时吞吐量提升2.6倍。
- **VLM扩展定律**：VLM的计算成本呈现**预处理驱动的“分辨率拐点”**。当输入图像分辨率超过模型内部固定的预处理裁剪阈值时，计算成本保持恒定；低于该阈值时，成本急剧下降。此拐点位置由预处理流程决定，而非模型固有属性。
- **能效提升**：压缩模型在保持甚至提升语义准确性的同时，大幅降低了能耗。LLM在树莓派5上的能耗降低最高达62%，VLM在M2上的能耗降低37.5%。

**4. 研究意义和价值**
本研究首次在纯CPU环境下为本地LLM和VLM推理建立了量化的扩展定律，揭示了**输入令牌和有效图像像素是核心成本驱动因素**。其实际价值在于为边缘AI部署提供了明确的设计原则：
- **管理输入资源**：明确控制提示长度，并将图像分辨率调整至预处理阈值以下，可作为低成本、无损的能效优化杠杆。
- **优先采用压缩模型**：尤其是对于资源受限的嵌入式设备，压缩技术能显著提升可行性和能效。
- **系统级优化视角**：强调预处理配置应与模型参数一同作为部署规范，并为“绿色AI”在边缘计算中的实践提供了具体的测量数据和优化路径。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
论文旨在解决一个被忽视的关键问题：**在缺乏GPU的普通CPU硬件（如笔记本电脑、嵌入式设备）上，本地部署大型语言模型和视觉语言模型时，其计算成本、内存占用和能耗如何随输入复杂度（文本长度、图像分辨率）变化？** 现有研究大多关注GPU或数据中心环境，缺乏对CPU-only边缘推理的**系统性量化规律**。

### **核心创新点**
论文的创新点主要体现在以下三个层面：

1.  **首次系统性地揭示了CPU-only本地多模态推理的实证缩放定律**
    *   **LLM计算线性定律**：发现LLM的CPU和内存成本（以曲线下面积AUC衡量）与输入**令牌长度呈近似线性关系**。这一定律在高端（MacBook M2）和低端（Raspberry Pi 5）硬件上均成立。
    *   **VLM“分辨率拐点”定律**：发现VLM的计算成本并不随输入图像分辨率平滑增长，而是存在一个**由预处理决定的“拐点”**。当分辨率高于内部固定阈值时，计算成本恒定；低于该阈值时，成本急剧下降，而语义准确性保持不变。论文通过修改预处理阈值实验，证实该“拐点”是预处理流程的产物，而非模型固有属性。

2.  **提出并验证了一种统一的、面向系统的评估方法论**
    *   **度量标准**：采用**时间积分指标（AUC）** 来量化CPU、内存的“总工作量”，而非瞬时峰值，更能反映实际用户体验。
    *   **能耗测量**：结合USB功率计，实现了**按提示词（per-prompt）和按次运行（per-run）的能耗量化**，将性能与“绿色AI”直接关联。
    *   **硬件覆盖**：选择**MacBook Pro M2（高端消费级）和Raspberry Pi 5（低功耗嵌入式）** 作为代表性硬件平台，使结论兼具广度和现实意义。

3.  **实证了量子启发的模型压缩技术作为高效能“杠杆”的实际价值**
    *   论文不仅进行基准测试，还引入了 **CompactifAI（一种基于张量网络的量子启发压缩方法）** 作为关键变量。
    *   **效果验证**：压缩模型在**大幅减少参数量的同时，显著降低了CPU/内存AUC、能耗，并提高了推理速度**，且**语义准确性得以保持甚至提升**。例如，在Raspberry Pi 5上，压缩LLM的CPU和内存AUC分别降低60.5%和71.9%，能耗降低62%，速度提升2.6倍。
    *   **核心洞见**：压缩不仅是缩小模型尺寸，更是**系统级的效率倍增器**，尤其能在资源受限的硬件上，将原本不可行的本地推理变为可行。

### **解决方案**
论文通过一个严谨的“控制变量-测量-对比”实验框架来解决上述问题：

1.  **模型与框架**：选择LLaMA-3.1-8B / Qwen2-VL-2B及其CompactifAI压缩变体（Gilda / Axolotl）作为模型对，使用`llama.cpp`生态进行CPU推理。
2.  **输入复杂度控制**：
    *   **LLM**：使用语义连贯、长度递增的19个提示词序列。
    *   **VLM**：使用一张复杂的交通场景图，系统性地改变其输入分辨率（20个级别），并主动修改预处理分辨率阈值进行验证。
3.  **系统级度量**：以5Hz频率采样CPU/内存使用率，计算相对于空闲基线的AUC；使用USB功率计测量能耗。
4.  **准确性评估**：使用SimCSE计算模型输出与Gemini 2.5 Flash参考答案的语义相似度，作为自动化、可比的准确性代理指标。

### **实际价值与启示**
*   **对从业者的设计原则**：
    *   **将令牌和像素视为核心成本单元**进行管理。
    *   **在资源受限设备上默认使用压缩模型**。
    *   **将能耗（Wh/提示）作为关键性能指标**进行监控。
    *   **预处理配置（如分辨率限制）应与模型文件一同作为部署规范**，因其直接决定系统成本。
*   **对研究社区的贡献**：填补了CPU-only多模态推理量化规律的空白，为边缘AI、绿色AI的研究提供了新的基准和方法论参考。
*   **对可持续发展的意义**：证明了通过模型压缩和输入预处理，可以在不牺牲准确性的前提下，显著降低本地AI的能耗，推动更可持续的边缘计算。

**总结**：该论文的核心创新在于**首次通过严谨的系统性实验，揭示了CPU上本地LLM/VLM推理的定量缩放规律，并实证了先进模型压缩技术是实现高效、可持续边缘AI部署的有效途径**。其价值不仅在于发现的定律本身，更在于提供了一套可复现的评估框架和具有直接指导意义的工程实践原则。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决一个关键的实际问题：**在仅使用CPU的本地设备（如笔记本电脑和嵌入式系统）上部署大型语言模型（LLM）和视觉语言模型（VLM）时，其计算与能耗如何随输入复杂度（文本长度、图像分辨率）变化，以及如何通过模型压缩来优化效率。** 为此，论文提出了一套**基于曲线下面积（AUC）的统一系统级评测方法**，在MacBook Pro M2和Raspberry Pi 5两种代表性硬件上，对压缩与未压缩的模型对进行了系统性基准测试。研究发现了两条核心经验性扩展定律：**1）LLM的计算成本与输入令牌长度近似呈线性关系；2）VLM的计算成本存在一个由预处理（缩放/裁剪）决定的“分辨率拐点”，在此拐点之上计算成本恒定，之下则急剧下降。** 最终，论文验证了所采用的**量子启发式压缩方法（CompactifAI）** 能显著提升效率，在保持甚至提升语义准确性的同时，最高可减少71.9%的内存使用和62%的能耗，并实现高达2.6倍的推理加速，从而为在资源受限的边缘设备上实现可持续、高效能的本地AI部署提供了明确的量化依据和实用杠杆。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文在本地大语言模型（LLM）和视觉-语言模型（VLM）的CPU推理领域，提出了几个明确的创新点，主要体现在**研究视角、量化方法、发现规律和实用技术**的结合上。

### 1. **首次系统性地建立了CPU-only本地多模态推理的实证缩放定律**
   - **相比以往方法的改进/不同之处：**
     - 以往研究主要集中在GPU或数据中心环境下的模型性能（如吞吐量、延迟）或抽象计算量（如FLOPs）的缩放规律。
     - 少数关于边缘AI的研究也未能**量化CPU推理成本如何随输入复杂度（文本长度、图像分辨率）演变**。
   - **解决的具体问题/带来的优势：**
     - 填补了在**纯CPU、资源受限的边缘设备**上部署LLM/VLM时，缺乏可预测性计算成本模型的空白。
     - 为开发者和系统设计者提供了**关键的设计依据**，使其能够根据设备的计算和能源预算，合理规划输入长度和图像分辨率。

### 2. **发现了VLM推理中由预处理主导的“分辨率拐点”现象，并揭示了其本质**
   - **相比以往方法的改进/不同之处：**
     - 以往对VLM计算成本的研究可能默认其随输入分辨率平滑增长。
     - 本文通过实验首次明确揭示并验证：VLM的计算成本在达到内部预处理钳位（clamp）分辨率后**保持恒定**，低于该值则**急剧下降**，且该“拐点”完全由预处理流程（如`llama.cpp`中的resize-and-clamp操作）决定，而非模型固有属性。
   - **解决的具体问题/带来的优势：**
     - 澄清了一个重要的系统级误解，指出**有效像素数**而非**名义像素数**才是计算成本的决定因素。
     - 为优化提供了明确的、低成本的杠杆：**通过调整预处理管道中的分辨率钳位值，可以在不损失精度的情况下显著降低计算和能源成本**。这比修改模型本身要简单得多。

### 3. **在统一的框架下，跨设备、跨模态量化了量子启发压缩技术（CompactifAI）对系统级指标的实际影响**
   - **相比以往方法的改进/不同之处：**
     - 传统的模型压缩研究（如量化、剪枝）评估指标多集中于模型大小、FLOPs或GPU上的准确率/延迟。
     - 本文创新性地在**两种代表性CPU硬件**（高端笔记本M2和低功耗嵌入式树莓派5）上，系统测量了压缩对**CPU利用率曲线下面积（AUC）、内存AUC、吞吐量和每提示词能源消耗**的全面影响。
   - **解决的具体问题/带来的优势：**
     - 证明了压缩不仅是“让模型能跑起来”的工具，更是强大的**效率倍增器**。特别是在资源紧张的设备（如树莓派）上，压缩能带来**不成比例的巨大收益**（例如，LLM的CPU和内存使用降低超60%，能耗降低超60%）。
     - 提供了压缩技术在实际部署环境中的**性能-效率权衡**的具体数据，指导开发者针对不同硬件选择合适的压缩模型。

### 4. **提出并应用了一套针对本地AI推理的、基于曲线下面积（AUC）和每任务能源消耗的统一评测方法**
   - **相比以往方法的改进/不同之处：**
     - 边缘AI的能效评估虽受关注，但多数工作仍假设GPU加速平台，或缺乏对**单次推理任务**完整生命周期的资源与能源集成测量。
     - 本文采用**时间积分**的AUC方法来度量CPU/内存的“总工作量”，并结合USB功率计进行**每提示词能源核算**，提供了比峰值功率或平均利用率更全面的成本视图。
   - **解决的具体问题/带来的优势：**
     - 实现了对间歇性、交互式本地AI负载（如单次对话或图像问答）计算成本的**精准、可比较的量化**。
     - 将**能源消耗**确立为核心性能指标（Wh/提示词，Wh/次运行），推动了面向可持续发展的“绿色AI”从训练阶段向**部署阶段**的延伸，使开发者能直观评估不同模型和设置对设备续航和运营成本的影响。

### 总结
本文的核心创新在于将**基础研究（发现缩放定律）** 与**应用工程（量化压缩效益、提供实测方法）** 紧密结合，针对“**在纯CPU设备上高效运行本地大模型**”这一具体且日益重要的场景，提供了从理论认知到实践工具的完整贡献。其发现的定律和验证的方法，为构建更高效、更可持续的边缘AI系统提供了直接、可操作的指导原则。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过系统性的基准测试，揭示了在仅使用CPU的本地设备上运行大型语言模型（LLM）和视觉语言模型（VLM）时的计算与能耗规律，并评估了一种量子启发的压缩方法的效果。

### 一、 实验设置与评估框架

#### 1. **硬件平台（“数据集”）**
论文未使用传统意义上的数据集，而是将**硬件平台**作为核心实验环境，以模拟真实的本地部署场景：
- **高端消费级**：Apple MacBook Pro M2 (8核CPU， 8GB统一内存)，代表主流笔记本电脑。
- **低功耗嵌入式**：Raspberry Pi 5 (ARM Cortex-A76， 4核， 8GB RAM)，代表资源受限的边缘设备。

#### 2. **模型对（“基线方法与实验组”）**
论文通过对比**原始模型**与其**压缩版本**来评估效果：
- **LLM对比对**：
    - **基线**：LLaMA-3.1-8B-Instruct (Q4KM量化版， 80亿参数)
    - **实验组**：Gilda v3 (使用CompactifAI压缩， 32亿参数)
- **VLM对比对**：
    - **基线**：Qwen2-VL-2B-Instruct (FP16精度， 20亿参数)
    - **实验组**：Axolotl (使用CompactifAI压缩， 11亿参数)

#### 3. **核心评价指标**
论文采用了系统级和任务级相结合的多维度指标：
- **系统资源消耗**：采用**曲线下面积（AUC）** 法，对**CPU利用率**和**内存使用量**进行时间积分，以衡量总计算负载。
- **能耗**：使用USB-C功率计测量**每提示（per-prompt）和每轮运行（per-run）的最大瓦时（Wh）**。
- **吞吐量**：**每秒生成令牌数（tokens/s）**。
- **语义准确性**：使用**SimCSE句子嵌入**计算模型输出与Gemini 2.5 Flash生成的标准答案之间的**语义相似度得分**，作为准确性的代理指标。

#### 4. **输入复杂性变量**
- **对于LLM**：使用19个语义难度相当但**令牌长度逐级递增**的提示（P1-P19），以探究计算成本与输入长度的关系。
- **对于VLM**：使用一张包含多实体的复杂交通场景图，将其**下采样至20种不同分辨率**，以探究计算成本与图像分辨率的关系。此外，通过修改`llama.cpp`的预处理代码，**人为调整内部分辨率钳制阈值**，以验证“分辨率拐点”的成因。

### 二、 关键实验结果与性能提升

#### 1. **两大经验性扩展定律（Scaling Laws）**
- **定律一：LLM计算成本与令牌长度近似线性相关**
    - **结论**：CPU和内存的AUC随输入令牌数量线性增长。**令牌数量是驱动计算成本的主要因素**，而非语义复杂性。
    - **压缩效果**：压缩模型Gilda显著降低了这种线性关系的**截距（固定开销）和斜率（每令牌成本）**。
        - 在MacBook M2上，CPU AUC降低31.3%， RAM AUC降低55.9%。
        - 在Raspberry Pi 5上，效果更显著：CPU AUC降低60.5%， **RAM AUC降低71.9%**。

- **定律二：VLM计算成本存在由预处理决定的“分辨率拐点”**
    - **结论**：VLM的计算成本并不随名义分辨率平滑增长。当图像分辨率超过模型内部预处理的**固定钳制阈值（如1024×720）** 时，计算成本保持恒定；低于该阈值时，计算成本急剧下降，而准确性保持不变。
    - **机制验证**：通过降低钳制阈值（至854×594和714×496），**“拐点”位置随之精确移动**，证明该现象完全由预处理流水线引起，而非模型固有架构特性。

#### 2. **压缩技术的综合效能提升**
- **效率提升**：
    - **LLM (Gilda)**：
        - **吞吐量**：在M2上加速2.1倍，在RPi 5上加速**2.6倍**。
        - **能耗**：在RPi 5上，每提示最大能耗降低约62%（从0.34 Wh降至0.13 Wh），每轮运行总能耗从6.50 Wh降至2.50 Wh。
    - **VLM (Axolotl)**：
        - **吞吐量**：在M2上平均加速1.8倍，在RPi 5上平均加速2.0倍。
        - **能耗**：在M2上，每提示最大能耗降低37.5%（从0.16 Wh降至0.10 Wh）。

- **准确性表现**：
    - **压缩模型在语义准确性上匹配或超越了原始模型**，表明CompactifAI压缩起到了结构化正则化的作用，去除了冗余参数。
        - **Gilda (LLM)**：在M2上平均准确率提升9.1%（80.0% → 89.1%），在RPi 5上提升13.8%（77.0% → 90.8%）。
        - **Axolotl (VLM)**：在M2上平均准确率提升6.9%（70.5% → 77.4%），在RPi 5上提升5.8%（70.9% → 76.7%）。

### 三、 核心结论与价值

论文的实验最终实现了以下效果：
1.  **量化了CPU-only场景的扩展规律**：首次系统性地揭示了LLM和VLM在仅CPU设备上，计算负载随输入复杂度（令牌长度、图像分辨率）变化的经验定律。
2.  **揭示了VLM效率的关键杠杆**：指出VLM的“分辨率拐点”是预处理产物，**通过调整输入分辨率至钳制阈值以下，可在不损失精度的情况下大幅降低计算成本和能耗**。这是一个低成本、易实施的优化手段。
3.  **验证了量子启发压缩的实际价值**：证明了CompactifAI压缩不仅是模型瘦身工具，更是**系统级的效率倍增器**。它在显著降低资源消耗和能耗的同时，**保持了甚至提升了模型的语义准确性**，尤其在资源受限的嵌入式设备（如Raspberry Pi 5）上效果极为显著。
4.  **提供了本地部署的实用设计原则**：论文结论直接转化为可操作的部署建议：**管理令牌和像素预算、默认使用压缩模型、将能耗作为核心性能指标进行监控**。

**总结**：该论文通过严谨的跨平台、跨模态实验，不仅得出了具有普适性的计算扩展定律，更重要的是，为在资源受限的CPU设备上高效、可持续地部署本地大模型提供了经过实证检验的技术路径（压缩）和系统优化策略（预处理配置）。其评估指标紧密结合实际部署的系统成本（计算、内存、能耗）和任务效果（语义准确性），结论具有明确的工程指导意义。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.16531v4)
- [HTML 版本](https://arxiv.org/html/2512.16531v4)
