# Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation

**相关性评分**: 6.0/10

**排名**: #27


---


## 基本信息

- **arXiv ID**: [2512.18028v1](https://arxiv.org/abs/2512.18028v1)
- **发布时间**: 2025-12-19T19:47:55Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

这篇论文介绍了一个名为Embodied4C的基准测试，用于评估视觉语言模型在具身导航中的推理能力，但未直接涉及推理效率、轻量架构或边缘部署等技术优化。

## 摘要

Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.

## 详细分析

## 论文《Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation》详细摘要

### 1. 研究背景和动机
当前，视觉-语言模型（VLMs）在具身智能（如自动驾驶、无人机、机器人操控）中的应用日益广泛，旨在通过统一的语言接口整合感知、推理与控制。然而，现有评测基准大多局限于单一平台或特定任务，未能系统性地评估**具身性**（如物理平台、传感器配置、模态对齐）对模型核心推理能力的影响。现有基准常因任务分布狭窄、静态传感器配置和选择题格式而存在**归纳偏差**，导致模型可能“过拟合”基准而非展现真正的泛化与推理能力。因此，亟需一个能够全面、闭环评估具身智能核心价值的基准。

### 2. 核心方法和技术创新
本文提出了 **Embodied4C**，一个首创的、闭环的具身推理评测基准，其核心创新在于：
- **多维度能力评估**：系统性地评估模型的四大核心推理能力——**语义**（对象类别/属性）、**空间**（位置/距离/拓扑）、**时间**（动态/时序依赖）和**物理**（动力学/约束/材质）理解。
- **异构具身平台**：首次在单一基准中统一评估三种异构平台：**自动驾驶车辆**（CARLA）、**空中无人机**（AirSim）和**机器人操纵臂**（RLBench），以探究模型对具身性的敏感性与泛化能力。
- **闭环任务设计**：包含两种互补的闭环任务：**视觉问答（VQA）** 用于评估场景理解，以及**视觉-语言导航（VLN）** 用于评估实际控制与决策，确保模型输出对环境产生因果影响。
- **防过拟合机制**：引入**领域外查询**、动态传感器配置和环境变化，专门探测模型的泛化能力与鲁棒性，防止平台特异性过拟合。
- **精细化评分协议**：采用开放式问答和基于GPT的语义评分机制，避免传统选择题或文本匹配指标（如BLEU）的偏差，更真实地反映模型的理解深度。

### 3. 主要实验结果
对10个前沿VLM和4个领域专用模型进行了全面评估，关键发现如下：
- **性能排名**：GPT-5-mini取得了最高的总体Embodied4C得分（39.59），其次是GPT-5（36.00）。Claude和LLaMA 4系列模型处于中游，而小型模型（如Qwen2.5-VL）和领域专用模型表现显著落后。
- **核心瓶颈**：**空间**和**时间**推理是所有模型的**主要性能瓶颈**，直接导致了VLN任务中控制执行的不稳定和低成功率。语义和物理理解相对较强。
- **关键洞见**：
    1.  **对齐优于规模**：GPT-5-mini的表现优于更大的GPT-5，表明**跨模态对齐质量和指令微调**比单纯的模型规模更重要。
    2.  **领域专家模型泛化性差**：为特定任务（如驾驶、无人机）训练的视觉-语言-动作模型（如Senna, OpenFly-Agent）在Embodied4C上表现近乎为零。它们严重过拟合于其训练分布，无法进行跨平台泛化或产生连贯的语言推理。
    3.  **泛化能力**：主流通用VLM在领域外问题上表现出较高的鲁棒性，而专用模型则完全失败。

### 4. 研究意义和价值
Embodied4C为具身人工智能领域提供了一个**标准化、精细化且具有高区分度的评估框架**。其价值在于：
- **诊断性强**：能够将模型在复杂任务中的失败归因于特定的能力缺陷（如空间感知不足），为模型改进提供了明确方向。
- **推动通用智能**：通过强调跨平台泛化、抗过拟合和核心推理能力，该基准鼓励开发**真正通用、鲁棒的具身智能体**，而非针对狭窄任务优化的专家系统。
- **开源与可复现**：论文承诺开源所有代码，将促进社区在该基准上进行公平比较和进一步研究，加速可靠自主系统的发展。

**总结**：Embodied4C填补了当前具身VLM评估的空白，首次将多平台、多维度能力评估与防过拟合设计相结合。其实验结果揭示了当前模型的优势与根本性局限，特别是空间-时间推理的不足和专用模型泛化能力的缺失，为未来研究指明了关键挑战。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Embodied4C

### **一、 论文拟解决的核心问题**
当前具身视觉语言导航（VLN）领域的评测基准存在**三大关键局限**，导致无法准确衡量模型在真实物理世界中的核心能力：

1.  **评测维度单一**：现有基准大多只关注下游任务性能（如导航成功率），而**未能系统性地分解和评估**支撑可靠性能的**四大基础推理能力**（语义、空间、时间、物理）。
2.  **泛化能力评估不足**：多数基准任务分布狭窄、传感器配置固定、采用选择题形式，导致模型容易**过拟合**（Inductive Bias），在基准上取得高分却缺乏真正的泛化能力。
3.  **“具身性”考量缺失**：现有工作通常只针对单一平台（如仅自动驾驶汽车或仅无人机），缺乏**跨异构具身平台**（自动驾驶车辆、无人机、机械臂）的统一评测，无法理解“具身性”选择（平台、传感器、模态对齐）如何影响感知、推理和控制。

**简言之，问题在于：缺乏一个能够系统、公正地评估VLN模型在多变、真实的物理世界中，其核心推理能力和泛化性的“图灵测试”。**

### **二、 核心创新点**
论文提出了 **Embodied4C** 基准，其创新性体现在**评测框架的设计理念和系统性**上：

1.  **首创“能力驱动”的闭环评测框架**：
    - **四大核心能力维度**：首次在统一基准中系统性地评估**语义、空间、时间、物理**理解。这18个子类别（如距离估计、短期记忆、模型动力学）提供了**细粒度、可归因的**性能分析。
    - **双任务闭环设计**：同时包含**视觉问答（VQA）** 和**视觉语言导航（VLN）** 任务。VQA隔离并评估“场景理解”，VLN评估“理解到行动的闭环控制”，二者结合全面衡量具身智能。

2.  **首次实现跨异构具身平台的统一评测**：
    - **三大平台**：在**自动驾驶（CARLA）、无人机导航（AirSim）、机器人操作（RLBench）** 三个差异巨大的仿真环境中构建评测任务。
    - **价值主张评估**：不仅评测能力，还首次明确评估智能体的**三大价值主张**：
        - **通用性（Generality）**：通过“领域外查询”、传感器/天气变化来测试。
        - **具身感知（Embodiment-awareness）**：通过动态传感器配置、平台切换来测试。
        - **交互性（Interactivity）**：通过VQA和VLN的闭环执行来测试。

3.  **精心设计的防过拟合评测机制**：
    - **开放式问答（非选择题）**：避免模型利用选项模式猜答案。
    - **考试式单次尝试（One-shot）**：VLN任务只给一次机会，模拟真实世界不可重复的场景。
    - **注入“领域外”常识问题**：在场景VQA中随机插入与当前场景无关的常识问题（如“高尔夫球中低于标准杆一杆的术语是什么？”），直接探测模型是否过拟合于当前视觉输入。

4.  **先进的自动化评分协议**：
    - **GPT作为评判员（VLM-Judge）**：对于开放式VQA答案，使用GPT-5根据语义对齐度进行连续评分（0-100分），优于传统的BLEU等词重叠指标。
    - **数值答案精确评分**：对于数值问题，使用基于相对偏差的确定性公式评分。
    - **VLN分级评分**：结合二进制（成功/失败）和基于距离的渐进式评分，奖励部分进展。

### **三、 解决方案与实现路径**
1.  **构建基准数据集与任务**：
    - 收集了 **1,149个** 手工制作的、非模板化的VQA问题对和 **58个** 目标导向的VLN任务指令。
    - 在三个仿真平台中设计动态场景，涵盖不同天气、光照、交通密度、物体变体。

2.  **定义标准化评测协议**：
    - 为所有模型提供**统一的系统提示**，确保评测条件一致。
    - 为每个平台定义**标准化的动作空间接口**（JSON格式），将模型输出解析为控制命令。
    - 制定详细的**评分公式**（论文中公式1-9）和**安全终止条件**（如碰撞、闯红灯）。

3.  **大规模模型评估与分析**：
    - 评估了**10个**预训练视觉语言模型（如GPT系列、Claude、LLaMA）和**4个**领域专家模型（如Senna, OpenVLA）。
    - 进行**多维度的结果分析**：总体分数、分平台性能、VQA vs. VLN差距、四大能力维度表现、泛化性分数，并辅以**主成分分析（PCA）** 揭示模型潜在的能力结构。

### **四、 关键发现与实际价值**
- **技术创新启示**：
    - **对齐优于规模**：GPT-5-mini（较小模型）总体表现优于GPT-5（较大模型），表明**跨模态对齐和指令微调比单纯增加参数规模更重要**。
    - **时空推理是主要瓶颈**：所有模型在**空间**（距离、方向）和**时间**（运动、记忆）推理上表现最差，这直接导致了VLN性能的下降。
    - **领域专家模型泛化性差**：为特定任务（如驾驶、抓取）训练的VLA模型在Embodied4C上接近零分。它们**过度拟合了其训练分布和具体动作空间**，无法进行跨平台泛化或产生合理的语言输出。

- **实际价值**：
    - **为社区提供诊断工具**：Embodied4C不仅是一个排名工具，更是一个**诊断框架**。研究者可以通过细粒度日志，定位模型的具体缺陷（例如，是距离估计不准还是短期记忆差）。
    - **指引未来研究方向**：结果明确指出，未来工作应聚焦于：1) 提升模型的**空间和时序**世界建模能力；2) 开发**更具泛化性的对齐方法**，而非训练狭窄的任务专家；3) 构建**更强大的感知-动作-语言**联合表示。
    - **推动鲁棒、通用自主智能体发展**：该基准鼓励开发不依赖于特定平台、能适应动态环境、真正理解物理世界约束的智能体，对自动驾驶、无人机、服务机器人等领域的实际应用具有重要指导意义。

**总结**：Embodied4C通过其**系统性、跨平台、防过拟合、能力可分解**的设计，填补了具身AI评测的关键空白，为衡量“什么对具身智能真正重要”设立了新的标准。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: ## 论文总结

这篇论文旨在解决当前具身视觉语言导航（VLN）评估中存在的**核心问题**：现有基准测试大多局限于单一平台或特定任务，无法系统性地衡量模型在异构物理平台（具身化）下的**核心推理能力**（语义、空间、时间、物理）和**关键价值主张**（通用性、具身感知、交互性），导致模型容易过拟合，其真实泛化能力与瓶颈难以诊断。

为此，论文提出了 **Embodied4C** 这一**闭环基准测试框架**。该框架首次在自动驾驶车辆、无人机和机械臂三种异构平台上，通过约1.1K个一次性推理的视觉问答（VQA）任务和58个目标导向的导航（VLN）任务，对模型的四种核心推理能力及三项价值主张进行联合评估。其方法创新在于设计了**考试式、一次性、非模板化的任务**，并引入**领域无关查询**来探测泛化能力，从而有效防止过拟合并实现细粒度能力归因。

**主要结论**显示：在全面评估了十种前沿视觉语言模型（VLM）和四种领域专用模型后，**跨模态对齐和指令微调比模型规模更重要**；**空间和时序推理是当前可靠具身能力的主要瓶颈**；而大多数领域专用模型（VLA）表现出严重的**具身过拟合**，在非原生平台或任务上性能近乎为零，缺乏真正的可迁移推理能力。该基准为开发鲁棒、通用的具身智能体提供了新的评估标准和分析工具。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Embodied4C》的创新点分析

这篇论文针对具身视觉语言导航（VLN）的评估基准提出了系统性的创新，旨在解决现有基准在衡量**具身智能核心能力**方面的不足。其核心创新点可归纳为以下五个方面：

### 1. **首个统一的多具身、多维度、闭环评估基准**
   - **改进/不同之处**：
     - **多具身统一评估**：现有基准（如Drive4C、AerialVLN、ARNOLD）通常只专注于单一平台（如仅自动驾驶、仅无人机或仅机械臂）。Embodied4C首次在**同一个基准框架内**集成了三种异构具身平台：**自动驾驶车辆、无人机和机器人机械臂**。
     - **四维核心能力评估**：不仅评估下游任务性能，还系统性地分解并评估四个基础推理维度：**语义、空间、时间和物理理解**。这18个子类别（如距离估计、短期记忆、模型动力学）提供了更细粒度的诊断。
     - **闭环设计**：基准包含**视觉问答（VQA）** 和**视觉语言导航（VLN）** 两个互补的闭环任务。VQA评估场景理解，VLN评估将理解转化为控制行动的能力，确保模型的输出对环境有因果影响。
   - **解决的问题/优势**：
     - **解决了评估片面性问题**：避免了模型在单一平台或单一任务类型上过拟合，能够真正测试模型的**跨平台泛化能力**和**综合推理能力**。
     - **提供了细粒度诊断**：通过分解核心能力，可以精确指出模型失败的具体原因（例如，是空间定位不准还是时间序列理解差），而不仅仅是给出一个笼统的成功率。
     - **更贴近现实**：闭环评估模拟了真实世界中智能体需要持续感知、推理并行动的模式。

### 2. **引入“具身智能图灵测试”概念与价值主张评估**
   - **改进/不同之处**：
     - **明确的价值主张维度**：除了技术能力，论文首次在基准中明确设计了对智能体**价值主张**的评估，包括：
       - **通用性**：通过**领域无关查询**、平台切换和领域偏移来测试。
       - **具身感知**：通过改变传感器配置（类型、位置、模态）来测试模型对物理配置的适应能力。
       - **交互性**：通过闭环的VQA和VLN任务，测试感知、推理和控制的紧密耦合。
     - **领域无关查询**：在VQA任务中插入与当前场景无关的常识问题（如“高尔夫球中低于标准杆一杆的术语是什么？”），专门用于**检测模型过拟合和评估其鲁棒性**。
   - **解决的问题/优势**：
     - **解决了基准“捷径学习”问题**：领域无关查询能有效区分模型是在进行真正的场景推理，还是仅仅记住了任务相关的模式。这**缓解了归纳偏差**。
     - **评估更全面**：不仅问“模型能不能做任务”，还问“模型是否具备一个实用具身智能体应有的属性（如通用、适应性强、可交互）”。
     - **促进通用智能体发展**：引导研究朝向开发能适应不同硬件、理解自身物理约束的通用智能体，而非针对特定平台优化的模型。

### 3. **精心设计的防过拟合评估协议与评分机制**
   - **改进/不同之处**：
     - **开放式问答与非模板化设计**：VQA问题采用自由形式、非模板化的自然语言，避免了多项选择题带来的答案匹配偏差。
     - **混合评分策略**：
       - 对于自由形式答案，使用**基于GPT的评判员**进行连续分数（0-100）评分，评估语义对齐而非词汇重叠。
       - 对于数值答案，使用**相对偏差公式**进行精确评分。
     - **VLN“单次尝试”考试式评估**：每个VLN任务只执行一次，采用**二元通过/失败**或**基于距离的渐进式评分**。这避免了通过多次试错来刷高成功率，更能反映现实世界“只有一次机会”的严苛性。
   - **解决的问题/优势**：
     - **提高了评估的信度和效度**：减少了模型通过记忆题目模式或利用选择题结构来获得高分的机会，迫使模型展现真正的推理能力。
     - **评分更符合人类判断**：GPT评分比传统的BLEU、METEOR等指标更能捕捉语义正确性。
     - **评估结果更可靠**：“单次尝试”设计防止了评估指标的虚高，使结果更能代表模型在部署时的真实表现。

### 4. **异构传感器配置与动态环境变化**
   - **改进/不同之处**：
     - **平台内传感器多样性**：在每个具身平台内，设计了多种不同的传感器套件（如自动驾驶有跑车、SUV、货车等多种配置，摄像头、雷达、激光雷达的组合各不相同）。
     - **动态环境扰动**：在仿真中引入了天气、光照、交通密度等变化。
     - **多机器人形态**：在机械臂任务中，切换使用Franka Panda、Sawyer、UR5等不同构型的机械臂。
   - **解决的问题/优势**：
     - **测试了传感器泛化能力**：模型必须学会从不同的传感器输入中提取有效信息，而不是依赖于固定的感知前端。
     - **测试了环境鲁棒性**：确保模型的能力不依赖于特定的环境条件。
     - **强调了“具身感知”**：迫使模型理解其“身体”（传感器配置和物理平台）如何影响其感知和行动能力。

### 5. **通过大规模评估揭示的关键洞见与结论**
   - **改进/不同之处**：
     - **广泛的模型对比**：评估了10个前沿预训练VLM（如GPT-5系列、Claude、LLaMA）和4个领域专用VLA模型（如Senna, OpenVLA），进行了跨平台的统一对比。
     - **得出了超越性能排名的深刻结论**：
       1.  **对齐优于规模**：表现最好的GPT-5-mini参数并非最大，表明**跨模态对齐和指令微调比单纯的模型规模更重要**。
       2.  **空间与时间推理是主要瓶颈**：所有模型在空间（如距离、方向）和时间（如运动预测、记忆）推理上得分显著低于语义和物理推理，这直接导致了VLN性能下降。
       3.  **领域专用VLA模型泛化能力极差**：为特定任务（如驾驶、飞行）训练的模型在Embodied4C上接近零分。它们**过度拟合了其训练环境的控制先验**，一旦绕过其动作头或更换环境，语言输出或控制策略完全失效，缺乏可迁移的场景理解能力。
   - **解决的问题/优势**：
     - **为社区提供了清晰的研发方向**：明确指出未来工作应重点提升模型的**空间和时间推理能力**，并致力于开发**真正通用的、与平台解耦的具身推理模型**，而非继续深耕狭窄的领域专用策略。
     - **实证了基准的有效性**：评估结果清晰地区分出了不同能力类型的模型，验证了Embodied4C作为诊断工具的价值。

### **总结**
Embodied4C的核心创新在于其**系统性**和**诊断性**。它不仅仅是一个新的任务集合，而是构建了一个**多维度的评估生态系统**，首次将**多具身平台、核心推理能力分解、防过拟合设计、价值主张评估**融为一体。它解决了现有基准评估维度单一、易过拟合、无法衡量泛化能力的关键问题，为开发**鲁棒、通用、可解释的具身智能体**提供了一个不可或缺的衡量标尺和研发指南。其开源承诺将进一步推动该领域的标准化和可比性研究。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 评估框架与核心设计
论文提出的 **Embodied4C** 是一个**闭环评估基准**，旨在系统性地衡量视觉语言模型在具身导航中的核心能力。其评估效果并非单一任务的性能提升，而是对模型**跨平台、跨模态、跨能力维度**的综合性诊断。

### 二、 使用的数据集与仿真环境
论文**未使用传统的静态数据集**，而是构建了一个动态、交互式的仿真基准，包含三个异构的具身平台：

| 具身平台 | 仿真环境 | 核心场景与特点 |
| :--- | :--- | :--- |
| **自动驾驶车辆** | CARLA | 涵盖高速公路、城市、郊区、乡村场景，可变交通密度与天气/光照扰动。 |
| **空中无人机** | AirSim | 覆盖多种户外环境（如废弃公园、非洲草原），可变高度、云台角度和天气条件。 |
| **机器人机械臂** | RLBench | 室内工作空间，包含拾取/放置、开/关、运动学交互等任务，物体在尺寸、形状、材质上存在变化。 |

**核心数据构成**：
- **VQA任务**：共 **1,149个** 手工构建的、非模板化的视觉问答对，用于评估场景理解。
- **VLN任务**：共 **58个** 目标导向的导航/操作任务，用于评估闭环控制与决策。
- **传感器配置**：**13种**不同的传感器设置（如不同数量/类型的摄像头、雷达、LiDAR），用于测试泛化性。
- **领域外查询**：注入与当前场景无关的常识问题，用于探测模型是否过拟合。

### 三、 评价指标
评估分为两个互补的部分，采用不同的评分机制：

#### 1. **视觉问答评分**
- **自由形式答案**：使用 **GPT-5 作为评判员**，根据预测答案与真实答案的语义和上下文对齐程度，给出 `[0, 100]` 的连续分数。
- **数值答案**：采用相对偏差公式计算分数，奖励接近真实值的答案。
- **聚合分数**：分别计算语义、空间、时间、物理四个核心能力维度的平均分，再汇总为总体VQA分数。

#### 2. **视觉语言导航评分**
- **简单任务**：采用**二元通过/失败**评分（0或100分）。
- **复杂任务**：采用**分级、基于距离的**评分方案。奖励向目标的部分进展，最高50分；完全满足所有目标条件则得100分。
- **安全终止**：发生闯红灯、车道入侵、碰撞等行为时，任务立即终止，只计算已获得的分数。

#### 3. **最终分数计算**
- 每个子基准（驾驶、空中、操作）的分数是VQA和VLN分数的平均值。
- 总体 **Embodied4C 分数** 是三个子基准分数的平均值。
- **泛化分数** 单独计算，用于诊断过拟合。

### 四、 对比的基线方法
论文对 **14个模型** 进行了全面评估，分为两大类：

#### 1. **预训练通用视觉语言模型**
- **大型前沿模型**：GPT-5系列 (GPT-5, GPT-5-mini, GPT-5-nano)、GPT-4o、Claude 4.5/3.7、LLaMA 4 Maverick。
- **中小型模型**：Qwen2.5-VL-3B-Instruct、Gemma3-4B-IT、FastVLM-0.5B。

#### 2. **领域专用视觉语言动作模型**
- **自动驾驶**：Senna
- **无人机导航**：OpenFly-Agent
- **机器人操作**：OpenVLA, MolmoAct

### 五、 关键性能结果与结论
根据论文中的表2和实验结果分析，主要结论如下：

#### 1. **整体性能排名**
- **最佳模型**：**GPT-5-mini** 获得了最高的总体Embodied4C分数 (**39.59**)，其次是 **GPT-5 (36.00)**。
- **第二梯队**：Claude 4.5 (31.78)、GPT-4o (31.29)、Claude 3.7 (28.01) 和 LLaMA 4 (29.56) 表现中等。
- **落后模型**：中小型模型（Qwen2.5-VL, Gemma3）分数显著较低（~20分）。
- **领域专用VLA模型彻底失败**：Senna、OpenFly-Agent、OpenVLA、MolmoAct 的总体分数接近 **0**。它们在VQA任务中无法产生连贯的语言输出，在VLN任务中无法将其控制策略迁移到新的仿真环境。

#### 2. **核心发现与结论**
- **对齐优于规模**：**GPT-5-mini** 的表现超越了更大的 **GPT-5**，表明**高效的跨模态对齐和校准的令牌交互比参数量更重要**。大模型若无强化的 grounding，其空间和时间推理会不稳定，导致VLN性能下降。
- **指令调优不可或缺**：缺乏指令调优的模型（如FastVLM和领域专用VLA）无法遵循任务结构和接口约定。
- **空间与时间推理是主要瓶颈**：即使是表现最好的模型，在**距离估计、方向判断、计数和长期记忆**等任务上得分也最低。这表明当前训练流程未能充分强制执行几何结构或时间连续性。
- **领域专用模型无法泛化**：所有测试的VLA模型在其原生训练域外都表现近乎为零。它们优化的是**狭窄的控制先验**，而非可迁移的场景理解和推理能力。
- **泛化能力**：大多数通用VLM在领域外问题上得分很高（如Claude 4.5达98.53），显示出强大的鲁棒性。而VLA模型和FastVLM在此项上急剧退化。

#### 3. **跨平台性能分析**
- **自动驾驶**：GPT-5-mini 领先 (40.11)。专用驾驶模型Senna表现不佳，表明其严重过拟合于其训练数据分布。
- **空中导航**：GPT-5-mini 同样领先 (40.21)。专用无人机模型OpenFly-Agent完全崩溃，因其过拟合于特定的仿真器动力学。
- **机器人操作**：GPT-5 (38.64) 和 GPT-5-mini (38.44) 最高。专用操作模型OpenVLA和MolmoAct几乎得零分。

### 六、 总结
Embodied4C的评估结果表明，当前**通用VLM展现出一定的泛化能力和具身意识**，但**空间和时间推理的缺陷直接导致了闭环执行能力的退化**。相反，**领域专用的VLA模型虽然在其特定任务上可能表现良好，但缺乏基本的、可迁移的场景理解能力，无法适应新的平台或环境**。这项研究强调，未来构建可靠的具身智能体需要**持续的世界建模和物理 grounded 的表征**，而非仅仅增加模型规模或训练狭窄的动作头。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.18028v1)
- [HTML 版本](https://arxiv.org/html/2512.18028v1)
