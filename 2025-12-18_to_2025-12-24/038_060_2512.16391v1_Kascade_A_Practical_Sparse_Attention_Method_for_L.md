# Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference

**相关性评分**: 6.0/10

**排名**: #38


---


## 基本信息

- **arXiv ID**: [2512.16391v1](https://arxiv.org/abs/2512.16391v1)
- **发布时间**: 2025-12-18T10:37:14Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee

## 关键词

Inference Efficiency, Inference Acceleration, Lightweight Architecture

## 一句话总结

Kascade是一种无需训练的稀疏注意力方法，通过跨层重用Top-k索引，显著提升长上下文LLM推理效率，适用于边缘部署场景。

## 摘要

Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.16391v1)
- [HTML 版本](https://arxiv.org/html/2512.16391v1)
