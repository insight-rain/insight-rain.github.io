# Collaborative Edge-to-Server Inference for Vision-Language Models

**相关性评分**: 6.0/10

**排名**: #37


---


## 基本信息

- **arXiv ID**: [2512.16349v1](https://arxiv.org/abs/2512.16349v1)
- **发布时间**: 2025-12-18T09:38:18Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Soochang Song, Yongjune Kim

## 关键词

Vision-Language Models, Edge Deployment, Inference Efficiency, Collaborative Inference, Communication Cost Reduction

## 一句话总结

该论文提出了一种协作式边缘到服务器推理框架，通过选择性重传策略减少视觉语言模型的通信成本，同时保持推理精度。

## 摘要

We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.16349v1)
- [HTML 版本](https://arxiv.org/html/2512.16349v1)
