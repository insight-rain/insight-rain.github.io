# Vidarc: Embodied Video Diffusion Model for Closed-loop Control

**相关性评分**: 8.0/10

**排名**: #5


---


## 基本信息

- **arXiv ID**: [2512.17661v1](https://arxiv.org/abs/2512.17661v1)
- **发布时间**: 2025-12-19T15:04:24Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

Vidarc是一种基于视频扩散的机器人闭环控制模型，通过掩码逆动力学和自回归生成实现高效推理和低延迟，适用于边缘部署。

## 摘要

Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.

## 详细分析

## 论文《Vidarc: Embodied Video Diffusion Model for Closed-loop Control》详细摘要

**1. 研究背景和动机**
机器人臂在数据稀缺环境下的操作是一项极具挑战性的任务，因其涉及复杂的本体动力学和多样化的场景。尽管基于视频的方法通过在海量互联网视频数据上预训练，在捕捉和迁移时空与物理交互方面展现出巨大潜力，但这些方法通常**未针对具身智能特有的闭环控制进行优化**，普遍存在**高延迟**和**对机器人本体动态建模不足**的问题。因此，亟需一种能够实现**快速、精准、可泛化**的闭环控制新方法。

**2. 核心方法和技术创新**
本文提出了 **Vidarc**，一种创新的自回归具身视频扩散模型，其核心创新在于：
- **闭环控制架构**：通过**自回归视频生成**结合**环境反馈重填充**机制，将实时观测融入生成过程，实现了真正的闭环控制，有效纠正累积误差。
- **低延迟推理**：采用**KV缓存**和**分块重填充**技术，大幅减少了推理时的计算开销，显著降低了延迟。
- **本体感知训练**：引入**掩码逆动力学模型**，并利用其学习到的**动作相关区域掩码**对视频扩散损失进行重加权，形成了**本体感知扩散损失**，使模型更关注对动作预测至关重要的机器人本体区域，提升了生成视频的“可执行性”。

**3. 主要实验结果**
在约一百万跨本体操作片段上预训练后，Vidarc在仿真和真实机器人平台上均取得了卓越性能：
- **成功率显著提升**：在真实世界部署中，成功率比当前最优基线（Vidar和Pi0.5）**至少高出15%**。
- **延迟大幅降低**：推理延迟相比Vidar降低了**91%**，向实时控制迈出关键一步。
- **强大的泛化与纠错能力**：在未见过的机器人平台和动态变化环境中（如任务执行时物体被移动），展现出优异的**泛化性和实时错误纠正能力**。
- **消融实验验证**：实验证实了闭环控制机制和本体感知损失对性能提升的关键作用。

**4. 研究意义和价值**
Vidarc为数据稀缺下的机器人学习提供了一个高效、可泛化的新范式。它**首次将自回归视频扩散模型与闭环控制、低延迟推理及本体感知训练紧密结合**，解决了现有视频世界模型在具身控制中延迟高、对本体动态建模弱的核心痛点。这项工作推动了视频基础模型向实时、鲁棒的具身智能系统发展，对实现**可扩展、自适应**的复杂环境机器人操作具有重要的理论和应用价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：Vidarc

### **一、 论文旨在解决的核心问题**
论文针对**数据稀缺环境下机器人臂的精确闭环控制**这一核心挑战，具体分解为三个关键难题：

1.  **实时性与闭环控制缺失**：现有基于视频的机器人学习方法（如Vidar）多为**开环预测**，推理延迟高，无法在运行时根据环境反馈实时调整动作，导致在动态环境中鲁棒性差。
2.  **对机器人本体动态的建模不足**：通用视频生成模型将所有视觉特征平等对待，未能**重点关注与机器人动作高度相关的区域**（如机械臂、末端执行器）。这导致生成的视频在关键运动区域存在伪影或物理不一致，进而影响下游逆动力学模型的动作解码精度。
3.  **跨平台泛化与数据效率**：在数据稀缺的新平台、新任务上，模型需要快速适应并保持高性能，同时避免因收集海量演示数据而产生的高昂成本。

### **二、 核心创新点与技术方案**
Vidarc 通过一个集成框架解决了上述问题，其创新主要体现在以下三个紧密耦合的方面：

#### **1. 自回归闭环视频生成与推理加速**
*   **技术方案**：
    *   **因果自回归训练**：采用 `CausVid` 方法，将视频扩散模型改造为**帧-by-帧的因果生成模型**。在生成每一帧时，模型只能关注之前已生成的（或真实的）帧，这符合机器人感知-决策的时序逻辑。
    *   **闭环推理与重填充机制**：在推理时，**关键创新在于使用从真实环境收集的最新观测**，而非模型自身生成的预测，来“重填充”自回归生成过程的上下文。这通过算法1中的 `chunk_prefill` 步骤实现，**有效桥接了训练与推理的差距，防止了误差累积**，实现了真正的闭环控制。
    *   **KV缓存加速**：借鉴大语言模型推理优化，引入**键值（KV）缓存**来避免在生成序列帧时重复计算已生成帧的特征，并结合上述重填充机制，大幅降低了推理延迟。

#### **2. 本体感知的扩散损失**
*   **技术方案**：
    *   利用**掩码逆动力学模型** 学习到的**动作相关掩码**。该掩码能自动识别图像中与机器人动作预测最相关的像素区域（如机械臂）。
    *   将此掩码作为权重，**重新加权视频扩散模型的训练损失**（公式7）。这使得模型在训练时更专注于学习机器人本体和其交互区域的精确动态，生成“更具可操作性”的视频，从而提升了后续动作解码的准确性。

#### **3. 两阶段训练与高效架构**
*   **技术方案**：
    *   **大规模跨本体预训练**：在约100万条包含不同机器人平台和人类操作的视频片段上进行预训练，学习通用的时空交互先验。
    *   **轻量级微调**：在新平台上，只需少量演示数据（如20个任务episode）进行微调，主要调整模型适应新的相机视角和机器人外观。**掩码逆动力学模型在此过程中提供了关键的、与本体动态相关的归纳偏置**。
    *   **模型分解**：延续 `π = G ∘ I` 的范式，将策略分解为**视频生成模型（G）** 和**逆动力学模型（I）**。这种解耦允许G利用海量互联网视频数据预训练，而I负责处理机器人特定的动作空间，兼顾了通用性与特异性。

### **三、 解决方案总结**
Vidarc 的解决方案可以概括为：**一个以“掩码”为桥梁、以“闭环”为核心、以“缓存”为加速器的自回归视频世界模型**。

1.  **用“掩码”连接感知与动作**：通过MIDM学习到的动作掩码，一方面提升了逆动力学模型的动作预测鲁棒性（过滤背景干扰），另一方面通过**本体感知损失**引导视频生成模型聚焦关键区域，解决了“建模不足”问题。
2.  **用“闭环”实现实时纠偏**：通过**因果自回归架构**和**基于真实观测的重填充机制**，将环境反馈无缝接入生成过程，实现了低延迟的闭环控制，解决了“实时性”和“开环”问题。
3.  **用“预训练+微调”保证泛化与效率**：大规模跨本体预训练提供了强大的物理和交互先验，结合掩码机制和少量数据微调，使其能快速适应新平台，解决了“数据稀缺”和“泛化”问题。

**最终价值**：Vidarc 在保持视频世界模型强大生成和物理推理能力的同时，首次将其成功应用于**低延迟、高精度的实时机器人闭环控制**，在仿真和真实世界任务中均取得了显著优于前沿基线（Vidar, Pi0.5）的成功率，并将延迟降低了91%，为构建可扩展、自适应、数据高效的通用机器人系统提供了新的方向。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决数据稀缺环境下机器人臂操控任务中，现有视频生成模型难以实现**低延迟、高精度闭环控制**的核心问题。为此，作者提出了 **Vidarc** 框架，它由一个**自回归视频扩散模型**和一个**掩码逆动力学模型**构成。其核心创新在于：通过**KV缓存和重预填充机制**将实时环境反馈融入自回归视频生成，实现快速闭环；并利用逆动力学模型学习到的**动作相关掩码**来重新加权扩散损失，使模型更关注机器人本体动态，从而生成更具可执行性的视频。实验表明，该方法在仿真和真实世界任务中取得了**显著更高的成功率**（例如在真实部署中比基线高出至少15%），同时将推理延迟降低了**91%**，并展现出强大的跨平台泛化与实时纠错能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## Vidarc 论文创新点分析

这篇论文提出了 **Vidarc** 框架，旨在解决数据稀缺环境下机器人臂操控任务中，现有视频基础模型在**实时性、闭环控制和具身适应性**方面的核心瓶颈。其创新点明确且具有系统性，具体如下：

### 1. **创新的模型架构：自回归具身视频扩散模型 + 掩码逆动力学模型**
   - **改进/不同之处**：
     - **以往方法**：多数视频世界模型（如Vidar）采用**双向扩散**或**非因果生成**架构，生成整个视频序列后再解码动作。这导致高延迟，且无法在生成过程中融入实时环境反馈。
     - **Vidarc**：将**因果自回归视频生成**与**掩码逆动力学模型**深度结合。视频模型以自回归方式逐帧/逐块预测未来观测，而逆动力学模型则通过一个**可学习的掩码**聚焦于机器人臂等动作相关区域来解码动作。
   - **解决的问题/带来的优势**：
     - **实现低延迟闭环控制**：自回归生成天然支持序列预测，结合下文将提到的重填充机制，使得模型能基于最新环境观测快速生成下一帧，为实时控制奠定基础。
     - **增强动作预测的鲁棒性**：掩码逆动力学模型能自动忽略视觉干扰（如复杂背景），只关注与机器人动力学相关的关键像素，提升了动作解码在陌生环境中的准确性和泛化能力。

### 2. **闭环控制机制：基于KV缓存和重填充的环境反馈集成**
   - **改进/不同之处**：
     - **以往方法**：通常是**开环**的。模型基于初始观测生成整个动作序列或视频，执行期间不根据实际环境状态进行调整。一旦出现偏差，错误会持续累积导致任务失败。
     - **Vidarc**：在推理时引入 **“重填充”机制**。具体流程为：1) 用历史真实观测初始化KV缓存；2) 自回归生成未来若干帧的预测；3) 执行解码出的动作；4) **用执行后获取的新真实观测，替换掉缓存中对应的旧预测帧**，然后继续生成。这形成了一个感知-行动-修正的闭环。
   - **解决的问题/带来的优势**：
     - **解决误差累积问题**：通过不断用真实观测“修正”模型的想象轨迹，防止了自回归预测中常见的漂移和误差放大。
     - **赋予实时纠错能力**：当环境发生动态变化（如物体被意外移动）时，系统能通过最新的观测及时调整后续计划，如图4所示，从而在动态场景中仍能保持较高成功率。
     - **降低计算延迟**：重填充只更新缓存中最新部分，避免了为整个历史序列重新计算KV缓存，显著减少了“预填充”阶段的耗时。

### 3. **具身感知损失函数：利用动作掩码重新加权扩散损失**
   - **改进/不同之处**：
     - **以往方法**：标准视频扩散模型的训练损失对所有像素一视同仁，平等地重建整个画面。这可能导致模型对背景等无关细节渲染得很好，但对**机器人本体（如机械臂）的运动学和外观**建模不精确，产生伪影（如图3）。
     - **Vidarc**：提出 **“具身感知损失”**。利用从掩码逆动力学模型中学习到的**动作相关区域掩码**，在训练视频扩散模型时，**加大这些关键区域（如机械臂）的损失权重**。公式为 `(1 + η * Mask) ⊙ Loss`。
   - **解决的问题/带来的优势**：
     - **提升视频预测的“可行动性”**：迫使视频模型更精准地生成与机器人动作执行直接相关的视觉特征，使生成的视频在物理和运动学上更一致。
     - **桥接视频生成与动作解码**：使视频模型的优化目标与下游逆动力学模型的需求对齐，生成对动作解码更友好的视频表示，从而间接提升整体控制性能。消融实验（表1）证实了该损失的有效性。

### 4. **系统级优化：针对实时推理的KV缓存加速与轻量采样**
   - **改进/不同之处**：
     - **以往方法**：像Vidar这样的模型，由于非因果结构和大量采样步骤，**端到端延迟极高**（表3中达34.3秒），完全无法用于需要实时响应的机器人控制。
     - **Vidarc**：充分利用**自回归架构的KV缓存特性**，在生成序列时复用已计算的特征，避免重复计算。同时，在保证性能的前提下，**大幅减少推理时的扩散采样步数**（真实世界实验从20步降至5步）。
   - **解决的问题/带来的优势**：
     - **实现数量级的延迟降低**：论文报告相比Vidar**延迟降低了91%**（从34.3秒降至3.03秒），使其向实时控制（与Pi0.5等VLA模型差距大幅缩小）迈出了关键一步。
     - **保持高性能**：通过闭环纠错和具身感知损失，即使使用更少的采样步数（意味着更快的生成速度），模型仍能保持甚至超越基线方法的任务成功率。

### 总结
Vidarc的核心创新在于**系统性地重构了视频模型用于机器人控制的范式**：
1.  **架构上**，从开环、非因果转向闭环、自回归。
2.  **训练上**，从通用视觉重建转向具身感知、任务驱动的重建。
3.  **推理上**，从高延迟批量生成转向低延迟、可交互的序列生成。

这些创新**共同解决**了将互联网规模预训练的视频先验知识，高效、鲁棒地应用于**数据稀缺、需要实时响应和强泛化能力的真实世界机器人控制**这一核心挑战。其实验结果（成功率提升15%以上，延迟降低91%）强有力地支撑了这些创新点的有效性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 核心评估效果
论文通过大量实验证明，**Vidarc** 在**数据稀缺**的机器人操作任务中，实现了**高性能、低延迟的闭环控制**，并展现出强大的**泛化与纠错能力**。

### 二、 使用的数据集
论文采用**大规模预训练 + 小样本微调**的策略。

| 数据集类型 | 数据集名称 | 规模/用途 | 来源/特点 |
| :--- | :--- | :--- | :--- |
| **预训练数据集** | Egodex, Agibot, RDT, RoboMind | 总计约 **100万** 个视频片段 | 来自人类和多种机器人平台（如Aloha, Franka），**公开可用**，用于学习通用时空动态。 |
| **微调数据集** | RoboTwin (仿真) | 20个任务 × 50条轨迹 = **1000条** | 在Agilex Aloha平台上收集，用于仿真环境评估。 |
| **微调数据集** | Vidarc (真实世界) | 219个任务，总计 **2307条** 高质量轨迹 | 在目标Aloha机器人平台上收集，用于真实世界部署评估。**相机和机械臂配置在预训练中未见过**。 |

### 三、 评价指标
1.  **任务成功率**：在固定次数的尝试中，成功完成指定任务的比率。这是评估控制性能的**核心指标**。
2.  **推理延迟**：
    *   **延迟**：从获取观察到输出下一个动作块所需的时间。**关键实时性指标**。
    *   **端到端生成成本**：生成整个任务所需所有视频帧/动作的总时间。
3.  **泛化能力**：在**未见过的任务、物体、环境**（包括动态干扰）上的成功率。
4.  **纠错能力**：通过案例研究，定性展示模型在出现预测偏差时，如何利用实时反馈进行修正。

### 四、 对比的基线方法
论文与两个当前**最先进的代表性方法**进行了全面对比：

1.  **Vidar**：基于视频扩散模型和掩码逆动力学模型的方法。代表了**视频世界模型**的最新进展。
2.  **Pi0.5**：一个强大的**视觉-语言-动作模型**。代表了**大规模机器人演示数据训练**的VLA模型方向。

### 五、 关键性能提升与结论
#### 1. 任务成功率（仿真与真实世界）
*   **仿真 (RoboTwin, 14个任务)**：
    *   **Vidarc**: **80.7%** (平均成功率)
    *   **Vidar**: 71.1%
    *   **Pi0.5**: 52.9%
    *   **结论**：Vidarc比Vidar和Pi0.5分别高出约 **9.6%** 和 **27.8%**。在需要双手协作的复杂任务（如“递麦克风”）上优势尤为明显（Vidarc: 65%， Vidar: 0%）。
*   **真实世界 (3种场景)**：
    *   **平均成功率**：Vidarc (**56%**) > Pi0.5 (41%) > Vidar (39%)。
    *   **动态场景**（人为移动目标物体）：Vidarc (**40%**) 显著优于 Vidar (**0%**)，证明了其**闭环纠错能力**。
    *   **结论**：Vidarc在真实、动态环境中实现了**最先进的成功率**，并具备强大的环境适应能力。

#### 2. 推理速度与延迟
*   **关键对比**：在生成64帧视频（6.4秒任务）的设置下：
    *   **延迟**：Vidarc (**3.03秒**) 比 Vidar (**34.3秒**) 降低了 **91%**，大幅逼近传统VLA方法Pi0.5 (0.482秒)。
    *   **端到端成本**：Vidarc (24.2秒) 也显著低于Vidar (34.3秒)。
    *   **结论**：通过**因果自回归生成**和**KV缓存重填充**机制，Vidarc在保持视频模型高保真度的同时，**极大降低了延迟**，使其更接近实时控制的要求。

#### 3. 泛化与鲁棒性
*   **零样本泛化**：在**未见过的任务和物体**上，Vidarc (56%) 的成功率显著高于Vidar (44%) 和 Pi0.5 (28%)。
*   **结论**：得益于大规模跨具身预训练和具身感知损失，Vidarc学习到了可迁移的物理交互先验，**泛化能力更强**。

#### 4. 消融实验结论
*   **具身感知损失**：移除后，仿真平均成功率从80.7%降至74.6%。证明该损失有效引导模型关注机器人相关区域，提升控制精度。
*   **闭环控制（重填充机制）**：移除后，成功率从80.7%降至66.8%。证明**利用真实环境反馈进行纠错**对于长时程任务至关重要。

### 总结
论文给出了**明确且全面的定量结果**。实验表明，Vidarc在**成功率、延迟、泛化能力和纠错鲁棒性**等多个关键指标上均**显著优于当前最先进的基线方法**，成功地将视频生成模型的强大先验与机器人闭环控制的实时性、精确性要求相结合，为数据稀缺下的机器人学习提供了一个高效且可扩展的新框架。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.17661v1)
- [HTML 版本](https://arxiv.org/html/2512.17661v1)
