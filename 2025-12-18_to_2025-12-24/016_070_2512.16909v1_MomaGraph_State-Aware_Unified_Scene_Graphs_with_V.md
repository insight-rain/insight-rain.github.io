# MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning

**相关性评分**: 7.0/10

**排名**: #16


---


## 基本信息

- **arXiv ID**: [2512.16909v1](https://arxiv.org/abs/2512.16909v1)
- **发布时间**: 2025-12-18T18:59:03Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

MomaGraph提出了一种用于具身任务规划的视觉-语言模型，通过统一场景图表示提升机器人推理能力，但未明确强调推理效率或轻量化架构。

## 摘要

Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

## 详细分析

## 论文摘要：MomaGraph

### 1. 研究背景和动机
家庭环境中的移动操作机器人需要同时具备导航和操作能力，这要求一种紧凑、语义丰富的场景表示，以捕捉物体的**位置**、**功能**以及**可交互部件**。场景图是一种自然的选择，但现有方法通常将空间关系与功能关系分离，将场景视为静态快照而忽略了物体状态和动态更新，并且未能突出与当前任务最相关的信息。为了克服这些局限，本文提出了**MomaGraph**，一种为具身智能体设计的统一场景表示。

### 2. 核心方法和技术创新
本文的核心贡献是一个完整的系统，包含三个关键部分：
*   **MomaGraph表示**：首次统一了空间关系与功能关系，并引入了部件级交互节点，构建了紧凑、动态且与任务对齐的结构化知识表示。
*   **MomaGraph-Scenes数据集与MomaGraph-Bench评测基准**：构建了首个大规模、任务驱动的家庭环境场景图数据集，并设计了一个系统性的评测套件，涵盖从高层规划到细粒度场景理解的六种推理能力。
*   **MomaGraph-R1模型**：基于Qwen2.5-VL-7B，采用**强化学习（DAPO算法）** 进行训练，并设计了基于图对齐的奖励函数。该模型能够预测任务导向的场景图，并在 **“先构图，后规划”** 的框架下作为零样本任务规划器使用，显著提升了推理的有效性和可解释性。

### 3. 主要实验结果
*   **基准测试**：在自建的MomaGraph-Bench上，MomaGraph-R1在**构图后规划**的设置下达到了**71.6%** 的整体准确率，比其基础模型（Qwen2.5-VL-7B）提升了**11.4%**，在所有开源模型中达到最优。
*   **有效性验证**：实验证明，统一的空间-功能图表示显著优于仅使用单一关系类型的图。同时，“先构图，后规划”的策略在所有测试模型上都比直接规划更可靠、更准确。
*   **泛化能力**：模型在公开的视觉对应基准（BLINK）和真实机器人实验中均表现出强大的泛化能力，在未见过的家庭场景中成功完成了开柜子、开电视等复杂任务。

### 4. 研究意义和价值
本研究为具身智能领域提供了**一个更全面、更实用的场景表示范式**。通过将空间布局与功能交互统一起来，MomaGraph使机器人能够像人类一样进行任务导向的感知和推理。其配套的数据集、基准和开源模型为社区提供了宝贵的研究资源，有望推动空间视觉语言模型与机器人学社区的深度融合，最终实现更智能、更自适应的具身智能体。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：MomaGraph

### **一、 论文旨在解决的核心问题**
论文指出，现有**场景图**作为机器人环境感知和任务规划的表示方法存在三大关键缺陷：
1.  **关系割裂**：现有方法通常只编码单一类型的关系（要么是**空间关系**，如“左边”，要么是**功能关系**，如“控制”），导致场景表示不完整，无法同时支持导航（需要空间信息）和操作（需要功能信息）。
2.  **静态与粗粒度**：大多数场景图是静态快照，无法适应环境中物体**状态变化**（如门从关到开），且缺乏对**可交互部件**（如把手、旋钮）的细粒度建模。
3.  **与任务脱节**：构建的场景图是通用的，没有根据当前**具体任务指令**进行聚焦和优化，导致规划效率低下，包含大量无关信息。

### **二、 核心技术创新点**
论文提出了一套完整的解决方案 `MomaGraph`，其创新点体现在**表示、数据、模型、评估**四个层面：

#### **1. 新颖的场景图表示：MomaGraph**
- **统一的空间-功能关系**：首次在同一个图结构中**同时编码**空间关系（9种，如 `left_of`, `close`）和功能关系（6种，如 `control`, `open/close`）。
- **状态感知与动态更新**：场景图能根据机器人的交互观察（如转动旋钮后炉灶点火）进行**动态更新**，从初始的模糊假设（多个旋钮对应一个炉灶）演变为确定、精确的表示（确认具体哪个旋钮控制该炉灶）。
- **任务导向与部件级节点**：根据自然语言任务指令（如“打开冰箱”），构建**最小且充分**的任务相关子图。图中节点不仅包含物体，还包含完成任务必须交互的**部件**（如冰箱的`把手`）。
- **结果**：形成了一个**紧凑、动态、任务对齐**的结构化场景表示，为具身智能体提供了更全面的感知和规划基础。

#### **2. 首个大规模数据集与评估基准**
- **`MomaGraph-Scenes` 数据集**：首个大规模、细粒度标注的**任务驱动型场景图**数据集。包含1050个子图、6278张多视角图像，覆盖350多个家庭场景和93种任务指令，**联合标注**了空间与功能关系及部件级交互元素。
- **`MomaGraph-Bench` 评估套件**：首个系统化评估场景图对任务规划影响的基准。它设计了**六大核心推理能力**（动作序列、空间、功能、前提与效果、目标分解、视觉对应）和**四个难度层级**（T1单步到T4动态重规划），全面衡量模型从细粒度理解到高层规划的能力。

#### **3. 基于强化学习的视觉-语言模型：MomaGraph-R1**
- **“构图再规划”范式**：提出 `Graph-then-Plan` 框架。模型不是直接从图像生成规划，而是**先**根据任务指令和多视角观察生成一个结构化的 `MomaGraph` 场景图，**然后**基于此图进行任务规划。这提高了推理的可靠性和可解释性。
- **基于图的奖励函数进行RL训练**：在 `Qwen2.5-VL-7B` 基础上，使用 **DAPO 强化学习算法** 在 `MomaGraph-Scenes` 上训练。其核心是精心设计的**图对齐奖励函数**，从**动作类型预测、边关系匹配、节点完整性**三个方面引导模型生成准确、任务相关的场景图。
- **零样本任务规划器**：训练后的 `MomaGraph-R1` 既能预测场景图，又能作为零样本任务规划器，在 `Graph-then-Plan` 框架下工作。

### **三、 解决方案的验证与效果**
论文通过严谨的实验验证了其解决方案的有效性：
1.  **基准测试领先**：在 `MomaGraph-Bench` 上，`MomaGraph-R1` 在 **构图再规划** 模式下达到 **71.6%** 的整体准确率，比最佳开源基线提升 **+11.4%**，性能与闭源大模型（GPT-5, Claude）相当。
2.  **关键结论验证**：
    - **构图再规划优于直接规划**：实验证明，即使对于强大的闭源模型，先构图再规划的策略也能显著提高规划准确性和鲁棒性。
    - **统一关系优于单一关系**：消融实验表明，仅包含空间或功能关系的场景图在规划任务上表现均不如统一的关系图。
3.  **泛化与迁移能力**：
    - 在公开的视觉对应基准 **BLINK** 上取得开源模型最佳性能，证明其提升了**多视角一致性感知**能力。
    - **真实机器人实验**：在全新的家庭场景中，部署在移动操作机器人上，成功完成了“开柜门”、“关灯”等需要空间、功能和部件级推理的任务，整体复杂任务成功率达 **70%**。

### **四、 实际价值与贡献总结**
- **理论/方法贡献**：提出了一种更符合具身智能需求的**新一代场景图表示范式**，将空间、功能、动态、任务焦点融为一体。
- **资源贡献**：发布了**首个**统一空间-功能关系的场景图**数据集**和**系统化评估基准**，为社区提供了重要的研究和评测基础。
- **工程/应用贡献**：证明了通过**强化学习**和**结构化中间表示**，可以显著提升开源VLM的场景理解和任务规划能力，使其在机器人应用中达到实用水平，为开发更智能、自适应的家庭服务机器人提供了可行的技术路径。

**核心思想**：让机器像人一样，根据当前任务“有选择地、动态地”理解环境——先看全局布局，再聚焦关键物体和它们之间的操作关系，并随着交互更新认知。`MomaGraph` 正是实现这一认知过程的结构化“思维导图”。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决现有场景图在具身任务规划中的三个核心局限：**关系类型单一**（仅空间或功能）、**静态表示**（无法适应动态变化）和**缺乏任务相关性**。为此，论文提出了 **MomaGraph**，一个**统一的空间-功能关系场景图表示**，并引入了**部件级交互节点**以提供更细粒度的结构化知识。为支撑该表示，论文贡献了首个大规模、任务驱动的场景图数据集 **MomaGraph-Scenes** 和系统评估套件 **MomaGraph-Bench**。基于此，论文开发了 **MomaGraph-R1**，一个通过强化学习训练的 7B 视觉语言模型，它能够在 **“先构图，后规划”** 的框架下，从多视角图像和任务指令中**零样本生成任务导向的场景图并执行规划**。实验表明，该方法在基准测试中达到了开源模型的最高水平（71.6% 准确率，比最佳基线提升 11.4%），并能有效泛化至公开基准和真实机器人实验，证明了其统一场景表示在提升规划准确性和鲁棒性方面的显著优势。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning》针对具身智能体在家庭环境中的任务规划问题，提出了一套完整的解决方案。其核心创新点明确且具有递进关系，具体如下：

---

### 1. **提出了统一空间-功能关系的场景图表示（MomaGraph）**
- **改进/不同之处**：
    - **以往方法**：现有的场景图通常只编码单一类型的关系，要么是**空间关系**（如`left_of`, `close`），用于描述物体布局；要么是**功能关系**（如`control`, `activate`），用于描述物体间的交互与操控。两者被割裂研究。
    - **本文方法**：**MomaGraph首次将空间关系与功能关系统一在同一个图结构中**。图中的每条边同时包含空间关系（如方向、距离）和功能关系（如控制、调整）标签。
- **解决的问题与优势**：
    - **解决不完整、不可操作的表征问题**：单一关系图无法同时回答“物体在哪？”和“物体怎么用？”这两个具身任务规划必需的问题。例如，只知道遥控器在电视前面（空间）不够，还必须知道它能“控制”电视（功能）。
    - **提供更全面的规划基础**：统一的表征使智能体能够同时推理空间可达性约束和功能交互逻辑，为生成可行、合理的动作序列提供了更坚实的基础。

### 2. **引入了状态感知的动态场景图更新机制**
- **改进/不同之处**：
    - **以往方法**：大多数场景图研究将场景视为**静态快照**，无法处理物体状态变化（如门从关到开）或由交互引发的动态关系变化。
    - **本文方法**：论文明确将场景图定义为随时间步 `t` 变化的动态结构 `𝒢_𝒯^(t)`。设计了一个更新函数 `𝒰`，允许智能体根据执行动作后的新观察 `s_(t+1)`，来**修正图中不确定的假设关系**（如从多个可能的旋钮中确认哪一个真正控制了目标炉灶）。
- **解决的问题与优势**：
    - **解决现实环境中的歧义性问题**：在存在多个同类物体的场景中（如多个旋钮），仅凭外观无法确定功能对应关系。通过观察动作引发的状态变化来更新图，可以**消解歧义，建立唯一、可靠的对象对应关系**。
    - **实现自适应与环境共变**：使场景图成为一个**活的、可演化的**世界模型，能够反映智能体交互后环境的最新状态，支持更长期的序列任务规划。

### 3. **构建了首个大规模、任务驱动的统一场景图数据集与评测基准**
- **改进/不同之处**：
    - **以往资源**：现有数据集（如ConceptGraphs, OpenFunGraph）要么只标注空间关系，要么只标注功能关系，且缺乏与具体任务指令的紧密对齐。
    - **本文资源**：
        1. **MomaGraph-Scenes数据集**：首个**联合标注**空间与功能关系、并包含**部件级交互节点**（如把手、按钮）的大规模家庭环境数据集。它包含多视角图像、执行动作、交互部件及**任务对齐**的场景图标注。
        2. **MomaGraph-Bench评测基准**：首个系统化评估场景图对规划性能影响的基准，涵盖**六大推理能力**（动作序列、空间、功能、前提与效果、目标分解、视觉对应）和**四个难度层级**。
- **解决的问题与优势**：
    - **解决数据与评估缺失问题**：此前缺乏能够支撑“统一关系”表征学习和进行系统化评估的公共资源。该工作填补了这一空白。
    - **推动领域标准化与可比性**：为社区提供了高质量的训练数据和**系统化的评估工具**，使得不同方法在“场景理解如何提升任务规划”这一核心问题上有了公平、全面的比较标准。

### 4. **提出了“先构图后规划”范式，并训练了统一的VLM模型（MomaGraph-R1）**
- **改进/不同之处**：
    - **以往方法**：
        1. **直接规划**：让VLM直接从图像和指令生成动作计划，容易因视觉噪声和缺乏结构化推理而产生错误或遗漏步骤。
        2. **分离的构图与规划**：一些方法假设已有可靠场景图，或将构图与规划作为两个独立模块。
    - **本文方法**：
        1. **Graph-then-Plan框架**：强制模型先生成一个**任务特定的场景图**作为中间结构化表示，再基于此图进行高层规划。这相当于在原始感知和最终决策之间加入了一个**可解释的推理层**。
        2. **MomaGraph-R1模型**：基于Qwen2.5-VL-7B，使用**强化学习（DAPO算法）** 进行训练，其奖励函数专门针对生成**准确、任务相关的场景图**而设计（评估动作类型、边关系、节点完整性）。
- **解决的问题与优势**：
    - **提升规划准确性与鲁棒性**：结构化场景图作为“思维链”，迫使模型进行更深入的时空和功能推理，显著减少了直接规划中的幻觉和逻辑错误（论文实验显示，即使对于GPT-5等闭源模型，引入该框架也能提升性能）。
    - **实现零样本任务规划**：单个模型**同时具备构图和规划能力**，无需为每个新任务进行微调，展现了强大的泛化能力。
    - **提供可解释性**：生成的场景图为规划决策提供了明确的依据，使决策过程更透明、可调试。

### 5. **验证了统一表征与RL训练的有效性，并在真实机器人上成功部署**
- **改进/不同之处**：
    - **实验验证**：论文不仅进行了广泛的基准测试，还通过消融实验证明了**统一关系图优于单一关系图**，**RL训练优于纯监督微调（SFT）和上下文学习（ICL）**。
    - **真实系统部署**：在RobotEra Q5双手机器人平台上，在**未见过的家庭场景**中成功完成了开柜子、开微波炉、开电视、关灯等需要空间、功能和部件级推理的任务。
- **解决的问题与优势**：
    - **证实核心假设**：通过严谨的实验回答了“为什么需要统一关系？”和“为什么需要RL学习构图？”这两个核心问题，为方法设计提供了坚实依据。
    - **证明实用性与泛化性**：将方法成功迁移到存在感知噪声、控制不确定性的真实物理世界，证明了其**实际应用价值**和从仿真到现实的**强泛化能力**。

---

## 总结
本文的创新是一个**体系化**的贡献：从**表征定义**（MomaGraph）出发，到**数据与评估体系**（Scenes & Bench）的构建，再到**学习方法与框架**（RL训练的Graph-then-Plan）的创新，最后通过**全面实验与真实部署**完成闭环验证。其最核心的突破在于**打破了空间与功能表征的壁垒**，并通过动态更新和结构化推理，为具身智能体提供了更接近人类“任务导向感知”能力的紧凑、丰富且可操作的世界模型。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

### 一、 主要评估数据集与基准
论文构建并使用了以下核心数据集和评估基准：

1.  **MomaGraph-Bench**（核心评估基准）
    *   **性质**：作者提出的综合性评估套件，旨在系统评估**六种核心推理能力**（动作序列推理、空间推理、物体功能推理、前提与效果推理、目标分解、视觉对应）。
    *   **形式**：多选视觉问答任务。
    *   **规模**：包含294个室内场景、1446张多视角图像、352个任务导向场景图、1315个实例。
    *   **难度分级**：分为四个层级（T1-T4），从单步操作到复杂的多步重规划。

2.  **BLINK 基准**
    *   **目的**：用于评估模型的**视觉对应**和**跨视角一致性**能力，作为感知能力的补充验证。

3.  **MomaGraph-Scenes**（训练数据集）
    *   **性质**：作者构建的大规模、任务驱动的场景图数据集，用于模型训练。
    *   **内容**：统一标注了**空间关系**（9种）和**功能关系**（6种），包含物体及部件级交互节点。
    *   **规模**：约1050个子图、6278张多视角图像、覆盖超过350个家庭场景、93个不同任务指令。

4.  **真实机器人实验**
    *   **平台**：RobotEra Q5 双手机器人。
    *   **任务**：设计了4个代表性家庭任务（如开柜门、开微波炉、开电视、关灯），所有场景均为**未见过的**，以测试泛化能力。

### 二、 对比的基线方法
论文与广泛的基线模型进行了对比，涵盖闭源和开源模型：

*   **闭源模型（作为性能上限参考）**：
    *   Claude-4.5-Sonnet
    *   GPT-5
    *   Gemini-2.5-Pro
*   **开源视觉语言模型（主要对比对象）**：
    *   InstructBLIP-7B
    *   LLaVA-V1.5-7B
    *   DeepSeek-VL2 (4.5B)
    *   InternVL2.5-8B
    *   LLaVA-Onevision-7B
    *   **Qwen2.5-VL-7B-Instruct**（作为 **MomaGraph-R1** 的基座模型）

### 三、 关键性能指标与主要结论

#### 1. 在 MomaGraph-Bench 上的整体性能
*   **核心指标**：任务规划准确率（%）。
*   **主要结论**：
    *   **“图-再-规划”策略的有效性**：所有模型在**先生成场景图再进行规划**的设置下（w/ Graph），性能均优于**直接从图像规划**的设置（w/o Graph）。这验证了结构化中间表示对提升规划可靠性的价值。
    *   **MomaGraph-R1 的卓越性能**：
        *   在 **w/ Graph** 设置下，**MomaGraph-R1** 在 **MomaGraph-Bench** 上的**总体准确率达到 71.6%**。
        *   这比其基座模型 **Qwen2.5-VL-7B-Instruct**（60.2%）**提升了 11.4 个百分点**，证明了强化学习与图对齐奖励的有效性。
        *   **MomaGraph-R1 的性能与顶级闭源模型（Claude-4.5-Sonnet: 73.9%, GPT-5: 71.6%）相当，并显著超越了所有其他开源基线模型**。
    *   **对任务复杂度的鲁棒性**：随着任务难度从T1增加到T4，大多数开源模型性能急剧下降，而 **MomaGraph-R1 的性能下降幅度更小**，显示出其在长视野、复杂推理任务上更好的可扩展性。

#### 2. 在视觉对应任务上的性能
*   **核心指标**：在 BLINK 和 MomaGraph-Bench 视觉对应子任务上的准确率。
*   **主要结论**：
    *   **MomaGraph-R1 在 BLINK 基准上达到 63.5%**，在 MomaGraph-Bench 的对应任务上达到 **77.5%**。
    *   这**在所有开源模型中达到了最优性能**，分别比最佳竞争开源模型高出 **3.8%** 和 **4.8%**。
    *   这表明学习生成结构化场景图能有效提升模型的**跨视角一致性和感知可靠性**，减少幻觉。

#### 3. 消融实验与动机验证
*   **统一图 vs. 单一关系图**：实验证明，仅包含空间关系或仅包含功能关系的场景图，其规划性能（分别为59.9%和64.9%）均显著低于**统一了两种关系的 MomaGraph**（71.6%）。这验证了**空间-功能统一表示的必要性**。
*   **强化学习 vs. 其他训练方式**：与仅使用监督微调（SFT）和上下文学习（ICL）的基线相比，采用强化学习（RL）训练的 **MomaGraph-R1 在两项基准上均取得了显著更好的性能**（见表5），凸显了RL在优化图生成质量上的优势。

#### 4. 真实机器人实验效果
*   **核心指标**：任务成功率。
*   **主要结论**：
    *   在四个未见过的真实家庭任务中，系统能够成功完成。
    *   在一个复杂的多步长视野任务（“我需要更好的照明，打开离遥控器最近的灯，以便我找到它并打开显示器观看”）的定量评估中：
        *   场景图生成成功率为 **80%**。
        *   基于正确图谱的规划成功率为 **87.5%**。
        *   **整体任务成功率达到 70%**（10次试验）。
    *   这证明了 **MomaGraph-R1 生成的结构化表示能有效迁移到真实机器人平台，支持在未知环境中的鲁棒规划和执行**。

### 总结
论文通过系统的实验评估，全面证明了 **MomaGraph** 框架的有效性。其核心模型 **MomaGraph-R1** 在自建的综合基准上达到了开源模型的最高水平，与顶级闭源模型性能相当，并在视觉对应任务上表现出色。关键的**性能提升**（+11.4% over base model）主要归功于：1) **空间-功能统一的场景图表示**；2) **基于强化学习的“图-再-规划”范式**。最终，在真实机器人实验中的成功部署，验证了该方法的**实用性和泛化能力**。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.16909v1)
- [HTML 版本](https://arxiv.org/html/2512.16909v1)
