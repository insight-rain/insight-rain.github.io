# Scaling Laws for Energy Efficiency of Local LLMs

**相关性评分**: 6.0/10

**排名**: #36


---


## 基本信息

- **arXiv ID**: [2512.16531v4](https://arxiv.org/abs/2512.16531v4)
- **发布时间**: 2025-12-18T13:40:33Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Ander Alvarez, Alessandro Genuardi, Nilotpal Sinha, Antonio Tiene, Mikail Okyay, Bakbergen Ryskulov, David Montero, Samuel Mugel, Román Orús

## 关键词

Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

该论文通过系统基准测试和量子启发压缩技术，探索了本地语言和视觉语言模型在CPU上的计算扩展规律，旨在提高边缘设备上的推理效率和能源效率。

## 摘要

Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.16531v4)
- [HTML 版本](https://arxiv.org/html/2512.16531v4)
