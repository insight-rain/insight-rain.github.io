# PAACE: A Plan-Aware Automated Agent Context Engineering Framework

**相关性评分**: 7.0/10

**排名**: #17


---


## 基本信息

- **arXiv ID**: [2512.16970v1](https://arxiv.org/abs/2512.16970v1)
- **发布时间**: 2025-12-18T12:54:56Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Kamer Ali Yuksel

## 关键词

Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

PAACE是一个计划感知的自动代理上下文工程框架，通过上下文压缩和蒸馏技术优化LLM代理的工作流，显著降低推理成本并提升效率，适用于轻量级架构和边缘部署。

## 摘要

Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.16970v1)
- [HTML 版本](https://arxiv.org/html/2512.16970v1)
