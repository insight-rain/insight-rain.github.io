# Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations

**相关性评分**: 6.0/10

**排名**: #28


---


## 基本信息

- **arXiv ID**: [2512.21586v1](https://arxiv.org/abs/2512.21586v1)
- **发布时间**: 2025-12-25T09:11:14Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Xin Liu, Haoran Li, Dongbin Zhao

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

该论文提出了一种从视频中通过潜在表示进行行为克隆的样本高效框架，与视觉-语言-动作模型和推理效率相关，但未直接涉及轻量级架构或边缘部署。

## 摘要

Humans can efficiently extract knowledge and learn skills from the videos within only a few trials and errors. However, it poses a big challenge to replicate this learning process for autonomous agents, due to the complexity of visual input, the absence of action or reward signals, and the limitations of interaction steps. In this paper, we propose a novel, unsupervised, and sample-efficient framework to achieve imitation learning from videos (ILV), named Behavior Cloning from Videos via Latent Representations (BCV-LR). BCV-LR extracts action-related latent features from high-dimensional video inputs through self-supervised tasks, and then leverages a dynamics-based unsupervised objective to predict latent actions between consecutive frames. The pre-trained latent actions are fine-tuned and efficiently aligned to the real action space online (with collected interactions) for policy behavior cloning. The cloned policy in turn enriches the agent experience for further latent action finetuning, resulting in an iterative policy improvement that is highly sample-efficient.
  We conduct extensive experiments on a set of challenging visual tasks, including both discrete control and continuous control. BCV-LR enables effective (even expert-level on some tasks) policy performance with only a few interactions, surpassing state-of-the-art ILV baselines and reinforcement learning methods (provided with environmental rewards) in terms of sample efficiency across 24/28 tasks. To the best of our knowledge, this work for the first time demonstrates that videos can support extremely sample-efficient visual policy learning, without the need to access any other expert supervision.

## 详细分析

## 论文摘要：基于潜在表征的视频行为克隆（BCV-LR）

### 1. 研究背景和动机
强化学习（RL）在决策任务中表现出强大能力，但其训练通常依赖于精心设计的奖励信号和大量的环境交互，这限制了其应用范围。相比之下，视频是一种更易获取的监督信息源，人类可以轻松地从视频中学习技能。然而，让智能体仅从无动作标签的视频中进行模仿学习（ILV）极具挑战，主要困难在于高维视觉输入的复杂性以及动作信息的缺失。现有方法（如逆强化学习）通常样本效率不高。本文旨在探索**仅使用视频作为唯一专家监督信号**，实现**高样本效率**的视觉策略学习的可能性。

### 2. 核心方法和技术创新
本文提出了 **BCV-LR** 框架，包含**离线预训练**和**在线微调**两个阶段：
- **离线阶段**：
    - **自监督特征学习**：通过对比学习或时序关联任务，从视频帧中提取与决策相关的潜在特征，降低后续学习难度。
    - **无监督潜在动作提取**：训练一个世界模型和动作预测器，通过重构连续帧之间的潜在特征变化，以无监督方式预测出视频帧间的“潜在动作”。
- **在线阶段**：
    - **潜在动作微调与对齐**：利用智能体与环境交互收集的（无奖励）转移数据，微调预训练的潜在动作预测器，同时通过一个解码器将潜在动作对齐到真实动作空间。
    - **潜在策略克隆与迭代提升**：基于对齐后的潜在动作，通过行为克隆训练一个潜在策略。该策略反过来收集更高质量的经验数据，用于进一步微调潜在动作，形成**策略迭代提升的闭环**，极大提高了样本效率。

**核心技术创新**在于将复杂的视觉模仿问题分解为**特征提取、潜在动力学建模、动作对齐**三个子问题，并通过预训练-微调范式与迭代优化机制，实现了仅从视频中高效学习策略。

### 3. 主要实验结果
在 **Procgen（16个离散任务）** 和 **DMControl（8个连续任务）** 等具有挑战性的视觉控制基准上进行了广泛实验：
- **样本效率领先**：在仅允许 **10万次环境交互** 的严格限制下，BCV-LR在 **24/28 个任务** 上超越了最先进的ILV基线方法和（拥有环境奖励的）RL方法。
- **达到专家水平**：在多个任务（如“Bossfight”、“reacher_hard”）上，BCV-LR学习到的策略性能达到了专家水平。
- **消融实验验证**：实验证实了自监督特征预训练、潜在动作预训练以及在线微调对齐都是BCV-LR取得高性能的必要组件。
- **拓展性验证**：方法在少量视频数据下依然有效，并展示了跨任务预训练与迁移的潜力。

### 4. 研究意义和价值
- **理论价值**：本研究首次系统性地证明，**视频本身可以作为支持极高样本效率策略学习的唯一监督信号**，无需任何专家动作或奖励，为模仿学习开辟了新路径。
- **方法论贡献**：提出的BCV-LR框架为样本高效的视觉模仿学习提供了一个新颖、有效的解决方案，其“预训练-微调-迭代”的范式具有借鉴意义。
- **应用前景**：该方法降低了对昂贵交互数据和精确奖励设计的依赖，在**机器人操控、自动驾驶**等数据获取成本高或奖励难以设计的现实场景中具有重要的应用潜力，是迈向高效“看视频学习”智能体的关键一步。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **论文标题**
《Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations》

### **核心问题**
论文旨在解决 **“仅从视频中进行模仿学习”** 这一核心挑战。具体来说，它针对的是 **“视频模仿学习”** 问题，其目标是让智能体仅通过观看无动作标签的专家演示视频，就能学会执行任务，而**无需访问环境奖励信号或专家动作**。

现有方法的主要瓶颈在于：
- **样本效率低**：基于逆强化学习的方法需要大量环境交互来学习奖励函数和价值函数。
- **性能瓶颈**：基于行为克隆的方法在复杂的视觉连续控制任务中，难以从高维视频中准确预测专家动作，导致策略性能不佳。

因此，论文的核心问题是：**能否在仅使用视频作为监督信号的前提下，实现既高效（样本效率高）又有效（策略性能接近专家水平）的视觉策略学习？**

### **核心创新点**
论文提出了一个名为 **BCV-LR** 的新框架，其核心创新在于一个**两阶段、基于潜在表示的、迭代改进的**学习范式：

1.  **“视频 -> 潜在动作 -> 真实策略”的端到端框架**：首次系统性地证明了**仅凭视频**就能支持**极高样本效率**的视觉策略学习，无需任何其他专家监督（动作或奖励）。
2.  **分阶段的知识提取与对齐**：
    - **离线预训练阶段**：从无动作视频中提取“潜在动作”。
        - **自监督视觉编码器**：通过对比学习、时序关联等任务，从原始像素中提取与决策相关的潜在特征，降低后续学习难度。
        - **基于动力学的无监督目标**：训练一个世界模型和潜在动作预测器，通过重建连续帧之间的潜在特征变化，来推断出帧间隐含的“潜在动作”。使用**向量量化** 避免平凡解。
    - **在线微调阶段**：利用极少量无奖励的环境交互，将潜在知识对齐到真实环境。
        - **潜在动作微调与解码**：利用收集的环境转移数据，微调潜在动作预测器，同时通过一个解码器将潜在动作对齐到真实动作空间。关键点是**利用预训练的世界模型提供专家动力学约束**，以保持对动态的理解，提高微调鲁棒性。
        - **潜在策略的行为克隆**：训练一个潜在策略，直接克隆从专家视频中预测出的（已微调的）潜在动作。
3.  **“策略改进-数据收集”的迭代循环**：克隆出的策略用于与环境交互，收集质量更高的数据；这些数据反过来用于进一步微调潜在动作和解码器，从而提升策略性能。这种**闭环迭代机制**是达成高样本效率的关键。

### **解决方案的流程总结**
```
1. 离线阶段（仅用视频）：
   输入：专家视频帧 (o_i^v, o_{i+1}^v)
   -> 通过自监督任务训练视觉编码器 f，得到潜在特征 s_i^v
   -> 训练世界模型 w 和动作预测器 p，通过重建损失 L_la = ||w(s_i^v, VQ(p(s_i^v, s_{i+1}^v))) - s_{i+1}^v||^2 得到潜在动作 z_i^v

2. 在线阶段（少量无奖励交互）：
   a. 交互收集：用当前策略（f + π + d）与环境交互，收集带真实动作的转移数据 (o_t^e, a_t^e, o_{t+1}^e)。
   b. 动作微调与对齐：用收集的数据微调 p，并训练解码器 d 将 z_t^e 映射到真实动作 a_t^e。损失函数 L_ft 结合了动作预测损失和世界模型重建损失。
   c. 策略克隆：用微调后的 p 从专家视频中生成更好的潜在动作标签，通过行为克隆损失 L_bc = ||π(s_i^v) - z_i^v||^2 更新潜在策略 π。
   d. 迭代：更新后的策略回到步骤a收集更好数据，形成良性循环。

3. 最终策略：在线阶段结束后，组合 f, π, d 形成可直接在真实环境中执行的策略。
```

### **实际价值与意义**
- **理论价值**：首次实证表明，**视频本身可以作为一种极其样本高效的监督信号**，为模仿学习开辟了一条新的、更接近人类学习方式的路径。
- **应用价值**：
    - **降低数据获取门槛**：互联网上存在海量无标签演示视频（如教学视频、操作录像），BCV-LR 使得利用这些廉价、易得的数据训练智能体成为可能。
    - **提升安全性与效率**：在机器人、自动驾驶等真实交互成本高或危险的领域，能极大减少必要的物理交互次数。
    - **框架灵活性**：兼容不同的自监督任务，可适应不同领域（离散游戏、连续控制、机器人操作）。
- **实验结果支撑**：在 Procgen（16项）、DMControl（8项）、Metaworld（4项）共28项视觉任务上的实验表明，BCV-LR 在仅允许 **10万次环境交互** 的严格限制下，性能**显著超越**现有的ILV方法，甚至优于一些**使用环境奖励**的先进RL方法，在多项任务上达到专家水平的80%以上。

### **局限性**
论文也指出了其局限性与未来方向：
- **协变量偏移**：与所有行为克隆方法一样，在复杂的序列决策任务（如机器人 locomotion）上，性能增长会遇到瓶颈。
- **视频数据效率**：当视频数据量极少（如5k帧）时，性能会下降。
- **未来方向**：考虑与逆强化学习结合以克服协变量偏移；探索利用互联网规模跨领域视频进行预训练。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**仅从无动作标签的专家视频中进行高效模仿学习**的核心挑战，即如何在样本效率极高（仅允许少量环境交互）的情况下，从复杂视觉输入中学习到接近专家水平的策略。为此，论文提出了一个名为 **BCV-LR** 的两阶段框架：首先，在**离线预训练阶段**，通过自监督任务从视频中提取与动作相关的潜在特征，并利用基于动力学的无监督目标预测视频帧间的“潜在动作”；随后，在**在线微调阶段**，利用收集到的少量无奖励环境交互数据，微调这些潜在动作并将其对齐到真实动作空间，进而通过行为克隆训练策略，该策略又能收集更好的数据以迭代提升性能。实验表明，该方法在多个离散和连续视觉控制任务上，仅使用视频作为监督信号，其**样本效率超越了当前最先进的视频模仿学习方法，甚至超过了依赖环境奖励的强化学习方法**，首次证明了视频本身足以支持极高样本效率的视觉策略学习。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations》提出了一种名为 **BCV-LR** 的新框架，旨在仅使用无动作标签的专家视频进行样本高效的模仿学习。其核心创新点在于通过**潜在表示**和**两阶段训练**，解决了从视频中学习策略时面临的样本效率低、动作信息缺失和视觉输入复杂等关键挑战。

以下是其相对于已有工作的明确创新点，逐条列出并分析：

### 1. **首次系统性地论证了“仅凭视频即可实现样本高效的视觉策略学习”**
   - **相比以往方法的改进/不同之处**：
     - **主流方法依赖额外监督**：以往的模仿学习要么需要带动作标签的专家轨迹（行为克隆），要么需要环境奖励（强化学习或逆强化学习）。即使是“从视频模仿学习”的相关工作，也常常将视频作为**辅助信息**，与奖励或动作标签结合使用。
     - **BCV-LR的突破**：本文首次提出并验证了一个**完整的框架**，能够**仅以无动作视频作为唯一的专家监督信号**，在极少的真实环境交互步数（如10万步）内，学习到有效甚至专家级的策略。
   - **解决的具体问题/带来的优势**：
     - **降低了监督信息的获取门槛**：视频（如网络教程）远比精心设计的奖励函数或高质量带标签的专家数据更容易获取。
     - **开辟了更自然、更通用的学习范式**：使智能体能够像人类一样，通过“观看”进行高效学习，极大地拓宽了模仿学习的应用场景。

### 2. **提出“离线预训练潜在动作 + 在线对齐微调”的两阶段框架**
   - **相比以往方法的改进/不同之处**：
     - **传统ILV方法的局限**：现有方法要么直接通过逆强化学习从视频中拟合奖励函数（如LAIFO），这需要大量交互来训练奖励和值网络；要么尝试直接从视频帧预测动作（如BCO, ILPO），在复杂的视觉连续控制任务中性能瓶颈明显。
     - **BCV-LR的创新流程**：
       1. **离线阶段**：在无动作视频上，通过自监督任务预训练视觉编码器，提取与决策相关的潜在特征。然后，训练一个世界模型和潜在动作预测器，通过**基于动力学的无监督目标**（如重建下一帧的潜在特征）来预测视频帧之间的“潜在动作”。
       2. **在线阶段**：利用智能体在无奖励环境中收集的少量交互数据，对预训练的潜在动作进行**微调**，并通过一个解码器将其**对齐到真实动作空间**。同时，训练一个潜在策略来克隆这些对齐后的动作。
   - **解决的具体问题/带来的优势**：
     - **化解了动作缺失的难题**：通过无监督方式从视频动态变化中“蒸馏”出隐含的动作表示（潜在动作），绕过了直接预测真实动作的困难。
     - **实现了极高的样本效率**：离线阶段从海量视频中提取了丰富的先验知识（视觉表示和动态规律），在线阶段只需极少的交互来“校准”和适应真实环境，避免了从零开始的大量探索。
     - **形成了迭代改进的良性循环**：克隆的策略能收集更好的数据，用于进一步微调潜在动作，从而进一步提升策略，形成高效的正反馈。

### 3. **设计了针对不同任务域的自适应自监督视觉编码器预训练方法**
   - **相比以往方法的改进/不同之处**：
     - **通用性与灵活性**：BCV-LR没有固守单一的自监督任务，而是根据任务特点选择或组合不同的目标。
       - 对于**视觉复杂、动态相对简单**的任务（如Procgen游戏），采用**对比学习 + 图像重建**的组合，使编码器关注与动作高度相关的相对位置差异和关键像素。
       - 对于**部分可观测、动态复杂**的任务（如DMControl连续控制），采用**基于原型的时序关联任务**（使用Sinkhorn-Knopp算法），使编码器能够理解关键的时序信息。
   - **解决的具体问题/带来的优势**：
     - **提升了潜在特征的质量**：定制的自监督任务能更有效地从高维像素中提取出与下游策略学习高度相关的特征，缓解了视觉理解的难度。
     - **增强了框架的普适性和鲁棒性**：能够灵活适应离散控制、连续控制乃至机器人操作等不同领域和不同视觉特性的任务。

### 4. **引入世界模型作为潜在动作微调的“动力学约束”**
   - **相比以往方法的改进/不同之处**：
     - **在线微调的关键设计**：在在线阶段，当使用智能体自身收集的（非专家级）交互数据来微调潜在动作预测器时，BCV-LR**同时利用预训练的世界模型计算重建损失**（公式3中的第二项）。
     - **对比基线**：一些方法（如UPESV）在在线阶段会冻结预训练知识，导致适应能力不足；而直接微调又可能因在线数据分布与专家视频不同而学偏。
   - **解决的具体问题/带来的优势**：
     - **保持了专家动态先验**：世界模型在专家视频上训练，蕴含了专家的动态知识。这个约束项迫使潜在动作在适应非专家动作标签时，仍能保持对专家动力学的理解。
     - **提升了微调的鲁棒性和最终性能**：避免了因在线数据质量不佳而导致的性能退化，确保了从视频中提取的“动作本质”在适配过程中不被破坏。消融实验证明了该设计对连续控制任务至关重要。

### 5. **在广泛且具有挑战性的基准测试中实现了样本效率的领先**
   - **相比以往方法的改进/不同之处**：
     - **全面的评估**：在**28个任务**（16个Procgen离散任务、8个DMControl连续任务、4个Metaworld机械臂操作任务）上进行了系统实验，任务覆盖广、难度高。
     - **严格的对比**：不仅超越了所有先进的ILV基线（UPESV, BCO, ILPO, LAIFO），甚至在**样本效率上超越了依赖环境真实奖励的先进RL方法**（如PPO, DrQ-v2, TACO）。
   - **解决的具体问题/带来的优势**：
     - **提供了强有力的实证支撑**：大规模实验确凿地证明了所提框架的有效性和优越性，使其结论非常坚实。
     - **确立了新的性能标杆**：在仅使用视频监督的前提下，BCV-LR在24/28的任务上取得了最佳样本效率，为“视频作为高效监督”这一命题树立了新的State-of-the-art。

### 总结
BCV-LR的核心创新在于**将无监督表示学习、模型预测与行为克隆巧妙地结合在一个两阶段框架中**，创造性地通过“潜在动作”这一中介，桥接了无动作视频与可执行策略之间的鸿沟。其每一项创新都直指ILV领域的核心痛点——**样本效率**和**监督信息稀缺**，并通过系统性的设计（自适应预训练、动力学约束微调、迭代优化）予以解决，最终在多个维度上显著超越了已有方法。这项工作为样本高效的视觉模仿学习开辟了一条切实可行的新路径。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果总结

该论文提出了一种名为 **BCV-LR** 的新框架，旨在仅使用无动作标签的专家视频（即“视频模仿学习”， ILV）来实现**极高样本效率**的视觉策略学习。实验表明，BCV-LR 在多种具有挑战性的视觉控制任务上，仅需极少的真实环境交互，就能达到接近甚至达到专家水平的性能，其样本效率超越了现有的 ILV 方法和部分依赖环境奖励的强化学习方法。

### 一、 使用的数据集与任务
论文在三个主流的视觉控制基准上进行了广泛测试，涵盖了离散和连续控制：
1.  **Procgen Benchmark**：包含 **16个** 程序生成的离散控制视频游戏任务（如 Bigfish, Maze, Bossfight）。特点是视觉风格多变，动态相对简单。
2.  **DeepMind Control Suite (DMControl)**：包含 **8个** 连续控制机器人任务（如 reacher_hard, finger_spin）。特点是状态部分可观测，动态复杂。
3.  **Metaworld**：包含 **4个** 机器人连续操作任务（如 Faucet-open, Drawer-open）。用于展示在更复杂操作任务上的泛化能力。

**专家视频数据**：对于 Procgen，使用预训练的 RL 智能体生成 8M 步的专家轨迹。对于 DMControl 和 Metaworld，使用训练好的 RL 智能体（1M 步）生成 100k 步的专家轨迹。

### 二、 评价指标
- **主要指标**：在**严格限制的环境交互步数**下，学得策略在对应任务上的**性能得分**。
    - **Procgen**：使用游戏内置的标准化得分。
    - **DMControl**：使用任务特定的奖励得分（范围通常为 0~1000）。
    - **Metaworld**：任务成功率。
- **核心对比维度**：**样本效率**，即在给定的少量交互步数内，能达到多高的性能。
- **辅助指标**：与专家性能的归一化比值（`Video-norm Mean`），用于衡量模仿的接近程度。

### 三、 对比的基线方法
论文与两大类基线方法进行了对比：

**1. 视频模仿学习 (ILV) 基线**（仅能访问专家视频）：
- **UPESV**、**BCO**、**ILPO**：与 BCV-LR 类似，都属于基于行为克隆（预测动作）的 ILV 方法。
- **LAIFO**：基于逆强化学习（预测奖励）的先进 ILV 方法。

**2. 在线强化学习 (RL) 基线**（能访问环境真实奖励，作为性能上限参考）：
- **Procgen**：**PPO**（标准RL）、**LAPO**（利用视频预训练策略的增强RL）。
- **DMControl**：**DrQ-v2**（先进的视觉连续控制RL）、**TACO**（结合自监督任务的先进RL）。

### 四、 关键性能结果与结论

#### 1. 总体样本效率领先
在 **Procgen (16个任务)** 和 **DMControl (8个任务)** 的共24个任务上，BCV-LR 在仅允许 **100k 环境交互步数** 的极端限制下，其平均性能**超越了所有对比的 ILV 和 RL 基线方法**。

- **Procgen (表1)**：
    - BCV-LR 平均得分为 **13.8**，远超其他 ILV 方法（UPESV: 9.0, BCO: 4.8, ILPO: 2.1, LAIFO: 2.2）。
    - 甚至超过了使用真实奖励的 RL 方法 PPO (2.3) 和 LAPO (6.8)。
    - 平均达到了专家性能的 **79%**，在多个任务（如 Maze, Bigfish）上达到专家水平。

- **DMControl (表2 及 图3)**：
    - BCV-LR 平均得分为 **604**，显著高于其他 ILV 方法（LAIFO: 158, BCO: 336, UPESV: 18）。
    - 在 6/8 的任务上，表现优于使用真实奖励的先进 RL 方法 TACO (310) 和 DrQ-v2 (232)。
    - 在仅 **50k 甚至 20k 步**（见附录表4、5）的交互下，BCV-LR 仍能学习到有效策略，而多数基线方法此时性能极低或无法学习。

#### 2. 核心结论
- **首次证明**：仅使用视频作为**唯一**的专家监督信号，可以实现**极端样本高效**的视觉策略学习。无需专家动作标签或环境奖励。
- **方法有效性**：BCV-LR 的两阶段框架（**离线潜在表示预训练 + 在线微调与对齐**）是成功的。其关键创新——通过自监督任务提取动作相关特征、基于动力学的无监督潜在动作预测、以及利用预训练世界模型约束的在线微调——共同促成了高效的样本利用。
- **优势场景**：在交互成本极高或奖励难以设计的场景（如机器人操作）中，BCV-LR 提供了一种极具潜力的替代方案。

#### 3. 其他重要发现
- **模块消融实验 (图4)**：移除自监督特征预训练 (`w/o lf`)、潜在动作预训练 (`w/o la`) 或在线微调 (`w/o ft`) 都会导致性能显著下降，验证了每个组件的必要性。
- **视频数据效率 (图5)**：仅需 **50k** 步的视频数据，BCV-LR 就能达到接近专家的性能，展示了其对视频数据量的高效利用。
- **多任务与跨任务适应 (表3)**：在多个任务视频上预训练的 BCV-LR 模型，能在未见过的任务上进行有效的策略模仿，展示了其利用大规模互联网视频数据的潜力。
- **局限性**：与所有基于行为克隆的方法一样，BCV-LR 受**协变量偏移**问题限制，在需要长序列决策的复杂运动任务（如 `walker_walk`）上，当交互步数足够多时，性能可能被基于逆RL的方法超越（附录B.9）。

**总结**：BCV-LR 在实验评估中实现了**里程碑式**的效果，证明了视频本身足以支撑高效模仿学习，并在样本效率这一关键指标上全面超越了现有方法，为样本稀缺场景下的智能体学习开辟了新路径。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.21586v1)
- [HTML 版本](https://arxiv.org/html/2512.21586v1)
