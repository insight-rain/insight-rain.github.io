# StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision

**相关性评分**: 8.0/10

**排名**: #14


---


## 基本信息

- **arXiv ID**: [2512.21970v1](https://arxiv.org/abs/2512.21970v1)
- **发布时间**: 2025-12-26T10:34:20Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

StereoVLA通过立体视觉增强VLA模型，提出几何-语义特征提取模块和辅助深度估计任务，提升空间感知和模型收敛速度，适用于机器人操作。

## 摘要

Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.

## 详细分析

## 论文摘要：StereoVLA: 利用立体视觉增强视觉-语言-动作模型

**1. 研究背景和动机**
视觉-语言-动作模型为机器人提供了从视觉输入和语言指令直接映射到动作的端到端框架。然而，现有VLA模型大多依赖单目RGB图像，缺乏精确的几何感知能力，限制了其在需要精细空间操作的场景中的应用。虽然已有工作尝试引入深度传感器、腕部相机等多视角方案，但这些方案存在视野受限、硬件复杂或对透明物体感知不佳等问题。受人类双目视觉启发，立体相机能提供鲁棒的几何线索且部署简单，但其在VLA模型中的应用尚未得到充分探索。本研究旨在系统性地将立体视觉的几何优势融入VLA模型。

**2. 核心方法和技术创新**
本文提出了**StereoVLA**模型，其核心创新在于：
- **几何-语义特征提取模块**：该模块巧妙地融合了来自**FoundationStereo**（专精于立体匹配的视觉基础模型）的**稠密几何特征**与来自**PrismaticVLM**（基于SigLIP和DINOv2）的**单目语义特征**。通过空间对齐和通道拼接，生成了同时具备几何精度和语义丰富性的混合视觉表征。
- **交互区域深度估计辅助任务**：在训练阶段，模型被额外要求预测以机械爪和目标物体为中心的“交互区域”内采样点的度量深度。这迫使模型关注对操作至关重要的精细空间细节，加速了模型收敛并提升了操作精度。

**3. 主要实验结果**
在包含通用任务、抓取条形物体（不同朝向）以及抓取中小型物体的综合评测中，StereoVLA显著优于基线模型（如π0.5、GraspVLA、SpatialVLA的立体适配版本）。特别是在最具挑战性的小物体抓取任务上，StereoVLA取得了30%的成功率，而其他基线模型全部失败。消融实验验证了几何特征（选择过滤后的代价体积`V_c'`最优）、语义特征融合以及交互区域深度估计任务的有效性。此外，与单目、前+腕部、前+侧部等多种相机配置的对比表明，**立体配置在任务性能、对相机位姿变化的鲁棒性以及部署简易性之间取得了最佳平衡**。

**4. 研究意义和价值**
本研究首次系统地将立体视觉引入VLA框架，并通过创新的特征融合与辅助任务设计，成功地将立体视觉的几何优势与视觉语言模型的语义理解能力相结合。StereoVLA在需要高精度空间感知的机器人操作任务上展现了卓越性能，为未来开发更鲁棒、更通用的具身智能体提供了新的技术路径。同时，对不同相机设置的全面比较为机器人感知系统的实际部署提供了有价值的参考。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：StereoVLA

### **一、 核心问题**
论文旨在解决当前**视觉-语言-动作模型**在机器人操作任务中面临的**几何感知能力不足**的关键挑战。具体问题包括：
- **深度模糊**：传统VLA模型依赖单目RGB图像，缺乏精确的深度和三维空间信息，导致在需要精细空间感知的操作（如抓取细长、小型物体）中表现不佳。
- **补充传感器的局限性**：现有方案（如腕部相机、深度传感器、额外第三人称相机）各有缺陷：视野受限、对透明/反光物体敏感、硬件复杂、泛化性差。
- **立体视觉的潜力未被挖掘**：尽管立体相机能像人眼一样提供鲁棒的几何线索，且硬件部署简单，但在VLA框架中尚未得到系统性探索和有效利用。

### **二、 核心创新点**
论文提出了 **StereoVLA** 模型，其创新主要体现在**架构设计**和**训练策略**上：

1.  **几何-语义特征提取模块**
    - **问题**：立体图像对左右视图差异细微，直接输入现有多相机VLA模型效果不佳。
    - **解决方案**：
        - **几何特征提取**：利用专为立体视觉预训练的 **FoundationStereo** 模型，提取其**过滤后的代价体积**作为密集的几何特征源。该特征包含了通过注意力机制增强的长程空间关联，比原始代价体积或相关性体积更具信息量。
        - **语义特征提取**：为避免几何模型语义信息不足，从**左视图**提取语义特征。使用 **SigLIP**（用于高级语义）和 **DINOv2**（用于视觉细节）。
        - **特征融合**：将几何与语义特征在**通道维度**上进行拼接，再通过MLP投影器生成统一的视觉表征。这种方法避免了序列拼接带来的计算开销增加。

2.  **交互区域深度估计辅助任务**
    - **问题**：均匀采样图像点进行深度估计会包含大量无信息的背景，分散模型对关键区域的注意力，降低训练效率。
    - **解决方案**：在训练阶段引入一个辅助的**度量深度估计任务**，但**仅在与任务相关的“交互区域”**（即夹爪和目标物体周围的区域）内采样点。这迫使模型专注于学习对操作至关重要的精细几何细节，加速了收敛并提升了操作精度。

3.  **系统性利用立体视觉范式**
    - 论文首次将成熟的立体视觉基础模型（FoundationStereo）与大规模VLA框架深度结合，证明了立体配置在**性能、鲁棒性和部署简易性**之间的最佳平衡。
    - 通过精心策划的合成数据集（500万条轨迹）和严格的评估协议，为立体视觉在VLA中的应用建立了基准。

### **三、 解决方案总结**
论文通过一个**端到端的框架**解决了上述问题：
1.  **输入**：立体RGB图像对 + 语言指令。
2.  **处理**：
    - 通过 **Geometric-Semantic Feature Extraction** 模块生成融合了几何精确性和语义丰富性的视觉令牌。
    - 视觉令牌与语言令牌一同输入大语言模型（InternLM-1.8B）进行联合推理。
    - 利用模型的中间特征，一个动作专家通过流匹配预测末端执行器的增量位姿。
3.  **训练**：在动作预测的主任务基础上，**协同训练**交互区域深度估计、目标物体2D边界框预测等辅助任务，形成多任务损失函数，全面提升模型的几何感知和语义 grounding 能力。
4.  **输出**：机器人动作序列。

### **四、 实际价值与效果**
- **性能提升**：在真实机器人实验中，StereoVLA在通用任务上比基线模型成功率**高出33%**，在抓取细长物体、小型物体等高精度任务上优势尤为显著。
- **鲁棒性强**：在相机位姿大幅变化的情况下，StereoVLA的性能下降幅度远小于依赖多视角（如前+侧）的模型，展现了出色的环境适应性。
- **部署优势**：仅需一个立体相机，避免了多相机系统的标定、同步和硬件复杂性，为移动机器人和人形机器人提供了更实用的感知方案。
- **指导意义**：论文对不同相机配置（单目、前+腕、前+侧、立体）进行了系统比较，为未来机器人感知系统的设计提供了实证依据。

**简而言之，StereoVLA 的核心创新在于创造性地将强大的立体视觉几何先验与VLA的语义理解能力相结合，并通过针对性的特征提取和训练任务设计，解决了VLA模型空间感知能力弱的根本问题，实现了更精确、更鲁棒的机器人操作。**


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决现有视觉-语言-动作模型因依赖单目视觉而缺乏精确几何感知能力，从而限制机器人精细操作性能的核心问题。为此，论文提出了StereoVLA框架，其核心创新在于设计了一个**几何-语义特征提取模块**，该模块通过融合来自立体视觉基础模型的密集几何特征与来自单目视觉基础模型的丰富语义特征，并辅以一个**交互区域深度估计**的辅助训练任务，以增强模型对关键空间细节的感知。实验结果表明，该方法在立体视觉设置下，相比多种基线模型在多项精细操作任务上取得了显著更高的成功率（例如在通用任务上提升33%），并且对相机位姿变化展现出更强的鲁棒性，验证了利用立体视觉几何线索有效提升VLA模型空间感知与操作精度的价值。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## StereoVLA 论文创新点分析

这篇论文针对现有视觉-语言-动作模型在几何感知上的不足，系统地引入了立体视觉，并提出了多项关键技术创新。其核心创新点如下：

### 1. **首次将立体视觉系统性地引入VLA模型**
   - **相比以往方法的改进/不同之处**：
     - 以往VLA模型主要依赖单目RGB（如OpenVLA）、腕部相机、深度传感器或多视角相机（如π0.5、GraspVLA）。这些方案各有局限：单目存在深度歧义；深度传感器对透明/反光物体噪声大；腕部相机视野受限且增加碰撞风险；多相机部署复杂且视角多样性影响泛化。
     - 本文首次提出并系统评估了**使用单个立体相机**作为VLA视觉输入的方案。立体相机模拟人类双目视觉，能直接提供鲁棒的几何线索（视差），且硬件部署简单。
   - **解决的具体问题/带来的优势**：
     - **解决了VLA模型几何感知能力弱的问题**，为机器人操作提供了精确的空间感知基础。
     - **简化了硬件设置**，避免了多相机系统的复杂部署和校准，降低了碰撞风险。
     - 实验证明，该设置在保持高性能的同时，对相机位姿变化具有**更强的鲁棒性**。

### 2. **提出几何-语义特征提取模块**
   - **相比以往方法的改进/不同之处**：
     - 现有方法处理多视角输入（如GraspVLA-S）时，通常独立编码左右图像，无法有效利用立体视图间**细微的差异**来提取密集几何特征。
     - 本模块创新性地**融合了来自不同基础模型的专长特征**：
       1. **几何特征**：从专为立体视觉预训练的**FoundationStereo**模型中，提取其**过滤后的代价体积**作为密集几何特征源。该特征经过了注意力机制的混合代价滤波，包含了长程空间关联，比原始代价体积或相关性体积信息更丰富。
       2. **语义特征**：从单目（左）视图，通过**SigLIP**（擅长高层语义）和**DINOv2**（擅长视觉细节）提取语义丰富的特征。
     - 采用**通道拼接**而非序列拼接的方式进行特征融合，在保证性能的同时减少了计算开销。
   - **解决的具体问题/带来的优势**：
     - **解决了立体视图差异细微、直接输入效果不佳的问题**，通过专用模型提取了高质量的几何特征。
     - **解决了纯几何特征缺乏语义信息、不利于语言指令跟随的问题**，通过融合语义特征实现了**几何精度与语义丰富性的统一**。
     - 消融实验表明，该设计是性能提升的关键，使用`V_c'`特征并融合语义信息取得了最高成功率。

### 3. **提出交互区域深度估计辅助任务**
   - **相比以往方法的改进/不同之处**：
     - 常见的辅助深度估计任务是在全图均匀采样点进行预测。这种方法会采样到大量无关的背景区域（如桌面、墙壁），对任务学习帮助有限。
     - 本文提出将采样点**限制在“交互区域”**——即夹爪和目标物体2D边界框所覆盖的区域。这迫使模型专注于学习对操作至关重要的精细几何细节。
   - **解决的具体问题/带来的优势**：
     - **解决了均匀采样效率低、干扰模型学习的问题**，使训练注意力集中在任务相关区域。
     - **增强了模型对物体几何及其与夹爪空间关系的理解**，从而提升了操作精度。
     - **加速了模型收敛**，实验表明该策略在训练早期即带来显著性能提升，且最终效果优于全局采样策略。

### 4. **在立体设定下进行了全面的相机配置对比研究**
   - **相比以往方法的改进/不同之处**：
     - 以往工作缺乏在不同相机配置（单目、立体、前+腕、前+侧）下，使用**同等规模数据**和**可比训练条件**的公平比较。
     - 本文系统地生成了不同相机位姿随机化范围的数据集，并在此框架下重新训练/微调了多个SOTA基线模型（SpatialVLA-D, π0.5, GraspVLA等），进行了严谨对比。
   - **解决的具体问题/带来的优势**：
     - **首次量化评估了立体配置相对于其他主流配置的优劣**，为社区提供了实用的部署指南。
     - 结论表明：立体配置在**性能、鲁棒性和部署简易性之间取得了最佳平衡**。特别是在相机位姿变化较大时，立体配置的鲁棒性显著优于依赖未对齐多视角（如前+侧）的方案。

### 总结
StereoVLA 的核心创新在于**方法论层面**，而非单纯依赖数据规模。它通过**新颖的模型架构（几何-语义特征提取）** 和**训练策略（交互区域深度估计）**，有效地解锁了立体视觉为VLA模型带来的几何感知潜力，最终在需要精细空间感知的操作任务上实现了大幅性能提升，并对实际部署中常见的视角变化展现了强大适应性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

### 一、 主要数据集
1.  **合成数据集**：使用 MuJoCo 和 Isaac Sim 生成 **500万条** 合成拾取-放置轨迹，渲染 **256x256** 分辨率的立体图像对，并模拟真实 ZED Mini 相机的基线参数。
2.  **互联网规模数据集**：整合了 GRIT 数据集，以增强模型的语义基础能力。
3.  **真实世界评估**：在包含多种物体（条形物体、中小型物体、日常物品）的复杂场景中进行测试，并设置了不同相机位姿随机化范围。

### 二、 评价指标
- **核心指标**：**任务成功率**。每个任务集进行15次试验，总计450次试验。
- **严格评估标准**：
    - 每次试验仅允许**单次执行**（一次抓取前闭合、一次抓取后张开）。
    - 禁用“夹爪粘滞”启发式方法，防止掩盖决策不准确的问题。
    - 仅当任务**完全完成**时才计为成功，无部分积分。

### 三、 基线方法对比
由于现有VLA模型未专门针对立体视觉设计，论文对以下SOTA开源模型使用相同的合成立体数据集进行训练/微调，以进行公平比较：
- **SpatialVLA**：原设计使用单目RGB估计深度。论文额外引入其变体 **SpatialVLA-D**，使用FoundationStereo从立体输入估计高质量深度。
- **π₀.₅**：以其强适应性著称。论文将其适应为立体输入版本 **π₀.₅-S**。
- **GraspVLA**：以大规模合成数据预训练为特色。论文将其适应为立体输入版本 **GraspVLA-S**。

### 四、 关键性能结果与结论

#### 1. 立体设置下的任务性能（图3）
- **总体优势**：StereoVLA在所有任务类别中均取得**最高且最稳定**的成功率。
- **高精度任务表现突出**：
    - **抓取条形物体**（0°, 45°, 90°方向）：StereoVLA取得接近完美或完美的结果，而所有基线模型表现明显更差。随着物体方向角度增大（对深度估计误差更敏感），所有模型性能均下降，但StereoVLA下降幅度最小。
    - **抓取中小型物体**：在最具挑战性的抓取小物体（1~2 cm）任务中，StereoVLA取得了**30.0%** 的成功率，而**所有基线模型均完全失败**（0%）。
- **定性分析**：基线模型（如π₀.₅、SpatialVLA）常因空间感知不精确而过早闭合夹爪。GraspVLA-S在编码独立立体视图时，难以从细微视差中持续捕获精细空间关系。

#### 2. 不同相机设置的性能与鲁棒性对比（表II）
论文系统比较了四种常见相机配置在三种相机位姿随机化范围（小、中、大）下的表现：
- **小范围随机化**：`前视+侧视`配置的GraspVLA表现最佳（82.5%），StereoVLA紧随其后（79.3%）。
- **中/大范围随机化**：StereoVLA展现出**最强的鲁棒性**，成功率分别为**71.9%** 和**61.3%**，均**排名第一**。
    - `前视+侧视`配置性能随随机化增大而**急剧下降**（从82.5%降至24.1%），因为从未对齐的视图中提取一致空间线索非常困难。
    - `前视+腕部`配置性能衰减较缓，但初始性能较低。
    - `单目`配置始终表现最差。

**结论**：立体配置在**任务性能、对相机位姿变化的鲁棒性以及部署简易性**之间取得了最佳平衡。

#### 3. 消融实验验证设计有效性
- **几何-语义特征提取模块**（表I，图5）：
    - **特征选择**：使用过滤后的代价体积 `V_c'` 作为几何特征源，结合语义特征，取得了**最高成功率（77.0%）**。仅使用立体特征（无语义）时，模型更易抓错物体。
    - **特征融合**：通道级联在性能和计算效率上均优于序列级联。
- **交互区域深度估计任务**（图6）：
    - 在**交互区域**（夹爪与目标物体周围）采样点进行深度估计，相比在整个图像均匀采样或完全不进行深度预测，能**最有效地提升模型性能并加速收敛**。

### 五、 性能提升量化总结
- 在立体设置下的**通用任务**中，StereoVLA相比基线实现了**33%** 的更高成功率。
- 在**需要高精度的任务**（如抓取条形物体、小物体）上，优势更为显著，部分任务实现从0%到30%的突破。
- 在**相机位姿大幅变化**的挑战性场景下，StereoVLA展现了**最强的鲁棒性**，性能衰减远小于依赖多视图对齐的配置。

**核心结论**：StereoVLA通过创新的**几何-语义特征提取**和**交互区域深度估计**任务，有效利用了立体视觉提供的丰富几何线索，在保持强大语义理解的同时，显著提升了VLA模型的空间感知与精细操作能力，并在部署鲁棒性上具有明显优势。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.21970v1)
- [HTML 版本](https://arxiv.org/html/2512.21970v1)
