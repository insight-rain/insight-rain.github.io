# Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone

**相关性评分**: 8.0/10

**排名**: #3


---


## 基本信息

- **arXiv ID**: [2512.22615v2](https://arxiv.org/abs/2512.22615v2)
- **发布时间**: 2025-12-27T14:46:24Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Inference Acceleration, Edge Deployment

## 一句话总结

这篇论文介绍了基于扩散语言模型的Dream-VL和Dream-VLA模型，在视觉语言和机器人控制任务中表现出色，并强调了其并行生成和快速收敛特性，与推理效率和边缘部署相关。

## 摘要

While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.

## 详细分析

## 论文摘要：Dream-VL & Dream-VLA

**1. 研究背景和动机**
当前基于自回归（AR）的大型视觉语言模型（VLMs）在序列生成任务上取得了显著成功，但其**顺序生成特性**在处理复杂的视觉规划（如多步推理）和动态机器人控制（如动作序列生成）任务时存在局限性。为了克服这些限制，本研究探索了基于**扩散大语言模型（dLLMs）** 构建视觉语言模型的潜力，旨在利用其**双向和并行生成**的优势来提升模型在规划与控制任务中的性能。

**2. 核心方法和技术创新**
- **模型架构创新**：提出了 **Dream-VL**，一个基于扩散模型的开放视觉语言模型（dVLM）。其核心是将视觉信息与扩散语言模型主干进行深度融合，构建了一个非自回归的生成框架。
- **技术扩展**：在 Dream-VL 的基础上，通过在海量开放的机器人数据集上进行持续预训练，进一步开发了 **Dream-VLA**，一个基于 dLLM 的视觉-语言-动作模型（dVLA）。
- **关键优势**：扩散模型主干**固有的双向性**使其天然适合**动作分块（chunking）和并行生成**，这为需要长序列规划和快速推理的机器人任务提供了更优的底层支持，并能在下游微调中实现**更快的收敛速度**。

**3. 主要实验结果**
- **Dream-VL** 在多项基准测试中达到了与顶尖开源 AR-VLM 相当的性能，并在**视觉规划任务**中展现出更优的潜力。
- **Dream-VLA** 在多个机器人操控基准测试中取得了领先性能：
    - **LIBERO** 任务：平均成功率高达 **97.2%**。
    - **SimplerEnv-Bridge**：总体平均分为 **71.4%**。
    - **SimplerEnv-Fractal**：总体平均分为 **60.5%**。
- 这些成绩超越了包括 $π_0$ 和 GR00T-N1 在内的领先模型。实验同时验证了在不同训练目标下，dVLM 在下游任务中普遍优于 AR 基线模型。

**4. 研究意义和价值**
本研究证明了**扩散模型作为视觉-语言-动作任务主干网络的巨大潜力**。Dream-VL 和 Dream-VLA 的成功不仅为复杂视觉推理和机器人控制提供了性能更强、效率更高的新范式，其**开放模型**的发布也将推动社区在非自回归多模态模型及其在具身智能等领域的进一步研究与应用。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### 核心问题
论文旨在解决传统**自回归大视觉语言模型**在**复杂视觉规划**和**动态机器人控制**任务中的局限性。这些局限性主要体现在：
- **顺序生成**：自回归模型逐词生成，效率较低，难以处理需要并行或双向推理的任务。
- **动作规划**：在视觉-语言-动作任务中，自回归模型在动作分块和并行生成方面存在固有困难，导致收敛慢、性能受限。

### 核心创新点
论文的核心创新是**首次将基于扩散的大语言模型作为主干，构建了开放的视觉-语言模型和视觉-语言-动作模型**，具体包括：

1.  **架构创新**：提出 **Dream-VL**，一个基于扩散的开放视觉-语言模型。它利用扩散语言模型的**原生双向特性**，克服了自回归模型顺序生成的限制。
2.  **任务扩展**：在 Dream-VL 基础上，通过持续预训练，推出 **Dream-VLA**，一个基于扩散的视觉-语言-动作模型。该模型专为机器人控制任务设计。
3.  **性能突破**：Dream-VLA 在多个机器人任务基准测试中取得了**最先进的性能**，证明了扩散主干在VLA任务中的优越性。

### 解决方案
论文通过以下方式解决了上述问题：

- **技术路径**：
    - **主干网络替换**：用**扩散大语言模型** 替代传统的自回归LLM作为核心处理单元。扩散模型在训练时学习从噪声中重建数据，其生成过程本质上是**非自回归**和**双向**的。
    - **模型构建**：
        - **Dream-VL**：将视觉编码器与扩散LLM结合，构建通用的dVLM。它在多项视觉-语言基准测试上达到了与顶尖开源AR-VLM相当的水平，并在**视觉规划任务**中展现出**更优潜力**。
        - **Dream-VL**：在Dream-VL基础上，使用**开放的机器人数据集进行持续预训练**，注入动作规划和控制的先验知识，从而得到专用于机器人任务的dVLA。

- **关键优势与实现机制**：
    - **并行生成与动作分块**：扩散主干的**双向性**使其天然适合**并行生成动作序列**，并能更好地将高层指令分解为连贯的动作块，这直接加速了机器人策略的学习。
    - **快速收敛**：在下游任务微调时，得益于其结构优势，Dream-VLA实现了**显著更快的收敛速度**。
    - **卓越性能**：在LIBERO、SimplerEnv等具身智能基准测试中，Dream-VLA的成功率**全面超越**了如 $π_0$、GR00T-N1 等领先模型，验证了其解决方案的有效性。

### 实际价值
- **为具身智能和机器人学提供了新范式**：证明了基于扩散的架构在需要复杂时序规划和实时控制的VLA任务中具有巨大潜力。
- **推动开源研究**：论文发布了Dream-VL和Dream-VLA模型，为社区提供了强大的开源工具，便于进一步探索dVLM和dVLA的应用与改进。
- **性能标杆**：在多个关键基准上设定了新的性能标杆，为后续研究提供了明确的对比基线。

**总结**：该论文的核心创新在于**架构范式的转变**——从自回归转向扩散模型作为VL/LA模型的主干，成功解决了AR模型在视觉规划和机器人控制任务中并行处理能力不足、收敛慢的问题，并通过开源模型和顶尖性能验证了该路径的可行性与优越性。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决传统自回归视觉语言模型在复杂视觉规划和动态机器人控制任务中，因序列生成特性而存在的效率与性能瓶颈。其核心方法是提出了一种基于扩散语言模型的新型架构，构建了名为Dream-VL的开放视觉语言模型，并进一步扩展为支持机器人控制的Dream-VL-A模型。该方法利用扩散模型固有的双向和并行生成能力，显著提升了模型在视觉推理和动作规划任务中的表现。最终，模型在多个标准机器人操作基准测试中取得了领先的性能，验证了扩散骨干网络在视觉-语言-动作任务中的优越潜力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文的核心创新在于**将扩散语言模型（dLLM）作为骨干网络，构建了开放视觉-语言（VLM）和视觉-语言-动作（VLA）模型**，以解决传统自回归（AR）模型在复杂任务上的局限性。具体创新点如下：

---

### 1. **架构创新：采用扩散语言模型作为视觉-语言模型的主干**
- **相比以往方法的改进/不同之处**：
    - 传统的主流视觉-语言模型（VLMs）普遍基于**自回归（AR）大型语言模型**（如GPT系列）构建，其生成过程是**顺序的、单向的**。
    - 本文提出的Dream-VL/Dream-VLA则首次系统地**基于扩散语言模型（dLLM）构建**。扩散模型本身具有**双向、非自回归**的生成特性。
- **解决的具体问题/带来的优势**：
    - **解决顺序生成的瓶颈**：AR模型在需要长序列规划或复杂推理的任务中，错误会累积，且生成速度受序列长度限制。扩散模型能并行生成所有token，**更适合需要全局规划和长程依赖的任务**，如视觉规划。
    - **提升任务潜力**：论文指出，Dream-VL在视觉规划任务上展现出**优于顶级AR-VLM的潜力**，证明了该架构在复杂场景理解与规划方面的优势。

### 2. **任务扩展创新：构建首个基于dLLM的视觉-语言-动作模型**
- **相比以往方法的改进/不同之处**：
    - 将视觉-语言模型扩展到机器人控制领域（VLA）时，以往工作（如$π_0$, GR00T-N1）大多仍沿用AR骨干。
    - 本文在Dream-VL基础上，通过**在开放机器人数据集上持续预训练**，推出了**Dream-VLA**，这是首个基于扩散骨干的VLA模型。
- **解决的具体问题/带来的优势**：
    - **动作分块与并行生成的天然适配性**：机器人控制中的“动作块”生成需要协调多个连续步骤。dLLM的**双向性使其天生适合对动作序列进行分块和并行建模**，而不是像AR模型那样必须一步步串行预测。
    - **显著加速下游微调收敛**：论文强调，这种架构使得模型在针对具体机器人任务进行微调时，**收敛速度显著快于AR基线**，提高了训练效率。

### 3. **性能创新：在多个基准测试中达到领先水平**
- **相比以往方法的改进/不同之处**：
    - 在视觉-语言任务上，Dream-VL在多项基准测试中与使用开放数据训练的顶级AR-VLM**性能相当**，打破了“扩散模型在理解任务上不如AR模型”的潜在偏见。
    - 在机器人VLA任务上，Dream-VLA在**LIBERO、SimplerEnv-Bridge、SimplerEnv-Fractal**三个关键基准上超越了当时的领先模型（如$π_0$和GR00T-N1），取得了最高平均成功率。
- **解决的具体问题/带来的优势**：
    - **证明了dLLM骨干的通用性和竞争力**：不仅在规划类任务有潜力，在标准VLM评测上也不逊色，同时在机器人具体控制任务上能实现**最先进的性能**（如LIBERO上97.2%的平均成功率），解决了AR模型在动态控制任务中可能存在的效率与效果瓶颈。
    - **提供高性能开放模型**：论文发布了Dream-VL和Dream-VLA，为社区提供了强大的开源基线，促进了扩散模型在具身AI领域的研究。

### 4. **验证了扩散骨干在下游任务中的普遍优势**
- **相比以往方法的改进/不同之处**：
    - 论文不仅提出了新模型，还**系统性地验证了dVLMs在不同训练目标下的多种下游任务中，均超越AR基线**。
- **解决的具体问题/带来的优势**：
    - **提供了强有力的实证依据**：这一验证表明，dLLM骨干的优势不是任务特异的，而是具有**普适性**的。它解决了社区对“何时以及为何使用扩散骨干”的疑问，为其在更广泛的视觉-语言任务中的应用铺平了道路。

---

**总结**：本文的核心创新是**架构范式的转变**——从主导的AR骨干转向扩散骨干，并成功将其应用于VLM和VLA两大领域。它解决了AR模型在**复杂规划、动作序列生成**方面的固有缺陷，带来了**并行生成、快速收敛、卓越规划能力**等优势，并通过开源模型和实证验证推动了该方向的发展。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

论文通过实验验证了所提出的 **Dream-VL** 和 **Dream-VLA** 模型在多个任务和数据集上的优越性能，并与当前主流基线方法进行了对比。

### 一、 评估数据集与指标

#### **1. Dream-VL (视觉语言模型) 评估**
- **任务类型**：通用视觉语言理解与视觉规划任务。
- **评估基准**：论文提及在“各种基准测试”上进行评估，虽然没有明确列出具体数据集名称，但指出这些是**开放数据**上训练的模型常用的基准，旨在衡量模型的综合视觉语言理解能力。
- **关键对比维度**：与顶尖的**自回归（AR）视觉语言模型**在开放数据训练下的性能进行对比。

#### **2. Dream-VLA (视觉语言动作模型) 评估**
- **训练数据**：在**开放机器人数据集**上通过持续预训练得到。
- **评估数据集与指标**：
    - **LIBERO**：评估机器人长视野任务的成功率。指标为**平均成功率**。
    - **SimplerEnv-Bridge** 与 **SimplerEnv-Fractal**：评估机器人操作任务性能。指标为**总体平均分**。

### 二、 对比的基线方法

1.  **对于 Dream-VL**：与在开放数据上训练的**顶级自回归（AR）视觉语言模型**进行对比。
2.  **对于 Dream-VLA**：与当前领先的机器人模型进行对比，明确提及：
    - **$π_0$**
    - **GR00T-N1**

### 三、 关键性能提升与结论

#### **Dream-VL 的主要结论**
- **性能相当**：在多种通用视觉语言基准测试上，Dream-VL 的性能与顶尖的 AR-VLM 相当。
- **核心优势显现**：在**视觉规划任务**中，Dream-VL 展现出**更优的潜力**。这得益于其扩散主干网络的**双向建模能力**，更适合进行复杂的序列规划和推理。

#### **Dream-VLA 的主要性能结果**
Dream-VLA 在三个机器人操作基准上均取得了**顶级（top-tier）性能**，具体数据如下：

| 数据集 | Dream-VLA 性能 | 对比基线 | 结论 |
| :--- | :--- | :--- | :--- |
| **LIBERO** | **97.2%** 平均成功率 | 优于 $π_0$, GR00T-N1 等 | 在长视野、多阶段任务中表现卓越。 |
| **SimplerEnv-Bridge** | **71.4%** 总体平均分 | 优于 $π_0$, GR00T-N1 等 | 在桥接类操作任务中领先。 |
| **SimplerEnv-Fractal** | **60.5%** 总体平均分 | 优于 $π_0$, GR00T-N1 等 | 在复杂分形结构任务中表现最佳。 |

#### **技术优势验证**
1.  **收敛速度**：得益于扩散模型的**双向和并行生成**特性，Dream-VLA 在**下游微调时收敛速度显著更快**。
2.  **动作分块**：扩散主干网络**天生适合动作分块生成**，这为机器人控制提供了更高效、更自然的建模方式。
3.  **下游任务普适性**：论文验证了在不同训练目标的下游任务中，**扩散视觉语言模型（dVLMs）均超越了自回归基线**。

### 总结
论文给出了**明确的定量结果**，证明其提出的基于扩散语言模型的视觉语言与视觉语言动作模型不仅在标准基准上达到一流水平，更在**视觉规划**和**机器人控制**这类需要复杂序列生成和快速收敛的任务中，**显著超越了传统的自回归模型**，展示了扩散模型在该领域的独特优势和巨大潜力。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.22615v2)
- [HTML 版本](https://arxiv.org/html/2512.22615v2)
