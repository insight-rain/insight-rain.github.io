# Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding

**相关性评分**: 8.0/10

**排名**: #13


---


## 基本信息

- **arXiv ID**: [2512.22519v1](https://arxiv.org/abs/2512.22519v1)
- **发布时间**: 2025-12-27T08:31:25Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Khoa Vo, Taisei Hanyu, Yuki Ikebe, Trong Thang Pham, Nhat Chung, Minh Nhat Vu, Duy Nguyen Ho Minh, Anh Nguyen, Anthony Gunderman, Chase Rainwater, Ngan Le

## 关键词

Vision-Language-Action Model, VLA for Robotics, Inference Efficiency, Lightweight Architecture, Edge Deployment

## 一句话总结

该论文提出OBEYED-VLA框架，通过对象中心和几何感知的感知模块增强VLA模型，提升机器人操作在杂乱环境中的鲁棒性和泛化能力。

## 摘要

Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance.
  To address these issues, we propose OBEYED-VLA (OBject-centric and gEometrY groundED VLA), a framework that explicitly disentangles perceptual grounding from action reasoning. Instead of operating directly on raw RGB, OBEYED-VLA augments VLAs with a perception module that grounds multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. This module includes a VLM-based object-centric grounding stage that selects task-relevant object regions across camera views, along with a complementary geometric grounding stage that emphasizes the 3D structure of these objects over their appearance. The resulting grounded views are then fed to a pretrained VLA policy, which we fine-tune exclusively on single-object demonstrations collected without environmental clutter or non-target objects.
  On a real-world UR10e tabletop setup, OBEYED-VLA substantially improves robustness over strong VLA baselines across four challenging regimes and multiple difficulty levels: distractor objects, absent-target rejection, background appearance changes, and cluttered manipulation of unseen objects. Ablation studies confirm that both semantic grounding and geometry-aware grounding are critical to these gains. Overall, the results indicate that making perception an explicit, object-centric component is an effective way to strengthen and generalize VLA-based robotic manipulation.

## 详细分析

## 论文摘要：OBEYED-VLA：通过物体中心与几何感知实现抗干扰的视觉-语言-动作模型

**1. 研究背景和动机**
近年来，视觉-语言-动作模型在通用机器人操作方面取得了显著进展。然而，现有的VLA模型通常将感知与控制耦合在一个端到端优化的单一流程中，这可能导致**语言条件视觉感知能力退化**。在真实世界的桌面测试中，现有模型在目标缺失时仍会过度抓取、容易被杂物干扰，并且对背景外观过拟合。为了解决这些问题，本文旨在探索一种无需合成杂乱数据或额外感知训练目标的方法，以增强VLA在杂乱场景下的鲁棒性和泛化能力。

**2. 核心方法和技术创新**
本文提出了**OBEYED-VLA**框架，其核心创新在于**显式地将感知与动作推理解耦**。该框架引入了一个独立的感知模块，将原始多视角RGB观测转换为**任务条件化、物体中心化、几何感知**的观测。该模块包含两个互补阶段：
- **物体中心感知**：利用视觉语言模型，通过“标记集”提示策略，在多个相机视角中选择与任务相关的物体区域。
- **几何感知**：通过零样本深度估计器将选定的RGB区域转换为深度图，强调物体的3D结构而非外观。
处理后的观测被输入到一个预训练的VLA策略中，该策略仅在干净的单物体演示数据上进行微调，而感知模块保持冻结。

**3. 主要实验结果**
在真实的UR10e机器人桌面平台上进行了全面评估，OBEYED-VLA在四个具有挑战性的场景中均显著优于先进的VLA基线模型：
- **存在干扰物的场景**：在多达7个干扰物的情况下，成功率仍保持在80%左右，而基线模型则崩溃至10%以下。
- **目标缺失指令拒绝**：成功率达到约95%，能有效拒绝执行，而基线模型大多会错误抓取。
- **背景外观变化**：在多种背景干扰下保持高度稳定（成功率≥80%），对背景过拟合显著降低。
- **操作未见过的物体**：在场景完全由未见物体构成时，仍能可靠执行语言指令。
消融实验证实，两阶段物体中心感知和显式几何感知对性能提升都至关重要。

**4. 研究意义和价值**
本研究证明，将**感知作为一个显式、模块化的组件**，是增强VLA策略鲁棒性和泛化性的有效途径。OBEYED-VLA框架**无需收集复杂的杂乱场景数据或设计额外的感知损失函数**，提供了一种更高效、更通用的方案来提升VLA在真实世界杂乱环境中的可靠性。这项工作为构建更健壮、更专注于任务的具身智能系统提供了新的思路和强有力的基准。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心问题**
当前主流的**视觉-语言-动作模型**在端到端优化过程中，**感知与控制的紧密耦合**导致**语言条件视觉定位能力退化**。具体表现为：
- **过度抓取**：当指令目标不存在时，模型仍会执行抓取动作。
- **易受干扰**：在杂乱场景中，模型容易被无关物体（干扰物）分散注意力。
- **过拟合背景**：模型过度依赖背景外观等非本质线索，而非任务相关的物体语义和几何结构。

### **核心创新点**
提出了 **OBEYED-VLA** 框架，其核心创新在于**显式地将感知定位与动作推理解耦**，通过一个模块化的感知定位模块，为下游VLA策略提供**任务条件化、以物体为中心、几何感知**的观测输入。

#### **1. 架构创新：解耦的模块化设计**
- **传统VLA**：端到端单体架构，感知与动作联合优化，易导致视觉-语言对齐漂移。
- **OBEYED-VLA**：引入一个**冻结的感知定位模块**，该模块独立处理原始RGB观测，生成“净化”后的视觉输入，再馈送给下游VLA进行动作推理。这是一种**即插即用**的增强方案。

#### **2. 技术创新：双阶段感知定位**
感知定位模块包含两个互补阶段：

- **以物体为中心的语义定位**
    1.  **任务感知的基视角物体定位**：利用**VLM**（如Qwen3-VL）和**Set-of-Mark**视觉提示机制，根据语言指令在基视角图像中筛选出任务相关物体区域。
    2.  **跨视角区域匹配**：将基视角筛选出的物体裁剪图作为“视觉锚点”，引导VLM在腕部视角图像中匹配同一物体，实现跨视角的语义一致定位。

- **几何定位**
    1.  使用零样本深度估计器（Depth Anything v2）将RGB图像转换为深度图。
    2.  应用上一步得到的语义定位掩码，仅保留任务相关物体的深度信息，生成**掩码深度图**作为最终视觉输入。

#### **3. 训练范式创新**
- **仅需干净数据**：下游VLA策略**仅使用无干扰、单物体的演示数据进行微调**，无需合成杂乱场景数据或引入额外的感知辅助损失函数。
- **冻结感知模块**：感知定位模块在整个过程中保持冻结，确保了其定位能力的稳定性，并降低了计算成本。

### **解决方案总结**
**OBEYED-VLA通过一个前置的、模块化的感知系统，将杂乱的RGB观测“转换”为聚焦于任务相关物体几何结构的深度图。这使得下游VLA策略能够在“净化”的视觉空间中进行推理，从而显著提升其在干扰、背景变化、目标缺失及新物体泛化等挑战下的鲁棒性和可靠性。** 该方法本质上是将VLM强大的零样本语义理解能力与VLA的动作生成能力进行了有效且高效的结合。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对当前视觉-语言-动作模型在真实杂乱场景中因感知与控制耦合优化导致的**语言条件视觉定位能力退化**问题，提出了一种名为**OBEYED-VLA**的解耦框架。该框架的核心创新在于引入了一个独立的感知模块，该模块利用视觉语言模型进行**对象中心语义定位**，并结合**深度估计进行几何信息增强**，从而将原始的杂乱RGB输入转换为任务相关、对象中心且几何感知的清晰观测。实验表明，该方法仅需在干净的单物体演示数据上微调动作策略，就能在包含干扰物、目标缺失、背景变化及未见物体等四种挑战性场景中，显著提升VLA模型的**鲁棒性、指令遵循能力和泛化性能**，且无需合成杂乱数据或额外的感知监督目标。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding》提出的 **OBEYED-VLA** 框架，在提升视觉-语言-动作（VLA）模型在杂乱场景中的鲁棒性和泛化能力方面，做出了多项明确的创新。其核心思想是**将感知与动作推理显式解耦**，而非依赖端到端的单一模型优化。

以下是其相对于已有工作的主要创新点：

---

### 1. **架构创新：感知与动作的显式解耦与模块化设计**
- **相比以往方法的改进/不同之处**：
    - **传统VLA方法**：采用“单体式”架构，将视觉感知和动作推理紧密耦合，并仅通过动作预测损失进行端到端优化。这导致模型在学习控制策略时，可能牺牲或削弱了从预训练VLM继承的、精细的语言-视觉对齐能力。
    - **OBEYED-VLA**：提出一个**模块化框架**，在原始VLA模型前增加一个**独立的、冻结的感知模块**。该模块负责处理原始RGB观测，生成经过语义和几何“提纯”的观测，再输入给下游VLA进行动作推理。感知模块与动作策略是分离的。
- **解决的具体问题/带来的优势**：
    - **解决了“感知漂移”问题**：避免了在微调VLA以适应新任务时，因仅优化动作损失而导致的语言-视觉对齐能力退化。
    - **提升了模块复用性和灵活性**：同一个感知模块可以“即插即用”地搭配不同的VLA骨干模型、机器人平台或环境，无需为每个新设置重新设计或训练感知部分。
    - **降低了数据需求**：无需为了提升鲁棒性而收集大量包含合成杂乱场景的演示数据，也无需在VLA训练中引入复杂的辅助感知目标（如重建损失、对比对齐损失）。

### 2. **技术创新：双阶段、跨视角的物体中心语义感知**
- **相比以往方法的改进/不同之处**：
    - **传统方法**：一些工作也尝试利用VLM进行高层感知，但通常是在单视角上独立进行物体识别或分割，或者需要针对特定输出（如关键点、轨迹）对VLM进行大量微调。
    - **OBEYED-VLA**：设计了一个**两阶段的物体中心感知流程**：
        1.  **任务感知的基础视角物体定位**：在基础视角（通常视野更全）上，利用VLM和“标记集”提示，根据语言指令筛选出任务相关物体。
        2.  **跨视角区域匹配**：将上一步筛选出的物体裁剪图作为“视觉锚点”，引导VLM在手腕视角（通常视角刁钻、物体外观变化大）的图像中匹配对应区域。这是一个**基于参考的、跨视图的提示机制**。
- **解决的具体问题/带来的优势**：
    - **解决了手腕视角 grounding 脆弱的问题**：直接在手腕视角进行语言 grounding 非常困难，因为物体外观与VLM常见训练数据差异大。通过引入基础视角的参考裁剪图，极大地提升了跨视角物体关联的准确性和鲁棒性。
    - **实现了精确的语义筛选**：能够可靠地从杂乱场景中识别并隔离出与指令严格相关的物体区域，有效抑制了无关物体（干扰物）和背景的视觉干扰。这直接提升了在**高干扰物场景**和**空间关系指令**（如“左边的物体”）下的任务成功率。

### 3. **技术创新：显式的几何感知增强**
- **相比以往方法的改进/不同之处**：
    - **传统VLA方法**：主要依赖RGB图像作为视觉输入，模型可能过度依赖物体的颜色、纹理等外观特征，而非其几何结构。
    - **OBEYED-VLA**：在语义筛选的基础上，引入了一个**几何感知模块**。该模块使用零样本深度估计器将RGB转换为深度图，并**仅保留任务相关物体区域的深度信息**，生成掩码后的深度图作为VLA的最终视觉输入。
- **解决的具体问题/带来的优势**：
    - **减少了对表观特征的过拟合**：通过提供以深度信息为主的输入，迫使策略更多地关注物体的**三维形状和空间布局**，而非其表面的颜色或纹理。
    - **提升了泛化能力**：这一特性对于**操作未见过的物体**至关重要。即使新物体的外观与训练数据不同，只要其几何形状可抓取，策略仍能成功操作。消融实验证实，几何感知对未见物体任务的成功率有显著提升（约8个绝对百分点）。
    - **增强了背景变化的鲁棒性**：深度信息对背景的颜色、图案变化相对不敏感，因此有助于模型在**背景外观发生剧烈变化**时保持稳定的性能。

### 4. **训练范式创新：仅用干净数据训练，实现杂乱场景泛化**
- **相比以往方法的改进/不同之处**：
    - **传统提升鲁棒性的方法**：通常需要在训练数据中大量引入合成或真实的杂乱场景、干扰物，并可能需要额外的感知标注（如边界框、分割掩码）来设计辅助损失函数。这需要高昂的数据收集和标注成本。
    - **OBEYED-VLA**：其下游VLA策略**仅使用在干净、单物体场景下收集的演示数据进行微调**。所有应对杂乱、干扰、背景变化和未见物体的能力，都来自于前端的、冻结的感知模块对输入进行的“在线”提纯和转换。
- **解决的具体问题/带来的优势**：
    - **大幅降低了数据收集的难度和成本**：无需为应对各种复杂场景而采集海量、多样的机器人演示数据。
    - **实现了“训练简单，部署鲁棒”**：在训练阶段，只需要简单的单物体操作数据，这更容易通过遥操作获取。在部署时，通过感知模块的实时处理，策略自然具备了应对复杂场景的能力。
    - **提供了一种高效利用现有大规模VLM能力的新范式**：将互联网规模数据预训练出的强大VLM的零样本感知能力，以一种模块化的方式“注入”到机器人控制流程中，弥补了VLA在专门感知任务上的不足。

---

### 总结
OBEYED-VLA 的核心创新在于其**系统性的设计哲学**：它不试图通过修补单体VLA的损失函数或扩大数据集来解决问题，而是**重新思考了感知与控制在架构层面的关系**。通过引入一个**模块化、双阶段（语义+几何）、物体中心的感知前端**，它将VLM的强语义理解与VLA的强动作推理能力有效结合，在**不增加VLA训练复杂度**的前提下，显著解决了现有VLA在**干扰物敏感、背景过拟合、语言 grounding 退化、未见物体泛化差**等方面的关键瓶颈。其实验结果在真实机器人桌面上全面验证了该框架的有效性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

论文通过一系列精心设计的真实世界实验，全面评估了所提出的 **OBEYED-VLA** 框架的性能。实验旨在验证其在**抗干扰、鲁棒性和泛化性**方面的提升。

### 1. 实验设置与数据集

*   **机器人平台**： UR10e 六自由度机械臂，配备 Robotiq 2F-85 平行夹爪。
*   **视觉输入**： 双摄像头（肩部固定视角 + 腕部视角）。
*   **训练数据集**：
    *   **数据内容**： 2000 条真实世界示教数据，均为**单物体、无干扰**的拾放任务。
    *   **训练物体**： 8 种日常杂货物品（如番茄酱瓶、咖啡袋、芥末瓶等）。
    *   **语言指令**： 使用多种同义模板（如 “place `<object>` in the bin”），以减少对特定措辞的依赖。
*   **评估场景（四大挑战）**：
    1.  **干扰物体**： 场景中包含 1、4、7 个来自训练类别的干扰物。
    2.  **目标缺失拒绝**： 指令请求的物体不在场景中，模型应拒绝执行抓取。
    3.  **背景外观变化**： 改变桌布、背景板或在桌上放置彩色纸张。
    4.  **未见物体操作**： 使用 7 种**未在训练中见过**的新物体进行拾放，场景中包含未见过的干扰物。

### 2. 评价指标

*   **核心指标**： **任务成功率**。对于拾放任务，成功定义为正确识别并抓取目标物体，并将其放入指定容器。
*   **目标缺失任务**： 成功定义为**不执行任何抓取动作**。
*   **统计报告**： 所有结果均报告在多次运行（通常为 100 次）上的平均成功率，并附有 **95% 置信区间**。

### 3. 对比的基线方法

论文与当前最先进的 **VLA 模型**进行了对比，所有基线模型均在相同的**单物体、无干扰**数据集上进行微调：
*   **Pi-0**： 基于流匹配的 VLA 模型。
*   **Pi-0 FAST**： Pi-0 的加速版本。
*   **Pi-0.5**： 在更大规模多模态数据上共同训练的 VLA 模型。
*   **Gr00T 1.5**： NVIDIA 提出的通用具身智能体模型。

### 4. 关键性能提升与结论

#### (1) 抗干扰语言跟随能力（Q1）
*   **结果**： 随着干扰物数量从 0 增加到 7，基线 VLA 模型的成功率从 >80% **急剧下降至 <10%**。而 **OBEYED-VLA (Pi-0 和 Pi-0 FAST 版本) 在 1 个干扰物时成功率仍 >90%，在 7 个干扰物时仍能保持约 80%**。
*   **结论**： OBEYED-VLA 的感知接地模块有效过滤了干扰物，使策略能专注于任务相关物体，抗干扰能力显著优于基线。

#### (2) 目标缺失拒绝与空间推理（Q1）
*   **目标缺失拒绝**： OBEYED-VLA 成功率 **~95%**，而基线模型（除 Pi-0.5 的 ~40% 外）普遍在 **10-15%**。这表明基线模型倾向于“见物就抓”，而 OBEYED-VLA 能严格遵循语言指令的语义。
*   **空间推理**（如“抓取左边的物体”）： OBEYED-VLA 成功率 **~75%**，比最佳基线（Pi-0 FAST）高出 **超过 40 个百分点**。
*   **结论**： 显式的物体中心感知极大地增强了模型对指令可行性的判断和对空间关系的理解能力。

#### (3) 背景变化鲁棒性（Q2）
*   **结果**： 在四种逐渐增强的背景干扰下，**OBEYED-VLA 性能保持稳定（≥80%）**，仅从无干扰单物体场景略有下降。而所有基线模型均出现显著性能衰减，尤其是在桌面区域（如彩色纸张、桌布）发生变化时，Pi-0.5 甚至崩溃至接近零成功率。
*   **结论**： 通过将观察转换为以物体和几何为中心的表示，OBEYED-VLA 减少了对背景外观的过拟合，表现出优异的分布外鲁棒性。

#### (4) 对未见物体的泛化能力（Q3）
*   **结果**： 在场景完全由未见物体（1个目标+4个干扰物）构成的情况下，**OBEYED-VLA (Pi-0 FAST) 取得了最高成功率**，显著优于所有基线。基线模型在未见干扰物下性能大幅下降。
*   **结论**： **几何感知接地**（使用深度图）鼓励模型依赖物体的**3D几何结构而非外观纹理**，这是实现对新物体类别成功泛化的关键。

#### (5) 消融实验（Q4, Q5）
*   **两阶段物体中心接地的效果**： 移除跨视角参考匹配（单阶段接地）导致在4干扰物任务上成功率下降约16个百分点，空间推理任务下降约30个百分点。**证明了跨视角、基于参考的提示对于鲁棒语义消歧至关重要**。
*   **几何接地的效果**： 仅使用掩码后的RGB图像（无深度）输入，在未见物体任务上的成功率比完整模型下降8个百分点。**证明了深度表征提供的几何线索能减少对物体外观的依赖，提升泛化性能**。

### 5. 运行效率分析
*   **控制频率**： OBEYED-VLA 结合 Pi-0 和 Pi-0 FAST 的端到端控制周期分别为 **0.88秒** 和 **1.16秒**（约 **0.9-1.1 Hz**）。
*   **模块耗时**： 物体中心接地（调用VLM）是主要耗时部分（~0.41秒），但由于场景静态，该步骤仅在任务开始时执行一次。分割、几何转换和策略推理均在亚秒级完成。
*   **结论**： 虽然引入了额外模块，但整体速度足以支持其实验中的桌面操作任务。

### 总结
论文通过系统的定量实验证明，**OBEYED-VLA 框架在不依赖合成干扰数据或额外感知损失的情况下，显著提升了 VLA 模型在复杂、动态真实环境中的鲁棒性、指令遵循准确性和跨物体泛化能力**。其核心创新——**将感知接地显式化、模块化，并融合语义与几何信息**——被实验证实是有效的技术路径。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2512.22519v1)
- [HTML 版本](https://arxiv.org/html/2512.22519v1)
