# Efficient Online Reinforcement Learning for Diffusion Policy

**相关性评分**: 7.0/10

**排名**: #4


---


## 基本信息

- **arXiv ID**: [2502.00361v4](https://arxiv.org/abs/2502.00361v4)
- **发布时间**: 2025-02-01T07:55:06Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Haitong Ma, Tianyi Chen, Kai Wang, Na Li, Bo Dai

## 关键词

reinforcement learning (RL), mujoco, humanoid robot

## 一句话总结

该论文提出了一种高效的在线强化学习方法，通过重加权分数匹配优化扩散策略，在MuJoCo基准测试中显著提升了Humanoid和Ant等任务的性能。

## 摘要

Diffusion policies have achieved superior performance in imitation learning and offline reinforcement learning (RL) due to their rich expressiveness. However, the conventional diffusion training procedure requires samples from target distribution, which is impossible in online RL since we cannot sample from the optimal policy. Backpropagating policy gradient through the diffusion process incurs huge computational costs and instability, thus being expensive and not scalable. To enable efficient training of diffusion policies in online RL, we generalize the conventional denoising score matching by reweighting the loss function. The resulting Reweighted Score Matching (RSM) preserves the optimal solution and low computational cost of denoising score matching, while eliminating the need to sample from the target distribution and allowing learning to optimize value functions. We introduce two tractable reweighted loss functions to solve two commonly used policy optimization problems, policy mirror descent and max-entropy policy, resulting in two practical algorithms named Diffusion Policy Mirror Descent (DPMD) and Soft Diffusion Actor-Critic (SDAC). We conducted comprehensive comparisons on MuJoCo benchmarks. The empirical results show that the proposed algorithms outperform recent diffusion-policy online RLs on most tasks, and the DPMD improves more than 120% over soft actor-critic on Humanoid and Ant.

## 详细分析

## 论文摘要：用于扩散策略的高效在线强化学习

### 1. 研究背景和动机
扩散策略因其强大的表达能力和多模态特性，在模仿学习和离线强化学习中表现出色。然而，传统的扩散模型训练需要从目标分布中采样数据，这在**在线强化学习**中无法实现，因为我们无法从未知的最优策略中采样。此外，通过扩散过程反向传播策略梯度会带来巨大的计算成本和稳定性问题。因此，如何**高效地在在线强化学习中训练扩散策略**成为一个关键挑战。

### 2. 核心方法和技术创新
本文的核心贡献是提出了**重加权分数匹配**（Reweighted Score Matching, RSM）。该方法通过重新加权传统的去噪分数匹配损失函数，将其推广为一个更通用的损失函数族。
- **理论创新**：基于“扩散模型即噪声扰动能量模型”的观点，RSM在保留去噪分数匹配最优解和低计算成本的同时，**消除了对目标分布采样的需求**，并允许基于价值函数进行优化学习。
- **算法创新**：通过选择不同的加权函数，RSM可以求解两种常见的策略优化问题（策略镜像下降和最大熵策略），从而衍生出两个实用算法：
    - **扩散策略镜像下降**（Diffusion Policy Mirror Descent, DPMD）
    - **软扩散行动者-评论家**（Soft Diffusion Actor-Critic, SDAC）
- **技术优势**：与近期其他试图将扩散模型应用于在线RL的方法相比，RSM方法避免了有偏估计、巨大的内存/计算成本以及递归梯度传播的不稳定性，实现了**高效且可扩展**的训练。

### 3. 主要实验结果
在MuJoCo基准测试上进行的全面实验表明：
- 所提出的**DPMD和SDAC算法在大多数任务上优于近期的扩散策略在线RL基线方法**。
- 在复杂的运动任务上（如Humanoid和Ant），**DPMD相比经典的软行动者-评论家算法性能提升超过120%**，显著展示了扩散策略在在线RL中的潜力。
- 算法在计算时间和内存消耗上与轻量级基线相当，证明了其**高效性**。

### 4. 研究意义和价值
本研究弥合了基于能量模型的策略优化理论与在线强化学习实践之间的鸿沟。RSM不仅为在线RL中的扩散策略训练提供了一个**高效、通用的框架**，其思想也可应用于其他仅已知能量函数的概率建模问题（如玻尔兹曼采样）。这项工作推动了表达性策略家族在在线RL中的应用，为处理复杂、多模态的决策问题开辟了新途径。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文分析：《Efficient Online Reinforcement Learning for Diffusion Policy》

### **一、 核心问题**
论文旨在解决**扩散策略（Diffusion Policy）在在线强化学习（Online RL）中难以高效训练**的根本性难题。具体而言，存在两大挑战：

1.  **采样挑战**：传统扩散模型训练（如去噪分数匹配，DSM）需要从目标数据分布中采样。然而，在在线RL中，最优策略（即目标分布）是未知且正在被优化的，无法直接从中采样。
2.  **计算挑战**：若将反向扩散过程视为策略的参数化表示，并通过它反向传播策略梯度，会导致巨大的计算和内存开销，且训练不稳定，难以扩展到复杂任务。

### **二、 核心创新点**
论文提出了 **“重加权分数匹配”（Reweighted Score Matching, RSM）** ，这是一个用于训练扩散模型的通用损失函数族。RSM通过**重新加权**传统的去噪分数匹配损失，在**不改变最优解**的前提下，绕过了上述两大挑战。

#### **关键技术贡献**：
1.  **理论框架**：将扩散模型视为**噪声扰动的基于能量的模型（Noise-Perturbed EBMs）**。这建立了扩散策略与RL中两类常见的最优策略形式（基于能量的策略）之间的联系。
2.  **RSM损失函数**：
    ```math
    ℒ^g(θ; s, t) = ∫ g(a_t; s) ‖ s_θ(a_t; s, t) - ∇_{a_t} log p_t(a_t|s) ‖^2 da_t
    ```
    其中 `g(a_t; s)` 是任意严格正的重加权函数。通过精心设计 `g`，可以消除对目标分布 `p_0` 采样的依赖。
3.  **两个高效算法**：针对RL中两种核心的策略优化问题，提出了两种具体的、可计算的RSM损失，并推导出两个实用算法：
    - **DPMD (Diffusion Policy Mirror Descent)**：用于解决**策略镜像下降**问题。其损失函数允许从**当前策略** `π_old` 采样，并用 `Q` 函数进行指数加权。
    - **SDAC (Soft Diffusion Actor-Critic)**：用于解决**最大熵策略**问题。其核心是 **“反向采样技巧”** ，允许从一个**任意的采样分布** `h_t`（如均匀分布或当前策略）中采样，并通过一个条件高斯分布 `ϕ_{0|t}` 来估计与能量函数（`Q`函数）相关的权重。

### **三、 解决方案与优势**
#### **如何解决核心问题**：
- **解决采样挑战**：DPMD 从当前策略采样，SDAC 从自定义分布采样，**完全避免了从未知最优策略采样**。
- **解决计算挑战**：RSM导出的损失函数（`ℒ_DPMD` 和 `ℒ_SDAC`）在形式上与标准的去噪分数匹配（DSM）非常相似，**计算开销与DSM相当**，远低于通过整个扩散链反向传播梯度的方法。

#### **相比现有方法的优势**：
论文将现有扩散策略在线RL方法分为三类并指出其缺陷，而RSM方法克服了这些缺陷：
- **基于分数的玻尔兹曼采样**（如QSM）：直接对学习的能量函数（`Q`函数）求导得到分数，估计不准确，性能差。
- **反向扩散作为策略参数化**（如DACER）：通过扩散链反向传播梯度，计算和内存成本巨大。
- **其他方法**（如DIPO， QVPO）：或需要维护庞大的扩散缓冲区，或存在近似误差且无法处理负`Q`值。

**RSM方法在保持理论最优性的同时，实现了与标准扩散模型训练相近的高效性和稳定性。**

### **四、 实际价值与实验结果**
- **理论到实践的桥梁**：使两类理论上优美但实践中难以处理的基于能量的策略（镜像下降策略和最大熵策略）能够通过表达力极强的扩散模型高效实现。
- **卓越的性能**：在MuJoCo基准测试上，DPMD和SDAC在大多数任务上超越了近期的扩散策略在线RL基线。**在复杂的`Humanoid`和`Ant`任务上，DPMD相比经典算法SAC取得了超过120%的性能提升**，充分展现了扩散策略在在线RL中的潜力。
- **计算高效**：实验表明，DPMD/SDAC的GPU内存占用和训练时间与最轻量的基线方法（QSM）相当，远低于其他基于扩散的RL方法，证实了其“高效”的特性。

### **五、 总结**
该论文的核心创新在于**RSM这一理论框架**，它通过**广义化的损失函数重加权**，巧妙地解决了扩散策略在在线RL中训练的根本性障碍。由此衍生的**DPMD和SDAC算法**，首次实现了**高效、稳定且性能卓越**的扩散策略在线训练，弥合了基于能量的策略理论与表达性策略实践之间的鸿沟，为在线RL开辟了新的方向。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**扩散策略在在线强化学习中训练效率低下**的核心问题。传统扩散模型训练（如去噪分数匹配）需要从目标分布（即最优策略）中采样，这在在线RL中无法实现，而通过扩散过程反向传播策略梯度则计算成本高昂且不稳定。为此，论文提出了**重加权分数匹配（RSM）** 这一通用损失函数族，它通过重新加权损失函数，在保留去噪分数匹配最优解和低计算成本的同时，消除了对目标分布采样的需求。基于RSM，论文推导出两个可处理的损失函数，分别对应策略镜像下降和最大熵策略优化问题，从而提出了**DPMD**和**SDAC**两个高效算法。在MuJoCo基准测试上的实验结果表明，所提算法在大多数任务上超越了近期的扩散策略在线RL方法，其中DPMD在Humanoid和Ant任务上相比SAC取得了超过120%的性能提升，证明了扩散策略在在线RL中的巨大潜力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Efficient Online Reinforcement Learning for Diffusion Policy》针对在线强化学习中扩散策略训练的核心难题，提出了系统性的创新解决方案。其主要创新点可归纳如下：

### 1. **提出重加权分数匹配（Reweighted Score Matching, RSM）**
   - **改进/不同之处**：
     - 传统扩散模型训练（如去噪分数匹配，DSM）需要从**目标数据分布**（即最优策略）中采样，这在在线RL中是不可能的，因为最优策略是待学习的未知对象。
     - RSM通过引入一个严格正的**重加权函数** `g(a_t; s)`，对原始的分数匹配损失进行泛化。其核心形式为：
       ```math
       ℒ^g(θ; s, t) = ∫ g(a_t; s) ‖ s_θ(a_t; s, t) - ∇_{a_t} log p_t(a_t|s) ‖² da_t
       ```
       其中 `p_t` 是噪声扰动后的目标策略分布。
     - 这**摆脱了对目标策略样本的依赖**，允许损失函数在任意选择的分布（如当前策略 `π_old` 或一个设计好的采样分布 `h_t`）上进行积分，只要重加权函数 `g` 选择得当。
   - **解决的问题与优势**：
     - **根本性解决了在线RL中扩散策略训练的“采样难题”**：无需从最优策略（一个能量基模型，EBM）中采样，这是将扩散策略成功应用于在线RL的关键前提。
     - **保持了理论最优解**：论文证明了在适当选择 `g` 的情况下，最小化 `ℒ^g` 的最优解仍然是匹配噪声扰动后的目标分数 `∇ log p_t`，从而保证了策略优化的正确性。
     - **计算高效**：最终的损失形式可以转化为类似于标准DSM的、易于蒙特卡洛估计的表达式，避免了通过整个反向扩散过程进行反向传播带来的巨大计算和内存开销。

### 2. **基于RSM推导出两个高效实用算法：DPMD 和 SDAC**
   - **改进/不同之处**：
     - **DPMD (Diffusion Policy Mirror Descent)**：针对**策略镜像下降**问题（式(1)），其闭式解是一个以当前策略和Q函数加权的EBM（式(2)）。通过选择重加权函数 `g_MD = Z_MD(s) * p_t(a_t|s)`，RSM损失被转化为一个**仅需从当前策略 `π_old` 采样**的 tractable 损失 `ℒ_DPMD`（式(13)）。
     - **SDAC (Soft Diffusion Actor-Critic)**：针对**最大熵策略**问题（式(5)），其闭式解是Softmax策略。通过选择 `g_MaxEnt = h_t(a_t|s) * Z(s) * p_t(a_t|s)` 并引入**逆向采样技巧**，RSM损失被转化为一个**从任意设计分布 `h_t` 采样**的 tractable 损失 `ℒ_SDAC`（式(14)）。其中，`ϕ_{0|t}` 是一个与前向扰动核 `q_{t|0}` 成比例的高斯分布，使得采样无需依赖 `p_0`。
   - **解决的问题与优势**：
     - **为两个经典但实践困难的策略优化问题提供了高效的扩散策略解法**：
       - 策略镜像下降（与TRPO/PPO相关）和最大熵RL（与SAC相关）在连续动作空间中的理论最优解都是EBM，传统方法通常将其投影到高斯分布，损失了表达能力。
       - DPMD和SDAC利用扩散模型作为**噪声扰动的EBM**，能够精确表达这些复杂的多模态分布，从而**弥合了理论与实践的鸿沟**。
     - **计算成本与标准DSM相当**：两个算法的损失函数计算都类似于DSM，只涉及对得分网络输出的L2误差计算，避免了递归梯度传播（如DACER）或维护大型扩散缓冲区（如DIPO）带来的高昂成本。

### 3. **系统性地解决了扩散策略在在线RL中的应用障碍**
   - **改进/不同之处**：
     - 论文不仅提出了核心训练方法（RSM），还**深入探讨并提供了实际部署中的关键技巧**：
       1. **批量动作采样与选择**（式(16)）：采样多个动作后选择Q值最高的执行，以平衡扩散策略的探索性与高效利用。
       2. **对数似然近似**：在策略评估中，用添加的高斯噪声的概率来近似策略的对数概率，解决了扩散策略似然计算难的问题。
       3. **数值稳定性处理**：对DPMD使用Q值的滑动平均归一化；对SDAC使用 `logsumexp` 技巧，防止指数项爆炸。
       4. **采样分布 `h_t` 的灵活选择**：实验表明，均匀分布、当前策略或其扰动分布均表现良好，提供了灵活性。
   - **解决的问题与优势**：
     - **使理论算法具备实际可行性**：这些工程细节对于算法在复杂环境中的稳定性和性能至关重要。
     - **实现了效率与性能的平衡**：如表2所示，DPMD/SDAC在GPU内存占用和训练时间上，与最轻量的基线QSM相当，远低于其他复杂的扩散RL方法（如DACER, DPPO），同时取得了最优的性能。

### 4. **广泛的实验验证与卓越的性能表现**
   - **改进/不同之处**：
     - 在10个MuJoCo任务上进行了全面评估，对比了5种近期扩散RL方法和3种经典模型无关RL方法。
     - 结果（表1，图3）显示，**DPMD和SDAC在大多数任务上超越了所有基线**，尤其在复杂 locomotion 任务（HalfCheetah, Walker2d, Ant, Humanoid）上提升显著（例如，DPMD在Humanoid和Ant上相比SAC提升超过120%）。
     - 进行了**敏感性分析**（图4），验证了算法对扩散步数和噪声调度方案的鲁棒性。
   - **解决的问题与优势**：
     - **实证了所提方法的有效性和优越性**：强有力的实验证据表明，基于RSM的扩散策略训练能够充分释放扩散模型在在线RL中的潜力，取得SOTA性能。
     - **展示了扩散策略在在线RL中的真正潜力**：此前的工作或因偏差、或因计算成本过高而未能充分展现扩散模型的优势，本文的方法首次系统性地解决了这些问题，并取得了突破性的性能提升。

### 总结
本文的核心创新在于**理论、算法与工程实践的统一**：
1.  **理论层面**：提出RSM，为在只能获取能量函数（如Q函数）而无法采样的场景下训练扩散模型提供了通用框架。
2.  **算法层面**：将RSM具体应用于在线RL的两个核心问题，推导出高效、精确的算法DPMD和SDAC。
3.  **实践层面**：提供了完整的、稳定的实现方案，并通过大量实验证明了其卓越的性能和效率。

这些创新共同解决了长期存在的“如何高效、精确地在在线RL中训练表达力极强的扩散策略”这一难题，为能量基策略的理论研究与实践应用搭建了桥梁。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 实验数据集与评价指标
- **数据集**：论文在**MuJoCo v4**的10个连续控制基准任务上进行了全面评估，具体包括：
    - `HalfCheetah`, `Reacher`, `Humanoid`, `Pusher`, `InvertedPendulum`
    - `Ant`, `Hopper`, `Swimmer`, `Walker2d`, `InvertedDoublePendulum`
- **评价指标**：**平均回报（Average Return）**。在训练过程中，每25k次迭代（Humanoid为125k次）评估一次，每次评估运行20个回合，最终报告**最佳平均回报及其标准差**（基于5个随机种子）。

### 二、 对比的基线方法
论文与两大类基线方法进行了对比：

1.  **基于扩散策略的在线RL方法（主要对比对象）**：
    - **QSM**：通过Q函数微分得到分数函数，使用朗之万动力学采样。
    - **QVPO**：提出Q加权的变分目标训练扩散策略，但无法处理负Q值。
    - **DACER**：通过反向扩散过程反向传播梯度，并引入GMM熵调节器。
    - **DIPO**：两阶段方法，维护状态-动作粒子并用扩散模型拟合。
    - **DPPO**：构建包含扩散步骤和环境步骤的双层MDP，并应用PPO。

2.  **经典模型无关的在线RL方法**：
    - **PPO** (Proximal Policy Optimization)
    - **TD3** (Twin Delayed DDPG)
    - **SAC** (Soft Actor-Critic)

### 三、 关键性能结果与结论
论文提出的两种算法**DPMD**和**SDAC**在绝大多数任务上超越了所有基线方法，展现了扩散策略在在线RL中的巨大潜力。

#### 1. **整体性能领先**
- 在10个MuJoCo任务中，**DPMD**和**SDAC**在大多数任务上取得了**最佳或接近最佳的性能**。
- **DPMD**表现尤为稳健，在所有任务上均表现优异；**SDAC**在除`Ant`外的任务上与DPMD性能相当。

#### 2. **关键性能提升（与SAC对比）**
论文特别强调了在复杂 locomotion 任务上相对于经典算法SAC的显著提升：
- **Humanoid**: DPMD和SDAC均实现了**超过100%** 的性能提升。
- **Ant**: **DPMD实现了超过120%** 的性能提升。
- **HalfCheetah**: 最佳算法SDAC实现了约**36%** 的提升。
- **Walker2d**: DPMD实现了约**35%** 的提升。

#### 3. **与扩散策略基线的对比优势**
- **计算与内存效率**：DPMD/SDAC的计算时间和GPU内存消耗与最轻量的QSM相当，远低于其他需要反向传播或维护额外缓冲区的扩散方法（如DACER、DIPO、DPPO）。这表明**RSM方法在保持高性能的同时，没有引入额外的计算开销**。
- **稳定性与鲁棒性**：DPMD在所有任务上表现稳定。其他扩散基线在某些任务上存在严重失败：
    - QSM在`InvertedDoublePendulum`上失败（因Q函数不平滑导致分数估计差）。
    - QVPO在`Reacher`和`Pusher`上失败（无法处理负Q值）。
    - DACER在`InvertedPendulum`上失败（梯度不稳定）。

#### 4. **消融与敏感性分析**
- **扩散步数**：10步和20步性能相当且优于30步，最终选择20步。
- **噪声调度**：线性和余弦调度性能相似，均优于方差保持调度，最终选择余弦调度。
- 结果表明算法对超参数具有**鲁棒性**。

#### 5. **玩具实验验证**
在**2D高斯混合模型**和**Two-Moon分布**的Boltzmann采样任务中，验证了**RSM损失函数的有效性**。仅使用能量函数（无需真实样本）训练的扩散模型，其采样质量与使用真实样本训练的DDPM相当，并且显著优于传统的朗之万动力学方法。

### 四、 核心结论
1.  **技术创新成功转化**：论文提出的**Reweighted Score Matching (RSM)** 成功解决了在线RL中训练扩散策略的**采样挑战**（无法从最优策略采样）和**计算挑战**（反向传播梯度开销大），使理论上有吸引力的基于能量的策略得以高效实践。
2.  **性能突破**：DPMD和SDAC算法在标准基准测试中实现了**SOTA性能**，特别是在高维、复杂的机器人控制任务上，大幅超越了经典的SAC以及近期其他扩散RL方法，证明了**扩散策略在在线RL中的真正潜力**。
3.  **效率与效果兼备**：所提方法在取得卓越性能的同时，保持了与标准去噪分数匹配相近的**计算和内存效率**，为扩散策略的实际应用扫清了关键障碍。

**总结**：该论文通过理论创新（RSM）衍生出高效算法（DPMD/SDAC），在MuJoCo基准上取得了显著的性能提升，验证了其方法在保持高效计算的同时，能够释放扩散模型在表达复杂、多模态策略方面的优势，从而弥合了基于能量的策略理论与在线强化学习实践之间的差距。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2502.00361v4)
- [HTML 版本](https://arxiv.org/html/2502.00361v4)
