# Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults

**相关性评分**: 6.0/10

**排名**: #20


---


## 基本信息

- **arXiv ID**: [2501.15373v1](https://arxiv.org/abs/2501.15373v1)
- **发布时间**: 2025-01-26T03:03:02Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Xinyang Wang, Hongwei Zhang, Shimin Wang, Wei Xiao, Martin Guay

## 关键词

reinforcement learning (RL), robot control, quadruped robot locomotion, multi-agent RL, CTDE methods, Edge Deployment, humanoid robot, robot dog, mujoco, local observational information

## 一句话总结

这篇论文提出了一种结合高阶控制屏障函数和强化学习的框架，用于在非线性系统中平衡安全性和性能，但未直接涉及多智能体、边缘部署或特定机器人平台。

## 摘要

Merely pursuing performance may adversely affect the safety, while a conservative policy for safe exploration will degrade the performance. How to balance the safety and performance in learning-based control problems is an interesting yet challenging issue. This paper aims to enhance system performance with safety guarantee in solving the reinforcement learning (RL)-based optimal control problems of nonlinear systems subject to high-relative-degree state constraints and unknown time-varying disturbance/actuator faults. First, to combine control barrier functions (CBFs) with RL, a new type of CBFs, termed high-order reciprocal control barrier function (HO-RCBF) is proposed to deal with high-relative-degree constraints during the learning process. Then, the concept of gradient similarity is proposed to quantify the relationship between the gradient of safety and the gradient of performance. Finally, gradient manipulation and adaptive mechanisms are introduced in the safe RL framework to enhance the performance with a safety guarantee. Two simulation examples illustrate that the proposed safe RL framework can address high-relative-degree constraint, enhance safety robustness and improve system performance.

## 详细分析

## 论文摘要

### 1. 研究背景和动机
在基于强化学习（RL）的最优控制问题中，单纯追求性能可能损害系统安全，而过于保守的安全策略又会降低性能。如何在学习过程中平衡安全与性能是一个关键挑战。现有方法，如控制屏障函数（CBF），在处理**高阶相对度约束**（如机器人运动规划中的速度和加速度约束）以及**未知时变扰动/执行器故障**时存在局限性，要么计算成本高，要么鲁棒性不足。本文旨在解决存在上述复杂约束和未知扰动的非线性系统安全最优控制问题。

### 2. 核心方法和技术创新
本文提出了一个**自适应、基于保障的安全强化学习框架**，主要技术创新包括：
- **高阶互易控制屏障函数（HO-RCBF）**：新提出的一类CBF，能够处理学习过程中的高阶相对度状态约束，与RL有效结合。
- **梯度相似性与梯度操纵**：提出了**梯度相似性**概念来量化安全梯度与性能梯度之间的冲突关系。通过引入梯度操纵机制，调整保障控制项的方向，从而在保证安全的同时提升系统性能。
- **自适应高阶保障策略**：将保障增益 \(K_s\) 本身视为一个CBF，并为其设计自适应律。该机制能根据系统状态（如接近安全边界的程度、当前性能成本）动态调整增益大小，在需要时增强安全保障，在安全范围内则降低增益以优化性能，实现了安全与性能的**理论可证的加权平衡**。

### 3. 主要实验结果
通过两个仿真案例验证了所提框架的有效性：
1.  **倒立摆系统**：在存在未知执行器故障的情况下，与经典RL、基于惩罚项的RL和基于安全滤波器的RL相比，本文方法能**唯一**在满足角度（高阶约束）和角速度（一阶约束）安全约束的同时稳定系统。
2.  **移动机器人导航**：在包含障碍物（高阶约束）、工作区域边界（高阶约束）和速度限制（一阶约束）的复杂场景中，本文方法能成功处理**不同相对度的混合约束**，引导机器人安全到达目标点。同时，自适应机制与梯度操纵显著降低了累积控制成本，提升了性能。

### 4. 研究意义和价值
本工作为安全强化学习领域提供了重要的理论和实践贡献：
- **理论价值**：首次在保障式RL框架中系统性地解决了高阶约束问题，并提出了量化与平衡安全-性能冲突的严格理论方法（梯度相似性、自适应CBF），保证了闭环系统的渐近稳定性和安全性。
- **应用价值**：所提框架对未知扰动/故障具有鲁棒性，且计算效率较高（避免了实时求解拉格朗日乘子），能够直接应用于自动驾驶、机器人运动规划等存在复杂安全约束的实际系统，为实现**高性能且高可靠**的智能控制提供了新思路。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究问题**
论文旨在解决**基于强化学习（RL）的最优控制中，安全性与性能之间的平衡难题**。具体而言，针对**具有高阶相对度状态约束**的非线性系统，在存在**未知时变干扰/执行器故障**的情况下，如何设计一个既能**严格保证安全**，又能**提升系统性能**的学习控制框架。

### **核心创新点**

1.  **提出高阶互易控制屏障函数**
    -   **问题**：传统的控制屏障函数（CBF）通常只能处理相对度为1的安全约束。许多实际应用（如机器人避障、自适应巡航）的约束具有高阶相对度（即控制输入不直接出现在约束函数的导数中），现有基于CBF的安全RL方法无法直接处理。
    -   **创新**：受高阶零化CBF启发，本文首次提出了**高阶互易控制屏障函数**。HO-RCBF在接近安全集边界时趋于无穷大（而非像HO-ZCBF那样趋于零），这一特性使其更易于与RL算法集成，为处理高阶约束提供了理论工具。

2.  **设计自适应高阶安全防护策略**
    -   **问题**：现有基于安全防护的RL方法（如[16]）计算负担低，但仅限于低阶约束，且对干扰/故障的鲁棒性不足。而能处理高阶约束的方法（如[19]）需要实时计算拉格朗日乘子，计算成本高，且安全性易受干扰破坏。
    -   **创新**：
        -   **高阶防护控制器**：将低阶防护策略扩展为高阶形式，能够处理高阶约束。
        -   **理论证明**：严格分析了该控制器在未知有界干扰/故障下的**鲁棒安全性**。
        -   **自适应机制**：引入**梯度相似度**概念量化安全梯度与性能梯度之间的冲突关系。通过**梯度操纵**和**自适应增益**机制，动态调整防护控制器的方向和大小，从而在安全性和性能之间实现**可权衡的平衡**。

3.  **构建完整、鲁棒的自适应安全RL框架**
    -   **集成方案**：将HO-RCBF、自适应高阶防护策略、非线性干扰观测器、以及基于“经验模拟”的Actor-Critic神经网络在线学习算法相结合，形成了一个完整的闭环解决方案。
    -   **理论保证**：不仅证明了该框架能保证学习过程中的**安全性**（状态始终位于安全集内），还证明了闭环系统的**一致最终有界性**。在特定条件下，还能保证系统的**渐近稳定性**。

### **解决方案的总体思路**

1.  **安全建模**：针对高阶约束，构造HO-RCBF，将原安全集的保持问题转化为一个（通过HO-RCBF定义的）新集合的**前向不变性**问题。
2.  **控制器设计**：控制输入由两部分组成：
    ```math
    u = K(x, t) + u^s(x, K_s) - \hat{u}^f
    ```
    -   `K(x, t)`：由Actor-Critic网络学习的**性能驱动**的最优控制策略（目标）。
    -   `u^s(x, K_s)`：**基于HO-RCBF的自适应高阶防护控制器**，负责保证安全。其增益 `K_s` 可根据安全风险（如距离边界远近）和性能损失自适应调整。
    -   `\hat{u}^f`：**非线性干扰观测器**的估计值，用于补偿未知干扰/故障，提升鲁棒性。
3.  **在线学习与平衡**：在防护策略`u^s`的“保护”下，系统可以安全地探索。通过**梯度操纵**，当安全方向与性能方向冲突时，调整防护力的方向以减少对性能的负面影响；通过**自适应增益**，在状态安全时减小防护力以提升性能，在接近危险时增大防护力以保证安全。

### **实际价值**
-   **应用广泛**：为自动驾驶、机器人运动规划、多智能体协同等存在复杂安全约束（如速度、加速度、距离约束）的领域提供了**兼具安全性、最优性和鲁棒性**的强化学习解决方案。
-   **工程友好**：防护策略使用常数增益替代实时优化，**显著降低了计算负担**，更易于在实际系统中部署。自适应机制允许用户根据实际需求（控制频率、安全性等级）灵活调整安全与性能的侧重点。
-   **理论坚实**：提供了从安全保证、稳定性到学习收敛性的完整理论分析，增强了方法的可信度。

**总结**：本文的核心贡献在于**系统性地解决了安全RL中高阶约束和干扰鲁棒性两大挑战**。通过提出HO-RCBF这一新工具，并创新性地设计了融合梯度操纵与自适应机制的安全防护策略，构建了一个理论上严谨、计算上高效、且能权衡安全与性能的鲁棒自适应安全RL框架。两个仿真示例（倒立摆和移动机器人）验证了该方法在处理高阶约束、抵抗干扰以及提升性能方面的有效性。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对**存在高阶状态约束和未知时变扰动/执行器故障的非线性系统**，提出了一种**学习增强的安全保障控制框架**，旨在**平衡强化学习（RL）控制中的安全性与性能**。其核心创新在于：首先，提出了**高阶互易控制屏障函数（HO-RCBF）** 来处理学习过程中的高阶约束；其次，引入了**梯度相似性**概念来量化安全性与性能梯度之间的关系；最后，在安全RL框架中结合**梯度操纵和自适应机制**，通过一个自适应的高阶安全保障策略来动态权衡安全与性能。仿真结果表明，该框架不仅能有效处理高阶约束，还能在保证安全的前提下，增强系统对扰动的鲁棒性并提升控制性能。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文针对存在高阶状态约束和未知扰动的非线性系统，提出了一种**学习增强的自适应安全保护控制框架**。其核心创新点可归纳为以下四个方面：

---

### 1. **提出高阶互易控制屏障函数（HO-RCBF）**
- **改进/不同之处**：
    - 现有方法（如[16]中的安全保护RL）主要处理**相对度为1**的约束，而高阶零化CBF（HO-ZCBF）[20]在边界处趋于零，可能难以与RL集成。
    - 本文提出的**HO-RCBF**在系统轨迹接近安全集边界时趋于**无穷大**，这一特性使其更易于与RL算法结合，处理高阶约束。
- **解决的问题/优势**：
    - **解决了高阶相对度约束的处理难题**（例如机器人运动规划中的速度/加速度约束、交通合并问题）。
    - 为将CBF与RL结合以处理复杂约束提供了新的数学工具，扩展了安全RL的应用范围。

### 2. **提出“梯度相似性”概念与梯度操纵机制**
- **改进/不同之处**：
    - 现有安全保护方法[16]中，安全梯度与性能梯度可能冲突，导致保守解或性能下降。
    - 本文定义了**梯度相似度ρ**（公式16），量化安全梯度与性能梯度之间的夹角。通过引入**梯度操纵参数μ**，将安全梯度分解为与性能梯度平行和垂直的分量，并调整平行分量的大小。
- **解决的问题/优势**：
    - **显式地平衡了安全性与性能之间的冲突**。当ρ<0（梯度方向相反）时，通过减小μ可以降低安全策略对性能的负面影响。
    - 提供了理论依据和设计自由度，允许用户根据实际需求在安全与性能之间进行**可调节的权衡**，避免过于保守的控制策略。

### 3. **设计自适应高阶安全保护策略及其稳定性证明**
- **改进/不同之处**：
    - 现有安全保护RL[16]使用**固定增益Ks**，可能导致在离散时间实现时因控制频率不足而违反安全约束（见示例3.11），或导致保守性能。
    - 本文提出**自适应更新律**（公式21）用于动态调整Ks。Ks本身被视为一个CBF，通过投影算子保证其非负性，并设计更新律使其在需要时增大（保障安全），在安全范围内减小（提升性能）。
- **解决的问题/优势**：
    - **增强了在离散时间实现下的安全鲁棒性**，避免了因采样时间限制导致的安全违规。
    - **动态平衡安全与性能**：通过自适应机制，系统在初始阶段或接近约束边界时采用较大的Ks确保安全，在安全区域内则减小Ks以优化性能。
    - **提供了闭环系统渐近稳定性的严格证明**（定理2），这是许多现有安全保护RL方法所缺乏的。

### 4. **构建集成的鲁棒自适应安全RL框架**
- **改进/不同之处**：
    - 现有方法如[19]使用HO-ZCBF安全滤波器，需要实时计算拉格朗日乘子，计算成本高，且对扰动/故障敏感。
    - 本文框架**集成**了：1) **HO-RCBF**处理高阶约束；2) **梯度操纵**平衡性能；3) **自适应安全增益**；4) **非线性扰动/故障观测器**（公式23）补偿匹配扰动；5) **执行器-评论家神经网络**在线学习最优策略。
- **解决的问题/优势**：
    - **实现了对未知有界扰动/执行器故障的鲁棒性**，观测器放宽了对扰动上界已知的假设（仅需导数有界）。
    - **降低了计算负担**：用恒定增益/自适应增益替代实时拉格朗日乘子计算，更适合计算资源有限的平台。
    - **保证了学习过程中的安全性**（引理4.3）和复合状态的一致最终有界性（定理3），实现了**安全约束下的在线最优策略学习**。

---

## 总结：实际价值与贡献

| 创新点 | 关键技术贡献 | 解决的工程问题 |
| :--- | :--- | :--- |
| **HO-RCBF** | 为高阶约束提供新的CBF形式，易于与RL集成。 | 使RL能应用于具有复杂动态约束（如相对度>1）的实际系统（机器人、自动驾驶）。 |
| **梯度操纵** | 量化并调节安全与性能的冲突，提供设计参数μ。 | 允许用户根据实际需求灵活调整系统行为，避免传统方法“一刀切”的保守性。 |
| **自适应安全增益** | 将Ks设计为自适应CBF，动态调整。 | 提升离散时间实现下的安全鲁棒性，并在保证安全的前提下主动优化性能。 |
| **集成框架** | 融合观测器、自适应机制、在线RL，提供稳定性证明。 | 为存在建模不确定性、扰动和故障的复杂系统提供一个**理论坚实、计算可行、安全可靠**的端到端学习控制方案。 |

**仿真验证**（第5节）表明，所提框架在倒立摆和移动机器人案例中均能：
1. 处理**高阶和混合相对度约束**。
2. 在**未知扰动**下保持安全。
3. 相比固定增益安全保护RL和基于惩罚的方法，获得**更好的性能（更低成本）**。
4. 实现**安全约束下的稳定学习与收敛**。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过两个仿真案例（倒立摆系统和移动机器人系统）验证了所提出的**自适应高阶安全强化学习（Safe RL）框架**的有效性。实验主要评估了该方法在**处理高阶相对度约束、抗干扰/故障鲁棒性以及平衡安全与性能**方面的能力。

### 1. 实验设置与评价指标

#### 使用的“数据集”与系统模型
- **案例一：倒立摆系统**
    - **模型**：经典非线性动力学方程，状态为摆角 `θ` 和角速度 `θ̇`，控制输入为扭矩 `u`。
    - **安全约束**：
        1.  **高阶相对度约束**：摆角 `θ ≤ 0.8°`（相对度 `m=2`）。
        2.  **一阶相对度约束**：角速度 `θ̇ ≥ -2°/s`（相对度 `m=1`）。
    - **干扰/故障**：施加了包含常数项和正弦项的复合匹配干扰 `u^f(t)`。

- **案例二：移动机器人系统**
    - **模型**：双积分器动力学，状态为位置 `p` 和速度 `v`，控制输入为加速度 `u`。
    - **安全约束**（混合相对度）：
        1.  **高阶相对度约束**：避障约束 `∥p - c_i∥ ≥ r_i`（相对度 `m=2`）和圆形区域约束 `∥p∥ ≤ r`（相对度 `m=2`）。
        2.  **一阶相对度约束**：速度上下限约束 `v_min ≤ v ≤ v_max`（相对度 `m=1`）。

#### 核心评价指标
1.  **安全性**：状态轨迹是否始终满足所有安全约束（即保持在安全集 `𝒞` 内）。这是**定性/布尔型指标**。
2.  **性能**：使用标准LQR型代价函数衡量控制效果。
    ```math
    J = ∫_0^∞ (x^T Q x + u^T R u) dτ
    ```
    累积代价 `J` 越小，表示性能（稳定性、控制效率）越好。
3.  **鲁棒性**：在存在未知有界干扰或执行器故障 `u^f` 时，系统能否依然保证安全。
4.  **保守性**：通过最终轨迹与约束边界的接近程度、以及为保安全而牺牲的性能代价来间接衡量。

### 2. 对比的基线方法

论文将所提方法（**HO-RCBF + 自适应安全策略**）与以下三种主流安全RL方法进行了对比：

1.  **经典RL（无安全保证）**：使用Actor-Critic网络学习最优控制策略，但**未集成任何安全机制**。
2.  **基于惩罚项的安全RL**：将控制屏障函数（CBF）作为惩罚项加入代价函数中，试图在优化过程中兼顾安全（引用自文献 [14]）。
3.  **基于安全滤波器的安全RL**：使用高阶零ing控制屏障函数（HO-ZCBF）作为安全滤波器，实时将不安全策略投影到安全集合中（引用自文献 [19, 20]）。

### 3. 关键实验结果与性能结论

#### 案例一：倒立摆（验证鲁棒性与处理高阶约束能力）

- **无干扰场景**：
    - **经典RL**：成功稳定系统，但**严重违反**了摆角约束（`θ > 0.8°`）。
    - **惩罚法RL**、**安全滤波器RL**和**所提方法**：均能保证安全，未违反约束。
    - **结论**：在理想情况下，多种安全RL方法均有效。

- **有干扰场景**（核心创新点验证）：
    - **经典RL**：仍能稳定，但继续违反角度约束。
    - **惩罚法RL**：**无法保证安全**，速度约束被违反。论文指出其效果严重依赖于惩罚项设计。
    - **安全滤波器RL**：**无法保证安全**，速度约束被违反。原因是HO-ZCBF滤波器在存在未知干扰时，其安全条件会被破坏。
    - **所提方法（HO-RCBF + 安全策略）**：**成功保证所有约束**，并将系统稳定到原点。
    - **性能提升/结论**：
        - **主要结论**：所提方法在**存在未知匹配干扰/故障时，具有显著的鲁棒性优势**。这是相较于对比方法的关键改进。
        - **机制**：论文提出的高阶互易控制屏障函数（HO-RCBF）和对应的安全策略 `u^s` 不依赖于干扰的精确信息，仅需其有界，从而在理论上保证了鲁棒安全性。

#### 案例二：移动机器人（验证处理混合相对度约束与平衡安全性能能力）

- **对比1：处理高阶约束能力**
    - **经典RL**：找到最优路径但**无视所有位置和速度约束**。
    - **RCBF-based RL（仅处理一阶约束）**：能保证速度安全，但**无法处理高阶的位置约束**，导致机器人撞上障碍物或冲出圆形区域。
    - **所提方法（HO-RCBF-based RL）**：**成功同时满足一阶（速度）和高阶（位置）的所有约束**。
    - **结论**：所提方法**首次在安全RL框架内有效处理了高阶相对度状态约束**，解决了现有安全策略（如文献[16]）的局限性。

- **对比2：平衡安全与性能（自适应机制效果）**
    - **固定增益HO-RCBF RL** (`μ=0, γ=0, Y=0`)：能保证安全，但轨迹**非常保守**，机器人远离障碍物和边界，导致路径变长。
    - **所提自适应HO-RCBF RL** (`μ=0.5, Y=500, γ=0.001`)：同样保证安全，但轨迹**更接近最优**，机器人以更直接的路径到达目标。
    - **定量性能比较**：论文图6展示了从不同初始位置出发的**累积代价 `J`**。
        - 在所有测试的初始条件下，**自适应方法的 `J` 值均显著低于固定增益方法**。
        - 这表明通过**梯度操纵**（参数 `μ`）和**自适应增益调节**（`K_s` 的动态更新），所提方法在严格保证安全的前提下，**显著提升了系统性能**，减少了因过度保守而带来的控制消耗。

### 总结

- **定量结果**：论文提供了明确的**定性安全轨迹对比图**和**定量性能代价 `J` 的对比曲线**。`J` 值的降低是衡量其“平衡安全与性能”能力的核心定量指标。
- **核心验证点**：
    1.  **有效性**：能处理单一及混合高阶相对度约束。
    2.  **鲁棒性**：对未知匹配干扰/执行器故障具有强鲁棒性，这是相比许多现有安全滤波器方法的突出优势。
    3.  **优越性**：通过自适应机制，在同等安全水平下，获得了比固定增益安全策略更优的控制性能。
- **实际价值**：该框架为机器人、自动驾驶等安全关键系统在**复杂约束**和**不确定环境**下实现**高性能、高安全**的学习与控制提供了新的解决方案。其“即插即用”的安全策略形式，允许与任何已有的稳定控制器结合，增强了实用性。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2501.15373v1)
- [HTML 版本](https://arxiv.org/html/2501.15373v1)
