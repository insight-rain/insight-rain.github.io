# Utilizing Evolution Strategies to Train Transformers in Reinforcement Learning

**相关性评分**: 6.0/10

**排名**: #14


---


## 基本信息

- **arXiv ID**: [2501.13883v2](https://arxiv.org/abs/2501.13883v2)
- **发布时间**: 2025-01-23T17:56:40Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Matyáš Lorenc, Roman Neruda

## 关键词

Robot control, quadruped robot locomotion, reinforcement learning (RL), mujoco, humanoid robot

## 一句话总结

该论文探索了使用进化策略训练基于Transformer架构的强化学习智能体，在MuJoCo人形机器人运动环境中进行实验，展示了进化策略处理复杂模型训练的能力。

## 摘要

We explore the capability of evolution strategies to train an agent with a policy based on a transformer architecture in a reinforcement learning setting. We performed experiments using OpenAI's highly parallelizable evolution strategy to train Decision Transformer in the MuJoCo Humanoid locomotion environment and in the environment of Atari games, testing the ability of this black-box optimization technique to train even such relatively large and complicated models (compared to those previously tested in the literature). The examined evolution strategy proved to be, in general, capable of achieving strong results and managed to produce high-performing agents, showcasing evolution's ability to tackle the training of even such complex models.

## 详细分析

## 论文摘要

### 1. 研究背景和动机
强化学习是机器学习领域的核心难题之一。传统方法多基于梯度优化，而**进化策略**作为一种无导数优化算法，因其**高度并行化**和**强大探索能力**而备受关注。与此同时，**Transformer架构**在序列建模任务中展现出卓越性能，并已被引入强化学习领域，例如**决策Transformer**。然而，现有研究主要使用进化策略训练简单的全连接网络，对于更大、更复杂的Transformer模型是否有效尚不明确。本研究旨在探索**进化策略是否能够有效训练基于Transformer架构的智能体**，以验证无导数优化方法处理复杂模型的潜力。

### 2. 核心方法和技术创新
本研究采用**OpenAI-ES**这一高度并行化的自然进化策略作为核心优化器。主要技术创新与实验设计包括：
- **模型组合**：将OpenAI-ES与决策Transformer结合，在MuJoCo Humanoid和Atari Hero环境中进行在线强化学习训练。
- **实现修正**：自行实现了OpenAI-ES，并修正了原论文与代码中的不一致之处（如使用**真正的权重衰减**而非L2正则化），并优化了决策Transformer的序列存储逻辑以提升效率。
- **对比与消融实验**：
    - 设置了使用全连接网络的标准OpenAI-ES实验作为基线。
    - 探索了**预训练策略**：先用行为克隆模仿一个较弱模型，再用进化策略微调，以研究其对大模型训练的辅助作用。
    - 进行了**种群大小**、**是否使用虚拟批归一化**等关键超参数的消融研究。

### 3. 主要实验结果
- **可行性验证**：OpenAI-ES能够成功训练参数量达82.5万（Humanoid）和248.6万（Atari）的决策Transformer模型，并获得高性能智能体，证明了进化策略训练复杂模型的**能力**。
- **性能对比**：在Humanoid环境中，决策Transformer的最终性能**优于**作为基线的全连接网络。然而，其训练过程**稳定性稍差**（十次运行中有一次失败），且需要更大的种群规模和更长的计算时间。
- **预训练效果**：预训练策略并未带来优势。从预训练模型开始微调，需要大幅调低学习率和噪声标准差，导致**初期性能下降**且**后续学习速度更慢**。这表明梯度训练（行为克隆）与进化策略的目标存在本质差异。
- **环境扩展性**：在更复杂的Atari Hero环境中，进化策略同样能驱动模型性能逐步提升，但**单次迭代的墙钟时间显著增加**，凸显了大规模计算的需求。

### 4. 研究意义和价值
本研究的主要贡献在于**实证了进化策略能够训练Transformer这类复杂模型**，拓展了无导数优化方法在强化学习中的应用边界。其价值体现在：
- **方法学启示**：为结合进化策略与前沿神经网络架构（如Transformer）提供了可行性证明，鼓励在混合梯度与进化的先进算法中使用复杂模型。
- **深入洞察**：通过预训练实验的失败，揭示了梯度优化与分布式进化策略在目标上的根本不同——后者可能倾向于寻找**对参数扰动更鲁棒**的解，这对模型部署（如低精度硬件）具有潜在价值。
- **计算范式验证**：尽管训练大型Transformer需要巨大的计算资源（数百CPU核心），但进化策略的线性并行扩展性使其在理论上具备处理更大模型的潜力，为未来利用海量计算资源训练智能体指明了方向。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **核心创新点**
本文的核心创新点在于**首次系统性地将进化策略（Evolution Strategies, ES）与基于Transformer的强化学习模型（Decision Transformer）相结合**，并验证了这种无梯度优化方法在训练复杂序列模型进行强化学习任务时的可行性与有效性。

### **想要解决的核心问题**
1.  **技术验证问题**：进化策略这类无梯度黑盒优化方法，此前主要应用于训练参数较少、结构简单的（如前馈神经网络）策略模型。本文旨在探究其**能否有效训练参数规模更大、结构更复杂的Transformer架构模型**。
2.  **方法拓展问题**：Transformer在监督学习中表现出色，但在强化学习中通常依赖基于梯度的优化。本文试图**为Transformer策略模型提供一种替代的、基于进化的训练范式**，以利用进化策略的独特优势（如更好的探索性、易于并行化）。
3.  **实践可行性问题**：探索在**在线强化学习**设置下，使用进化策略训练Decision Transformer的实操细节、性能表现及潜在瓶颈。

### **解决方案与实验设计**
作者通过一系列严谨的实验来验证其核心主张：

1.  **方法选择**：
    - **进化策略**：采用**OpenAI-ES**，因其高度可并行化且是“最简单的”分布型进化策略。如果它有效，更复杂的进化算法理论上会表现更好。
    - **策略模型**：采用**Decision Transformer (DT)**，将强化学习视为序列建模问题，利用Transformer处理历史观测、动作和回报序列。

2.  **实验验证**：
    - **基准复现**：首先复现OpenAI-ES在MuJoCo Humanoid环境中训练前馈网络的结果，验证自实现代码的正确性。
    - **核心实验**：在相同环境中，使用OpenAI-ES训练Decision Transformer。**关键调整是大幅增加种群规模**（从5000增至20000），以应对模型参数量的增加（从16.6万增至82.5万）。
    - **预训练探索**：尝试先用行为克隆（模仿一个由SAC训练的小模型）预训练DT，再用ES微调，以探究是否有助于大模型训练。
    - **扩展性测试**：在更复杂的Atari Hero游戏环境中，训练参数量更大的DT模型（约248.6万参数），并将种群规模进一步增至40000，测试方法的极限。

3.  **关键技术细节与发现**：
    - **实现修正**：作者自实现了OpenAI-ES，并修正了与原论文/代码不一致的地方（如使用**真正的权重衰减**而非L2正则化；在参数更新时进行**不确定性归一化**以估计自然梯度）。
    - **性能表现**：实验表明，OpenAI-ES能够成功训练Decision Transformer，在Humanoid环境中取得了**优于基准前馈网络**的性能（见图2）。
    - **鲁棒性观察**：作者发现，进化策略训练出的模型可能具有**更强的鲁棒性**。例如，从预训练模型开始进行ES训练时，模型性能会先下降再上升，这被解释为ES在将“脆弱”的梯度训练模型“鲁棒化”，使其参数空间对扰动不敏感。
    - **计算需求**：成功训练大模型需要**巨大的计算并行度**（实验使用了300个CPU worker）。模型越大、环境越复杂，单次迭代的墙钟时间显著增加（见图4b）。
    - **当前局限**：在在线ES训练中，Decision Transformer的“回报条件”（return-to-go）令牌未被有效利用，模型会忽略它（见表I）。作者为此提出了一个未来的改进方案。

### **总结**
本文的**实际价值**在于**拓宽了进化策略在深度强化学习中的应用边界**，证明了即使对于Transformer这类复杂模型，无梯度优化依然是一条可行的技术路径。这为以下场景提供了新的可能性：
- 需要**极致探索**或模型**行为多样性**的任务。
- 策略模型包含**不可微模块**的场合。
- 拥有**大规模分布式CPU计算资源**，但缺乏GPU或需要避免梯度计算复杂性的情况。

尽管存在计算成本高昂、样本效率低于梯度方法等固有缺点，但这项工作为**进化策略与复杂模型架构的结合**奠定了实证基础，并启发了关于模型鲁棒性以及如何有效结合梯度与进化训练的未来研究方向。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在探索**无梯度优化方法（进化策略）训练复杂Transformer架构智能体的可行性**，以解决在强化学习环境中训练大规模、复杂模型（如决策Transformer）的挑战。研究提出并实现了一种基于**OpenAI-ES进化策略**的方法，直接优化决策Transformer的策略网络参数，并在MuJoCo Humanoid和Atari Hero等高维复杂环境中进行了系统实验。最终结果表明，尽管需要巨大的计算资源（数百CPU核心并行），但**进化策略能够成功训练参数规模达数百万的决策Transformer模型，使其达到高性能水平**，从而证明了进化方法对于训练复杂模型的有效性，并揭示了其可能产生比梯度方法更具鲁棒性策略的潜力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文的核心创新点在于**首次系统性地探索并验证了使用进化策略（Evolution Strategies, ES）直接训练基于Transformer架构的智能体策略在强化学习任务中的可行性**。具体创新点如下：

### 1. **将进化策略的应用范围扩展到复杂Transformer模型**
   - **相比以往方法的改进/不同之处**：
     - 以往研究（如OpenAI-ES原文）主要将进化策略应用于参数规模相对较小、结构简单的**前馈神经网络**（例如16.6万参数）。
     - 本文首次将进化策略（OpenAI-ES）应用于参数规模大、结构复杂的**Decision Transformer**模型（在Humanoid环境中约82.5万参数，在Atari环境中约248.6万参数）。
   - **解决的具体问题/带来的优势**：
     - 证明了进化策略这种**无导数优化方法**能够训练复杂序列模型，打破了“进化策略仅适用于简单模型”的潜在假设。
     - 为在强化学习中利用Transformer的强大表征能力提供了一种**无需梯度计算**的替代训练途径，这在模型包含不可微组件或需要更强探索性的场景中具有优势。

### 2. **在在线强化学习设置中训练原生Decision Transformer**
   - **相比以往方法的改进/不同之处**：
     - Decision Transformer原论文主要设计用于**离线强化学习**（通过行为克隆进行序列预测的监督学习）。
     - 本文尝试了使用梯度方法（如Online DT、TD3）在线训练DT但效果不佳，而**进化策略无需修改即可直接在线训练原生DT架构**。
   - **解决的具体问题/带来的优势**：
     - 提供了一种**即插即用**的在线训练方案，避免了为适配梯度下降而修改DT架构或训练流程的复杂性。
     - 展示了进化策略在处理**需要历史序列作为输入**的复杂策略模型时的通用性。

### 3. **探索了“预训练+进化微调”的混合范式并揭示了其局限性**
   - **相比以往方法的改进/不同之处**：
     - 提出并实验了先用**行为克隆**（梯度方法）预训练DT，再用进化策略微调的混合流程。
     - 与单纯使用进化策略从头训练相比，发现了该流程的**性能缺陷和理论冲突**。
   - **解决的具体问题/带来的优势**：
     - **揭示了梯度训练与进化策略在目标上的根本差异**：梯度训练追求**参数空间的最优点**（导致模型对参数扰动敏感/“脆弱”），而进化策略追求**分布层面的鲁棒性**（期望对参数扰动不敏感）。
     - 这一发现解释了为何简单的“先梯度后进化”串行混合可能效果不佳，为未来设计更有效的混合算法提供了关键见解（例如，可能需要同时优化或引入鲁棒性目标）。

### 4. **对OpenAI-ES算法进行了修正与澄清性实现**
   - **相比以往方法的改进/不同之处**：
     - 发现了原始OpenAI-ES论文与官方代码在**权重衰减**和**自然梯度估计**实现上的不一致。
     - 本文实现**严格遵循论文描述**：使用真正的权重衰减（而非L2正则化），并在更新时对梯度估计进行**不确定性归一化**（除以标准差）。
   - **解决的具体问题/带来的优势**：
     - 纠正了算法实现，确保了理论正确性。
     - **解耦了学习率与噪声标准差这两个超参数**，使它们可以独立调整，提高了算法可解释性和调优便利性。

### 5. **实证研究了大规模种群对训练复杂模型的必要性**
   - **相比以往方法的改进/不同之处**：
     - 通过消融实验证明，训练Transformer模型需要**显著增大种群规模**（相对于前馈网络）。
     - 例如，Humanoid DT需要2万个体（是前馈网络种群的4倍），Atari DT需要4万个体。
   - **解决的具体问题/带来的优势**：
     - 为使用进化策略训练大模型提供了关键的**超参数配置指导**：**更大的模型需要更大的种群来维持训练稳定性和成功率**。
     - 解释了减少种群会导致训练失败率升高，强调了足够的并行采样对于在高维参数空间中探索的重要性。

### 6. **验证了进化策略训练出的DT模型对“目标回报”不敏感**
   - **相比以往方法的改进/不同之处**：
     - 发现并验证了在在线进化训练中，Decision Transformer的**回报条件（return-to-go）机制失效**。
     - 模型性能不受输入的目标回报值影响（如表I所示，输入-1000到1,000,000的目标回报，最终性能无差异）。
   - **解决的具体问题/带来的优势**：
     - 指出了原生DT架构在在线无导数训练中的**一个局限**：缺乏机制驱动智能体去关注并实现特定的目标回报。
     - 为此提出了一个**未来的改进方案**：通过修改评估方式，将适应度函数与达成指定目标回报的能力挂钩，从而恢复DT的条件策略能力。

---

**总结**：本文的核心创新并非提出一个性能上超越现有方法的SOTA算法，而是进行了一项重要的**探索性验证**。它证明了进化策略能够训练复杂的Transformer强化学习模型，并在此过程中揭示了混合训练范式的内在矛盾、提供了大模型训练的关键配置见解、并指出了算法实现与架构适配上的改进方向。其价值在于**拓宽了无导数优化方法的应用边界**，并为未来结合进化策略与复杂模型的研究奠定了基础。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验效果总结

### 一、 实验设置与评价指标
- **实验环境（数据集）**：
    1.  **MuJoCo Humanoid**：连续控制环境，用于测试连续动作空间下的性能。
    2.  **Atari Hero**：视觉输入环境，用于测试高维、离散动作空间下的性能。
- **评价指标**：
    - **主要指标**：**平均回报（Mean Return / Fitness）**，即智能体在多个回合中获得的累积奖励的平均值。在Humanoid环境中，回报值在论文图表中进行了归一化（除以1000）。
    - **辅助指标**：**训练过程的鲁棒性**（通过十次独立运行的中位数和四分位数体现）和**训练时间**（单次迭代的挂钟时间）。

### 二、 基线方法与对比结果
论文主要进行了以下几组对比实验：

1.  **复现实验（基线）**：
    - **方法**：使用OpenAI-ES训练一个标准的**前馈神经网络（FF）**。
    - **目的**：验证自实现OpenAI-ES代码的正确性，并为后续实验提供一个性能基准。
    - **结果**：成功复现了原OpenAI-ES论文的结果，表明其实现是有效的。

2.  **核心实验：训练决策变换器（DT）**：
    - **方法**：使用OpenAI-ES直接训练**决策变换器（Decision Transformer）**。
    - **对比基线**：
        - **前馈网络基线（FF）**：在相同环境下，DT模型（82.5万参数）的性能**优于**更小的前馈网络（16.6万参数）。
        - **基于梯度的在线方法**：作者尝试了在线决策变换器和TD3算法。**在线决策变换器无法有效学习**；而经过修改的TD3算法在相似挂钟时间内训练后，其最终性能**低于**进化策略训练的DT（如图2所示，“TD3”水平线低于“DT”曲线的后期部分）。
    - **关键结论**：OpenAI-ES能够成功训练比传统前馈网络更大、更复杂的变换器模型，并且是当前**唯一无需修改即可直接应用于在线训练决策变换器的标准方法**。

3.  **消融实验与发现**：
    - **种群大小**：将种群规模减半会导致训练**成功率显著下降**（鲁棒性变差），表明大规模种群对于训练复杂模型是必要的。
    - **目标回报（Return-to-go）的影响**：在训练好的DT模型上测试不同初始目标回报值，发现**性能没有显著差异**（见表I）。这表明在纯粹的进化策略在线训练中，DT模型**忽略了**目标回报条件，这与它原本的离线设计初衷不同。
    - **预训练实验**：
        - **方法**：先用行为克隆（模仿一个由SAC训练的小模型）预训练DT，再用OpenAI-ES微调。
        - **结果**：预训练**并未带来优势**。微调初期性能会先下降（模型被“破坏”），之后才缓慢恢复。且为了不覆盖预训练成果，必须大幅调低学习率和噪声标准差，导致**收敛速度慢于从头开始训练**。
        - **深层发现**：这揭示了**基于梯度的训练与基于分布的进化策略之间存在目标冲突**。梯度训练追求参数空间中的精确最优点，而进化策略追求参数分布下的期望高性能，后者可能产生**对参数扰动更鲁棒的模型**。

4.  **扩展实验：Atari Hero环境**：
    - **方法**：使用更大的种群训练参数更多的DT模型（约248.6万参数）。
    - **结果**：OpenAI-ES能够使模型性能**逐步提升**（见图4a），证明了其处理更大模型和更复杂环境（图像输入）的能力。
    - **代价**：模型推理和环境交互时间大幅增加，导致**单次迭代的挂钟时间显著变长**（见图4b），凸显了大规模计算资源的需求。

### 三、 主要性能结论与定量结果
- **核心结论**：**进化策略（特别是OpenAI-ES）能够有效地从头开始训练决策变换器这类复杂的序列模型，用于在线强化学习任务。**
- **定量性能**：
    - 在MuJoCo Humanoid环境中，经过200次迭代，DT模型获得的**中位数回报（约8370，原始尺度）** 优于作为基线的前馈网络，也优于在相似时间内用TD3算法训练的DT。
    - 训练过程显示出**可接受的鲁棒性**（十次运行中多数成功），但存在因随机种子导致个别运行失败的情况。
- **未给出明确SOTA对比的原因**：
    - 论文的**主要目的不是击败现有方法**，而是**验证可行性**。作者明确指出，他们旨在证明进化策略能够训练复杂模型，从而鼓励在进化强化学习中使用此类模型，而非寻找决策变换器+进化的优势用例。
    - 因此，没有与最先进的离线RL或在线RL算法进行全面的性能竞赛比较。基线对比主要用于说明进化策略是当前训练在线DT的可行途径。

### 四、 实际价值与创新点
- **技术创新**：首次系统性地将**进化策略（OpenAI-ES）** 与**决策变换器**结合，用于**在线强化学习**，拓展了无导数优化方法的应用边界。
- **实际价值**：
    1.  **提供新工具**：为训练复杂、非标准或不可微分的策略网络（如变换器）提供了一种无需梯度计算的替代方案。
    2.  **揭示算法特性**：通过预训练实验，揭示了进化策略可能产生**更鲁棒模型**的潜力（对参数扰动不敏感），这对模型部署（如低精度硬件）有积极意义。
    3.  **明确计算需求**：实验证实，虽然算法高度可并行，但训练大型变换器模型**需要巨大的计算资源**（数百CPU核心），为实际应用提供了重要的成本参考。

**总结**：该论文通过实验成功证明，即使是最简单的进化策略（OpenAI-ES）也能训练参数规模达百万级的决策变换器，在连续控制和视觉输入任务中取得良好性能。其价值在于验证了“进化+变换器”这一技术路线的可行性，并为理解不同优化范式的差异提供了新的见解。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2501.13883v2)
- [HTML 版本](https://arxiv.org/html/2501.13883v2)
