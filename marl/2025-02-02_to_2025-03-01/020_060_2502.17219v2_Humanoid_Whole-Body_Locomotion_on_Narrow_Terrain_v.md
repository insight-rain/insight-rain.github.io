# Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning

**相关性评分**: 6.0/10

**排名**: #20


---


## 基本信息

- **arXiv ID**: [2502.17219v2](https://arxiv.org/abs/2502.17219v2)
- **发布时间**: 2025-02-24T14:53:45Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Weiji Xie, Chenjia Bai, Jiyuan Shi, Junkai Yang, Yunfei Ge, Weinan Zhang, Xuelong Li

## 关键词

Robot control, quadruped robot locomotion, Reinforcement Learning, local observational information

## 一句话总结

该论文提出了一种基于动态平衡和强化学习的人形机器人全身运动算法，用于在狭窄地形上仅使用本体感知实现稳定行走。

## 摘要

Humans possess delicate dynamic balance mechanisms that enable them to maintain stability across diverse terrains and under extreme conditions. However, despite significant advances recently, existing locomotion algorithms for humanoid robots are still struggle to traverse extreme environments, especially in cases that lack external perception (e.g., vision or LiDAR). This is because current methods often rely on gait-based or perception-condition rewards, lacking effective mechanisms to handle unobservable obstacles and sudden balance loss. To address this challenge, we propose a novel whole-body locomotion algorithm based on dynamic balance and Reinforcement Learning (RL) that enables humanoid robots to traverse extreme terrains, particularly narrow pathways and unexpected obstacles, using only proprioception. Specifically, we introduce a dynamic balance mechanism by leveraging an extended measure of Zero-Moment Point (ZMP)-driven rewards and task-driven rewards in a whole-body actor-critic framework, aiming to achieve coordinated actions of the upper and lower limbs for robust locomotion. Experiments conducted on a full-sized Unitree H1-2 robot verify the ability of our method to maintain balance on extremely narrow terrains and under external disturbances, demonstrating its effectiveness in enhancing the robot's adaptability to complex environments. The videos are given at https://whole-body-loco.github.io.

## 详细分析

## 论文摘要

**论文标题：** 基于动态平衡与强化学习的人形机器人狭窄地形全身运动

**1. 研究背景和动机**
人形机器人在复杂地形（尤其是狭窄路径和突发障碍）上的动态平衡与鲁棒运动能力仍远逊于人类。现有基于强化学习（RL）的算法多依赖于周期性步态或外部感知（如视觉、激光雷达），缺乏在不依赖外部感知（仅凭本体感知）情况下处理不可见障碍和突发失衡的有效机制。因此，亟需一种能够实现全身协调、动态平衡的鲁棒控制方法。

**2. 核心方法和技术创新**
本文提出了一种名为 **动态平衡人形机器人运动（DBHL）** 的新型RL框架。其核心技术创新包括：
- **基于零力矩点（ZMP）的动态平衡奖励**：将经典控制中的ZMP概念扩展至非平面地形，形成零力矩线（ZML）。设计奖励函数，鼓励ZML靠近机器人支撑多边形的中心，从而在不依赖外部感知的情况下引导策略学习动态平衡。
- **全身控制框架**：引入**角动量正则化**以减少不必要的身体旋转，并使用**乘性动作噪声**来约束上半身关节的运动范围，促进上下肢协调运动以辅助平衡。
- **奖励与价值函数向量化**：将不同奖励项与其对应的价值函数分开学习，避免小奖励项在累加中被淹没，提升了学习效率和稳定性。
- **非对称演员-评论家框架与领域随机化**：演员网络仅使用本体感知信息，确保策略在现实世界中的可迁移性；评论家网络利用特权信息（如高度场）辅助训练。广泛的领域随机化（如随机化动力学参数、外部扰动）实现了从仿真到现实的零样本迁移。

**3. 主要实验结果**
在仿真（Isaac Gym, MuJoCo）和真实世界（Unitree H1-2机器人）上进行了广泛评估：
- **性能对比**：在多种狭窄地形（平地、斜坡、楼梯）上，DBHL在成功率和平移距离上显著优于仅控制下半身或基于周期性步态的基线方法。
- **消融研究**：验证了ZMP奖励、奖励向量化和乘性动作噪声对提升动态平衡、学习速度和运动稳定性的关键作用。
- **现实世界验证**：机器人成功在仅25厘米宽的复杂路径（含斜坡、楼梯）上行走，并能有效应对**外部推力、移动棍棒绊脚、携带负载、密集锥形障碍**等多种突发扰动和挑战，展示了卓越的适应性和鲁棒性。

**4. 研究意义和价值**
本研究成功地将经典控制理论（ZMP）与前沿深度强化学习相结合，为人形机器人在**极端、非结构化环境**中实现鲁棒、动态的全身运动控制提供了新范式。其**仅依赖本体感知**的特性降低了对昂贵、易受干扰的外部传感器的依赖，增强了系统的实用性和可靠性。该工作推动了人形机器人向更复杂现实任务（如灾难救援、野外探索）迈进的步伐，具有重要的理论创新和实际应用价值。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究问题**
论文旨在解决**双足人形机器人在极端狭窄地形（如窄路、斜坡、楼梯）和突发干扰（如推搡、绊倒）下动态平衡能力不足**的问题。现有基于强化学习（RL）的步态控制方法主要依赖**周期性步态或外部感知（如视觉、激光雷达）**，在缺乏外部感知或需要快速、非周期性步态调整时（如突然失衡），机器人稳定性显著下降。

### **核心创新点**
1.  **将经典控制理论（ZMP）与强化学习深度融合**
    - **创新机制**：提出一种基于**零力矩点（Zero Moment Point, ZMP）的奖励函数**，将其整合到RL框架中，引导策略学习动态平衡。
    - **关键改进**：将ZMP概念扩展到非平面地形，引入**零力矩线（ZML）**，并通过奖励函数鼓励ZML靠近机器人支撑多边形的中心，从而在**仅使用本体感知（无外部视觉/LiDAR）** 的情况下实现稳定行走。

2.  **提出“全身协同控制”框架（DBHL）**
    - **整体设计**：构建了一个端到端的RL策略（DBHL），控制机器人全身（包括上肢摆动）以辅助动态平衡。
    - **关键技术组件**：
        - **角动量正则化**：通过惩罚不必要的身体旋转，促进上下肢协调运动。
        - **乘性动作噪声注入**：约束上肢关节的运动范围，提升策略的稳健性。
        - **奖励与价值函数向量化**：为每个奖励项（如ZMP奖励、速度跟踪奖励）独立学习一个价值函数，避免小幅度奖励项在累加时被忽略，提升学习效率。
        - **对称性正则化**：利用机器人身体的对称性提升样本效率和步态协调性。

3.  **实现了从仿真到现实的零样本迁移**
    - **方法**：在训练中采用了广泛的**域随机化**（包括随机化外部推力、延迟、摩擦系数、质量等），使得在仿真中训练的策略能够直接部署到真实机器人（Unitree H1-2）上。
    - **效果**：在真实世界中成功演示了在25cm窄道上行走、抗干扰、负载、跨越障碍物等一系列复杂任务。

### **解决方案路径**
1.  **问题建模**：将任务构建为**目标条件强化学习**问题，策略接收速度指令和本体感知信息，输出关节目标位置。
2.  **动态平衡核心**：
    - 推导ZMP与支撑多边形中心的关系，设计**ZMP距离奖励** `r_zmp`（式7），直接优化动态平衡指标。
    - 通过**非对称演员-评论家框架**，演员网络仅使用本体感知，评论家网络使用特权信息（仿真中可得）辅助训练，确保策略在现实中的可部署性。
3.  **全身运动优化**：
    - 引入**角动量奖励** `r_AM`（式8）减少整体旋转。
    - 使用**乘性动作噪声**（式10）约束动作幅度，鼓励平缓的上肢运动。
4.  **高效策略学习**：
    - 采用**课程学习**，逐步增加地形难度（路径变窄、坡度变陡）。
    - 通过**奖励向量化**（式11, 12）和**对称性损失**（式13）加速训练并提升策略质量。

### **实际价值与意义**
- **技术价值**：成功地将经典控制理论的物理洞察（ZMP）与现代数据驱动的RL方法结合，为**实现高动态、高适应性的人形机器人运动控制**提供了一条新路径。
- **应用价值**：证明了**仅凭本体感知**就能在极端地形和干扰下实现稳健行走，降低了对昂贵、易受干扰的外部感知系统的依赖，提升了机器人在**灾难救援、复杂工业环境**等场景下的实用性和可靠性。
- **方法论贡献**：提出的**奖励向量化、全身协同训练框架**等技术，对更广泛的足式机器人强化学习研究具有借鉴意义。

**总结**：该论文的核心是通过**ZMP驱动的奖励机制**和**创新的全身RL训练框架**，解决了人形机器人在**无外部感知条件下的极端地形动态平衡**难题，并通过系统的仿真与实物实验验证了其卓越性能。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决人形机器人在**极端狭窄地形**（如窄路、未知障碍）上**仅依靠本体感知**（无视觉/LiDAR）实现**动态平衡**和鲁棒全身运动控制的难题。针对现有基于强化学习的方法依赖周期性步态或外部感知、难以应对突发失衡的不足，论文提出了名为 **DBHL** 的新框架。其核心创新在于将经典控制理论中的**零力矩点**概念引入强化学习，设计了一种**ZMP驱动的奖励函数**，通过鼓励ZMP轨迹靠近支撑多边形中心来引导策略学习动态平衡；同时，框架集成了**全身控制**（利用上肢摆动辅助平衡）、**奖励向量化**、角动量正则化等关键技术。实验表明，该方法在仿真和真实Unitree H1-2机器人上均能有效穿越极窄地形、抵抗外部扰动（如推搡、绊脚），显著提升了在复杂环境下的稳定性与适应性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning》提出了一种名为**动态平衡人形机器人运动（DBHL）**的新方法。其核心创新点在于将经典控制理论中的动态平衡概念与深度强化学习框架深度融合，以解决人形机器人在**仅依赖本体感知**的情况下，在极端狭窄地形和突发扰动下保持稳定运动的问题。

以下是论文相对于已有工作的明确创新点及其分析：

---

### 1. **将零力矩点（ZMP）作为奖励函数集成到RL框架中**
   - **改进/不同之处**：
     - **以往方法**：ZMP是经典双足步行控制中的核心稳定性判据，通常用于基于模型的控制器（如MPC、WBC）进行轨迹规划和实时调整。在RL领域，先前工作多依赖**基于步态的周期性奖励**或**基于外部感知（如视觉、激光雷达）的条件奖励**来学习运动。
     - **本文方法**：创造性地将ZMP转化为一个可计算的**奖励项** `r_zmp`。它通过计算**支撑多边形中心**与**零力矩线（ZML）** 在水平面上的投影距离来量化动态平衡程度，并将此距离最小化作为学习目标。
   - **解决的问题/带来的优势**：
     - **解决动态平衡问题**：为策略提供了明确的、基于物理的稳定性学习信号，使机器人能学会在单足/双足支撑阶段快速调整质心和脚部位置以维持平衡，尤其是在失去平衡的临界时刻。
     - **实现无外部感知的鲁棒运动**：奖励计算依赖于可从仿真中获取的“特权信息”（如动量、接触力），但策略学习**仅基于本体感知**（关节状态、IMU等）。这使得训练出的策略能直接部署到真实世界，无需依赖容易失效或延迟的视觉/LiDAR传感器，从而能处理**不可观测的障碍**和**突然的失衡**。
     - **泛化至非平面地形**：通过引入ZML（一系列ZMP点构成的线）的概念，并将奖励设计为鼓励ZML靠近支撑多边形中心，该方法自然地推广到斜坡、楼梯等非平面复杂地形，无需显式、复杂地计算不规则地面的支撑多边形。

### 2. **构建了用于全身协调运动的整体控制框架与配套技术**
   - **改进/不同之处**：
     - **以往方法**：许多人形机器人RL控制器侧重于下半身步态，或将上半身动作与下半身解耦。对于全身协调，尤其是利用上肢摆动辅助动态平衡，缺乏有效的学习约束机制。
     - **本文方法**：提出了一个**全身控制框架**，明确鼓励上下肢协调。并引入了两项关键技术来达成此目标：
       1. **角动量正则化**：通过奖励项 `r_AM` 最小化机器人基座坐标系下的总角动量。这抑制了腿部摆动产生的不必要旋转，促使策略学习使用上肢运动来进行抵消，从而改善整体协调性和抗扰动能力。
       2. **乘性动作噪声注入**：在PD控制器的目标关节位置输入上添加与动作幅度成正比的噪声。这惩罚了大幅度的、可能不稳定的上肢动作，鼓励策略产生更谨慎、小幅度的上肢运动，增强了策略的鲁棒性。
   - **解决的问题/带来的优势**：
     - **实现真正的全身协同**：使机器人能够像人类一样，利用手臂的摆动来辅助平衡和转向，特别是在狭窄地形上需要精细调整重心时。
     - **提升运动质量和鲁棒性**：角动量正则化减少了“手舞足蹈”的不稳定模式，乘性噪声使策略对执行器误差和模型不确定性更不敏感，共同提升了运动的平滑性和可靠性。

### 3. **采用奖励与价值函数向量化技术**
   - **改进/不同之处**：
     - **以往方法**：标准RL做法是将所有奖励项（如跟踪命令、姿态、能量等）加权求和为一个标量奖励，并学习一个对应的标量价值函数。
     - **本文方法**：提出**奖励向量化**。每个奖励项（如ZMP奖励、速度跟踪奖励、角动量奖励等）保持独立，并为每个奖励项学习一个独立的价值函数头。最终的优势函数由这些独立价值函数的TD误差聚合计算。
   - **解决的问题/带来的优势**：
     - **解决奖励淹没问题**：在标量求和模式下，幅度较小的奖励项（如ZMP奖励）容易被幅度大的奖励项（如前进奖励）“淹没”，导致价值函数难以准确估计这些小项的变化，从而影响策略对其的学习。向量化确保了每个奖励目标的信号都能被清晰、独立地学习。
     - **提升学习效率和稳定性**：如图8所示，该方法显著加快了训练收敛速度，使策略能更高效地同时优化多个有时相互冲突的目标（如快速前进 vs. 保持稳定）。

### 4. **在极具挑战性的仅本体感知狭窄地形任务上验证了卓越性能**
   - **改进/不同之处**：
     - **以往方法**：同期工作（如BeamDojo）虽然也能通过狭窄区域，但**依赖LiDAR构建高程图**进行 foothold 规划。许多先进的RL人形 locomotion 工作主要在平坦或已知结构化地形上进行测试。
     - **本文方法**：专注于并系统验证了在**无任何外部感知**（纯本体感知）条件下，在宽度低至25cm的**狭窄路径、斜坡、楼梯**以及存在**未知移动障碍、外部推力、负载**等极端场景下的运动能力。
   - **解决的问题/带来的优势**：
     - **证明了方法的实用性与先进性**：通过仿真和Unitree H1-2真机实验，DBHL在成功率和行进距离上显著优于仅下半身控制的变体和官方相位步态基线（URG）。特别是在应对突发扰动（如被棍子绊、被推）时，展现了类似人类的快速动态平衡恢复能力。
     - **拓宽了人形机器人的应用边界**：表明在传感器受限或失效的灾难响应、工业巡检等场景中，人形机器人依然可以依靠本体感知完成复杂的移动任务。

---

**总结**：本文的核心创新在于**“古典控制智慧（ZMP）与现代学习框架（RL）的有机结合”**，并通过一系列精心设计的算法组件（向量化奖励、全身正则化等）将其实现。它解决了当前人形机器人RL控制中**动态平衡机制缺失**和**过度依赖外部感知**两大关键瓶颈，为实现更鲁棒、更通用、更接近人类运动能力的人形机器人 locomotion 提供了新的思路和有效的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过广泛的仿真和真实世界实验，系统地评估了所提出的 **DBHL** 方法在极端地形上的全身体运动性能。

### 一、 实验设置与评价指标

1.  **实验平台**：
    *   **仿真**：主要在 **Isaac Gym** 中进行大规模训练和主要评估，并在 **MuJoCo** 中进行高保真度的“仿真到仿真”验证和消融分析。
    *   **真实机器人**：在全尺寸双足机器人 **Unitree H1-2** 上进行零样本（zero-shot）部署验证。

2.  **评估场景（数据集）**：
    *   构建了三种主要的**狭窄地形**进行训练和评估：
        *   **狭窄平地**：宽度从1.0米逐渐减小至0.2米（课程学习）。
        *   **狭窄斜坡**：坡度从0逐渐增加至0.3。
        *   **狭窄楼梯**：台阶高度从0逐渐增加至0.12米。
    *   在真实世界中，搭建了包含**斜坡、桥梁、楼梯**的25厘米宽木质窄道，并在平地上设置了**按摩板、木块、石板路**等不规则障碍。

3.  **核心评价指标**：
    *   **成功率**：机器人成功行走距离 ≥ 4米的比例。
    *   **平均X方向位移**：评估期内机器人沿前进方向移动的平均距离。

### 二、 基线方法对比

论文与以下基线方法进行了详细对比，以回答不同研究问题：

1.  **DBHL w/o Upper**：**DBHL的消融变体**，固定上半身，仅使用下半身进行控制。用于验证**全身协调控制**的必要性。
2.  **Unitree RL Gym**：官方提供的基于相位的步态学习框架，主要遵循已有工作，使用**不对称演员-评论家架构**和**基于相位的奖励**来学习周期性步态。用于对比**基于动态平衡的方法**与**传统基于相位/步态的方法**的优劣。

### 三、 关键性能结果与结论

#### **Q1: DBHL在极端地形上的整体性能**
*   **结果**：如表III和图5所示，DBHL在**所有狭窄地形（平地、斜坡、楼梯）和所有难度等级下**，其**成功率和平均位移均显著优于两个基线方法**。
*   **结论**：
    *   **全身控制至关重要**：`DBHL w/o Upper` 性能急剧下降，表明在狭窄地形上行走需要上半身摆动来辅助动态平衡。
    *   **动态平衡机制更有效**：DBHL相比基于相位的`URG`方法表现出更强的鲁棒性，说明在复杂环境下，**脱离固定相位、基于ZMP的动态平衡机制**比学习周期性步态更具优势。

#### **Q2: ZMP奖励的作用分析**
*   **定量结果**：图6显示，在MuJoCo的高保真仿真中，**包含ZMP奖励的策略在大多数狭窄地形设置下，成功率和平均位移都明显高于不含ZMP奖励的变体**。
*   **定性分析**：图7展示了在受到外部推力时，有/无ZMP奖励的策略表现。**不含ZMP奖励的机器人迅速失去平衡摔倒**，其ZMP距离（衡量动态平衡的关键指标）发散；而**包含ZMP奖励的DBHL成功维持了稳定**，ZMP距离被有效约束。
*   **结论**：**ZMP奖励是DBHL实现动态平衡的核心**，它通过奖励函数引导策略将零力矩线保持在支撑多边形中心附近，从而显著提升了机器人在扰动和复杂地形下的稳定性。

#### **Q3: 其他设计选择的影响（消融研究）**
*   **奖励向量化**：图8表明，使用奖励向量化技术**显著加快了训练收敛速度**。这是因为每个奖励项对应独立的价值函数头，避免了小奖励项在累加和中被淹没，提升了学习效率。
*   **乘性动作噪声**：图9显示，**适度的乘性动作噪声**能有效约束上半身关节的运动范围，提升任务性能。噪声过大或过小都会导致不稳定或性能下降。

#### **Q4: 真实世界迁移能力**
*   **结果**：如图1和图10所示，部署在Unitree H1-2上的DBHL策略成功完成了多项极具挑战性的真实世界测试：
    1.  在25厘米宽的复杂窄道（含斜坡、楼梯）上稳定行走。
    2.  在窄道上抵抗**外部扰动**，如携带5kg负载、穿越密集锥形障碍、承受人力推搡、在杠杆化的棍子上行走。
    3.  在**不规则平面地形**上，成功越过按摩板、木块、石板路等障碍，并在过程中抵抗外力。
*   **结论**：DBHL通过**仅使用本体感知**和**广泛的域随机化**，实现了高质量的**零样本仿真到现实迁移**，证明了其在真实复杂、非结构化环境中的强大适应性和实用性。

### 总结
该论文通过严谨的实验设计，不仅提供了**明确的定量结果**（成功率、位移），还结合了**定性分析**（轨迹可视化）和**系统的消融研究**，全面验证了DBHL方法的有效性。核心结论是：通过将**ZMP驱动的动态平衡奖励**与**全身强化学习框架**相结合，DBHL使双足机器人能够在**仅依赖本体感知**的情况下，在**极端狭窄、不平整的地形**上实现**鲁棒、自适应**的运动，性能显著优于现有的基于相位或纯下半身控制的方法。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2502.17219v2)
- [HTML 版本](https://arxiv.org/html/2502.17219v2)
