# Learning Humanoid Locomotion with World Model Reconstruction

**相关性评分**: 6.0/10

**排名**: #17


---


## 基本信息

- **arXiv ID**: [2502.16230v1](https://arxiv.org/abs/2502.16230v1)
- **发布时间**: 2025-02-22T13:57:56Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Wandong Sun, Long Chen, Yongbo Su, Baoshi Cao, Yang Liu, Zongwu Xie

## 关键词

Robot control, quadruped robot locomotion, local observational information

## 一句话总结

该论文提出了一种基于世界模型重建的端到端学习方法，用于人形机器人在复杂地形中的盲行走，通过独立训练估计器重建世界状态以增强控制策略，并在真实世界场景中验证了其鲁棒性。

## 摘要

Humanoid robots are designed to navigate environments accessible to humans using their legs. However, classical research has primarily focused on controlled laboratory settings, resulting in a gap in developing controllers for navigating complex real-world terrains. This challenge mainly arises from the limitations and noise in sensor data, which hinder the robot's understanding of itself and the environment. In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains. We propose training an estimator to explicitly reconstruct the world state and utilize it to enhance the locomotion policy. The locomotion policy takes inputs entirely from the reconstructed information. The policy and the estimator are trained jointly; however, the gradient between them is intentionally cut off. This ensures that the estimator focuses solely on world reconstruction, independent of the locomotion policy's updates. We evaluated our model on rough, deformable, and slippery surfaces in real-world scenarios, demonstrating robust adaptability and resistance to interference. The robot successfully completed a 3.2 km hike without any human assistance, mastering terrains covered with ice and snow.

## 详细分析

## 论文摘要：《通过世界模型重建学习人形机器人运动》

### 1. 研究背景和动机
人形机器人旨在利用双腿在人类可及的环境中导航。然而，传统研究多集中于受控的实验室环境，导致在复杂真实地形上开发鲁棒控制器的能力存在差距。这一挑战主要源于传感器数据的局限性和噪声，阻碍了机器人对自身和环境的准确感知。因此，如何有效抑制噪声并利用看似不可用的信息来增强运动策略，成为亟待解决的问题。

### 2. 核心方法和技术创新
本文提出了**世界模型重建（WMR）**，一种用于在挑战性地形上实现“盲”人形运动的端到端强化学习方法。其核心创新在于：
- **显式世界重建**：训练一个**状态估计器**，从带噪声的传感器历史数据中显式重建完整的世界状态（包括去噪的传感器数据和特权信息，如地面摩擦、接触状态等），并将其作为运动策略的唯一输入。
- **梯度切断机制**：在估计器和运动策略之间**故意切断梯度反向传播**，确保估计器专注于精确的世界重建，不受策略学习目标的干扰，从而显著提高了重建精度。
- **人类运动数据驱动的指令空间**：利用运动捕捉数据集（AMASS）生成指令，使机器人的速度指令分布更符合人类自然运动模式，提升了指令跟踪的自然性和精确性。

### 3. 主要实验结果
- **仿真评估**：WMR框架在速度跟踪误差、重建精度等多个指标上均优于PPO、去噪世界模型学习等基线方法。梯度切断机制使重建精度提升了约40%。
- **实物部署**：在真实冰雪覆盖的户外环境中，搭载WMR控制器的G1人形机器人成功完成了**3.2公里的自主徒步**，稳定穿越了粗糙、碎石、斜坡及深雪等多种复杂地形，并能抵抗外力冲击和在低摩擦表面保持稳定。
- **零样本仿真到现实迁移**：经过单一阶段的训练后，控制器无需额外调整即可直接部署到真实世界，展示了强大的泛化能力和鲁棒性。

### 4. 研究意义和价值
本研究首次提出了一个通过**显式世界重建**来增强人形机器人运动控制的完整框架。其**梯度切断机制**为解决联合训练中目标冲突问题提供了简单有效的方案。该工作成功弥合了仿真与现实之间的差距，实现了人形机器人在复杂、非结构化户外环境中的**长距离、高鲁棒性自主运动**，为人形机器人的实际应用（如野外救援、户外作业）奠定了重要的技术基础。论文承诺将开源代码以完全复现结果。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **一、 论文旨在解决的核心问题**
论文旨在解决**双足人形机器人在复杂、非结构化真实地形（如粗糙、可变形、湿滑路面）上进行鲁棒、盲视（无外部视觉感知）运动控制**的难题。具体挑战包括：
- **传感器数据局限与噪声**：真实世界的传感器（如IMU、编码器）数据存在噪声且信息不完整，阻碍机器人准确感知自身状态（如基座速度）和环境属性（如地面摩擦系数）。
- **仿真到现实的鸿沟**：在仿真中训练的策略，因传感器模型不完美和环境简化，难以直接迁移到物理机器人上。
- **人形机器人控制的固有复杂性**：相比四足机器人，双足人形机器人具有更高的不稳定性、动态平衡要求和更复杂的动力学。

### **二、 核心创新点：世界模型重建框架**
论文提出了 **“世界模型重建”** 框架，其核心创新体现在以下三个紧密关联的方面：

1.  **显式世界状态重建与策略输入解耦**
    - **创新做法**：训练一个**估计器**，从带噪声的传感器历史数据中，**显式地重建**一个完整的、去噪的“世界状态”。这个重建状态（共192维，包括去噪的传感器数据和估计的“特权信息”）**作为运动策略的唯一输入**。
    - **与传统/其他学习方法的区别**：不同于将环境信息编码为隐变量或让策略直接处理原始噪声观测，WMR明确地输出可解释的物理量（如速度、摩擦系数、接触状态），使策略基于清晰、丰富的信息进行决策。

2.  **梯度截断机制**
    - **创新做法**：在**联合训练**估计器、价值网络和运动策略时，**故意切断从策略损失到估计器的梯度回传**。
    - **解决的关键问题**：确保估计器**专注于准确重建世界状态**这一目标，而不会因为策略学习目标的干扰（例如，策略可能倾向于利用估计器的错误来获取短期奖励）而“分心”或产生有偏估计。论文实验表明，该机制将重建准确率提升了约40%。

3.  **基于动作捕捉数据的命令空间**
    - **创新做法**：使用**AMASS人体动作捕捉数据集**中的根速度轨迹作为训练时的命令（目标速度）来源，而非在仿真中随机采样。
    - **带来的优势**：使机器人学习的速度命令跟踪更符合**人类的自然运动分布**（如平滑的速度过渡），从而生成更拟人、更高效的步态，提升了命令跟踪的精确性和整体运动表现。

### **三、 解决方案的总体框架与方法**
1.  **系统架构**：采用**非对称的演员-评论家**架构。
    - **估计器**：一个编码器-解码器网络（编码器为LSTM），输入为带噪声的传感器历史，输出重建的世界状态。
    - **价值网络**：输入为仿真中的无噪声“特权信息”，用于准确评估状态价值。
    - **运动策略**：输入为估计器重建的世界状态，输出关节目标位置，通过PD控制器转换为扭矩。

2.  **训练流程**：
    - **联合训练**：使用一个优化器同时优化估计器（重建损失）、价值网络（价值损失）和策略（PPO策略损失），但应用**梯度截断**。
    - **重建损失**：结合MSE损失（连续状态）、BCE损失（离散接触状态）和L1正则化（促进潜在表示稀疏性）。
    - **课程学习**：从平坦地面开始，逐渐增加地形难度（粗糙路面、障碍、斜坡、楼梯）。
    - **域随机化**：在仿真中对传感器数据、物理参数（摩擦、负载、重力等）添加噪声和扰动，以提升模型的鲁棒性和泛化能力。

3.  **奖励函数设计**：**刻意避免使用任何预设的运动参考轨迹**。奖励主要由任务奖励（跟踪指令速度）和正则化奖励（保持稳定、节能、平滑的运动）构成，让策略自主探索出高效的步态。

### **四、 实际价值与验证结果**
- **卓越的仿真性能**：在多项指标（速度跟踪误差、重建误差、平均奖励等）上全面超越基线方法（标准PPO、去噪世界模型学习等）。
- **成功的零次仿真到现实迁移**：训练后的策略**无需任何现实微调**，直接部署在G1人形机器人上。
- **强大的现实世界适应性**：机器人成功在**冰雪覆盖**的复杂户外环境（粗糙、碎石、斜坡、深雪）中稳定行走，能抵抗人为推搡，并在低摩擦表面保持稳定。
- **里程碑式成就**：在无任何人工辅助的情况下，完成了**单次3.2公里的徒步**，证明了其长期运行可靠性和对极端地形的适应能力。
- **准确的在线估计**：在现实部署中，框架能准确在线估计地面摩擦、负载质量等关键信息，并表现出自然的、速度自适应的步态。

### **总结**
本文的核心创新在于提出了一个**通过显式、精准的世界模型重建来赋能人形机器人运动控制**的端到端学习框架。其**梯度截断**机制是关键的技术洞察，保证了状态估计的纯净性和准确性。该方法**显著缩小了仿真与现实的差距**，首次实现了人形机器人在多种极端复杂地形上的长距离、高鲁棒性、完全自主的盲视 locomotion，是人形机器人走向实用化的重要一步。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决双足人形机器人在复杂真实地形（如粗糙、可变形、低摩擦表面）上实现鲁棒、盲视（无外部感知）运动控制的难题，其核心挑战在于传感器数据存在噪声和局限，阻碍了机器人对自身及环境的准确理解。为此，论文提出了**世界模型重建（WMR）**框架，这是一种端到端的强化学习方法，其核心创新在于训练一个**状态估计器**，从带噪声的传感器历史数据中显式地重建去噪后的传感器信息和特权信息（如地面摩擦、足部接触状态），并将重建后的完整世界状态作为运动策略的唯一输入。该方法的一个关键设计是**在估计器与策略网络之间切断梯度回传**，确保估计器专注于精确重建而不受策略学习目标的干扰。实验结果表明，该方法在仿真和现实世界中均显著提升了状态估计精度与运动控制性能，最终使机器人能够在冰雪覆盖等极端环境下完成长达**3.2公里**的完全自主徒步穿越，展现了强大的地形适应性和抗干扰能力。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文《Learning Humanoid Locomotion with World Model Reconstruction》的创新点分析

这篇论文针对双足人形机器人在复杂真实地形中的鲁棒运动控制问题，提出了一种名为**世界模型重建（World Model Reconstruction, WMR）**的端到端强化学习框架。其核心创新点明确且具有系统性，具体如下：

---

### 1. **首次提出基于显式世界重建的运动策略增强框架**
- **改进/不同之处**：
    - **以往方法**：许多基于学习的运动控制方法（如RMA、DreamWaQ等）通常将环境状态（如摩擦力、接触状态）编码为**隐式**的潜在变量（latent variables），或者仅估计部分状态（如根速度）。策略网络直接使用这些隐式或部分信息。
    - **本文方法**：WMR框架**显式地、完整地**重建了一个全面的“世界状态”（包括去噪的传感器数据和特权信息，如地面摩擦系数、接触掩码、负载、基座速度等，共192维）。**运动策略的输入完全且唯一地来源于这个显式重建的世界状态**。
- **解决的问题/带来的优势**：
    - **提高策略的鲁棒性与可解释性**：策略基于一个清晰、去噪且信息丰富的状态表示进行决策，而非原始噪声数据或模糊的隐变量。这使得策略在面对传感器噪声和未知扰动时更稳定。
    - **促进零样本仿真到现实迁移**：通过在仿真中使用域随机化模拟噪声，并训练估计器从带噪声的历史观测中重建干净的世界状态，有效缩小了仿真与现实之间的差距。这使得训练好的策略能够**直接（零样本）** 部署到真实世界的复杂地形上。

### 2. **引入简单有效的梯度截断机制**
- **改进/不同之处**：
    - **以往方法**：在联合训练状态估计器和运动策略时（如Denoising World Model Learning），策略学习的梯度会**反向传播到估计器网络**。这导致估计器的优化目标被污染，需要在“准确重建世界状态”和“辅助策略获得高回报”之间进行权衡，从而**分散了估计器的目标**，降低了重建精度。
    - **本文方法**：在估计器（Encoder-Decoder）和运动策略（Actor）之间**人为地切断了梯度流**。虽然两者使用同一个优化器进行联合训练，但策略损失 \(L_{\pi}\) 的梯度不会更新估计器的参数。
- **解决的问题/带来的优势**：
    - **确保估计器的专注性**：估计器可以**纯粹地、不受干扰地**专注于最小化世界状态的重建损失（\(\mathcal{L}_{\text{recon}}\)），其目标单一且明确。
    - **显著提升重建精度**：如表II和图4所示，引入梯度截断后，世界状态的重建误差（\(E_{\text{recon}}\)）降低了约40%。更高的重建精度直接为下游运动策略提供了更可靠的状态输入，这是实现鲁棒运动的基础。
    - **明确的模块化分工**：形成了“估计器负责精确感知，策略负责优化决策”的清晰架构，提升了系统的可分析性和稳定性。

### 3. **利用运动捕捉数据集构建指令空间**
- **改进/不同之处**：
    - **以往方法**：在训练中，给机器人的速度指令（线性速度和偏航角速度）通常是在仿真中**随机采样**的，其分布可能不自然（如突变频繁）。
    - **本文方法**：从大规模人类运动捕捉数据集（AMASS）中提取人体骨盆的根速度轨迹，将其作为机器人训练时的**指令空间**。这直接利用了与人类运动分布对齐的速度指令。
- **解决的问题/带来的优势**：
    - **生成更自然、拟人化的步态**：如图7所示，机器人能根据指令大小自动调整步频，在零速指令时保持稳定站立，在低速到高速行走时表现出平滑的步态过渡，模仿了人类的行走模式。
    - **提升指令跟踪的精确性与泛化性**：使用人类数据驱动的指令，使策略能学习到更符合生物力学特性的运动模式，从而在跟踪各种速度指令时更精确、更节能，也更容易适应真实世界中人类操作员给出的自然指令。

### 4. **采用无运动参考的奖励函数设计**
- **改进/不同之处**：
    - **以往方法**：许多强化学习工作会使用**运动先验或参考轨迹**（如通过模仿学习获取的步态数据）来设计奖励函数，以引导策略学习稳定的步态。
    - **本文方法**：**刻意避免使用任何预定义的运动参考**（如周期性触地模式、脚部抬升高度、参考轨迹跟踪等）。奖励函数仅由**任务奖励**（跟踪指令速度）和**正则化奖励**（保持稳定、节能、平滑的姿势）构成。
- **解决的问题/带来的优势**：
    - **激发策略的自主探索与适应性**：策略不被限制在特定的人类步态模板内，而是通过强化学习自主发现稳定、高效的步态。这使其能更好地适应非结构化的复杂地形和突发扰动。
    - **实现步态的自动切换**：如图7所示，策略能根据指令自动在站立、行走等模式间切换，并调整步频，表现出高度的自主性和灵活性。

---

### **总结：核心创新逻辑链**
这篇论文的创新点形成了一个紧密的闭环：
1.  **显式重建** 提供了一个干净、丰富的状态表示基础。
2.  **梯度截断** 保障了这个状态表示的高精度和可靠性。
3.  **人类指令空间** 引导策略学习更自然、高效的运动模式。
4.  **无参考奖励** 赋予策略在可靠状态输入下自主优化、适应复杂环境的最大自由度。

最终，这些创新共同作用，使得基于WMR框架训练的控制器能够在**真实世界**中（包括粗糙、可变形、倾斜、低摩擦的冰雪路面）完成长达**3.2公里**的完全自主徒步，展现了卓越的鲁棒性、适应性和抗干扰能力，实现了人形机器人盲 locomotion 领域的显著突破。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

该论文通过详尽的仿真与真实世界实验，全面评估了所提出的**世界模型重建（WMR）框架**在双足人形机器人运动控制上的性能。实验设计旨在验证其**去噪、状态估计和零样本仿真到现实迁移**的能力。

### 一、 主要实验效果
论文最终实现了以下**突破性效果**：
1.  **强大的零样本仿真到现实迁移**：训练好的策略无需任何微调，即可直接部署到真实机器人上。
2.  **复杂地形鲁棒穿越**：在真实冰雪覆盖的户外环境中，机器人成功自主完成了**3.2公里**的徒步，穿越了包括粗糙路面、碎石、斜坡和深雪在内的多种挑战性地形。
3.  **抗干扰能力**：能够抵抗来自人类的推搡冲击，并在低摩擦表面（如冰面）上保持稳定。
4.  **精确的状态重建与估计**：仅凭带噪声的机载传感器历史数据，能够高精度地重建世界状态（如基座速度、接触状态、地面摩擦系数、外部负载等）。

### 二、 使用的数据集
1.  **训练数据（仿真生成）**：
    *   使用 **IsaacLab** 仿真环境生成训练数据。
    *   通过**域随机化（Domain Randomization）** 模拟传感器噪声和环境变化（如摩擦系数、重力、关节参数扰动等），参数范围见论文附录表IV。
2.  **命令空间数据**：
    *   关键创新：使用 **AMASS（Archive of Motion Capture as Surface Shapes）** 运动捕捉数据集中的**人体根部位移速度**作为训练时的命令空间。
    *   **作用**：使机器人学习的速度命令分布更符合人类自然运动模式，提升了命令跟踪的自然性和鲁棒性。

### 三、 评价指标
论文使用了**仿真评估指标**和**真实世界定性/定量验证**相结合的方式。

#### **1. 主要仿真评估指标（见表II）**
*   **`E_vel`**：平均回合线性速度跟踪误差（越小越好）。
*   **`E_ang`**：平均回合角速度跟踪误差（越小越好）。
*   **`E_recon`**：平均世界状态重建误差（MSE，越小越好）。
*   **`M_terrain`**：平均通过的地形等级（在课程式训练中，越高表示策略能应对更复杂的地形）。
*   **`M_reward`**：平均回合奖励（越高越好）。

#### **2. 真实世界评估**
*   **定性**：展示在冰雪、斜坡、碎石等复杂地形的稳定行走视频/图像。
*   **定量**：
    *   **行走距离**：单次不间断行走3.2公里。
    *   **负载估计精度**：在真实机器人上添加-10kg至10kg负载，估计精度平均达到约 **0.86**（高相关度）。
    *   **状态重建可视化**：对比在仿真楼梯场景下，WMR预测的世界状态（如基座速度、接触掩码）与真实无噪声状态（Ground Truth），显示高度吻合。

### 四、 对比的基线方法
论文与以下**四个基线方法**进行了全面对比：
1.  **PPO**：标准的近端策略优化算法（带LSTM），**不包含任何世界模型重建模块**。用于验证重建机制本身的价值。
2.  **Denoising World Model Learning [4]**：当前**最先进的去噪世界模型学习方法**（编码器-解码器结构）。作为核心对比对象，以凸显WMR框架的创新性。
3.  **WMR without Gradient Cutoff**：**消融实验**。移除WMR框架中**估计器与策略间的梯度截断机制**，用于验证该设计的关键性。
4.  **WMR with Random Command Sample**：**消融实验**。将命令空间从AMASS数据改为**仿真中随机采样**，用于验证使用人类运动数据的重要性。

### 五、 关键性能提升与结论
根据论文表II、IV及正文分析，WMR框架在关键指标上全面领先：

| 指标 | WMR (Ours) 表现 | 关键对比与结论 |
| :--- | :--- | :--- |
| **重建误差 (`E_recon`)** | **0.098** (最低) | 相比 **Denoising World Model Learning (0.154)** 提升约 **36%**；相比 **无梯度截断的WMR (0.459)** 提升约 **79%**。**结论**：梯度截断机制极大提高了状态估计的专注度和准确性。 |
| **速度跟踪误差 (`E_vel`, `E_ang`)** | **0.156, 0.252** (最低) | 在所有基线中均为最优。相比基础PPO，误差显著降低。**结论**：准确的世界状态重建为策略提供了更干净、信息更丰富的输入，直接提升了控制性能。 |
| **平均奖励 (`M_reward`)** | **18.695** (最高) | 显著高于其他所有基线。**结论**：WMR框架训练出的策略整体性能更优，能获得更高的累积奖励。 |
| **地形通过能力 (`M_terrain`)** | **5.827** (最高) | 略高于其他方法，表明其策略能通过课程中更困难的地形。 |
| **命令跟踪自然性** | N/A (定性) | 相比随机命令采样，使用AMASS数据的WMR能产生更**符合人类步态**的速度切换和步频调整（见图7）。 |

### **核心结论总结**
1.  **梯度截断至关重要**：它是WMR框架成功的关键，使估计器能专注于重建任务，免受策略学习目标的干扰，从而获得**更精确、更可解释**的状态估计。
2.  **显式重建优于隐式表征**：WMR显式地重建完整的特权信息（共192维），并直接作为策略输入，这种方法比将信息编码在潜在变量中更有效，带来了显著的性能提升。
3.  **人类数据先验有效**：利用AMASS运动捕捉数据定义命令空间，使策略学习到更自然的运动模式，增强了泛化性和鲁棒性。
4.  **端到端训练实现Sim-to-Real**：通过域随机化和精心设计的框架，实现了**零样本**的仿真到现实迁移，在极其复杂的真实户外环境中展现了卓越的鲁棒性。

**局限性说明**：论文给出了明确的、丰富的定量结果（仿真指标）和定性验证（真实世界实验），结论坚实。其局限性主要在于**未利用地形高度图**，因此无法处理需要离散跳跃或高台阶的极端地形，这是作者提到的未来改进方向。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2502.16230v1)
- [HTML 版本](https://arxiv.org/html/2502.16230v1)
