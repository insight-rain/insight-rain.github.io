# Accelerating Model-Based Reinforcement Learning with State-Space World Models

**相关性评分**: 6.0/10

**排名**: #29


---


## 基本信息

- **arXiv ID**: [2502.20168v1](https://arxiv.org/abs/2502.20168v1)
- **发布时间**: 2025-02-27T15:05:25Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Maria Krinner, Elie Aljalbout, Angel Romero, Davide Scaramuzza

## 关键词

Robot control, quadruped robot locomotion, multi-agent RL, CTDE methods, Edge Deployment, mujoco, local observational information

## 一句话总结

该论文提出了一种基于状态空间世界模型的加速模型强化学习方法，在四旋翼飞行任务中显著提升训练效率，但与部分关键词如多智能体RL和CTDE方法相关性较弱。

## 摘要

Reinforcement learning (RL) is a powerful approach for robot learning. However, model-free RL (MFRL) requires a large number of environment interactions to learn successful control policies. This is due to the noisy RL training updates and the complexity of robotic systems, which typically involve highly non-linear dynamics and noisy sensor signals. In contrast, model-based RL (MBRL) not only trains a policy but simultaneously learns a world model that captures the environment's dynamics and rewards. The world model can either be used for planning, for data collection, or to provide first-order policy gradients for training. Leveraging a world model significantly improves sample efficiency compared to model-free RL. However, training a world model alongside the policy increases the computational complexity, leading to longer training times that are often intractable for complex real-world scenarios. In this work, we propose a new method for accelerating model-based RL using state-space world models. Our approach leverages state-space models (SSMs) to parallelize the training of the dynamics model, which is typically the main computational bottleneck. Additionally, we propose an architecture that provides privileged information to the world model during training, which is particularly relevant for partially observable environments. We evaluate our method in several real-world agile quadrotor flight tasks, involving complex dynamics, for both fully and partially observable environments. We demonstrate a significant speedup, reducing the world model training time by up to 10 times, and the overall MBRL training time by up to 4 times. This benefit comes without compromising performance, as our method achieves similar sample efficiency and task rewards to state-of-the-art MBRL methods.

## 详细分析

## 论文摘要

**论文标题**：使用状态空间世界模型加速基于模型的强化学习

### 1. 研究背景和动机
基于模型的强化学习（MBRL）因其卓越的样本效率在机器人学习领域备受关注。然而，传统的MBRL方法（如Dreamer系列）通常使用循环神经网络（RNN）作为世界模型的核心，其序列处理的串行特性导致模型训练速度缓慢，成为MBRL应用于复杂现实场景（如机器人敏捷控制）的主要瓶颈。本研究旨在解决这一关键问题，**通过引入并行化的序列模型来显著加速世界模型的训练**，从而提升MBRL的整体训练效率。

### 2. 核心方法和技术创新
本文提出了一种名为 **S5WM** 的新型MBRL方法，其核心创新点在于：
- **并行化状态空间模型（SSM）**：用现代可并行化的状态空间模型（特别是S5架构）替代传统的循环状态空间模型（RSSM）作为世界模型的动力学序列模型。S5利用**并行扫描**操作，将序列维度的计算复杂度从O(L)降低至O(log L)，从而实现了对长序列的高效并行训练。
- **特权世界模型**：针对部分可观测环境（如视觉输入），提出在训练世界模型时使用**特权信息**（如低维状态观测）作为解码目标，而非重建高维图像。这为编码器提供了更强的监督信号，鼓励其学习对策略优化更相关的潜在表示，同时降低了计算开销。
- **可重置的SSM设计**：扩展了S5模型，使其能够根据情节连续性预测来重置隐藏状态，防止无关信息在不同训练情节间传播，这对于处理现实任务中的序列边界至关重要。

### 3. 主要实验结果
研究在具有复杂动力学的**真实世界四旋翼敏捷飞行任务**（状态观测的Split-S赛道和视觉观测的Figure-8赛道）上进行了验证：
- **训练速度大幅提升**：与DreamerV3相比，S5WM将**世界模型训练时间减少了高达10倍**，整体MBRL训练时间减少了高达**4倍**。
- **性能无损**：在实现加速的同时，S5WM达到了与最先进MBRL方法（DreamerV3）**相当的样本效率和最终任务奖励**，在仿真和真实部署中均能成功完成高速飞行任务。
- **特权信息的有效性**：在视觉任务中，使用特权信息进行解码的S5WM变体表现出更高的样本效率。
- **成功仿真到现实迁移**：训练出的策略成功部署到真实四旋翼飞行器上，完成了高速竞速飞行。

### 4. 研究意义和价值
本工作通过将高效的序列建模架构（SSM）与MBRL框架相结合，**在保持MBRL高样本效率优势的同时，显著突破了其训练速度瓶颈**。这使MBRL更适用于对训练时间敏感的现实机器人应用场景。所提出的S5WM框架为加速机器人技能学习提供了一个有前景的方向，其方法具有通用性，可扩展至其他机器人领域（如移动、操作）。同时，关于利用特权信息辅助训练的思路，也为处理高维、部分可观测输入提供了有效的解决方案。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **研究背景与核心问题**
论文旨在解决**基于模型的强化学习（MBRL）训练速度慢**的瓶颈问题。MBRL（如Dreamer系列）虽然比无模型方法（MFRL）样本效率更高，但因其需要额外训练一个**世界模型（World Model）**来模拟环境动态，导致计算复杂度高、训练时间长，限制了其在复杂现实场景（如机器人控制）中的应用。

### **核心创新点**
论文提出了 **S5WM（State-Space World Model）** 方法，主要包含两大创新：

1.  **使用可并行化的状态空间模型（SSM）替代传统的循环神经网络（RNN）作为世界模型的序列建模骨干**：
    - **问题**：传统MBRL（如DreamerV3）使用循环状态空间模型（RSSM），其RNN的序列计算本质导致训练无法并行化，成为计算瓶颈。
    - **解决方案**：采用 **S5 SSM** 作为动态模型。S5利用**并行扫描（parallel scan）** 操作，可将序列维度上的计算并行化，显著降低计算复杂度。
    - **关键技术改进**：对S5进行了改造，使其能够处理**序列重置（resetability）**，以应对训练数据中 episode 边界的不连续性，防止无关信息跨episode传播。

2.  **在部分可观测环境中引入特权信息（Privileged Information）训练世界模型**：
    - **问题**：在视觉MBRL中，从高维图像中学习有意义的潜在表示非常困难，且部分可观测性进一步增加了难度。
    - **解决方案**：在仿真训练阶段，让世界模型访问**特权状态信息**（如精确的无人机位姿、速度等低维状态），并让解码器重建这些特权状态，而非原始图像。
    - **优势**：
        - 为编码器学习提供更强的监督信号。
        - 鼓励潜在表示捕获与策略优化更相关的信息，而非专注于图像细节。
        - 计算上更高效（重建低维状态 vs. 高维图像）。

### **解决方案的实施路径**
1.  **架构设计**：在DreamerV3框架基础上，用S5块替换RSSM。世界模型包含编码器、S5序列模型、以及奖励、连续性和（特权）状态解码器。
2.  **训练流程**：遵循标准MBRL三步迭代循环：
    - 用真实交互数据训练世界模型（**可并行化，加速关键**）。
    - 在世界模型生成的“想象”轨迹中训练演员-评论家（Actor-Critic）。
    - 用最新策略在真实环境中收集新数据。
3.  **实际应用与验证**：在具有非线性复杂动态的**真实世界敏捷四旋翼无人机飞行任务**（竞速赛道）中进行了全面验证，包括完全可观测（状态输入）和部分可观测（视觉输入）环境。

### **实际价值与效果**
- **显著加速**：世界模型训练时间**最高减少10倍**，整体MBRL训练时间**最高减少4倍**。
- **性能无损**：在样本效率和最终任务奖励上，达到了与SOTA MBRL方法（DreamerV3）相当的水平。
- **现实可行性**：加速效果使得MBRL在需要快速迭代的复杂机器人学习任务中更具实用价值。论文成功实现了从仿真到真实世界的策略迁移，在真实无人机上完成了高速飞行任务。
- **系统贡献**：为解决MBRL的计算瓶颈提供了一个高效、可并行化的新架构选择，并展示了利用仿真中特权信息来提升视觉MBRL性能的有效途径。

**总结**：该论文的核心是通过**引入可并行的SSM架构**和**利用特权信息监督**，在**不牺牲样本效率的前提下，大幅提升了MBRL的训练速度**，从而推动了MBRL在计算资源受限或需要快速部署的现实机器人任务中的应用。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**模型强化学习（MBRL）训练速度慢**的核心瓶颈问题，该问题源于其世界模型（尤其是基于RNN的序列模型）训练时的顺序计算开销。为此，论文提出了一种名为 **S5WM** 的新方法，其核心创新在于**使用可并行化的状态空间模型（SSM，特别是S5架构）替代传统的循环状态空间模型（RSSM）作为世界模型的动力学序列模型**，并引入了一种**利用特权状态信息（而非原始图像）来训练视觉世界模型**的架构，以处理部分可观测环境。最终，该方法在保持与先进MBRL方法（如DreamerV3）相当样本效率和任务性能的同时，**显著加速了训练过程**：世界模型训练时间最高减少10倍，整体MBRL训练时间最高减少4倍，并在真实世界的敏捷四旋翼飞行任务中得到了验证。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇题为《Accelerating Model-Based Reinforcement Learning with State-Space World Models》的论文，针对模型强化学习（MBRL）训练速度慢的核心瓶颈，提出了一套系统的创新方法。其核心创新点可归纳为以下四条：

---

### 1. **采用可并行化的状态空间模型作为世界模型的序列建模骨干**
- **相比以往方法的改进/不同之处**：
    - **传统方法**：主流的MBRL方法（如Dreamer系列）通常使用**循环状态空间模型（RSSM）** 作为世界模型的动态模型。RSSM本质上是循环神经网络（RNN），其计算是**顺序的**，无法并行处理序列，导致训练速度成为瓶颈，尤其对于长序列。
    - **本文方法**：提出使用**现代状态空间模型（SSM）**，特别是**S5模型**，来替代RSSM。SSM通过**并行扫描（Parallel Scan）** 操作，能够将序列维度的计算并行化。
- **解决的具体问题/带来的优势**：
    - **显著加速世界模型训练**：这是论文最核心的贡献。通过并行化序列计算，**世界模型的训练时间最高可减少10倍**。这直接解决了MBRL因需同时训练世界模型和策略而导致**总体训练时间过长**的问题，使得MBRL在需要快速迭代的复杂现实场景（如机器人学习）中更具实用性。
    - **保持建模能力**：S5 SSM同样擅长捕捉长期时空依赖关系，因此替换RSSM后，并未牺牲世界模型的预测精度和样本效率。

### 2. **提出了支持“重置”的、适用于MBRL的S5世界模型架构**
- **相比以往方法的改进/不同之处**：
    - **基础SSM的局限**：标准的SSM（如S4）在序列建模时，隐藏状态会持续跨序列传递，这**不适合MBRL中常见的分幕式（episodic）环境**，因为不同幕（episode）之间的状态不应有连续性。
    - **本文方法**：对S5模型进行了关键改造，**引入了基于幕终止信号的隐藏状态重置机制**。具体做法是扩展了并行扫描的二元操作符，使其能够根据预测的“幕是否继续”信号（`c_t`）来决定是累积状态还是重置状态。
- **解决的具体问题/带来的优势**：
    - **正确处理分幕式任务**：确保了世界模型在想象（imagination）或预测时，不会将无关的上一幕信息带入当前幕，从而**提高了动态建模的准确性和稳定性**。这是将SSM成功应用于MBRL的关键工程创新。

### 3. **在训练阶段为世界模型引入特权信息解码**
- **相比以往方法的改进/不同之处**：
    - **传统视觉MBRL**：在基于视觉的MBRL中，世界模型的解码器通常被训练来**重建高维图像观测**。这存在两个问题：(1) 图像重建计算成本高（需要CNN解码器）；(2) 重建像素的监督信号可能与控制任务不直接相关。
    - **本文方法**：在**仿真训练阶段**，让世界模型解码器去重建**低维特权状态信息**（如无人机的位置、速度等），而非原始图像。这些特权信息在真实部署时是不可得的，仅在仿真中可用。
- **解决的具体问题/带来的优势**：
    - **提供更强的学习信号**：重建低维状态比重建图像更容易，且为编码器学习提供了**更直接、与任务更相关的监督**。这鼓励潜在表征捕获对策略优化至关重要的信息，而非无关的视觉细节。
    - **缓解部分可观测性问题**：在视觉输入部分可观测的环境中，这一方法有助于学习更丰富、更准确的潜在状态表示，从而**提升了在视觉任务上的样本效率和最终性能**。实验表明，此方法对基于S5WM的方法提升尤为明显。

### 4. **提出了解耦的奖励平滑课程学习策略**
- **相比以往方法的改进/不同之处**：
    - **常见的奖励塑形**：在模型无关RL中，常通过动态调整奖励函数来设置课程（由易到难），但这在MBRL中会导致世界模型需要“忘记”旧奖励并重新学习新奖励，**损害训练稳定性**。
    - **本文方法**：将总奖励分解为**固定奖励**和**平滑奖励**两部分。`r = r_nom + r_aug`。`r_nom`（如进度奖励、基础动作惩罚）从一开始就启用；`r_aug`（如鼓励动作平滑的惩罚项）也从头开始学习，但仅在智能体掌握基本技能后才**加入到价值函数的目标中**。
- **解决的具体问题/带来的优势**：
    - **稳定地提升策略质量**：允许智能体先探索并找到可行解（仅优化`r_nom`），再在此基础上优化动作平滑度等高级要求（加入`r_aug`）。这避免了直接修改奖励函数对世界模型造成的干扰。
    - **促进仿真到现实的迁移**：通过课程学习得到的**更平滑、更可行的控制指令**，减少了策略在仿真中学到激进、不切实际行为的可能，从而**缩小了仿真与现实之间的差距**，提高了部署成功率。

---

### **总结**
本文的创新是一个**系统性的工程与算法结合方案**，而非单一突破。其核心主线是：**1. 用可并行的SSM大幅加速MBRL的瓶颈（世界模型训练）；2. 通过架构改造（重置机制、特权信息解码）和训练技巧（奖励课程）来保证并提升加速后方法的性能与鲁棒性。** 最终，在复杂的真实世界四旋翼敏捷飞行任务上，实现了**高达4倍的整体训练加速，且不损失样本效率和最终性能**，显著提升了MBRL在机器人等实际应用中的可行性。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果总结

### 一、 实验任务与数据集
论文在两个**真实世界无人机竞速任务**上进行了评估：
1.  **基于状态的Split-S任务**：无人机在复杂Split-S赛道高速飞行，观测为**24维状态向量**（位置、速度、姿态等）。
2.  **基于视觉的Figure-8任务**：无人机在Figure-8赛道飞行，观测为**64x64x3的RGB图像**，更具挑战性（高维、部分可观测）。

**实验环境**：使用Flightmare和Agilicious软件栈进行仿真训练，并在**真实无人机平台**上进行了部署验证。

### 二、 评价指标
论文使用了以下核心指标进行评估：
- **任务奖励**：训练过程中，每个Episode获得的累积奖励，用于衡量**样本效率**和策略性能。
- **单圈时间**：完成赛道一圈所需的时间（秒），衡量策略的**敏捷性与效率**。
- **成功率**：成功完成飞行的比率（%），衡量策略的**鲁棒性**。
- **训练时间**：分为世界模型训练、策略优化、数据收集三个阶段的时间，以及**总训练步长时间**，用于衡量**计算效率**。

### 三、 基线方法对比
论文与以下两类主流方法进行了全面对比：

| 方法类别 | 具体方法 | 说明 |
| :--- | :--- | :--- |
| **基于模型的RL** | **DreamerV3** | 当前SOTA的MBRL方法，使用循环状态空间模型作为世界模型骨干。 |
| | **DreamerV3-P** | DreamerV3的变体，在视觉任务中使用特权信息（解码状态而非图像）。 |
| **无模型RL** | **PPO** | 在状态任务中使用标准PPO。 |
| | **非对称PPO** | 在视觉任务中使用，为Critic提供特权信息以辅助训练。 |

### 四、 关键性能结果与结论

#### 1. 性能与样本效率（不妥协性能）
- **结论**：S5WM在**样本效率和最终任务奖励**上与SOTA的DreamerV3方法**表现相当**，均显著优于无模型的PPO。
- **数据支持**：
    - 在状态和视觉任务中，S5WM与DreamerV3均能收敛到接近最大奖励，而PPO在有限交互预算下表现不佳。
    - 最终**单圈时间和成功率**与DreamerV3持平（见下表）。

**仿真性能对比（关键指标）**
| 任务 | 方法 | 单圈时间 (仿真) | 成功率 (仿真) | 单圈时间 (真实) | 成功率 (真实) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **状态Split-S** | **S5WM** | **4.82s** | **100%** | **4.87s** | **100%** |
| | DreamerV3 | 4.79s | 100% | 4.92s | 100% |
| | PPO | 4.76s | 100% | - | - |
| **视觉Figure-8** | **S5WM** | **3.77s** | **100%** | **3.81s** | **100%** |
| | DreamerV3 | 3.87s | 100% | 3.74s | 100% |
| | PPO (非对称) | 4.62s | 44% | - | - |

#### 2. 核心贡献：训练速度大幅提升
- **结论**：S5WM实现了**显著的训练加速**，且**不牺牲性能**。这是论文最核心的技术贡献。
- **数据支持**：
    - **世界模型训练加速高达10倍**：在状态任务中，通过并行化序列建模，世界模型训练阶段速度提升一个数量级。
    - **整体MBRL训练加速高达4倍**：在状态任务中，总训练步长时间减少至1/4。
    - **视觉任务加速达2倍**：当计算瓶颈部分转移到图像编码时，加速效果依然明显。

**训练时间剖析（状态Split-S任务）**
- **S5WM总步长时间**：显著低于DreamerV3。
- **关键瓶颈突破**：世界模型（WM）训练时间大幅缩短，成为主要加速来源。
- **策略训练（AC）**：由于依赖策略展开，无法并行，因此时间相近。

#### 3. 特权世界模型的有效性
- **结论**：在部分可观测的视觉任务中，使用特权信息（解码低维状态而非图像）训练世界模型，能**提升样本效率**。
- **数据支持**：S5WM和DreamerV3使用特权解码器（-P变体）后，学习曲线上升更快，最终性能更好。这对S5WM尤其有益，因其后验表示仅依赖于当前观测。

#### 4. 仿真到现实的成功迁移
- **结论**：训练出的策略能够**成功迁移到真实世界**，完成高速、敏捷的飞行任务。
- **数据支持**：上表显示，在真实无人机上部署后，S5WM保持了与仿真中相近的单圈时间和100%的成功率，证明了方法的**实际可行性与鲁棒性**。

### 五、 其他重要分析结果
1.  **想象精度**：S5WM的世界模型能够生成**长达50步的准确想象轨迹**，与真实轨迹吻合，证明了其动力学建模的有效性。
2.  **消融实验关键发现**：
    - **想象视野**：`H=10`在样本效率和计算成本间取得了最佳平衡。
    - **奖励平滑课程**：延迟引入平滑奖励项，能先让智能体找到可行解，再优化动作平滑度，有效提升了真实世界部署的可行性。
    - **并行vs循环结构**：循环变体（后验依赖历史）在部分可观测任务中样本效率略高，但丧失了并行加速的优势，凸显了设计权衡。

### 总结
论文通过系统的实验评估，有力地证明了其提出的 **S5WM方法在保持SOTA样本效率和最终性能的同时，实现了高达4-10倍的训练加速**。这一突破通过**在无人机竞速的仿真与真实任务**中得到验证，并深入分析了特权信息、关键超参数等因素的影响，为加速MBRL在复杂机器人任务中的应用提供了切实可行的方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2502.20168v1)
- [HTML 版本](https://arxiv.org/html/2502.20168v1)
