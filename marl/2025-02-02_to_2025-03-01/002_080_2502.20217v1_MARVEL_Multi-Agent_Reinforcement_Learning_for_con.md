# MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View multi-robot Exploration in Large-scale environments

**相关性评分**: 8.0/10

**排名**: #2


---


## 基本信息

- **arXiv ID**: [2502.20217v1](https://arxiv.org/abs/2502.20217v1)
- **发布时间**: 2025-02-27T15:58:42Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Jimmy Chiun, Shizhe Zhang, Yizhuo Wang, Yuhong Cao, Guillaume Sartoretti

## 关键词

Robot control, quadruped robot locomotion, multi-agent RL, CTDE methods, Edge Deployment, mujoco, local observational information

## 一句话总结

MARVEL是一个基于多智能体强化学习的框架，用于具有受限视野的多机器人协作探索大规模环境，通过图注意力网络和信息驱动动作剪枝策略提升协调决策能力。

## 摘要

In multi-robot exploration, a team of mobile robot is tasked with efficiently mapping an unknown environments. While most exploration planners assume omnidirectional sensors like LiDAR, this is impractical for small robots such as drones, where lightweight, directional sensors like cameras may be the only option due to payload constraints. These sensors have a constrained field-of-view (FoV), which adds complexity to the exploration problem, requiring not only optimal robot positioning but also sensor orientation during movement. In this work, we propose MARVEL, a neural framework that leverages graph attention networks, together with novel frontiers and orientation features fusion technique, to develop a collaborative, decentralized policy using multi-agent reinforcement learning (MARL) for robots with constrained FoV. To handle the large action space of viewpoints planning, we further introduce a novel information-driven action pruning strategy. MARVEL improves multi-robot coordination and decision-making in challenging large-scale indoor environments, while adapting to various team sizes and sensor configurations (i.e., FoV and sensor range) without additional training. Our extensive evaluation shows that MARVEL's learned policies exhibit effective coordinated behaviors, outperforming state-of-the-art exploration planners across multiple metrics. We experimentally demonstrate MARVEL's generalizability in large-scale environments, of up to 90m by 90m, and validate its practical applicability through successful deployment on a team of real drone hardware.

## 详细分析

## 论文摘要：MARVEL：面向大规模环境中受限视场多机器人探索的多智能体强化学习

### 1. 研究背景和动机
多机器人探索旨在通过团队协作高效绘制未知环境地图。现有规划器大多假设机器人配备全向传感器（如激光雷达），但对于小型机器人（如无人机），由于有效载荷限制，通常只能搭载轻量级的**受限视场传感器**（如摄像头）。这为探索任务增加了复杂性，不仅需要优化机器人位置，还需规划传感器朝向。传统方法（如前沿法）在长期规划、避免重复探索方面存在不足，而基于强化学习的方法则面临**动作空间巨大、奖励稀疏、信用分配困难**等挑战。本研究旨在开发一个能够有效协调多机器人、适应不同传感器配置的探索框架。

### 2. 核心方法和技术创新
本文提出了 **MARVEL**，一个基于多智能体强化学习的神经框架，其核心创新点包括：
- **基于图注意力网络的策略与评论家网络**：利用堆叠的掩码自注意力层处理环境图，智能融合**前沿分布**和**朝向特征**，增强了模型对大规模局部地图的理解和非近视决策能力。
- **信息驱动的动作剪枝策略**：为应对受限视场带来的巨大动作空间（位置-朝向对），该方法为每个候选位置仅保留信息增益最高的几个朝向，显著降低了决策复杂度。
- **特权评论家网络**：在集中式训练阶段，评论家网络利用全局真实信息（如完整地图）来更好地评估每个智能体的贡献，改善了多智能体协作中的信用分配问题。
- **去中心化执行**：训练后的策略允许每个机器人仅根据局部观察进行独立决策，适应真实世界的异步执行。

### 3. 主要实验结果
在100个未见过的90m×90m大规模室内环境中的评估表明：
- **性能领先**：MARVEL在**轨迹长度**（完成99%探索）和**90%覆盖率所需路径长度**上均优于Nearest、MMPF、NBVP和Learnt-Greedy等先进基线方法，例如在4个智能体时轨迹长度减少14.2%。
- **高成功率与强泛化性**：在所有测试中均达到**100%的成功率**，且结果方差最低，展现了卓越的鲁棒性和泛化能力。
- **卓越的适应性**：无需重新训练，即可适应**不同的团队规模（2、4、8个机器人）**、**不同的传感器视场角（90°、120°、180°）** 和**不同的传感器感知范围（10m、12m、15m）**。
- **实物验证**：成功在Crazyflie无人机集群上进行了部署实验，验证了其在实际硬件上的应用潜力。

### 4. 研究意义和价值
MARVEL为解决**受限视场多机器人协同探索**这一实际问题提供了高效、通用的学习框架。其技术贡献在于通过创新的神经网络架构和训练机制，有效克服了该领域长期存在的动作空间爆炸和协作信用分配难题。该框架展现出的**强大性能、泛化能力和即插即用的适应性**，使其在搜索救援、场景重建等需要轻量级机器人团队进行自主探索的实际应用中具有重要的实用价值。同时，为将学习型方法扩展到更复杂的3D环境探索奠定了基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析：MARVEL

### **一、 论文旨在解决的核心问题**
论文致力于解决**在大型未知环境中，使用受限视场（FoV）传感器的多机器人协同探索问题**。具体挑战包括：
- **传感器限制**：小型机器人（如无人机）通常只能搭载轻量级、方向性传感器（如摄像头），其**视场有限**，这增加了规划复杂度，不仅需要选择机器人位置，还需优化传感器朝向。
- **大规模动作空间**：受限FoV使得**视点规划（位置+朝向）的动作空间变得极其庞大**，传统方法难以高效处理。
- **多智能体协调**：需要多个机器人进行**高效、非短视的协作**，以最小化总探索轨迹长度或时间，避免重复探索。
- **泛化性与适应性**：算法需要能**适应不同的团队规模、传感器配置（FoV、感知范围）和环境拓扑**，而无需重新训练。

### **二、 核心创新点**
MARVEL 提出了一种基于**多智能体强化学习（MARL）的神经框架**，其创新主要体现在以下四个方面：

1.  **基于图注意力网络的策略与评论家架构**
    - **编码器-解码器结构**：使用堆叠的**掩码自注意力层**处理环境图。图中的节点代表可到达位置，边代表无碰撞路径。
    - **创新性特征融合**：
        - **前沿分布特征**：为每个图节点计算一个36维向量，表示其周围不同朝向下的可观测前沿（未知区域边界）数量，通过1D卷积编码后与节点基础特征（相对位置、效用、智能体占用等）**智能融合**。
        - **朝向增强特征**：在解码器中，为每个候选动作（目标节点+朝向）融合了**朝向特定的特征**（如该朝向的FoV覆盖范围、该位置的历史探索朝向），使网络能同时推理位置和朝向的价值。
    - **特权评论家网络**：采用集中式训练（CTDE），评论家网络在训练时可访问全局真实信息（如完整地图），以更好地评估个体贡献和进行**信用分配**，从而学习更优的协作策略。

2.  **信息驱动的动作剪枝策略**
    - 为应对庞大的动作空间（所有相邻节点×所有可能朝向），提出一种**启发式剪枝方法**。
    - 对于每个候选目标节点，仅保留能观测到最多前沿的 **前3个最佳朝向**，形成最终的候选动作对 `(节点, 朝向)`。
    - 若无前沿可观测，则利用A*路径到最近前沿的引导信号来采样朝向。
    - **作用**：大幅缩小动作空间，聚焦于高信息增益的动作，提升学习效率和决策质量。

3.  **针对受限FoV探索的强化学习问题建模与奖励设计**
    - **状态表示**：将环境建模为动态增长的无碰撞图，观察状态包括该图、所有智能体位置以及每个节点的前沿分布。
    - **奖励函数**：精心设计包含四个部分的奖励：
        - `r_o`：基于新视点可观测的前沿数量（个体即时收益）。
        - `r_h`：朝向奖励，鼓励智能体朝向通往最近前沿的路径方向。
        - `r_t`：团队共享奖励，基于团队整体观测到的新前沿数量。
        - `r_f`：任务完成奖励。
    - 该奖励结构**平衡了短期信息增益、长期路径朝向和团队协作**，有效缓解了稀疏奖励问题。

4.  **强大的泛化能力与零样本适应性**
    - 论文强调并验证了MARVEL在**未经额外训练**的情况下，能够：
        - 适应**不同的机器人数量**（2、4、8个）。
        - 适应**不同的传感器FoV**（90°, 120°, 180°）。
        - 适应**不同的传感器感知范围**（10m, 12m, 15m）。
        - 在**大规模（90m×90m）且结构多样的室内环境**中保持高性能和100%的成功率。

### **三、 解决方案的流程总结**
1.  **环境建模**：将探索环境表示为动态构建的无碰撞图 `G=(V, E)`。
2.  **观察构建**：每个智能体获取包含图节点特征、智能体位置和节点前沿分布的局部观察。
3.  **动作生成**：
    - 使用**信息驱动剪枝**从当前节点的邻居中生成一组候选`(节点, 朝向)`动作对。
    - **策略网络**（图注意力网络）接收观察，融合前沿与朝向特征，输出每个候选动作的概率分布。
    - 智能体依概率选择并执行动作（移动至目标节点并调整至目标朝向）。
4.  **训练**：采用**Soft Actor-Critic (SAC)算法**，利用**特权评论家网络**进行集中式训练，以学习协作策略。
5.  **执行**：训练后的策略可**分散式执行**，每个智能体仅凭局部观察做出决策，实现实时协同探索。

### **四、 实际价值与验证**
- **性能优越**：在大量测试环境中，MARVEL在**总轨迹长度、90%覆盖率速度**等关键指标上均优于前沿的基于前沿、采样和学习的基线方法。
- **计算高效**：决策时间稳定在0.2秒以内，适合实时应用。
- **现实可行性**：成功在**真实的Crazyflie无人机集群**上进行了部署验证，证明了其处理实际机器人动力学和噪声的能力。
- **开源**：代码和模型已公开，促进了相关领域的研究和应用。

**结论**：MARVEL通过**创新的图神经网络架构、特征融合技术、动作剪枝策略和强化学习框架**，系统地解决了受限FoV下的大规模多机器人协同探索难题，并在性能、泛化性和实际部署能力方面展现了显著优势。其工作为资源受限平台（如无人机）的高效自主探索提供了强有力的解决方案。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文旨在解决**搭载受限视场（FoV）传感器（如相机）的多机器人系统在大规模未知环境中进行高效协同探索**的难题。该问题不仅需要规划机器人的位置，还需优化其传感器朝向，且面临巨大的动作空间和稀疏奖励等挑战。为此，论文提出了 **MARVEL** 框架，其核心是**基于图注意力网络（GAT）和前沿与朝向特征智能融合的分散式策略**，并采用**集中训练分散执行（CTDE）的多智能体强化学习（MARL）范式**进行训练。为了应对庞大的视点规划动作空间，论文还创新性地引入了**基于信息增益的动作剪枝策略**。实验结果表明，MARVEL 在多种指标上**显著优于现有的前沿探索规划器**，能够适应不同的团队规模和传感器配置而无需重新训练，并在真实无人机硬件上成功验证了其实际部署的可行性和有效性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View multi-robot Exploration in Large-scale environments》针对**受限视场角（FoV）多机器人协同探索**这一特定且具有挑战性的问题，提出了一套完整的神经框架。其核心创新点明确且具有递进性，具体如下：

---

### 1. **针对受限FoV的专用多智能体强化学习（MARL）框架**
- **改进/不同之处**：以往的多机器人探索研究大多假设机器人搭载全向传感器（如LiDAR），或仅在简单的小规模仿真环境中测试。本文首次系统性地将MARL应用于**大规模室内环境**中，且专门针对**轻量级、方向性传感器（如相机）的受限FoV特性**进行算法设计。
- **解决的问题/优势**：解决了小型机器人（如无人机）因有效载荷限制只能使用相机等受限FoV传感器时，探索效率低下的核心问题。该框架使机器人不仅需要规划位置，还需规划传感器朝向，从而适应了真实世界小型机器人的硬件约束。

### 2. **基于图注意力网络（GAT）与前沿-朝向特征智能融合的编码器-解码器架构**
- **改进/不同之处**：
    - **编码器**：使用堆叠的**掩码自注意力层**处理环境图。与普通GAT或Transformer不同，其注意力被限制在图的邻接矩阵内，使节点特征能通过多层网络逐步聚合非邻接节点的信息，更高效地学习大规模环境的空间结构。
    - **特征融合**：创新性地将**节点属性特征**与**前沿分布特征**进行融合。前沿分布通过在每个节点采样36个均匀朝向的FoV并记录可观测前沿数量来获得，为网络提供了丰富的局部环境信息。
    - **解码器**：引入了**朝向增强特征**。除了选择下一个路径点，还为每个候选路径点计算了两种朝向特征：1) 以该朝向为中心的FoV覆盖范围；2) 该点已被探索过的朝向历史。这些特征与路径点特征拼接，使策略网络能同时输出最优的**路径点-朝向对**。
- **解决的问题/优势**：
    - 解决了传统前沿方法或简单RL策略在受限FoV下**空间短视**的问题，使智能体能够基于对大规模局部地图的理解做出**非短视的决策**。
    - 显式地建模朝向信息，直接优化了受限FoV传感器的观测质量，避免了因朝向不佳导致前沿破碎、需要重复访问的问题。

### 3. **信息驱动的动作剪枝策略**
- **改进/不同之处**：在庞大的动作空间（所有相邻节点×所有可能朝向）中，不是进行全量评估，而是为每个相邻节点预先选择**信息增益最高的前3个朝向**，形成候选动作对。若无前沿可观测，则使用A*路径信息来采样朝向。
- **解决的问题/优势**：
    - 直接解决了**受限FoV导致动作空间爆炸**的挑战，将连续/高维的朝向决策空间离散化为少量高价值选项。
    - 大幅降低了策略网络的输出维度和学习难度，同时保证了动作的**高信息潜力**，提高了学习效率和最终策略的质量。

### 4. **具有特权信息的注意力评论家网络**
- **改进/不同之处**：采用集中式训练分散式执行（CTDE）范式。评论家网络在训练时可以访问**全局真实信息**（如完整地图），并采用注意力机制，将其他智能体的动作作为键值对，来增强当前智能体节点的特征表示。
- **解决的问题/优势**：
    - 解决了多智能体协同中的**信用分配难题**。评论家能更准确地评估单个智能体动作对团队全局收益的贡献，从而引导策略网络学习更有效的协作行为。
    - **特权学习**提升了训练的稳定性和策略的协作性能。

### 5. **强大的泛化性与适应性（无需重新训练）**
- **改进/不同之处**：论文通过大量实验证明，在固定传感器配置（120° FoV, 10m范围）下训练的单一模型，无需任何微调或重新训练，即可直接适应：
    - **不同的机器人团队规模**（2、4、8台）。
    - **不同的传感器FoV**（90°、180°）。
    - **不同的传感器范围**（12m、15m）。
    - **从仿真到真实无人机平台的迁移**。
- **解决的问题/优势**：
    - 解决了学习型方法通常泛化能力差、需要为每种配置重新训练的问题。
    - 证明了MARVEL学习到的是**通用、鲁棒的探索与协作策略**，而非过拟合到特定设置，这大大提升了其实用价值和部署便利性。

### 6. **面向大规模环境的系统性评估**
- **改进/不同之处**：与许多先前工作仅在少数（<10）场景中测试不同，本文在**5663张地图上训练**，并在**100张未见过的、大规模（90m×90m）地图**上进行全面评估，报告了轨迹长度、90%覆盖率耗时、重叠率、成功率等多个指标的平均值和方差。
- **解决的问题/优势**：
    - 提供了令人信服的、统计意义显著的性能证据，证明了算法在**复杂、大规模拓扑环境**中的有效性和稳定性。
    - 严格的评估标准凸显了算法相对于传统方法（如最近前沿法、势场法、采样法）在**效率、成功率和鲁棒性**上的全面优势。

---

## 总结
MARVEL的核心贡献在于**将一个具有实际硬件约束（受限FoV）的复杂多机器人协同问题，成功地转化为一个可学习、可泛化、且性能优越的MARL框架**。其创新不是单一的“银弹”，而是一套环环相扣的技术组合：**图注意力架构**处理空间关系，**前沿-朝向融合**提供精准感知，**动作剪枝**管理复杂度，**特权评论家**优化协作，共同赋予了算法**强大的决策能力、卓越的泛化性和切实的实用价值**，为小型无人机集群在未知大场景中的自主探索提供了有力的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 实验与评估效果总结

论文通过广泛的仿真和实物实验，全面评估了MARVEL框架的性能，证明了其在**大规模、受限视场（FoV）多机器人探索任务**中的优越性。

### 一、 使用的数据集与评价指标

#### 数据集
- **训练集**：5,663个随机生成的大规模2D室内环境地图。
- **测试集**：100个未见过的、独立生成的环境地图，用于公平评估泛化能力。
- **环境规模**：所有地图均为 **90m × 90m** 的大规模室内场景，远超许多现有研究的小规模测试环境。
- **实物实验场地**：在一个 **4m × 4m** 的真实飞行场地中使用Crazyflie 2.1无人机进行验证。

#### 核心评价指标
论文采用了四个关键指标进行综合评估：
1.  **轨迹长度**：完成整个探索任务（达到99%覆盖率）时，所有机器人**总轨迹长度的最大值**。**越低越好**。
2.  **90%覆盖率轨迹长度**：探索到环境90%区域时所需的轨迹长度。衡量**前期探索效率**。**越低越好**。
3.  **重叠率**：每个时间步中，多个机器人传感区域重叠部分的比率。衡量**协作效率与冗余度**。**越低越好**（但需结合探索效率综合判断）。
4.  **成功率**：在最大步数（128步）内成功完成探索（达到99%覆盖率）的测试场景百分比。**越高越好**。

### 二、 对比的基线方法
论文与四种代表性的多机器人探索规划器进行了对比：
1.  **Nearest Frontier**：经典前沿法，选择最近的前沿作为目标。
2.  **MMPF**：基于人工势场的方法，使用聚类后的前沿产生吸引力，机器人间产生排斥力。
3.  **NBVP**：基于采样的“下一最佳视点”规划器，在随机采样的路径中选择信息增益最高的。
4.  **Learnt-Greedy**：一种基于图注意力网络的深度强化学习方法（作者团队前期工作），但采用**贪婪策略**选择朝向（即选择当前可见前沿最多的方向）。

### 三、 关键性能提升与结论

根据论文中的表I、II、III及分析，MARVEL在以下方面表现出显著优势：

#### 1. 整体探索效率最高
- **在标准配置（4智能体，120° FoV）下**，MARVEL的**总轨迹长度**比表现最好的基线（NBVP）**减少了14.2%**。
- **在90%覆盖率指标上**，MARVEL同样领先，意味着它能更快地覆盖大部分未知区域。
- **成功率高达100%**，在所有100个测试场景中均能成功完成探索，而部分基线方法（如MMPF）在2智能体时成功率仅为45%，显示了MARVEL出色的**鲁棒性和泛化能力**。

#### 2. 卓越的协调与泛化能力
- **适应不同团队规模**：在2、4、8个智能体的测试中，MARVEL在轨迹长度和成功率上均全面领先或并列最优。
- **适应不同传感器配置**：
    - **不同FoV（90°, 180°）**：无需重新训练，MARVEL在两种FoV下的轨迹长度均显著优于所有基线（表II）。
    - **不同传感范围（12m, 15m）**：同样无需重新训练，在比训练时（10m）更大的传感范围下，性能进一步提升（轨迹更短），展示了框架对额外感知信息的有效利用能力（表III）。

#### 3. 计算效率与决策质量
- MARVEL的每次决策时间稳定在 **0.2秒以下**，适合实时应用。
- 与**Learnt-Greedy**的对比尤为重要：虽然两者使用相似的网络结构，但MARVEL引入了**朝向特征融合**和**信息驱动的动作剪枝**。结果表明，Learnt-Greedy在探索后期（处理零散前沿时）效率下降，导致总轨迹更长。这证明了MARVEL的**非短视决策能力**更强，能更好地进行长期规划。

#### 4. 实际部署可行性
- 在实物无人机实验（图4）中，MARVEL成功部署并适应了真实硬件的运动动力学，验证了其**从仿真到现实的迁移潜力**。

### 结论
论文给出了**明确且强有力的定量结果**。实验表明，MARVEL不仅在**探索效率（轨迹长度）和鲁棒性（成功率）** 上全面超越了现有的前沿方法、采样方法和早期学习型方法，还展现了**强大的零样本泛化能力**，能够适应不同的机器人数量、传感器视场和感知范围，无需额外训练。这使其成为面向实际应用（尤其是搭载轻量级相机的小型无人机集群）的、极具前景的大规模协同探索解决方案。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2502.20217v1)
- [HTML 版本](https://arxiv.org/html/2502.20217v1)
