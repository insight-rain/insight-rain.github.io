# SMART: Self-supervised Multi-task pretrAining with contRol Transformers

**相关性评分**: 6.0/10

**排名**: #21


---


## 基本信息

- **arXiv ID**: [2301.09816v1](https://arxiv.org/abs/2301.09816v1)
- **发布时间**: 2023-01-24T05:01:23Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Yanchao Sun, Shuang Ma, Ratnesh Madaan, Rogerio Bonatti, Furong Huang, Ashish Kapoor

## 关键词

reinforcement learning (RL), multi-agent RL, CTDE methods, Edge Deployment, humanoid robot, robot dog, local observational information

## 一句话总结

这篇论文提出了一种基于控制变换器的自监督多任务预训练框架SMART，旨在提升序列决策任务（包括强化学习）的学习效率和跨任务泛化能力，但与四足机器人、人形机器人等具体机器人类型和边缘部署的直接相关性较弱。

## 摘要

Self-supervised pretraining has been extensively studied in language and vision domains, where a unified model can be easily adapted to various downstream tasks by pretraining representations without explicit labels. When it comes to sequential decision-making tasks, however, it is difficult to properly design such a pretraining approach that can cope with both high-dimensional perceptual information and the complexity of sequential control over long interaction horizons. The challenge becomes combinatorially more complex if we want to pretrain representations amenable to a large variety of tasks. To tackle this problem, in this work, we formulate a general pretraining-finetuning pipeline for sequential decision making, under which we propose a generic pretraining framework \textit{Self-supervised Multi-task pretrAining with contRol Transformer (SMART)}. By systematically investigating pretraining regimes, we carefully design a Control Transformer (CT) coupled with a novel control-centric pretraining objective in a self-supervised manner. SMART encourages the representation to capture the common essential information relevant to short-term control and long-term control, which is transferrable across tasks. We show by extensive experiments in DeepMind Control Suite that SMART significantly improves the learning efficiency among seen and unseen downstream tasks and domains under different learning scenarios including Imitation Learning (IL) and Reinforcement Learning (RL). Benefiting from the proposed control-centric objective, SMART is resilient to distribution shift between pretraining and finetuning, and even works well with low-quality pretraining datasets that are randomly collected.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.09816v1)
- [HTML 版本](https://arxiv.org/html/2301.09816v1)
