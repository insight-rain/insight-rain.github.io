# Emergent collective intelligence from massive-agent cooperation and competition

**相关性评分**: 7.0/10

**排名**: #6


---


## 基本信息

- **arXiv ID**: [2301.01609v2](https://arxiv.org/abs/2301.01609v2)
- **发布时间**: 2023-01-04T13:23:12Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Hanmo Chen, Stone Tao, Jiaxin Chen, Weihan Shen, Xihui Li, Chenghui Yu, Sikai Cheng, Xiaolong Zhu, Xiu Li

## 关键词

reinforcement learning (RL), multi-agent RL, massive-agent cooperation and competition, collective intelligence, self-play, curriculum learning, pixel-to-pixel policy network

## 一句话总结

该论文通过大规模多智能体强化学习研究人工集体智能的涌现，涉及合作与竞争环境，但与四足机器人、人形机器人等具体机器人控制应用关联较弱。

## 摘要

Inspired by organisms evolving through cooperation and competition between different populations on Earth, we study the emergence of artificial collective intelligence through massive-agent reinforcement learning. To this end, We propose a new massive-agent reinforcement learning environment, Lux, where dynamic and massive agents in two teams scramble for limited resources and fight off the darkness. In Lux, we build our agents through the standard reinforcement learning algorithm in curriculum learning phases and leverage centralized control via a pixel-to-pixel policy network. As agents co-evolve through self-play, we observe several stages of intelligence, from the acquisition of atomic skills to the development of group strategies. Since these learned group strategies arise from individual decisions without an explicit coordination mechanism, we claim that artificial collective intelligence emerges from massive-agent cooperation and competition. We further analyze the emergence of various learned strategies through metrics and ablation studies, aiming to provide insights for reinforcement learning implementations in massive-agent environments.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.01609v2)
- [HTML 版本](https://arxiv.org/html/2301.01609v2)
