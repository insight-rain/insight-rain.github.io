# Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints

**相关性评分**: 6.0/10

**排名**: #20


---


## 基本信息

- **arXiv ID**: [2301.10923v2](https://arxiv.org/abs/2301.10923v2)
- **发布时间**: 2023-01-26T04:05:40Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Dohyeong Kim, Kyungjae Lee, Songhwai Oh

## 关键词

reinforcement learning (RL), multi-agent RL, CTDE methods, Edge Deployment, humanoid robot, robot dog, local observational information

## 一句话总结

这篇论文提出了一种基于信任区域的安全分布强化学习算法（SDAC），用于处理机器人任务中的多重约束，但未明确涉及四足机器人、多智能体RL、CTDE方法、边缘部署、人形机器人、机器狗或局部观测信息。

## 摘要

In safety-critical robotic tasks, potential failures must be reduced, and multiple constraints must be met, such as avoiding collisions, limiting energy consumption, and maintaining balance. Thus, applying safe reinforcement learning (RL) in such robotic tasks requires to handle multiple constraints and use risk-averse constraints rather than risk-neutral constraints. To this end, we propose a trust region-based safe RL algorithm for multiple constraints called a safe distributional actor-critic (SDAC). Our main contributions are as follows: 1) introducing a gradient integration method to manage infeasibility issues in multi-constrained problems, ensuring theoretical convergence, and 2) developing a TD($λ$) target distribution to estimate risk-averse constraints with low biases. We evaluate SDAC through extensive experiments involving multi- and single-constrained robotic tasks. While maintaining high scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in multi-constrained tasks and 1.78 times fewer constraint violations in single-constrained tasks compared to safe RL baselines. Code is available at: https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.10923v2)
- [HTML 版本](https://arxiv.org/html/2301.10923v2)
