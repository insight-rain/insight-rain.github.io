# Human-Timescale Adaptation in an Open-Ended Task Space

**相关性评分**: 7.0/10

**排名**: #4


---


## 基本信息

- **arXiv ID**: [2301.07608v1](https://arxiv.org/abs/2301.07608v1)
- **发布时间**: 2023-01-18T15:39:21Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreya Pathak, Nicolas Perez-Nieves, Nemanja Rakicevic, Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, Lei Zhang

## 关键词

reinforcement learning (RL), meta-reinforcement learning, adaptation, open-ended task space, scaling laws, attention-based memory architecture, automated curriculum

## 一句话总结

这篇论文提出了一种基于大规模元强化学习的自适应智能体，能够在开放式的3D环境中快速适应新任务，类似于人类的学习速度。

## 摘要

Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.07608v1)
- [HTML 版本](https://arxiv.org/html/2301.07608v1)
