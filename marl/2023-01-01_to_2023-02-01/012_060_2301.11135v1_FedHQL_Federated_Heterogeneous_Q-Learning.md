# FedHQL: Federated Heterogeneous Q-Learning

**相关性评分**: 6.0/10

**排名**: #12


---


## 基本信息

- **arXiv ID**: [2301.11135v1](https://arxiv.org/abs/2301.11135v1)
- **发布时间**: 2023-01-26T14:39:34Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Flint Xiaofeng Fan, Yining Ma, Zhongxiang Dai, Cheston Tan, Bryan Kian Hsiang Low, Roger Wattenhofer

## 关键词

reinforcement learning (RL), multi-agent RL, CTDE methods

## 一句话总结

FedHQL是一种联邦强化学习算法，针对异构代理（如不同策略参数化）设计，通过分布式学习提升样本效率，但与机器人控制、四足机器人运动等具体应用无直接关联。

## 摘要

Federated Reinforcement Learning (FedRL) encourages distributed agents to learn collectively from each other's experience to improve their performance without exchanging their raw trajectories. The existing work on FedRL assumes that all participating agents are homogeneous, which requires all agents to share the same policy parameterization (e.g., network architectures and training configurations). However, in real-world applications, agents are often in disagreement about the architecture and the parameters, possibly also because of disparate computational budgets. Because homogeneity is not given in practice, we introduce the problem setting of Federated Reinforcement Learning with Heterogeneous And bLack-box agEnts (FedRL-HALE). We present the unique challenges this new setting poses and propose the Federated Heterogeneous Q-Learning (FedHQL) algorithm that principally addresses these challenges. We empirically demonstrate the efficacy of FedHQL in boosting the sample efficiency of heterogeneous agents with distinct policy parameterization using standard RL tasks.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.11135v1)
- [HTML 版本](https://arxiv.org/html/2301.11135v1)
