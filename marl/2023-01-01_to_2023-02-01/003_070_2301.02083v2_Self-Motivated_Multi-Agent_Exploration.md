# Self-Motivated Multi-Agent Exploration

**相关性评分**: 7.0/10

**排名**: #3


---


## 基本信息

- **arXiv ID**: [2301.02083v2](https://arxiv.org/abs/2301.02083v2)
- **发布时间**: 2023-01-05T14:42:39Z
- **相关性评分**: 7.0/10
- **是否相关**: 是

## 作者

Shaowei Zhang, Jiahan Cao, Lei Yuan, Yang Yu, De-Chuan Zhan

## 关键词

reinforcement learning (RL), multi-agent RL, CTDE methods, local observational information

## 一句话总结

该论文提出了一种自激励多智能体探索方法，通过平衡个体探索与团队协作，在合作多智能体强化学习中提升任务性能。

## 摘要

In cooperative multi-agent reinforcement learning (CMARL), it is critical for agents to achieve a balance between self-exploration and team collaboration. However, agents can hardly accomplish the team task without coordination and they would be trapped in a local optimum where easy cooperation is accessed without enough individual exploration. Recent works mainly concentrate on agents' coordinated exploration, which brings about the exponentially grown exploration of the state space. To address this issue, we propose Self-Motivated Multi-Agent Exploration (SMMAE), which aims to achieve success in team tasks by adaptively finding a trade-off between self-exploration and team cooperation. In SMMAE, we train an independent exploration policy for each agent to maximize their own visited state space. Each agent learns an adjustable exploration probability based on the stability of the joint team policy. The experiments on highly cooperative tasks in StarCraft II micromanagement benchmark (SMAC) demonstrate that SMMAE can explore task-related states more efficiently, accomplish coordinated behaviours and boost the learning performance.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.02083v2)
- [HTML 版本](https://arxiv.org/html/2301.02083v2)
