# Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State

**相关性评分**: 6.0/10

**排名**: #23


---


## 基本信息

- **arXiv ID**: [2301.06889v2](https://arxiv.org/abs/2301.06889v2)
- **发布时间**: 2023-01-13T18:55:58Z
- **相关性评分**: 6.0/10
- **是否相关**: 是

## 作者

Washim Uddin Mondal, Vaneet Aggarwal, Satish V. Ukkusuri

## 关键词

reinforcement learning (RL), multi-agent RL, CTDE methods

## 一句话总结

该论文提出了一种基于平均场控制的近似方法，用于处理具有不可分解共享全局状态的多智能体强化学习问题，并分析了近似误差和算法复杂度。

## 摘要

Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\mathcal{O}(e)$ where $e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$. The size of the agent population is denoted by the term $N$, and $|\mathcal{X}|, |\mathcal{U}|$ respectively indicate the sizes of (local) state and action spaces of individual agents. The approximation error is found to be independent of the size of the shared global state space. We further demonstrate that in a special case if the reward and state transition functions are independent of the action distribution of the population, then the error can be improved to $e=\frac{\sqrt{|\mathcal{X}|}}{\sqrt{N}}$. Finally, we devise a Natural Policy Gradient based algorithm that solves the MFC problem with $\mathcal{O}(ε^{-3})$ sample complexity and obtains a policy that is within $\mathcal{O}(\max\{e,ε\})$ error of the optimal MARL policy for any $ε>0$.

## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2301.06889v2)
- [HTML 版本](https://arxiv.org/html/2301.06889v2)
