# PALo: Learning Posture-Aware Locomotion for Quadruped Robots

**相关性评分**: 8.0/10

**排名**: #5


---


## 基本信息

- **arXiv ID**: [2503.04462v1](https://arxiv.org/abs/2503.04462v1)
- **发布时间**: 2025-03-06T14:13:59Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Xiangyu Miao, Jun Sun, Hang Lai, Xinpeng Di, Jiahang Cao, Yong Yu, Weinan Zhang

## 关键词

Robot control, quadruped robot locomotion, multi-agent RL, CTDE methods, Edge Deployment, mujoco, local observational information

## 一句话总结

PALo提出一种基于深度强化学习的端到端框架，用于四足机器人在复杂地形上的姿态感知运动控制，通过非对称演员-评论家架构和定制化训练课程实现敏捷与鲁棒性的平衡，并成功迁移到现实世界。

## 摘要

With the rapid development of embodied intelligence, locomotion control of quadruped robots on complex terrains has become a research hotspot. Unlike traditional locomotion control approaches focusing solely on velocity tracking, we pursue to balance the agility and robustness of quadruped robots on diverse and complex terrains. To this end, we propose an end-to-end deep reinforcement learning framework for posture-aware locomotion named PALo, which manages to handle simultaneous linear and angular velocity tracking and real-time adjustments of body height, pitch, and roll angles. In PALo, the locomotion control problem is formulated as a partially observable Markov decision process, and an asymmetric actor-critic architecture is adopted to overcome the sim-to-real challenge. Further, by incorporating customized training curricula, PALo achieves agile posture-aware locomotion control in simulated environments and successfully transfers to real-world settings without fine-tuning, allowing real-time control of the quadruped robot's locomotion and body posture across challenging terrains. Through in-depth experimental analysis, we identify the key components of PALo that contribute to its performance, further validating the effectiveness of the proposed method. The results of this study provide new possibilities for the low-level locomotion control of quadruped robots in higher dimensional command spaces and lay the foundation for future research on upper-level modules for embodied intelligence.

## 详细分析

## PALo：四足机器人姿态感知运动学习论文摘要

### 1. 研究背景和动机
随着具身智能的快速发展，四足机器人在复杂地形上的运动控制成为研究热点。传统方法通常仅关注速度跟踪，而许多实际任务（如运送物品、通过狭窄空间）要求对机器人身体姿态（高度、俯仰角、横滚角）进行精确控制。现有工作大多忽视了显式的姿态控制，或为了稳定性牺牲了敏捷性。因此，本文旨在平衡四足机器人的敏捷性与鲁棒性，提出一种能够同时跟踪速度与姿态的端到端控制框架。

### 2. 核心方法和技术创新
本文提出了 **PALo**，一个基于深度强化学习的姿态感知运动控制框架。其核心技术创新包括：
- **高维命令空间**：将控制命令从传统的3维（线速度、角速度）扩展为**6维**，新增了身体高度、俯仰角和横滚角的实时跟踪。
- **非对称演员-评论家架构**：演员网络仅接收本体感知信息，而评论家网络利用仿真中的特权信息（如地面高度、摩擦力）提供更丰富的学习信号，提升了数据效率和策略鲁棒性。
- **定制化训练课程**：综合了**地形课程**（从易到难）、**奖励课程**（从速度跟踪逐步引入姿态跟踪）和**命令课程**（动态调整命令采样范围），稳定了训练过程。
- **对抗运动先验**：采用AMP技术，利用从平坦地形策略收集的专家数据生成风格奖励，引导策略学习自然、稳定的步态，特别是在复杂地形上减少了身体碰撞。
- **领域随机化**：在仿真训练中随机化动力学参数（如摩擦、质量、PD增益），以缩小**仿真到现实的差距**，实现无需微调的直接部署。

### 3. 主要实验结果
- **仿真实验**：在Isaac Gym中训练的策略能有效跟踪动态变化的6D命令，在平坦、波浪、斜坡、楼梯等多种地形上均表现出鲁棒的运动和姿态控制能力。
- **实物实验**：策略**无需微调**直接部署在Unitree A1机器人上，在草地、橡胶垫、沙地、上下坡、上下楼梯等9种真实结构化与非结构化地形上成功导航，成功率接近100%，并实现了对俯仰、横滚角命令的实时精准跟踪。
- **消融实验**：验证了AMP对复杂地形通过能力的关键提升作用、奖励课程的有效性，以及简单的MLP历史编码器（5步观测）足以胜任此项任务。

### 4. 研究意义和价值
PALo的研究意义与价值主要体现在：
- **技术贡献**：首次在端到端强化学习框架中实现了对四足机器人6维（速度+姿态）命令的同步、敏捷、鲁棒跟踪，显著提升了机器人的运动可控性和环境适应性。
- **实践价值**：成功实现了从仿真到现实的无缝迁移，证明了该方法在真实复杂场景中部署的可行性，为执行如货物平稳运输、狭小空间探查等需要精细身体控制的任务奠定了基础。
- **前瞻性**：该工作为四足机器人的底层运动控制开辟了更高维命令空间的可能性，并为未来集成高层运动规划、视觉导航乃至大语言模型等上层智能模块提供了可靠的底层支撑。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## PALo论文分析：核心创新、问题与解决方案

### 一、 论文拟解决的核心问题
论文旨在解决四足机器人**在复杂地形上同时实现敏捷性与鲁棒性**的挑战。具体而言，它指出了当前主流研究的两个主要局限：
1.  **控制维度单一**：大多数基于强化学习的运动控制器仅关注**三维速度跟踪**（线速度x, y，角速度z），而忽略了**身体姿态**（高度、俯仰角、横滚角）的主动控制。
2.  **敏捷性与稳定性的权衡**：许多工作为了稳定性而惩罚非标准身体姿态，牺牲了机器人在执行复杂任务（如穿越狭小空间、运输物品时保持平衡）时所需的**姿态灵活性**。

### 二、 核心创新点
论文的核心创新在于提出了一个**端到端的姿态感知运动学习框架**，其创新性主要体现在以下三个层面：

#### 1. **控制范式创新：从3D速度到6D姿态感知命令**
- **扩展命令空间**：将控制命令从传统的3维（速度）扩展为**6维**，新增了**身体高度、俯仰角、横滚角**的跟踪目标。这使得机器人能根据任务需求主动调整姿态，而不仅仅是移动。
- **实际价值**：使机器人能够执行更复杂的任务，例如：
    - **匍匐穿越**低矮空间（控制高度和俯仰）。
    - **倾斜行走**以在斜坡上保持身体水平（控制横滚和俯仰）。
    - **平稳运输**物品（通过姿态调整抵消运动扰动）。

#### 2. **方法框架创新：集成多种先进技术的训练方案**
- **非对称演员-评论家架构**：
    - **演员网络**：仅接收**本体感知信息**（关节状态、IMU等）和6D命令，适用于真实部署。
    - **评论家网络**：在训练时接收**特权信息**（地面高度、摩擦系数、接触力等），提供更优的学习信号。
    - **优势**：克服了部分可观性挑战，提升了数据效率和策略性能，避免了传统的“教师-学生”蒸馏范式效率低下的问题。

- **定制化课程学习**：
    - **地形课程**：从简单地形（平坦）逐步过渡到复杂地形（波浪、斜坡、楼梯、离散障碍）。
    - **奖励课程**：分阶段引入奖励项。早期侧重速度跟踪，中期逐步加入姿态跟踪，后期整合全部6D命令跟踪。
    - **命令课程**：动态调整命令的采样范围和难度（如爬楼梯时针对性采样俯仰角命令）。
    - **优势**：稳定了训练过程，使策略能逐步掌握高维、复杂的控制任务。

- **对抗运动先验的应用与生成**：
    - **应用AMP**：使用AMP风格奖励替代复杂的手工设计辅助奖励项，引导策略学习出**自然、稳定、节能**的步态。
    - **数据生成创新**：AMP的专家数据并非来自动物动捕，而是**从机器人自身在平坦地形上训练出的稳定策略中收集**。这些数据随后用于在复杂地形训练中引导策略，实现了**从简单到复杂的知识迁移**。

#### 3. **系统实现创新：有效的仿真到现实迁移**
- **领域随机化**：对机器人动力学（质量、摩擦、PD增益等）、动作延迟、传感器噪声进行大规模随机化。
- **无微调直接部署**：策略完全在仿真中训练（使用Isaac Gym），然后**直接部署到真实的Unitree A1机器人上，无需任何真实世界微调**。
- **实际验证**：在多种结构化与非结构化地形（草地、沙地、斜坡、楼梯等）上成功验证了6D命令跟踪和运动能力，证明了其有效**跨越了仿真到现实的鸿沟**。

### 三、 解决方案总结
论文通过一个名为 **PALo** 的集成框架解决了上述问题，其解决方案路径清晰：

1.  **问题建模**：将姿态感知运动控制建模为一个**部分可观马尔可夫决策过程**。
2.  **算法设计**：
    - 采用**PPO**作为基础RL算法。
    - 设计包含**任务奖励**（6D跟踪）、**风格奖励**（AMP）、**正则化奖励**的复合奖励函数。
    - 构建**非对称演员-评论家网络**处理信息不对称。
3.  **训练策略**：
    - 采用**三管齐下的课程学习**（地形、奖励、命令）引导训练。
    - 运用**领域随机化**提升策略的鲁棒性和泛化能力。
    - 创新性地**生成并利用AMP专家数据**促进复杂地形下的稳定运动。
4.  **评估与验证**：
    - 在仿真中验证6D命令的动态跟踪能力。
    - **在真实世界中零样本部署**，测试其在多种挑战性地形上的运动与姿态控制性能，并通过消融实验验证了AMP、课程学习等关键组件的有效性。

**结论**：PALo的核心创新在于**首次将高维身体姿态控制与速度控制无缝集成到一个端到端的强化学习框架中**，并通过一系列精心设计的方法（非对称架构、定制课程、AMP）成功实现了在复杂地形上**既敏捷又鲁棒**、且能**直接仿真到现实**的姿态感知运动。这为四足机器人执行更复杂、更贴近实际需求的任务奠定了重要的底层控制基础。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对四足机器人传统运动控制方法通常只关注速度跟踪、缺乏对机身姿态（高度、俯仰角、横滚角）的显式控制，从而限制了其在复杂任务（如运输物品、穿越狭小空间）中的适应性问题，提出了一种名为 **PALo** 的端到端深度强化学习框架。该框架的核心创新在于将运动控制问题扩展为对**6维命令**（线性速度、角速度、身体高度、俯仰角、横滚角）的同步跟踪，并采用**非对称的演员-评论家架构**结合**定制化的课程学习**（包括地形、奖励和命令课程）进行训练。此外，论文利用**对抗运动先验**来引导策略学习自然、稳定的步态。最终，该方法在仿真中训练的策略能够**无需微调直接迁移到真实世界的Unitree A1机器人上**，在多种结构化与非结构化地形上实现了敏捷、鲁棒的姿态感知运动，显著提升了四足机器人在高维命令空间下的运动可控性和环境适应性。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## PALo论文创新点分析

这篇论文提出了一种用于四足机器人姿态感知运动控制的端到端深度强化学习框架PALo。相对于已有工作，其明确的创新点如下：

---

### 1. **从3D速度跟踪扩展到6D姿态感知控制**
- **改进/不同之处**：传统的大多数基于强化学习的四足运动控制方法仅关注3维速度命令（x, y线速度，z轴角速度）的跟踪。PALo将控制维度扩展为**6维命令**，在速度命令基础上，**新增了对机器人身体高度、俯仰角（pitch）和横滚角（roll）的实时跟踪控制**。
- **解决的问题/优势**：
    - **解决了任务适应性不足的问题**：许多现实任务（如运输物体、通过狭窄空间、工业检测）不仅需要移动到目标点，还需要精确控制身体姿态以保持稳定、避免碰撞或获得更好视野。纯速度控制无法满足这些需求。
    - **增强了机器人的整体可控性和敏捷性**：机器人可以在运动过程中主动调整姿态，以应对复杂地形或任务约束，而不是被动地保持一个固定或中立的姿态。

### 2. **提出定制化的分层课程学习策略**
- **改进/不同之处**：论文没有使用单一的课程，而是设计了一个**组合式课程学习框架**，包含：
    1.  **地形课程**：根据智能体的表现，动态调整训练地形的难度（从平坦到波浪、斜坡、楼梯、离散障碍等）。
    2.  **奖励课程**：**分阶段引入奖励项**。早期专注于速度跟踪，中期逐步引入身体高度、俯仰和横滚跟踪的奖励，后期使用完整的组合奖励进行综合训练。
    3.  **命令课程**：动态调整6D命令的采样范围和分布（例如，针对上下楼梯任务，专门调整俯仰角命令的分布）。
- **解决的问题/优势**：
    - **解决了高维命令空间下的训练不稳定和收敛困难问题**。直接在所有复杂地形上训练完整的6D控制策略非常困难。
    - **通过渐进式学习，使策略能够稳健地掌握同时进行运动和姿态调整的复杂技能**，提高了训练成功率和最终策略的性能。

### 3. **采用非对称演员-评论家架构与历史编码器结合**
- **改进/不同之处**：
    - **非对称架构**：**演员网络**仅接收本体感知信息（部分可观），而**评论家网络**在训练时可以访问仿真中的“特权信息”（如地面高度、摩擦系数、接触力等）。这不同于传统的师生蒸馏范式。
    - **历史编码器**：使用一个简单的MLP编码过去5个时间步的观测和动作，为演员网络提供短期记忆，以应对POMDP问题。
- **解决的问题/优势**：
    - **解决了仿真到现实迁移中的数据效率问题**。传统的师生蒸馏方法数据效率低，且学生策略性能常低于教师策略。非对称架构让评论家利用特权信息提供更优的学习信号，从而更高效地训练一个仅依赖真实世界可用信息的演员策略。
    - **使策略能够更好地适应现实世界中的部分可观测性**，提升了在真实部署中的鲁棒性。

### 4. **创新性地应用对抗运动先验于姿态感知运动**
- **改进/不同之处**：PALo使用**AMP来生成风格奖励**，但其专家数据并非来自动物动作捕捉，而是**来自机器人自身在平坦地形上预先训练好的策略所生成的运动数据**。随后，这些“稳定步态”数据被用于在复杂地形上训练时提供风格引导。
- **解决的问题/优势**：
    - **解决了复杂地形上奖励函数设计难题**。手动设计奖励函数以在复杂地形上同时保持自然步态和精确的姿态跟踪极其困难。
    - **AMP提供了一种隐式的、数据驱动的风格约束**，鼓励策略在复杂地形上（如离散障碍、楼梯）产生稳定、自然的运动，**显著减少了身体碰撞和任务失败**（消融实验证明，在复杂地形上，使用AMP的策略成功率远高于未使用的）。

### 5. **实现了无需微调的高质量仿真到现实迁移**
- **改进/不同之处**：论文强调，训练完成的策略**直接部署到真实的Unitree A1机器人上，未进行任何额外的现实世界微调或领域自适应**。这通过结合**领域随机化**和上述核心方法实现。
- **解决的问题/优势**：
    - **直接解决了sim-to-real的核心挑战**。通过在仿真中广泛随机化动力学参数（摩擦、质量、PD增益、动作延迟等）和施加随机扰动，策略学会了应对现实世界的不确定性和变化。
    - **降低了部署成本和风险**，避免了在真实机器人上进行耗时且可能损坏机器人的策略微调过程，证明了框架的实用性和鲁棒性。实验在多种结构化与非结构化地形（草地、沙地、斜坡、楼梯等）上取得了高成功率。

---

**总结**：PALo的核心创新在于**将四足机器人的低层运动控制从“速度跟踪”提升到了“姿态感知的全身运动控制”**，并通过一套精心设计的**课程学习、非对称学习架构、AMP应用和领域随机化组合拳**，成功解决了由此带来的高维控制、训练不稳定和仿真到现实迁移的难题，显著增强了机器人在复杂场景下的适应性、敏捷性和任务执行能力。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验效果概述
论文提出的 **PALo** 框架在仿真和真实世界实验中，成功实现了四足机器人在复杂地形上对 **6维控制指令**（线性速度、角速度、身体高度、俯仰角、滚转角）的**敏捷、鲁棒且无需视觉的**姿态感知运动控制。核心效果是：**单一策略**即可泛化到多种地形和动态指令组合，并实现了**从仿真到真实世界的零调优部署**。

### 二、 数据集与评价指标

#### 1. 数据集
- **训练数据**：**无外部数据集**。所有训练均在Isaac Gym仿真器中通过强化学习智能体与环境交互生成。
- **AMP专家数据**：采用**自生成**模式。先在平坦地形训练一个基础策略，然后**采集该策略在平坦地形上的运动数据**，作为后续在复杂地形训练时的对抗运动先验专家数据。
- **测试地形**：
    - **仿真**：平坦地形、波浪地形、粗糙斜坡、上下楼梯、离散障碍地形、平坦粗糙地形。
    - **真实世界**：草地、橡胶跑道、软海绵垫、上坡、下坡、沙地、上行木楼梯、下行木楼梯、台阶穿越。

#### 2. 主要评价指标
论文评估主要分为**定量指标**和**定性/成功率指标**。

- **定量指标**：
    1.  **6D指令跟踪误差**：通过绘制**目标指令**与**实际状态**的曲线图，直观展示跟踪性能（如图4，图6）。具体包括：
        - 线性速度 (`v_x`, `v_y`)
        - 角速度 (`ω_z`)
        - 身体高度 (`h`)
        - 俯仰角 (`θ`)
        - 滚转角 (`φ`)
    2.  **平均任务奖励**：在仿真中，使用公式(4)定义的奖励函数计算智能体在一段测试内的平均奖励，用于对比不同方法或消融实验的性能（如图7，图8，图9）。

- **定性/成功率指标**：
    1.  **地形通过成功率**：在真实世界九种不同地形上各进行5次测试，记录成功穿越的次数。
    2.  **碰撞次数/提前终止率**：在仿真复杂地形测试中，记录因身体碰撞导致训练回合提前终止的次数，用于衡量策略的鲁棒性和安全性。
    3.  **sim-to-real转移成功性**：通过真实机器人成功执行多种地形穿越和动态姿态调整任务来**定性证明**。

### 三、 基线方法对比与性能分析
论文**未设置传统的、与已有SOTA方法的直接定量对比**（如对比RMA、AMP等方法在相同指标上的数值）。作者的解释和评估重点在于**框架内部组件的有效性验证**和**整体任务的实现**。

#### 1. 主要对比方式：消融实验
论文通过系统的消融研究来证明各核心组件的贡献，这实质上是与**简化版PALo**进行对比。

| 消融组件 | 对比设置 | 关键性能结论与提升 |
| :--- | :--- | :--- |
| **对抗运动先验** | 有AMP vs. 无AMP | **在复杂地形上性能提升显著**：<br>- **平坦地形**：影响不大，平均奖励接近。<br>- **离散障碍/楼梯地形**：**无AMP**的策略发生多次碰撞，任务完成率近乎为零；**有AMP**的策略成功完成所有测试，无碰撞终止。说明AMP对于在复杂地形上学习稳定、自然的步态至关重要。 |
| **定制化训练课程** | 使用奖励课程 vs. 不使用 | **显著提升复杂地形性能**：<br>- 在上楼梯等地形中，使用奖励课程（先学速度跟踪，再逐步引入姿态跟踪）能**降低碰撞率，提高移动性**。证明了分阶段学习对处理多任务目标的有效性。 |
| **指令采样策略** | 正态分布采样 vs. 均匀分布采样 | **正态分布采样略优**：<br>- 对于角速度和姿态指令，正态分布采样在方向跟踪上表现稍好。<br>- 均匀分布采样在测试中出现了2次因碰撞导致的提前终止。 |
| **历史编码器架构** | MLP-5 vs. MLP-50 vs. TCN-50 | **简单短期记忆已足够**：<br>- **MLP-5**（基于最近5个时间步的MLP编码器）性能**略优于**更长的历史编码或更复杂的TCN结构。<br>- 结论：对于底层盲运动控制，短期历史信息已足够，简化网络更有效。 |

#### 2. 整体性能结论
- **6D指令跟踪**：仿真实验（图4）显示，在平坦地形上动态跟踪6D指令的误差极小（高度指令在极端姿态冲突时有可理解的小幅退化）。在波浪、斜坡、楼梯等地形上均表现出**一致且鲁棒**的跟踪性能。
- **Sim-to-Real转移**：
    - **成功率**：在真实世界9类地形测试中，除上行木楼梯（5次中失败1次，因碰撞触发安全协议）外，其余地形**成功率100%**。
    - **姿态控制**：真实世界实验（图6）表明，机器人能够快速、稳定地跟踪实时变化的俯仰和滚转角指令，证明了其**卓越的姿态控制能力**和**仿真的有效性**。
- **框架价值**：实验综合表明，**PALo**通过整合**6D指令空间、AMP、不对称演员-评论家架构、以及定制化课程**，成功实现了传统方法（仅速度跟踪）难以完成的**敏捷且鲁棒的姿态感知运动**，并解决了sim-to-real转移的挑战。

### 四、 未提供明确定量对比的原因分析
论文未与其它前沿方法进行数值对比，可能基于以下原因：
1.  **研究重点不同**：本文核心创新点是**扩展控制指令维度**（从3D速度到6D姿态+速度），并实现其端到端学习。许多SOTA方法（如RMA、ANYmal系列工作）主要关注**在3D速度指令下的鲁棒性**或**结合视觉**，直接进行“苹果对苹果”的公平比较较为困难。
2.  **评估范式**：本文采用“**任务完成**”和“**组件消融**”作为主要评估方式，旨在验证**所提框架自身能否实现既定目标**（姿态感知运动）以及**各模块是否有效**，而非在标准基准上争夺排名。
3.  **实用性证明**：作者可能认为，在真实四足机器人（Unitree A1）上**零调优成功部署**并完成多样复杂任务，是比仿真分数更有力的性能证明。

**总结**：论文通过详尽的消融实验和真实的物理部署，强有力地证明了PALo框架在实现四足机器人**高维（6D）姿态感知运动控制**方面的有效性和实用性。其评估虽未提供与传统基线的直接数值对比，但通过**组件有效性分析**和**全面的现实世界演示**，清晰地展示了方法的技术优势和创新价值。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2503.04462v1)
- [HTML 版本](https://arxiv.org/html/2503.04462v1)
