# Runtime Learning of Quadruped Robots in Wild Environments

**相关性评分**: 8.0/10

**排名**: #2


---


## 基本信息

- **arXiv ID**: [2503.04794v2](https://arxiv.org/abs/2503.04794v2)
- **发布时间**: 2025-03-02T02:32:55Z
- **相关性评分**: 8.0/10
- **是否相关**: 是

## 作者

Yihao Cai, Yanbing Mao, Lui Sha, Hongpeng Cao, Marco Caccamo

## 关键词

Robot control, quadruped robot locomotion, multi-agent RL, CTDE methods, Edge Deployment, mujoco, local observational information

## 一句话总结

该论文提出了一种四足机器人在动态野外环境中的运行时学习框架，结合高性能深度强化学习代理和高保证物理模型控制器，实现安全自适应运动。

## 摘要

This paper presents a runtime learning framework for quadruped robots, enabling them to learn and adapt safely in dynamic wild environments. The framework integrates sensing, navigation, and control, forming a closed-loop system for the robot. The core novelty of this framework lies in two interactive and complementary components within the control module: the high-performance (HP)-Student and the high-assurance (HA)-Teacher. HP-Student is a deep reinforcement learning (DRL) agent that engages in self-learning and teaching-to-learn to develop a safe and high-performance action policy. HA-Teacher is a simplified yet verifiable physics-model-based controller, with the role of teaching HP-Student about safety while providing a backup for the robot's safe locomotion. HA-Teacher is innovative due to its real-time physics model, real-time action policy, and real-time control goals, all tailored to respond effectively to real-time wild environments, ensuring safety. The framework also includes a coordinator who effectively manages the interaction between HP-Student and HA-Teacher. Experiments involving a Unitree Go2 robot in Nvidia Isaac Gym and comparisons with state-of-the-art safe DRLs demonstrate the effectiveness of the proposed runtime learning framework.

## 详细分析

## 论文摘要

### 1. 研究背景和动机
四足机器人在野外复杂环境（如森林、灾区）中展现出巨大应用潜力，但其安全导航面临严峻挑战。**深度强化学习** 虽能生成高效的运动策略，但其“黑盒”特性难以提供可验证的安全性保证，尤其是在动态、不可预测的野外环境中，预训练的策略容易因**领域差距**而失效，导致安全风险。现有安全架构（如神经Simplex）依赖静态的、基于物理模型的高保障模块，在环境动态变化时可能因模型失配而失效。因此，如何在保证**实时安全**的前提下，让机器人在运行中持续学习并适应动态环境，成为一个关键开放问题。

### 2. 核心方法和技术创新
本文提出了一个**运行时学习框架**，其核心创新在于控制模块内两个交互式组件：
- **HP-Student**：一个DRL智能体，在安全空间内进行**自我学习**以追求高性能，在安全边界附近则向HA-Teacher**学习安全策略**。
- **HA-Teacher**：一个基于简化物理模型、可验证的控制器，**专司安全保障**。其创新性在于采用**实时物理模型、实时动作策略和实时控制目标**，能根据最新的传感器数据动态更新，有效应对环境变化。
- **协调器**：管理两者切换。当机器人状态从安全学习空间进入边际安全空间时，HA-Teacher接管控制并生成安全经验元组供HP-Student学习，确保其最终能学到**既安全又高性能**的策略。

### 3. 主要实验结果
在Nvidia Isaac Gym中使用Unitree Go2机器人进行仿真实验，环境包含非结构化地形与动态障碍。与先进的安全DRL方法（CLF-DRL、Phy-DRL）及单独HA-Teacher对比表明：
- **安全性**：本文框架在运行时学习过程中实现了**零跌倒**，并能将机器人状态始终维持在安全集内，显著优于对比方法。
- **性能**：经过学习后，HP-Student在导航任务中**成功率更高、旅行时间更短、能耗更低**，平衡了安全与效率。单独HA-Teacher虽安全但性能（速度、能耗）较差。

### 4. 研究意义和价值
本工作为解决**安全关键自主系统在动态环境中的持续学习**这一根本难题提供了新思路。其价值在于：
- **理论价值**：提出了一个融合可验证控制与数据驱动学习的**新型安全学习范式**，通过“教学相长”机制，为DRL提供了可靠的安全边界与学习引导。
- **实际价值**：框架使四足机器人能在**保证实时安全的前提下**，于复杂野外环境中持续适应与优化，提升了其在搜救、勘探等实际任务中的**鲁棒性、自主性和实用性**。开源代码为后续研究提供了重要基础。

## 问答对

### 问题 1

**Q**: 这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？

**A**: ## 论文核心分析

### **一、 论文旨在解决的核心问题**
论文旨在解决**四足机器人在动态、不可预测的野外环境中进行运行时学习（Runtime Learning）时的安全问题**。具体而言，它针对以下两个关键挑战：
1.  **安全挑战**：当基于深度强化学习（DRL）的智能体在运行时学习过程中产生可能导致安全违规（如摔倒、碰撞）的动作时，如何**及时纠正其不安全的学习行为，并保证机器人的实时安全**？
2.  **适应挑战**：如何让机器人的控制系统能够**有效适应动态变化的野外环境**（如从不平坦地面过渡到冰雪路面），确保在这些未知或变化条件下依然保持安全？

### **二、 论文的核心创新点**
论文提出了一个名为 **“运行时学习框架”** 的整体解决方案，其核心创新在于控制模块内两个交互式、互补的组件设计：

1.  **高性能学生（HP-Student）**：
    -   一个基于**深度强化学习（DRL）** 的智能体。
    -   创新在于其**双模式学习机制**：
        -   **自我学习（Self-Learning）**：在安全边界内（自学习空间 `𝕃`）探索高性能策略，以优化任务表现（如导航速度、能效）。
        -   **教学式学习（Teaching-to-Learn）**：当接近安全边界时，从“教师”那里学习安全策略，将安全经验存入回放缓冲区，从而将安全约束内化到其策略中。

2.  **高保障教师（HA-Teacher）**：
    -   一个基于**简化、可验证物理模型**的控制器。
    -   **核心创新在于其“实时性”**，专门为解决动态环境下的模型失配问题而设计：
        -   **实时物理模型**：使用最新的传感器数据（`𝐬(𝑘)`）在线更新模型参数（`𝐀(𝐬(𝑘)), 𝐁(𝐬(𝑘))`），使其更贴合当前环境。
        -   **实时控制目标**：根据当前状态动态计算一个位于安全区域内的“补丁中心”（`𝐞ₖ*`）作为控制目标，引导机器人返回安全区域。
        -   **实时动作策略**：通过求解一组线性矩阵不等式（LMIs，**定理1**）在线计算控制律（`𝐅ₖ`），该策略被**数学证明**能在考虑模型不确定性的情况下，保证在规定的教学时段内将机器人状态从边际安全空间（`𝕊∖𝕃`）驱动回自学习空间（`𝕃`），同时满足所有动作约束。

3.  **协调器（Coordinator）与整体框架**：
    -   创新性地定义了**自学习空间（`𝕃`）** 和**边际安全空间（`𝕊∖𝕃`）**，为安全切换提供了清晰的数学边界。
    -   协调器根据状态是否从 `𝕃` 进入 `𝕊∖𝕃` 来触发 **HA-Teacher 接管控制**，并管理两者间的切换逻辑。
    -   整个框架形成了 **“感知-导航-学习-控制”的闭环系统**，使安全的学习与适应能在机器人运行时在线进行。

### **三、 解决方案的总体思路与方法**
论文通过 **“HA-Teacher 保障安全底线，HP-Student 追求高性能，两者协同实现安全运行时学习”** 的思路解决问题。

**解决路径如下：**

1.  **问题建模**：
    -   用数学集合严格定义了安全集（`𝕊`）、动作集（`𝔸`）和自学习空间（`𝕃`），将安全问题转化为状态和动作的集合约束问题。

2.  **安全机制设计（HA-Teacher）**：
    -   当 HP-Student 的控制使机器人状态触及边际安全边界时，协调器触发切换。
    -   HA-Teacher **立即接管**，利用最新的环境信息更新其模型和目标，并执行其**被证明安全**的动作策略（`𝐚_HA`），确保机器人不违反安全约束（`𝕊` 和 `𝔸`）。
    -   同时，HA-Teacher 产生的（状态，安全动作，下一状态，奖励）**经验元组被记录并注入 HP-Student 的经验回放池**。这就是“教学”过程。

3.  **学习与适应机制（HP-Student）**：
    -   在安全的自学习空间内，HP-Student 通过标准的 DRL（演员-评论家架构）自我优化，最大化包含稳定性、导航效率和能量消耗的复合奖励。
    -   通过从回放池中采样 HA-Teacher 的安全经验进行学习，HP-Student **逐渐学会在接近边界时应该如何行动**，从而将安全知识融入自身策略。
    -   随着学习进行，HP-Student 因不安全行为而触发教师干预的次数会减少，最终学会在复杂环境中**同时保持高性能和高安全性**。

4.  **验证与评估**：
    -   在 NVIDIA Isaac Gym 仿真环境中，使用 Unitree Go2 机器人模型，在包含非结构化地形、移动障碍物的动态野外环境中进行测试。
    -   与最先进的安全 DRL 方法（如 CLF-DRL, Phy-DRL）以及单独使用 HA-Teacher 进行对比。
    -   **实验结果**表明，该框架：
        -   **显著提升了安全性**：在长期学习过程中机器人摔倒次数远少于对比方法。
        -   **实现了安全与性能的平衡**：最终学习到的策略在成功完成导航任务、旅行时间和能量消耗方面均优于单独使用 HA-Teacher，且安全性远超其他纯 DRL 方法。

### **总结**
这篇论文的核心价值在于提出并验证了一个**兼具“可验证安全性”和“数据驱动高性能”** 的运行时学习框架。它通过一个**实时更新的、可证明安全的模型化控制器（HA-Teacher）** 为 **DRL 智能体（HP-Student）** 的学习过程提供了一个**安全“护栏”和“教练”**，从而解决了在开放、动态环境中部署学习型机器人所面临的核心安全难题。其创新点不仅在于两个组件的设计，更在于它们之间基于严格数学定义的交互机制，为安全关键的自主系统学习提供了一个有前景的架构范式。


### 问题 2

**Q**: 请用一段话总结这篇论文，明确说明：论文试图解决的核心问题；提出的主要方法或框架；最终取得的主要效果或结论。要求语言简洁、信息密度高，不要复述摘要原文。

**A**: 这篇论文针对四足机器人在动态野外环境中运行时，深度强化学习（DRL）策略难以保证安全且无法持续适应环境变化的核心问题，提出了一种**运行时学习框架**。该框架的核心创新在于设计了一个由**高性能学生（HP-Student，DRL智能体）** 和**高保障教师（HA-Teacher，可验证的物理模型控制器）** 组成的交互式控制模块，并由一个协调器管理。HP-Student负责通过自我学习和向教师学习来优化性能；HA-Teacher则利用实时更新的物理模型和“实时补丁”，在机器人状态接近安全边界时介入，确保安全并“教导”学生。实验表明，该框架在仿真动态野外环境中，相比现有安全DRL方法，能**在保证机器人零跌倒等安全性的同时，显著提升导航任务的成功率、缩短行进时间并降低能耗**，实现了安全与高性能的兼顾。


### 问题 3

**Q**: 这篇论文相对于已有工作有哪些明确的创新点？请逐条列出，并对每一条说明：相比以往方法改进或不同之处在哪里，以及该创新解决了什么具体问题或带来了什么优势。

**A**: ## 论文创新点分析

这篇论文《Runtime Learning of Quadruped Robots in Wild Environments》针对四足机器人在动态、不可预测的野外环境中安全运行与学习的核心挑战，提出了一套运行时学习框架。其核心创新点在于**将高性能但安全性难以验证的深度强化学习（DRL）与可验证但性能受限的物理模型控制相结合，并设计了一种新颖的交互机制**。具体创新点如下：

---

### 1. **提出了“HP-Student”与“HA-Teacher”互补交互的运行时学习架构**
   - **相比以往方法的改进/不同之处**：
     - **传统DRL方法**：通常采用预训练后零样本部署、微调或域随机化。这些方法在部署后模型是静态的，无法在运行时根据新环境持续、安全地学习和适应。
     - **传统容错DRL/运行时保障架构**：如神经Simplex架构，通常将DRL（高性能模块，HPM）与一个**静态的、基于模型的**高保障模块（HAM）并行运行。HAM作为“安全网”，仅在DRL可能违反安全时接管。但其假设HAM的模型在实时环境中始终有效，这在动态野外环境中不成立。
     - **本文创新**：提出了 **“HP-Student” (高性能学生)** 和 **“HA-Teacher” (高保障教师)** 两个组件。HA-Teacher**不是静态的**，而是能够根据实时传感器数据更新其物理模型、控制目标和动作策略的**动态、自适应**的保障模块。
   - **解决的具体问题/带来的优势**：
     - **解决了模型失配问题**：在动态野外环境中，地面属性、坡度等实时变化会导致预先设计的物理模型失效。HA-Teacher的实时更新能力使其能适应这种变化，确保安全备份始终可靠。
     - **实现了安全的在线学习**：HP-Student可以在安全的边界内（自学习空间 `𝕃`）进行自我探索以追求高性能。一旦接近安全边界，HA-Teacher会介入接管控制，**同时**将其安全的控制经验（“教学数据”）提供给HP-Student学习。这解决了“如果DRL智能体的动作导致安全违规，如何及时纠正其不安全学习并保证机器人安全”这一开放问题。
     - **优势**：形成了一个“教学相长”的闭环。HA-Teacher不仅保障了即时安全，还作为“教师”引导HP-Student学习安全策略，最终目标是让HP-Student自身学会在复杂环境中既安全又高性能地运行。

### 2. **设计了具有“实时性”和“可验证性”的HA-Teacher模块**
   - **相比以往方法的改进/不同之处**：
     - **传统模型控制器**：基于固定的、简化的物理模型，控制目标和策略也是预设的。在环境剧烈变化时，其控制命令可能变得不可行甚至危险。
     - **本文创新**：HA-Teacher的核心在于其三个“实时”特性：
       1. **实时物理模型**：根据最近时刻的机器人状态 `𝐬(𝑘)` 线性化并更新模型矩阵 `(𝐀(𝐬(𝑘)), 𝐁(𝐬(𝑘)))`。
       2. **实时控制目标**：当HP-Student控制的状态误差 `𝐞(𝑘)` 进入边际安全空间时，HA-Teacher会设定一个新的、位于自学习空间内部的“补丁中心” `𝐞𝑘* = χ·𝐞(𝑘)` 作为实时控制目标，引导状态安全返回。
       3. **实时动作策略**：通过求解一个线性矩阵不等式（LMI）问题（**定理1**），在线计算反馈增益矩阵 `𝐅𝑘`，生成控制动作 `𝐚_HA = 𝐅𝑘 · (𝐞(𝑡) − 𝐞𝑘*)`。
   - **解决的具体问题/带来的优势**：
     - **解决了静态HAM在动态环境中的失效问题**：通过实时更新，HA-Teacher的模型和控制律能够紧跟环境变化，确保了其作为安全备份的**可靠性**和**有效性**。
     - **提供了形式化的安全保证**：**定理1** 为HA-Teacher的动作策略提供了严格的理论证明，确保在其控制下，机器人的状态能在有限的教学周期 `τ` 内，从边际安全空间 `𝕊∖𝕃` 安全地返回到自学习空间 `𝕃`，并且整个过程始终满足安全集 `𝕊` 和动作集 `𝔸` 的约束。这赋予了整个框架**可验证的安全性**。
     - **优势**：将自适应控制与形式化验证相结合，使安全模块既灵活又可靠。

### 3. **明确定义了“自学习空间”与“教学空间”，并设计了相应的协调器逻辑**
   - **相比以往方法的改进/不同之处**：
     - **传统安全架构的切换**：通常基于简单的阈值或故障检测，切换逻辑可能不够精细，容易导致频繁切换或切换不及时。
     - **本文创新**：
       - 明确定义了**安全集 `𝕊`**、**自学习空间 `𝕃`**（`𝕃 ⊂ 𝕊`，`η` 因子引入缓冲）和**教学空间（边际安全空间）`𝕊∖𝕃`**。
       - 协调器根据精确的数学条件（公式5）进行切换：当状态从 `𝕃` 进入 `𝕊∖𝕃` 时，触发HA-Teacher接管。
       - **定理2** 进一步为教学周期 `τ` 提供了下界指导，确保HA-Teacher有足够时间将状态引导回 `𝕃`。
   - **解决的具体问题/带来的优势**：
     - **解决了安全与性能的权衡问题**：`𝕃` 的设定为HP-Student提供了一个**安全的探索区域**，使其能专注于学习高性能策略，而无需时刻担心轻微波动触发安全接管。缓冲区的存在考虑了系统响应延迟，使切换更鲁棒。
     - **实现了平滑、有原则的模块交互**：清晰的集合定义和切换条件使得HP-Student和HA-Teacher的职责分明，交互逻辑可预测、可分析，避免了随意切换带来的不稳定。
     - **优势**：为运行时学习提供了一个结构化和理论支撑的决策框架。

### 4. **在统一的仿真环境中进行了综合评估，验证了框架的有效性**
   - **相比以往方法的改进/不同之处**：
     - **许多研究** 可能只单独评估学习性能或控制器稳定性。
     - **本文创新**：在Nvidia Isaac Gym中构建了包含非结构化地形、移动障碍物的动态野外环境，对提出的**完整框架**进行了端到端评估。不仅与纯HA-Teacher比较，还与最先进的**安全DRL基线（CLF-DRL, Phy-DRL）** 进行了全面对比。
   - **解决的具体问题/带来的优势**：
     - **实证了创新点的实际价值**：实验结果（表I，图4，图5）明确显示：
       - **安全性**：本文框架在训练过程中跌倒次数为0，显著优于基线方法。
       - **性能**：经过足够的学习后，本文框架（`rtl-ep-4500`）在成功到达目的地、旅行时间和能量消耗上均优于纯HA-Teacher，并且比基线DRL方法更安全、学习更高效。
       - **解决了“如何适应动态野外环境以保证安全”的问题**：框架成功让机器人在复杂动态环境中通过运行时学习，最终获得了**既安全（无跌倒碰撞）又高效（时间短、能耗低）** 的导航策略。
     - **优势**：提供了强有力的实验证据，表明所提出的理论框架能够在实际的、复杂的仿真场景中实现其设计目标——**安全的自适应学习**。

---

**总结**：本文的核心创新在于一个**动态、可验证的安全教师（HA-Teacher）** 与一个**持续学习的学生（HP-Student）** 通过**形式化定义的交互协议**协同工作的新范式。它突破了传统静态安全备份在动态环境中的局限，也克服了纯DRL方法安全性不足的缺陷，为实现四足机器人在真实、不可预测的野外环境中长期自主、安全且高效地运行提供了有前景的解决方案。


### 问题 4

**Q**: 论文在实验或评估中最终实现了怎样的效果？请说明使用了哪些数据集和评价指标，与哪些基线方法进行了对比，以及在关键指标上的主要性能提升或结论。如果论文未给出明确的定量结果，也请说明原因。

**A**: ## 论文实验与评估效果分析

### 一、 实验设置与评价指标

#### 1. **实验平台与数据集**
- **仿真平台**：使用 **Nvidia Isaac Gym** 构建动态野外环境。
- **机器人平台**：使用 **Unitree Go2** 四足机器人模型。
- **环境**：模拟了包含非结构化地形、可移动石块、树木、岩石等障碍物的动态野外环境。机器人任务为按顺序导航至一系列预设的路径点。

#### 2. **主要评价指标**
评估分为**安全性**和**任务性能**两大类：

| 类别 | 具体指标 | 说明 |
| :--- | :--- | :--- |
| **安全性** | **是否摔倒 (Is Fall)** | 机器人是否在运行中失去平衡摔倒。 |
| | **是否碰撞 (Collision)** | 机器人是否与环境中的障碍物发生碰撞。 |
| | **安全集合规性** | 机器人状态（如质心速度、高度）是否始终保持在预定义的安全集 `𝕊` 内（见公式39）。 |
| **任务性能** | **任务成功 (Success)** | 机器人是否成功到达最终目的地。 |
| | **跟随路径点数量 (Num (wp))** | 成功导航经过的路径点数量。 |
| | **旅行时间 (Travel Time)** | 完成导航任务所花费的总时间。 |
| | **平均功率 (Avg Power)** | 电机平均功耗（瓦特）。 |
| | **总能耗 (Total Energy)** | 完成导航任务的总能量消耗（焦耳）。 |
| **学习过程** | **累计摔倒次数** | 在整个学习周期（多个训练周期）中机器人摔倒的总次数。 |
| | **周期回报 (Episodic Returns)** | 深度强化学习（DRL）智能体在每个训练周期获得的总奖励，反映学习收敛性和策略质量。 |

### 二、 对比的基线方法
论文将提出的**运行时学习框架 (Ours)** 与以下三种方法进行了对比：
1.  **CLF-DRL**：一种基于控制李雅普诺夫函数（CLF）的安全DRL方法。
2.  **Phy-DRL**：一种将物理模型嵌入DRL框架的方法。
3.  **HA-Teacher (单独)**：仅使用论文框架中的高保障教师模块（基于物理模型的控制器）进行控制，作为纯安全但性能可能受限的基准。

### 三、 关键实验结果与性能提升

根据论文中的**表 I** 和**图 4、5**，主要结论如下：

#### 1. **安全性方面**
- **摔倒与碰撞**：
    - **CLF-DRL** 和 **Phy-DRL** 在训练早期和中期均出现了**摔倒**或**碰撞**问题，表明其无法在动态环境中保证运行时安全。
    - **HA-Teacher (单独)** 和 **Ours (运行时学习框架)** 在整个实验过程中**均未发生摔倒**。这验证了HA-Teacher模块提供的可验证安全保障的有效性。
- **状态轨迹**：**图 4(b)(c)** 显示，在使用本文框架时，机器人的质心速度和高度轨迹能很好地维持在安全边界内，证明了框架在运行时维持安全的能力。
- **累计摔倒次数**：**图 4(a)** 清晰表明，本文框架在整个学习过程中的累计摔倒次数**显著低于** CLF-DRL 和 Phy-DRL。

#### 2. **任务性能与效率方面**
- **导航成功率与效率**：
    - **CLF-DRL**：未能成功完成导航任务（`Success = No`）。
    - **Phy-DRL**：在训练4500周期后能完成导航，但**旅行时间更长（56.63秒）**。
    - **HA-Teacher (单独)**：能安全完成任务，但因其保守性，**性能最差（旅行时间59.27秒，能耗最高）**。
    - **Ours (运行时学习框架)**：在训练3000周期后即能成功、安全地完成任务，并且在训练4500周期后实现了**最佳性能**：
        - **旅行时间最短：45.38秒**（相比Phy-DRL提升约20%，相比HA-Teacher提升约23%）。
        - **总能耗最低：22277.53焦耳**。
- **学习收敛性**：**图 5** 显示，本文框架的HP-Student在学习过程中获得的周期回报**收敛更快、更稳定且最终值更高**，表明其能更有效地学习到高性能策略。

#### 3. **核心结论**
- **实现了安全与性能的平衡**：本文提出的 **HA-Teacher 与 HP-Student 协同框架**，成功解决了在动态野外环境中四足机器人**运行时学习的安全保障问题**，同时通过HP-Student的自学习和向HA-Teacher学习，**显著提升了导航任务的性能（速度、能效）**。
- **超越了现有安全DRL方法**：与CLF-DRL和Phy-DRL相比，本文框架在**安全性上具有绝对优势**（零摔倒），在**任务性能上也更优**。
- **验证了“教学-学习”机制的有效性**：实验结果表明，HP-Student能够从HA-Teacher的干预中学习到安全边界处的控制策略，从而逐步内化安全约束，最终形成既安全又高性能的复合策略。

**总结**：论文通过详实的仿真实验和与主流基线的对比，定量化地证明了其运行时学习框架能够使四足机器人在复杂动态环境中，实现**安全、自适应且高效**的导航与学习。所有关键指标（安全性、任务完成时间、能耗）均支持这一结论。


## 相关链接

- [arXiv 页面](https://arxiv.org/abs/2503.04794v2)
- [HTML 版本](https://arxiv.org/html/2503.04794v2)
